const g=(o,a)=>{const i=o.toLowerCase(),e=a.toLowerCase(),s=[];let n=0,l=0;const c=(t,p=!1)=>{let r="";l===0?r=t.length>20?`… ${t.slice(-20)}`:t:p?r=t.length+l>100?`${t.slice(0,100-l)}… `:t:r=t.length>20?`${t.slice(0,20)} … ${t.slice(-20)}`:t,r&&s.push(r),l+=r.length,p||(s.push(["strong",a]),l+=a.length,l>=100&&s.push(" …"))};let h=i.indexOf(e,n);if(h===-1)return null;for(;h>=0;){const t=h+e.length;if(c(o.slice(n,h)),n=t,l>100)break;h=i.indexOf(e,n)}return l<100&&c(o.slice(n),!0),s},d=Object.entries,y=Object.keys,f=o=>o.reduce((a,{type:i})=>a+(i==="title"?50:i==="heading"?20:i==="custom"?10:1),0),$=(o,a)=>{var i;const e={};for(const[s,n]of d(a)){const l=((i=a[s.replace(/\/[^\\]*$/,"")])==null?void 0:i.title)||"",c=`${l?`${l} > `:""}${n.title}`,h=g(n.title,o);h&&(e[c]=[...e[c]||[],{type:"title",path:s,display:h}]),n.customFields&&d(n.customFields).forEach(([t,p])=>{p.forEach(r=>{const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"custom",path:s,index:t,display:u}])})});for(const t of n.contents){const p=g(t.header,o);p&&(e[c]=[...e[c]||[],{type:"heading",path:s+(t.slug?`#${t.slug}`:""),display:p}]);for(const r of t.contents){const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"content",header:t.header,path:s+(t.slug?`#${t.slug}`:""),display:u}])}}}return y(e).sort((s,n)=>f(e[s])-f(e[n])).map(s=>({title:s,contents:e[s]}))},m=JSON.parse("{\"/\":{\"/\":{\"title\":\"小熊学Java\",\"contents\":[{\"header\":\"公众号\",\"slug\":\"公众号\",\"contents\":[\"另外，推荐大家关注小熊同学的公众号：小熊学Java\",\"每周更新免费的学习资料，不定期福利和最新的技术文章，网站内容慢于公众号半个月左右。\"]}]},\"/about/changelog.html\":{\"title\":\"更新日志\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"更新日志\",\"为什么会存在更新日志这篇呢？\",\"第一：对自己知识努力持续输出的一种记录、激励，让自己更有成就感\",\"第二：本网站会持久维护的证明\",\"第三：对更新的内容做记录，方便读者查看自己想看的内容\",\"第四：网站更新内容慢于公众号更新一周\",\"具体更详细的更新内容，会体现在公众号上，公众号上优先发表，这里只针对大方向进行说明\",\"点击查看公众号\"]},{\"header\":\"2023更新日志\",\"slug\":\"_2023更新日志\",\"contents\":[]},{\"header\":\"2023-07-15\",\"slug\":\"_2023-07-15\",\"contents\":[\"添加：\",\"增加架构设计 相关文章\",\"面试专栏 增加ElasticSearch面试题\",\"修复：\",\"修复部分面试题有错别字的情况\",\"完善部分面试题解答不全的问题\"]},{\"header\":\"2023-06-15\",\"slug\":\"_2023-06-15\",\"contents\":[\"1、Java练习生 框架组件 添加 - 流程引擎activity、Java可视化报表笔记\"]},{\"header\":\"2023-01-11\",\"slug\":\"_2023-01-11\",\"contents\":[\"1、面试专栏 添加 - Spring 事务失效的8个场景、Spring MVC执行流程\"]},{\"header\":\"2022更新日志\",\"slug\":\"_2022更新日志\",\"contents\":[\"文章类型\",\"内容\",\"更新日期\",\"状态\",\"开发必备\",\"添加 - [通用设计] java构建树的笔记\",\"2022-12-18\",\"完成\",\"面试专栏\",\"添加 - [尚硅谷150道MySQL面试题]\",\"2022-12-18\",\"完成\",\"面试专栏\",\"添加 - [底层源码] HashMap、Spring refresh、Spring bean面试题\",\"2022-12-18\",\"完成\",\"学习笔记\",\"添加 - Java设计模式的笔记\",\"2022-12-04\",\"完成\",\"学习笔记\",\"完善 - IDEA的安装和使用的笔记\",\"2022-12-04\",\"完成\",\"架构设计\",\"添加 - 分布式ID生成方案的文章\",\"2022-11-27\",\"完成\",\"学习笔记\",\"添加 - IDEA的安装和使用的笔记\",\"2022-11-27\",\"完成\",\"面试专栏\",\"完善 - MySQL专题的面试题\",\"2022-11-20\",\"完成\",\"码不停题\",\"添加 - 双指针的刷题记录\",\"2022-11-13\",\"完成\",\"架构设计\",\"添加 - 架构体系的笔记\",\"2022-11-13\",\"完成\",\"码不停题\",\"添加 - 字符串的刷题记录\",\"2022-11-06\",\"完成\",\"面试专题\",\"添加 - JVM的面试题\",\"2022-11-06\",\"完成\",\"面试专题\",\"添加 - MySQL专题的面试题\",\"2022-10-23\",\"完成\",\"学习笔记\",\"添加 - CAT链路追踪 、HATEOAS的学习笔记\",\"2022-10-23\",\"完成\",\"码不停题\",\"添加 - 哈希表的刷题记录\",\"2022-10-16\",\"完成\",\"学习笔记\",\"添加 - Elasticsearch的学习笔记\",\"2022-10-16\",\"完成\",\"码不停题\",\"添加 - 栈、堆、队列的刷题记录\",\"2022-10-09\",\"完成\",\"面试专栏\",\"添加 - ArrayList扩容机制源码分析的面试题\",\"2022-10-09\",\"完成\",\"码不停题\",\"添加 - 二叉树的刷题记录\",\"2022-10-03\",\"完成\",\"面试专栏\",\"添加 - 计算机网络面试题\",\"2022-10-03\",\"完成\",\"码不停题\",\"添加 - 数组、链表、二分查找的刷题记录\",\"2022-09-24\",\"完成\",\"面试专栏\",\"添加 - Spring Cloud Alibaba 面试题\",\"2022-09-24\",\"完成\"]}]},\"/about/communicate.html\":{\"title\":\"交流学习\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"交流\",\"有些读者看了我的介绍视频或网站后，会问一下问题，我非常乐意解答，看到了会回复大家的；\"]},{\"header\":\"学会提问\",\"slug\":\"学会提问\",\"contents\":[\"我们要学会提问，提问的智慧可能比你敲代码更重要\",\"首先在什么样场景，达到什么样的效果？这样问更容易分析解答你的问题，防止解答的人来问你的需求场景\",\"拒绝无场景问题，没有场景我也帮不了\"]},{\"header\":\"简略回答\",\"slug\":\"简略回答\",\"contents\":[\"大部分能上必应/度娘解决的，我会直接告诉你百度，因为你自己寻找答案的过程也是理解运用的过程，节省彼此的时间\"]}]},\"/about/donate.html\":{\"title\":\"赞助支持\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"如果您正在使用这个网站对你有帮助并感觉良好，或者是想支持我持续输出，您可以通过如下任意方式支持我：\",\"关注我的个人公众号【小熊学Java】\",\"点击查看\",\"分享这个网站，让更多的人参与一起卷\",\"也可打赏一杯☕，作为持续创作的动力\"]}]},\"/about/feedback.html\":{\"title\":\"问题反馈\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"反馈\",\"在整理文章和笔记的时候，可能会有一些错误或者错别字等，难免有疏漏和错误的地方；如有错误和问题，请添加我的微信反馈给我。 你在观看的同时，发现错误进行反馈，也是保证文章质量，为每一个读者看到一篇好的文章！\"]},{\"header\":\"如何反馈\",\"slug\":\"如何反馈\",\"contents\":[\"请指出哪篇文章，哪个地方，以及文章链接\",\"请指出存在什么问题\"]},{\"header\":\"反馈奖励\",\"slug\":\"反馈奖励\",\"contents\":[\"将获取一份Java学习资料，你可以说明资料名称，我这边寻找后发你\",\"点击查看个人微信\"]}]},\"/about/me.html\":{\"title\":\"关于小熊\",\"contents\":[{\"header\":\"❤️ 小熊同学\",\"slug\":\"小熊同学\",\"contents\":[\"❤️ 毕业接近二年的Java小白\",\"♣️ 喜欢新的技术，喜欢折腾\",\"🎈 什么都会一点，但好像又不会\",\"🚀 欢迎交流，也希望那些卷的朋友带我一下，一起卷\"]},{\"header\":\"🔍 技术栈\",\"slug\":\"技术栈\",\"contents\":[\"Java、Spring Cloud、RabbitMQ、Linux、Elasticsearch等\",\"HTML、VUE\",\"MySQL、Redis\",\"Git、SVN、Docker\"]},{\"header\":\"📞联系\",\"slug\":\"联系\",\"contents\":[\"点击查看\",\"(左边是公众号，右边是个人微信)\"]}]},\"/about/originalIntention.html\":{\"title\":\"制作初衷\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"说明\",\"在大学期间，也是摸鱼了两三年，一直没有一个学习的思路，后面慢慢逼迫自己学习框架等技术；\\n毕业之后，自己也是在一直摸索，闲下来也不知道该学些什么，虽然每次都有做笔记，但是查找也不是很方便，于是整理了一下，部署了这个网站！同时也是对自己学习知识的一种记录与激励吧！\\n同时，也希望这个网站上面的知识可以帮助到你，提供一些Java学习路上\"]}]},\"/architecture/\":{\"title\":\"架构设计\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"说明\",\"自己的目标也是成为一名架构师，也为此一直努力着，不断在摸索，总结......\"]}]},\"/dev-necessary/\":{\"title\":\"开发必备\",\"contents\":[]},\"/interview/\":{\"title\":\"面试专栏\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"说明\",\"Java基础：要了解一下Java概述，熟悉基本语法、数据类型、异常的处理，重点是面向对象，封装、继承、多态；面试中高频问点就是多线程与并发、JVM，还有集合的源码等\",\"Java高级：高级部分包括Java反射，JVM、并发编程高级篇等\",\"计算机基础：计算机基础主要是操作系统、计算机网络、计算机组成原理、数据结构与算法等\",\"数据库: 数据库主要以MySQL、Redis为主，这两个基本上是主要问的\",\"Java框架: 框架这部分包含很多，入门框架有Spring、Spring MVC、Mybatis，目前SSH很少使用了，微服务生态这边有Spring Boot、Spring Cloud\",\"分布式：市面上流行的分布式框架，分布式缓存，消息队列，数据库等\",\"源码剖析：面试中必问源码的面试题\",\"架构设计面试：自己总结学习的一些架构设计方面的面试题\"]}]},\"/program-nav/\":{\"title\":\"导航\",\"contents\":[]},\"/study-tutorial/\":{\"title\":\"Java学习手册\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"自己学习Java过程中，总结的一些笔记，教程，方便自己查漏补缺，同时，给大家学习提供一个思路(参考)\"]}]},\"/architecture/basic/evolution.html\":{\"title\":\"架构演进之路\",\"contents\":[{\"header\":\"1、单体架构\",\"slug\":\"_1、单体架构\",\"contents\":[\"公司发展的初期，资金少、用户少，需要的软件产品的数据和并发量都比较小，这个时期大多数的软件系统只需要单一服务器就可以满足需求，所有的业务逻辑都在单一应用系统，单应用、单数据库。数据库部署在和应用相同的虚拟机或服务器上，或者放置在另外一台机器上。此时的架构图如下：\",\" 或 \",\"操作系统：windows、linux\",\"应用服务器：tomcat、jetty、jboos、apache、weblogic、websphere...\",\"数据库：mysql、oracle、db2...\",\"应用系统：可以用java、php、asp等各种语言开发\",\"这种架构模式优点很明显：\",\"节省服务器资源，投入少\",\"管理简单：上线、部署、监控、问题排查等都比较简单\",\"开发简单：软件系统功能整合在一起，不需要考虑太多服务依赖等问题，代码管理也比较简单明了。\",\"测试简单\",\"随着公司和业务进入快速发展时期，软件系统面临来自多方面的考验：\",\"单体架构的缺点也越发的凸显出来：\",\"可用性差： 应用和数据库都是单点，无论应用还是数据库出现问题，整个系统的就会不可用了\",\"稳定性差： 系统耦合度高，新增或者修改任何一个功能，哪怕只是一行代码，也需要重启服务器，此时系统是不可用的\",\"性能差：单一的应用服务器和数据库服务器，性能总会有上限的，当用户变多或者准确的说相同时刻并发访问多时，系统就容易挂掉了\"]},{\"header\":\"2、分布式架构\",\"slug\":\"_2、分布式架构\",\"contents\":[\"单体架构有着明显的缺陷，随着系统访问量的增多，这些缺陷越来越凸显，为了解决这些缺陷，架构升级了，变成了分布式架构。分布式，就是多个实例提供服务。下面我们来简单介绍下常见的一些解决方案。\"]},{\"header\":\"1、应用集群\",\"slug\":\"_1、应用集群\",\"contents\":[\"反向代理服务器：把用户请求反向路由到应用服务器，常见的反向代理服务器是Nginx或HAProxy\",\"应用服务器：集群化部署\",\"数据库服务器：主从部署\",\"架构优点：\",\"可用性高：代理服务器、应用服务器、数据库服务器都是做了集群，当某台机器挂掉后，其他机器能够几乎无感的接替下任务\",\"性能比单体架构高： 用户的请求分发到多个应用服务器上，整体性能接近单体结构的三倍\",\"安全性高： 外网用户访问的是反向代理服务器，应用和数据库隔离在内网中\"]},{\"header\":\"2、分布式缓存\",\"slug\":\"_2、分布式缓存\",\"contents\":[\"缓存分为多级缓存，比如本地缓存（JVM中），分布式缓存服务器（Redis集群等）。本地缓存的访问速度更快一些，但是受应用服务器内存限制，其缓存数据量有限，而且会出现和应用程序争用内存的情况。远程分布式缓存可以使用集群的方式，部署大内存的服务器作为专门的缓存服务器，可以在理论上做到不受内存容量限制的缓存服务。常见缓存服务器包括Redis、Memcached等。使用缓存后，数据访问压力得到有效缓解。\"]},{\"header\":\"3、业务拆分\",\"slug\":\"_3、业务拆分\",\"contents\":[\"业务进一步发展，用户越来越多，系统又出现了瓶颈，此时整个电商系统可以做系统拆分了，系统拆分分为水平拆分和垂直拆分。\",\"水平拆分：\",\"拆分成商品、订单、交易、用户、支付等多个系统，每个系统都都是多台服务器构成的集群。\",\"垂直拆分：\",\"将一些公共业务和服务，如用户中心拆分成注册登录中心和用户中心，短信、文件、消息等各种公共服务，从业务系统中拆分剥离出来。\",\"这种架构的优势也比较明显，一方面，应用系统增加了，能够响应用户的请求也会变多，另一方面公共服务能够提供给所有的应用使用，达到服务复用的效果。但是大家需要注意的是数据库有可能只是一个，而单一数据库服务器的处理能力必然是有限的，随着用户并发量的持续增多，数据库将会是系统的瓶颈。\"]},{\"header\":\"4、分库分表和读写分离\",\"slug\":\"_4、分库分表和读写分离\",\"contents\":[\"读写分离：\",\"在网站的用户达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。目前大部分的主流数据库都提供主从热备功能，通过配置数据库的主从关系，可以将一台数据库服务器的数据更新同步到另外的数据库服务器上。网站利用数据库的这一功能，实现数据库读写分离，从而改善数据库负载压力。\",\"应用服务器在写数据的时候，访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库，这样当应用服务器读数据的时候，就可以通过从数据库获得数据。\",\"分库分表：\",\"随着数据库中的数据量越来越大，相应的，查询所需要的时间也越来越多，这个时候，相当于数据的处理遇到了瓶颈，另一方面单库发生意外的时候，需要修复的是所有的数据，而多库中的一个库发生意外的时候，只需要修复一个库。基于此，分库分表就成了必然。分库分表的策略很多，如按照用户、订单、交易、商品等进行分库，不同的数据库中按照时间进行分表。\",\"分库分表带来性能上的显著提升，但相应的管理和维护成本也比较高，比如数据库服务器的维护、分表策略的维护。为了便于应用程序访问分库分表、读写分离后的数据库，通常在应用服务器端使用专门的数据访问模块，使数据库的分库分表和读写分离对应用透明。\"]},{\"header\":\"5、静态化和CDN\",\"slug\":\"_5、静态化和cdn\",\"contents\":[\"随着网站业务不断发展，用户规模越来越大，和中国复杂的网络环境，不同地区的用户访问网站时，速度差别也极大。为了提供更好的用户体验，留住用户，网站需要加速网站访问速度。主要手段有使用页面的静态化和CDN。\",\"操作方式上把一些页面，比如某些商品的详情信息，在发布商品时将页面静态化，静态化页面和静态资源可以放在CDN服务器，部署在网络服务提供商的机房，用户在访问静态资源时，可以很好的利用CDN的优点，从距离自己最近的网络提供商机房获取数据。\"]},{\"header\":\"6、异步解耦\",\"slug\":\"_6、异步解耦\",\"contents\":[\"应用之间的服务存在互相调用的情况，但有些场景下，并不需要同步调用，比如某个业务完成后，需要短信通知对方，而短信接收的时间晚几秒钟都是可以接受的，此时就不需要同步处理了，我们可以使用消息队列，把发送短信的内容扔到消息队列中，达到异步处理的效果，从而增强业务系统的性能，此时对于服务之间也达到了解耦的功能，服务之间的依赖减少了。\"]},{\"header\":\"3、微服务架构\",\"slug\":\"_3、微服务架构\",\"contents\":[\"微服务架构是分布式架构的深化，分布式架构偏向于部署和环境，比如上边提到的应用、数据库、缓存等，在多台机器上进行部署，就属于分布式。微服务架构通过业务拆分实现服务组件化，通过组件组合快速开发系统，业务单一的服务组件又可以独立部署，使得整个系统变得清晰灵活。\",\"大量的分布式服务又使得架构实现面临问题，如服务注册发现，服务统一接入和权限控制，服务的负载均衡，服务配置的集中管理，服务熔断，服务监控等。\",\"所以微服务架构是由这些基础的服务组件和业务微服务组件共同组成：\",\"服务注册发现组件： 进行服务治理\",\"服务网关组件：提供统一入口和权限控制\",\"负载均衡组件：提供客户端或服务器端的负载均衡\",\"集中配置组件：提供服务集中管理\",\"熔断器组件：提供服务熔断\",\"服务追踪组件：提供服务监控\",\"采用微服务架构后，项目可以快速迭代与持续交付。但是也带了一些弊端，开发人员除了需要关注业务逻辑实现外还需要考虑业务的一系列问题，比如服务注册，服务发现，服务通讯，负载均衡，服务熔断，服务超时等，这些是非常重要的。大多数时候，我们需要依赖第三方库或者组件来提供这些服务，例如Hystrix，Eureka、Zookeeper等组件，在其服务组织中起到了广泛的应用。\"]}]},\"/architecture/basic/method.html\":{\"title\":\"架构视角\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"架构图是为了表示软件系统的整体轮廓和各个组件之间的相互关系和约束边界，以及软件系统的物理部署和软件系统的演进方向的整体视图。要让干系人理解、遵循架构决策，就需要把架构信息传递出去，架构图就是一个很好的载体。不同的视角和角色，关注点也是不同的，看到的架构图是不一样的。\"]},{\"header\":\"1、业务架构\",\"slug\":\"_1、业务架构\",\"contents\":[\"使用者：CEO、CIO、CTO、产品总监\",\"核心业务流程：\",\"核心能力:\"]},{\"header\":\"2、功能架构\",\"slug\":\"_2、功能架构\",\"contents\":[\"使用者：产品总监、产品经理\",\"**示例：**黑马头条功能架构图\"]},{\"header\":\"3、系统架构\",\"slug\":\"_3、系统架构\",\"contents\":[\"使用者：系统架构师\"]},{\"header\":\"4、 技术架构\",\"slug\":\"_4、-技术架构\",\"contents\":[\"使用者：系统架构师\",\"示例一：https://www.processon.com/view/5f2a0bfb1e08533a629b7ed3\",\"示例二：冷链项目技术架构图\"]},{\"header\":\"5、数据架构\",\"slug\":\"_5、数据架构\",\"contents\":[\"使用者：CTO、系统架构师、数据架构师\",\"示例一：数据模型\",\"示例二：大数据平台架构\"]},{\"header\":\"6、部署架构\",\"slug\":\"_6、部署架构\",\"contents\":[\"使用者：运维架构师\",\"示例一：https://www.processon.com/view/5f2a03cf637689168e49e3fa\",\"示例二：冷链项目部署架构图\"]}]},\"/architecture/basic/principles.html\":{\"title\":\"架构原则\",\"contents\":[{\"header\":\"1、 解耦\",\"slug\":\"_1、-解耦\",\"contents\":[\"在软件工程中，耦合指的就是对象之间的依赖性。对象之间的耦合度越高，维护成本越高。因此对象的设计应使类和构件之间的耦合最小。软件设计中通常用耦合度和内聚度作为衡量模块独立程度的标准。划分模块的一个准则就是高内聚低耦合。\",\"耦合性存在于各个领域，而非软件设计中独有的，理论上说绝对的零耦合是做不到的，但可以通过一些方法将耦合降至最低，降低耦合度即可理解为解耦，在设计上解耦的核心思想是【彼此独立，互不依赖】。\"]},{\"header\":\"2、 分层\",\"slug\":\"_2、-分层\",\"contents\":[\"分层结构是最为流行、应用最广泛的应用软件的设计方式。在应用了分层结构的系统中，各个子系统按照层次的形式组织起来，上层使用下层的各种服务，而下层对上层一无所知。每一层都对自己的上层隐藏其下层的细节。\",\"经典三层架构：\",\"在软件架构中，经典三层架构自顶向下由用户界面层、业务逻辑层、数据访问层组成。在提出该分层架构的时代，多数系统往往较为简单，本质上都是一个单体架构的数据库管理系统。这种分层架构有效地隔离了业务逻辑与数据访问逻辑，使得这两个不同关注点能够相对自由和独立地演化。经典的三层架构如下所示：\",\"分层的设计原则是：保证同一层的组件处于同一个抽象层次。即所谓的“单一抽象层次原则”。这一原则可以运用到分层架构中。比如下图所示：\"]},{\"header\":\"3、 封装\",\"slug\":\"_3、-封装\",\"contents\":[\"假设我们有一个程序，它在逻辑上有一些不同的对象，并且这些对象彼此之间会相互交流。\",\"在一个类中，当每个对象的状态保持相对孤立，就实现了封装。其余的对象并不能观察到这个对象的状态。他们能做到的只有调用一些被称作“方法”的通用功能。\",\"因此，对象使用方法掌控着自己的状态，除非明确允许，没有其他人可以接触到它。如果你想和某个对象交流，你需要使用提供的方法。但在默认情况下，你无法改变对象的状态。\"]}]},\"/architecture/basic/system.html\":{\"title\":\"架构体系\",\"contents\":[{\"header\":\"1、系统与子系统\",\"slug\":\"_1、系统与子系统\",\"contents\":[\"系统：泛指由一群有关联的个体组成，根据某种规则运作，能完成个别元件不能单独完成的工作的群体。\",\"关联：系统是由一群有关联的个体组成的，没有关联的个体堆在一起不能成为一个系统。例如，把一个汽车发动机和一堆苹果放在一起不能称之为一个系统，把发动机、底盘、轮胎、车架组合起来才能成为一台汽车，构成一个系统。\",\"规则：系统内的个体需要按照指定的规则运作，而不是单个个体各自为政。规则规定了系统内个体分工和协作的方式。例如，汽车发动机负责产生动力，然后通过变速器和传动轴，将动力输出到车轮上，从而驱动汽车前进。\",\"能力：系统能力与个体能力有本质的差别，系统能力不是个体能力之和，而是产生了新的能力。例如，汽车能够载重前进，而发动机、变速器、传动轴、车轮本身都不具备这样的能力。\",\"子系统：子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中的一部分。 子系统的定义和系统定义是一样的，只是观察的角度有差异，一个系统可能是另外一个更大系统的子系统。\",\"以微信为例来做一个分析：\",\"微信本身是一个系统，包含聊天、登录、支付、朋友圈等子系统。\",\"朋友圈这个系统又包括动态、评论、点赞等子系统。\",\"评论这个系统可能又包括防刷子系统、审核子系统、发布子系统、存储子系统。\",\"评论审核子系统不再包含业务意义上的子系统，而是包括各个模块或者组件，这些模块或者组件本身也是另外一个维度上的系统。例如，MySQL、Redis 等是存储系统，但不是业务子系统\"]},{\"header\":\"2、模块、组件、服务\",\"slug\":\"_2、模块、组件、服务\",\"contents\":[\"模块：是一套一致而互相有紧密关连的软件组织。它分别包含了程序和数据结构两部分。现代软件开发往往使用模块作为合成的单位\",\"组件：自包含的、可编程的、可重用的、与语言无关的软件单元，组件可以很容易被用于组装应用程序中\",\"模块和组件都是系统的组成部分，只是从不同的角度拆分系统而已。例如：\",\"从逻辑的角度来拆分系统后，得到的单元就是“模块”；从物理的角度来拆分系统后，得到的单元就是“组件”。\",\"划分模块的主要目的是职责分离；划分组件的主要目的是单元复用。\",\"例如我们要做一个学生信息管理系统，这个系统从逻辑的角度来拆分，可以分为：登录注册模块、个人信息模块、个人成绩模块；从物理的角度来拆分，可以拆分为应用程序、 Nginx、Web 服务器、MySQL等\",\"**服务：**服务和组件有某种相似之处：它们都将被外部的应用程序使用。两者之间最大的差异在于：组件是在本地使用的（例如Jar文件）；而服务是运行起来的，要通过同步或异步的远程接口来远程使用（例如RESTFul接口、web service、消息系统、RPC，或者socket）\",\"服务是可以单独运行，并且对外提供功能的一种形式。可以将一个复杂的项目分解成多个服务。当某一个服务挂掉时不会拖垮整个系统。如果没有服务化，每当一个新的功能被添加到系统中就会影响到所有功能；如果采取服务化，每个服务只对其上下游的服务负责。\"]}]},\"/architecture/design/taobaoid.html\":{\"title\":\"淘宝数据库，主键如何设计的？\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"聊一个实际问题：淘宝的数据库，主键是如何设计的？\",\"某些错的离谱的答案还在网上年复一年的流传着，甚至还成为了所谓的MySQL军规。其中，一个最明显的错误就是关于MySQL的主键设计。\",\"大部分人的回答如此自信：用8字节的 BIGINT 做主键，而不要用INT。 错 ！\",\"这样的回答，只站在了数据库这一层，而没有 从业务的角度 思考主键。主键就是一个自增ID吗？站在2022年的新年档口，用自增做主键，架构设计上可能连及格都拿不到 。\"]},{\"header\":\"1、自增ID的问题\",\"slug\":\"_1、自增id的问题\",\"contents\":[\"自增ID做主键，简单易懂，几乎所有数据库都支持自增类型，只是实现上各自有所不同而已。自增ID除了简单，其他都是缺点，总体来看存在以下几方面的问题：\",\"1.可靠性不高\",\"存在自增ID回溯的问题，这个问题直到最新版本的MySQL 8.0才修复。\",\"2.安全性不高\",\"对外暴露的接口可以非常容易猜测对应的信息。比如：/User/1/这样的接口，可以非常容易猜测用户ID的值为多少，总用户数量有多少，也可以非常容易地通过接口进行数据的爬取。\",\"3.性能差\",\"自增ID的性能较差，需要在数据库服务器端生成。\",\"4.交互多\",\"业务还需要额外执行一次类似 last_insert_id() 的函数才能知道刚才插入的自增值，这需要多一次的网络交互。在海量并发的系统中，多1条SQL，就多一次性能上的开销。\",\"5.局部唯一性\",\"最重要的一点，自增ID是局部唯一，只在当前数据库实例中唯一，而不是全局唯一，在任意服务器间都是唯一的。对于目前分布式系统来说，这简直就是噩梦。\"]},{\"header\":\"2、 业务字段做主键\",\"slug\":\"_2、-业务字段做主键\",\"contents\":[\"为了能够唯一地标识一个会员的信息，需要为 会员信息表 设置一个主键。那么，怎么为这个表设置主键，才能达到我们理想的目标呢？ 这里我们考虑业务字段做主键。表数据如下：\",\"在这个表里，哪个字段比较合适呢？\",\"选择卡号（cardno）\",\"会员卡号（cardno）看起来比较合适，因为会员卡号不能为空，而且有唯一性，可以用来 标识一条会员记录。\",\"mysql> CREATE TABLE membermaster -> ( -> cardno CHAR(8) PRIMARY KEY, -- 会员卡号为主键 -> membername varchar(32), -> memberphone varchar(11), -> memberpid TEXT, -> memberaddress TEXT, -> sex char(1), -> birthday DATETIME -> ); Query OK, 0 rows affected (0.02 sec) \",\"不同的会员卡号对应不同的会员，字段“cardno”唯一地标识某一个会员。如果都是这样，会员卡号与会员一一对应，系统是可以正常运行的。\",\"但实际情况是， 会员卡号可能存在重复使用 的情况。比如，张三因为工作变动搬离了原来的地址，不再到商家的门店消费了 （退还了会员卡），于是张三就不再是这个商家门店的会员了。但是，商家不想让这个会 员卡空着，就把卡号是“10000001”的会员卡发给了王五。\",\"从系统设计的角度看，这个变化只是修改了会员信息表中的卡号是“10000001”这个会员 信息，并不会影响到数据一致性。也就是说，修改会员卡号是“10000001”的会员信息， 系统的各个模块，都会获取到修改后的会员信息，不会出现“有的模块获取到修改之前的会员信息，有的模块获取到修改后的会员信息，而导致系统内部数据不一致”的情况。因此，从 信息系统层面 上看是没问题的。\",\"但是从使用 系统的业务层面 来看，就有很大的问题 了，会对商家造成影响。\",\"比如，我们有一个销售流水表（trans），记录了所有的销售流水明细。2020 年 12 月 01 日，张三在门店购买了一本书，消费了 89 元。那么，系统中就有了张三买书的流水记录，如下所示：\",\"接着，我们查询一下 2020 年 12 月 01 日的会员销售记录：\",\"mysql> SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -> FROM demo.trans AS a -> JOIN demo.membermaster AS b -> JOIN demo.goodsmaster AS c -> ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 张三 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.00 sec) \",\"如果会员卡“10000001”又发给了王五，我们会更改会员信息表。导致查询时：\",\"mysql> SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -> FROM demo.trans AS a -> JOIN demo.membermaster AS b -> JOIN demo.goodsmaster AS c -> ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 王五 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.01 sec) \",\"这次得到的结果是：王五在 2020 年 12 月 01 日，买了一本书，消费 89 元。显然是错误的！\",\"结论：千万不能把会员卡号当做主键。\",\"选择会员电话 或 身份证号\",\"会员电话可以做主键吗？不行的。在实际操作中，手机号也存在 被运营商收回 ，重新发给别人用的情况！\",\"那身份证号行不行呢？好像可以。因为身份证决不会重复，身份证号与一个人存在一一对 应的关系。可问题是，身份证号属于个人隐私 ，顾客不一定愿意给你。要是强制要求会员必须登记身份证号，会把很多客人赶跑的。其实，客户电话也有这个问题，这也是我们在设计会员信息表的时候，允许身份证号和电话都为空的原因。\",\"所以，建议尽量不要用跟业务有关的字段做主键。毕竟，作为项目设计的技术人员，我们谁也无法预测在项目的整个生命周期中，哪个业务字段会因为项目的业务需求而有重复，或者重用之类的情况出现\",\"经验：刚开始使用 MySQL 时，很多人都很容易犯的错误是喜欢用业务字段做主键，想当然地认为了解业务需求，但实际情况往往出乎意料，而更改主键设置的成本非常高。\"]},{\"header\":\"3、淘宝的主键设计\",\"slug\":\"_3、淘宝的主键设计\",\"contents\":[\"在淘宝的电商业务中，订单服务是一个核心业务。请问， 订单表的主键 淘宝是如何设计的呢？是自增ID吗？打开淘宝，看一下订单信息：\",\"从上图可以发现，订单号不是自增ID！我们详细看下上述4个订单号：\",\"1550672064762308113 1481195847180308113 1431156171142308113 1431146631521308113 \",\"订单号是19位的长度，且订单的最后5位都是一样的，都是08113。且订单号的前面14位部分是单调递增的，大胆猜测，淘宝的订单ID设计应该是：订单ID = 时间 + 去重字段 + 用户ID后6位尾号，这样的设计能做到全局唯一，且对分布式系统查询及其友好。\"]},{\"header\":\"4、推荐主键设计\",\"slug\":\"_4、推荐主键设计\",\"contents\":[\"非核心业务 ：对应表的主键自增ID，如告警、日志、监控等信息。\",\"核心业务 ：主键设计至少应该是全局唯一且是单调递增。全局唯一保证在各系统之间都是唯一的，单调递增是希望插入时不影响数据库性能。\",\"这里推荐最简单的一种主键设计：UUID。\"]},{\"header\":\"1、UUID的特点：\",\"slug\":\"_1、uuid的特点\",\"contents\":[\"全局唯一，占用36字节，数据无序，插入性能差。\"]},{\"header\":\"2、认识UUID：\",\"slug\":\"_2、认识uuid\",\"contents\":[\"为什么UUID是全局唯一的？\",\"为什么UUID占用36个字节？\",\"为什么UUID是无序的？\",\"MySQL数据库的UUID组成如下所示：\",\"UUID = 时间+UUID版本（16字节）- 时钟序列（4字节） - MAC地址（12字节）\",\"我们以UUID值e0ea12d4-6473-11eb-943c-00155dbaa39d举例：\",\"为什么UUID是全局唯一的？\",\"在UUID中时间部分占用60位，存储的类似TIMESTAMP的时间戳，但表示的是从1582-10-15 00：00：00.00到现在的100ns的计数。可以看到UUID存储的时间精度比TIMESTAMPE更高，时间维度发生重复的概率降低到1/100ns。时钟序列是为了避免时钟被回拨导致产生时间重复的可能性。MAC地址用于全局唯一。\",\"为什么UUID占用36个字节？\",\"UUID根据字符串进行存储，设计时还带有无用\\\"-\\\"字符串，因此总共需要36个字节。\",\"为什么UUID是随机无序的呢？\",\"因为UUID的设计中，将时间低位放在最前面，而这部分的数据是一直在变化的，并且是无序。\"]},{\"header\":\"3、改造UUID\",\"slug\":\"_3、改造uuid\",\"contents\":[\"若将时间高低位互换，则时间就是单调递增的了，也就变得单调递增了。MySQL 8.0可以更换时间低位和时间高位的存储方式，这样UUID就是有序的UUID了。\",\"MySQL 8.0还解决了UUID存在的空间占用的问题，除去了UUID字符串中无意义的\\\"-\\\"字符串，并且将字符串用二进制类型保存，这样存储空间降低为了16字节。可以通过MySQL8.0提供的uuid_to_bin函数实现上述功能，同样的，MySQL也提供了bin_to_uuid函数进行转化：\",\"SET @uuid = UUID(); SELECT @uuid,uuid_to_bin(@uuid),uuid_to_bin(@uuid,TRUE); \",\"通过函数uuid_to_bin(@uuid,true)将UUID转化为有序 UUID 了。全局唯一 + 单调递增，这不就是我们想要的主键！\"]},{\"header\":\"4、有序UUID性能测试\",\"slug\":\"_4、有序uuid性能测试\",\"contents\":[\"16字节的有序UUID，相比之前8字节的自增ID，性能和存储空间对比究竟如何呢？\",\"我们来做一个测试，插入1亿条数据，每条数据占用500字节，含有3个二级索引，最终的结果如下所示：\",\"从上图可以看到插入1亿条数据有序UUID是最快的，而且在实际业务使用中有序UUID在 业务端就可以生成 。还可以进一步减少SQL的交互次数。\",\"另外，虽然有序UUID相比自增ID多了8个字节，但实际只增大了3G的存储空间，还可以接受。\",\"在当今的互联网环境中，非常不推荐自增ID作为主键的数据库设计。更推荐类似有序UUID的全局唯一的实现。\",\"另外在真实的业务系统中，主键还可以加入业务和系统属性，如用户的尾号，机房的信息等。这样的主键设计就更为考验架构师的水平了。\"]},{\"header\":\"5、如果不是MySQL8.0 肿么办？\",\"slug\":\"_5、如果不是mysql8-0-肿么办\",\"contents\":[\"手动赋值字段做主键！\",\"比如，设计各个分店的会员表的主键，因为如果每台机器各自产生的数据需要合并，就可能会出现主键重复的问题。\",\"可以在总部 MySQL 数据库中，有一个管理信息表，在这个表中添加一个字段，专门用来记录当前会员编号的最大值。\",\"门店在添加会员的时候，先到总部 MySQL 数据库中获取这个最大值，在这个基础上加 1，然后用这个值作为新会员的“id”，同时，更新总部 MySQL 数据库管理信息表中的当 前会员编号的最大值。\",\"这样一来，各个门店添加会员的时候，都对同一个总部 MySQL 数据库中的数据表字段进 行操作，就解决了各门店添加会员时会员编号冲突的问题。\"]},{\"header\":\"6、其他主键设计\",\"slug\":\"_6、其他主键设计\",\"contents\":[\"之前我发布了一篇分布式ID的解决方案的文章，更多设计可参考：\",\"https://mp.weixin.qq.com/s/znQy21thOMq7UVcV4PMoiA\",\"https://www.javaxiaobear.cn/architecture/distributed/distributedId.html\"]}]},\"/architecture/distributed/consistency.html\":{\"title\":\"分布式理论和一致性算法\",\"contents\":[{\"header\":\"1、什么是分布式系统\",\"slug\":\"_1、什么是分布式系统\",\"contents\":[\"分布式系统是一个硬件或软件组成分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统\"]},{\"header\":\"2、分布式系统的特征\",\"slug\":\"_2、分布式系统的特征\",\"contents\":[\"分布性\",\"分布式系统的多台计算机都会在空间上随意分布的，同时，机器的分布情况也会随时变动\",\"对等性\",\"分布式系统中的计算机没有主/从之分，既没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本 (Replica)是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。 在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取到该数据，这是解决分布式系统数据丢失问题 最为有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理\",\"并发性\",\"在“问题的提出”部分，我们已经提到过与“更新的并发性”相关的内容。在一个计算机网络中，程序运行过程中的并发性操作是非常常见的行为，例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储等，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。\",\"缺乏全局时钟\",\"前面提到，一个典型的分布式系统是由一系列在空间上随意分布的多个进程组成的，具有明显的分布性，这些进程之间通过交换消息来进行相互通信。因此，在分布式系统中，很难定义两个事件究竞谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。关于分布式系统的时钟和事件顺序中已经做了非常深刻的讲解.\",\"故障总是会发生\",\"组成分布式系统的所有计算机，都有可能发生任何形式的故障。一个被大量工程实践所检验过的黄金定理是:任何在设计阶段考虑到的异常情况，一定会在系统实际运行中发生，并且，在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以，除非需求指标允许，在系统设计时不能放过任何异常情况。\"]},{\"header\":\"3、分布式环境的各种问题\",\"slug\":\"_3、分布式环境的各种问题\",\"contents\":[\"通信异常\",\"从集中式向分布式演变的过程中，必然引人了网络因素，而由于网络本身的不可靠性、因此也引人了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险，网络光纤、路由器或是 DNS等硬件设备或是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。 另外，即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远大于单机操作。通常我们认为在现代计算机体系结构中，单机内存访问的延时在纳秒数量级(通常是10ns左右)，而正常的一次网络通信的延迟在0.1- 1ms左右(相当于内存访问延时的105~106倍)，如此巨大的延时差别，也会影响消息的收发的过程，因此消息丢失和消息延迟变得非常普遍\",\"网络分区\",\"当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信。而另一些节点则不能我们将这个现象称为网络分区，就是俗称的“脑裂”。当网络分区出现时 分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事务处理，这就对分布式一致性提出了非常大的挑战。\",\"三态\",\"前面提到，在分布式系统环境下，网络可能存在各种各样的问题；因此分布式系统的每一次请求和响应，存在特有的“三态”概念，即成功、失败与超时。 在传统的单机系统中，应用程序在调用一次函数之后，能够得到一个非常明确的响应，成功或失败；而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况下，网络通信也能接收到成功或失败的响应，但是当网络出现异常时，就可能出现超时的情况，通常有以下两种情况：\",\"请求时消息丢失\",\"响应时消息丢失\",\"节点故障\",\"节点故则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或“死”现象。通常根据经验来说，每个节点都有可能会出现故障，并且每天都在发生\"]},{\"header\":\"4、分布式理论\",\"slug\":\"_4、分布式理论\",\"contents\":[]},{\"header\":\"1、CAP定理\",\"slug\":\"_1、cap定理\",\"contents\":[\"CAP理论告诉我们，一个分布式系统不可能同时满足一致性 (C:Consistency)、可用性(A: Availability) 和分区容错性 (P: Partition tolerance) 这三个基本需求，最多只能同时满足其中的两项。\",\"Consistency 一致性：访问分布式系统中任意节点，总能返回一致的结果 \",\"Every read receives the most recent write or an error\",\"Availability 可用性：分布式系统总能向客户端返回响应 \",\"Every request receives a (non-error) response, without the guarantee that it contains the most recent write\",\"Partition tolerance 分区容忍：当分布式系统节点间通信发生了消息丢失或消息延迟，仍然允许系统继续运行 \",\"The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\",\"CAP 定理：最多三选二，无法兼得，通常在 CP 或者 AP 之间做出选择\",\"不一致的产生\",\"client 向 Node1 写入新值 v1\",\"写入成功 Node1 更新成 v1\",\"Node1 在没有将变更同步到 Node2 时，就向客户端返回了应答\",\"client 发起向 Node2 的读操作\",\"返回了旧值 v0（不一致）的结果\",\"保证一致性\",\"client 向 Node1 写入新值 v1\",\"写入成功 Node1 更新成 v1，此时不能立刻向 client 返回应答，而是需要将 v1 同步到 Node2\",\"同步 v1 成功\",\"此时才能向 client 返回应答\",\"如果此时 client 再去访问 Node2，不会出现不一致的情况\",\"保 CP 失 A\",\"当发生了网络分区，Node1 与 Node2 已经失去了联系，这时仍想对外提供服务（保 P）\",\"client 向 Node1 写入新值 v1\",\"写入 Node1 成功，但无法同步至 Node2\",\"这时为了保证一致性，Node1 不能向 client 返回应答，造成操作挂起无法完成（失去可用性）\",\"保 AP 失 C\",\"当发生了网络分区，Node1 与 Node2 已经失去了联系，这时仍想对外提供服务（保 P）\",\"client 向 Node1 写入新值 v1\",\"写入 Node1 成功，但无法同步至 Node2\",\"为了保证可用性，向 client 返回了应答（但牺牲了一致性）\",\"一致性级别\",\"CP 和 AP 之间需要做权衡，其实根据需求不同，也可以将一致性划分成几个级别，在这些级别里做一个权衡。\",\"强一致性：系统写入什么，读出来的也会是什么，但实现起来往往对性能影响较大，例如之前 CP 的例子\",\"例如：火车站售票，有就是有，没有就是没有，不能出现不一致的情况\",\"典型算法：Paxos、Raft、ZAB\",\"弱一致性：系统写入成功后，不承诺立刻可以读到写入的值，也不承诺具体多久后数据能达到一致，还可以细分为：\",\"会话一致性，同一个客户端会话中可以保证一致，其它会话不能保证\",\"用户一致性，同一个用户中可以保证一致，其它用户不能保证\",\"例如：网上购物，在商品详情页看到库存量还有好多，下单的瞬间才被提示“库存量不足”，实际上商品详情页展示的库存并不是最新的数据，只是在某个流程上才会做准确的检查\",\"最终一致性：是弱一致性的特例，保证在一定时间内，能够达到一个一致的状态\",\"例如：转账，转账完成后，会有一个提示，您的转账会在 24 小时内到账，一般用户也能接受，但最终必须是一致的\",\"典型协议：Gossip\"]},{\"header\":\"2、BASE理论\",\"slug\":\"_2、base理论\",\"contents\":[\"BASE是 Basically Available (基本可用)， Soft state (软状态) 和Eventually consistent(最终一致性) 三个短语的简写，是由来自eBay的架构师Dan Pritchet在其文章BASE An Acid Alternative中提出的。 BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP定理逐步演化而来 consistency)，但每个应用都可以根据的，其核心思想是即使无法做到强一致性(Strong自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventualconsistency)。\",\"Basically Available (基本可用)\",\"是指分布式系统出现不可预知故障的时候，允许损失部分可用性——但绝不等价于系统不可用。以下举例说明\",\"响应时间上的丢失：正常情况下，一个搜索引擎需要在0.5s之内返回给用户，但由于故障（机房断电或断网故障），查询响应时间增加到了1~2s\",\"Soft state (软状态)\",\"弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时\",\"Eventually consistent(最终一致性)\",\"最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。\"]},{\"header\":\"5、分布式一致性算法\",\"slug\":\"_5、分布式一致性算法\",\"contents\":[]},{\"header\":\"1、Paxos 算法\",\"slug\":\"_1、paxos-算法\",\"contents\":[\"问题提出\",\"集群中有 N 个节点，如果一个节点写入后要求同步到剩余 N-1 个节点后再向客户端返回 ok，虽然看起来最保险，但其中任意一个节点同步失败，势必造成整个集群不可用，能否在此基础上稍微提高可用性呢？\",\"答案是 （写）多数派，集群节点设置为奇数，同步超过集群中 N/2 个节点成功，则向客户端返回 ok，但存在顺序性问题，如 3 描述\",\"多数派写操作成功后的读一致性暂不考虑，思考下图中的两项操作，都满足了多数派通过，但 S3 这台服务器并没有与 S1，S2 达成一致，要达到多数派内部一致性\",\"Paxos\",\"Paxos 是一种共识算法，目的是解决之前提到的写多数派时的顺序性问题\",\"Paxos 角色划分：集群中的每个节点都可以充当\",\"Proposer：负责生成提案\",\"注意：Paxos 算法允许有多个 Proposer 同时提案，但可能会引起活锁问题\",\"Acceptor：负责批准提案\",\"Acceptor 如果只有一个的话，存在单点问题，因此应当有多个\",\"Learner：负责获取提案，Acceptor 批准提案后，会将提案发送给所有 Learner\",\"执行一个修改操作，不是一上来就能执行，分成两个阶段：\",\"准备阶段：Proposer负责接收 client 请求并产生提案，必须由多数派 Acceptor 批准通过提案\",\"接受阶段：提案通过后，再将要执行的修改操作广播给 Acceptor，这次仍然多数派通过，此修改才能生效，可以返回响应给客户端\",\"算法要点：\",\"整个算法分成两个阶段：预备阶段，前两个箭头，接受阶段，后两个箭头。 \",\"预备阶段的目的是：第一拦截掉旧的提案，第二找到最新的 acceptValue\",\"对于 Proposer \",\"预备阶段只发送提案号，接受阶段发送提案号 + 值\",\"提案号 n 唯一且全局递增，大的提案号有更高优先级\",\"如果见到最新已接受值，就会替换掉 Proposer 自己原来的值，保证一致性\",\"对于 Acceptor 会持久化以下信息 \",\"minN（最小提案号），会在预备阶段和接受阶段被更新为更大提案号，会用来决定 Proposer 是否能选中提案\",\"acceptN（已接受提案号）和 acceptValue（已接受值），会在接受阶段被更新，如果 minN > n 则不会更新\",\"例1\",\"P 广播提案号 1\",\"有 3 个 A 接到提案，此时满足 n > minN，将 minN 更新为 1\",\"3个 A 成功返回，P 收到的应答过半，但没有遇到更大的 acceptNo 和 acceptValue，因此使用自己的 value X\",\"P 广播提案号和值 1:X\",\"3 个 A 接到提案号和值，更新状态，返回 minN 值 1 给 P\",\"P 收到过半应答，并检查发现没有出现 minN > 1，便选中提案值 X\",\"例2\",\"S1 广播提案号 1，想把值更新为 X\",\"S5 广播提案号 2，想把值更新为 Y\",\"S1、S2、S3 已经经历了 Accept 阶段并选中值 X\",\"关键点，S3 也接到了 S5 的prepare 提案，这时是否会有不一致的情况呢？\",\"此时 S3 状态已将 acceptN 和 acceptValue 分别更新为 1:X；再返回 S5 的 ack 时就会将 1:X 返回给 S5\",\"S5 用返回的 X 替换掉了自己原有的值 Y，并执行后续流程，后续都会同步为 X\",\"例3\",\"S1 广播提案号 1，想把值更新为 X\",\"S5 广播提案号 2，想把值更新为 Y\",\"S1、S2、S3 已经经历了 Accept 阶段，与例2 不同的是，值 X 还未选中\",\"关键点，S3 也接到了 S5 的prepare 提案，这时是否会有不一致的情况呢？\",\"此时 S3 状态将 acceptN 和 acceptValue 分别更新为 1:X；再返回 S5 的 ack 时就会将 1:X 返回给 S5\",\"S5 用返回的 X 替换掉了自己原有的值 Y，并执行后续流程，后续都会同步为 X\",\"例4\",\"S1 广播提案号 1，想把值更新为 X\",\"S5 广播提案号 2，想把值更新为 Y\",\"关键点，S3 还未经历 Accept 阶段时，就拿到了 S5 的 prepare 提案，这时是否会有不一致的情况呢？\",\"S3 在接到 S1 的 accept 请求时，n>=minN 条件不成立，因此没有更新 acceptN 和 acceptValue，并且返回的 minN 是 2\",\"对 S1 来说，S3 返回的 minN 大于 n，选中失败；想更新 X 需要发起新一轮提案\",\"对 S5 来说，accept 阶段发送的是它自己的 2:Y，后续会把值同步为 Y\",\"例5\",\"回顾最早提到的顺序性问题，看 Paxos 能否解决它\",\"下图演示了 Paxos 是如何解决顺序性问题的，分析步骤参考例3\",\"Paxos 缺点\",\"效率较低，两轮操作只能选中一个值\",\"难于理解\",\"活锁问题\",\"Paxos 是允许多个 Proposer 的，因此如果按上图所示运行，则后一个提案总会让前面提案选中失败，显然死循环\",\"参考资料\",\"https://www.youtube.com/watch?v=JEpsBg0AO6o&t=41s Raft 作者讲解 Paxos\"]},{\"header\":\"2、Raft 算法\",\"slug\":\"_2、raft-算法\",\"contents\":[\"另一种共识算法，目的是比 Paxos 更易理解，Raft正是为了探索一种更易于理解的一致性算法而产生的。它的首要设计目的就是易于理解，所以在选主的冲突处理等方式上它都选择了非常简单明了的解决方案\",\"整个 Raft 算法分解为三部分：\",\"Leader 选举\",\"① 只有一个 Server 能作为 Leader\",\"② 一旦此 Server 崩溃，选举新 Leader\",\"执行操作，以日志复制为例（Log replication）\",\"① 由 Leader 执行自己的日志记录\",\"② 将日志复制到其它 Server，会覆盖掉不一致的部分\",\"③ 多数派记录日志成功，Leader 才能执行命令，向客户端返回结果\",\"确保安全\",\"① 保证日志记录的一致性\",\"② 拥有最新日志的 Server 才能成为 Leader\",\"Leader 选举\",\"Leader 会不断向选民发送 AppendEntries 请求，证明自己活着\",\"选民收到 Leader AppendEntries 请求后会重置自己的 timeout 时间\",\"选民收不到 AppendEntries 请求超时后，转换角色为候选者，并将任期加1，发送 RequestVote 请求，推选自己\",\"选民收到第一个 RequestVote，会向该候选者投一票，这样即使有多个候选者，必定会选出一个 Leader，选票过半即当选，如果落选会变回选民\",\"每一任期最多有一个 Leader，但也可能没有（选票都不过半的情况，需要再进行一轮投票，timeout 在 T~2T 间随机）\",\"任期由各个 server 自己维护即可，无需全局维护，在超时后加1，在接收到任意消息时更新为更新的任期，遇到更旧的任期，视为错误\",\"执行操作（以日志复制为例）\",\"客户端发送命令至 Leader\",\"Leader 将命令写入日志（S1虚框），并向所有选民发送 AppendEntries 请求\",\"多数派通过后，执行命令（即提交，S1虚框变实），此时就可以向客户端返回结果\",\"在后续的 AppendEntries 请求中通知选民，选民执行命令（即提交，S2,S3,S4,S5虚框变实）\",\"如果选民挂了，则 Leader 会不断尝试，待到选民重启，会将其缺失的日志陆续补齐\",\"确保安全\",\"Leader 日志的完整性\",\"Leader 被认为拥有最完整的日志\",\"一旦 Leader 完成了某条命令提交，那么未来的 Leader 也必须存有该条命令提交信息\",\"投票时，会将候选者最新的 <Term，Index> 随 RequestVote 请求发送，如果候选者的日志还没选民的新，则投否决票\",\"图中 S2 如果超时，发起选举请求，其它服务器只会对它投否决票，因为它的 Index 比其它人都旧\",\"图中 S5 如果超时，发起选举请求，其它服务器也不会选它，因为他的 Term 太旧\",\"选民日志的一致性\",\"以 Leader 为准，对选民的日志进行补充或覆盖\",\"AppendEntries 请求发送时会携带最新的 <Term,Index,Command> 以及上一个的 <Term,Index>\",\"如果选民发现上一个的 <Term,Index> 能够对应上则成功，否则失败，继续携带更早的信息进行比对\",\"图中 Leader 发送了 <3,4,Command> 和 <2,3> 给 follower，follower 发现 <2,3> 能够与当前最新日志对应，这时直接执行 <3,4,Command> 即可\",\"图中 Leader 发送了 <3,4,Command> 和 <2,3> 给 follower，follower 发现 <2,3> 不能与当前最新日志对应，会央求 Leader 发送更早日志\",\"Leader 这次发送了 <3,4,Command> ， <2,3,Command> ，<1,2> 给 follower，follower 发现 <1,2> 能够与当前最新日志对应，这时补全 <3,4,Command> ， <2,3,Command> 即可\",\"参考资料\",\"https://www.youtube.com/watch?v=vYp4LYbnnW8 Raft 作者讲解 Raft\",\"https://raft.github.io/ Raft 资源\",\"https://raft.github.io/raftscope/index.html Raft 可视化\"]},{\"header\":\"3、一致性Hash算法\",\"slug\":\"_3、一致性hash算法\",\"contents\":[\"它是为了解决在服务器增、删时普通 hash 算法造成数据大量迁移问题的\",\"普通 hash 算法\",\"假设有 3 台服务器，10 个 key 在服务器上的分布如下图所示\",\"添加一台服务器后，数据分布变成下图，可以看到除了一个 key（上下颜色相同的）以外，其它 key 都得迁移\",\"一致性 hash 算法\",\"假设有 3 台服务器，10 个 key 在服务器上的分布如下图所示\",\"添加一台服务器后，数据分布变成下图，发现仅有 3 个key 需要迁移（上下颜色不同的）\"]}]},\"/architecture/distributed/distributedId.html\":{\"title\":\"分布式ID的设计与实现\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"demo地址：https://gitee.com/javaxiaobear/distributedid.git\"]},{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"ID，全称Identifier，中文翻译为标识符，是用来唯一标识对象或记录的符号。比如我们每个人都有自己的身份证号，这个就是我们的标识符，有了这个唯一标识，就能快速识别出每一个人。\",\"**在计算机世界里，复杂的分布式系统中，经常需要对大量的数据、消息、HTTP 请求等进行唯一标识。**比如对于分微服务架构的系统中，服务间相互调用需要唯一标识，幂等处理，调用链路分析，日志追踪的时候都需要使用这个唯一标识，此时我们的系统就迫切的需要一个全局唯一的ID。\",\"另外随着社会的发展，各种金融、电商、支付、等系统中产生的数据越来越多，对数据库进行分库分表是比较常见的，而分库后则需要有一个唯一ID来标识一条数据或消息，单个数据库的自增ID显然不能满足需求，此时也会需要一个能够生成全局唯一ID的系统。\"]},{\"header\":\"2、为什么需要分布式ID?\",\"slug\":\"_2、为什么需要分布式id\",\"contents\":[\"在以往单体系统，ID我们常用主键自增进行设置，这种ID生成方法在单体项目是可行的，但是对于分布式系统，分库分表之后，就不适应了，比如订单表数据量太大了，分成了多个库，如果还采用数据库主键自增的方式，就会出现在不同库id一致的情况，很显然不符合业务的，那出现这个情况，有没有办法能够解决呢？\",\"分库分表之后，可以根据取余或者主键奇偶性等方式分别插入不同的库不同表。但很显然也不符合业务，还得去做额外的计算\"]},{\"header\":\"3、特点\",\"slug\":\"_3、特点\",\"contents\":[\"全局唯一：就是说不能出现重复的ID，既然是唯一标识，这是最基本的要求\",\"趋势递增：\",\"先来了解下什么是趋势递增？\",\"简单说就是在一段时间内，生成的ID是递增的趋势，而不强求下一个ID必须大于前一ID。例如在一段时间内生成的ID在【0，1000】之间，过段时间生成的ID在【1000，2000】之间。\",\"为什么要趋势递增？\",\"目前大部分的互联网公司使用了开源的MySQL数据库，存储引擎选择InnoDB。MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS数据库使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键，这样在插入新的数据时B-tree的结构不会时常被打乱重塑，能有效的提高存取效率。\",\"单调递增：通俗的说就是下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。\",\"信息安全：如果ID是连续递增的，那么恶意用户可以根据当前ID推测出下一个ID，爬取系统中数据的工作就非常容易实现，直接按照顺序访问指定URL即可；如果是订单号就更加危险，竞争对手可以直接知道系统一天的总订单量。所以在一些应用场景下，会需要ID无规则、不规则，切不易被破解。\"]},{\"header\":\"4、常用方法\",\"slug\":\"_4、常用方法\",\"contents\":[\"解决方案一般有以下8种，可以根据自己项目需求进行设计调整\",\"UUID\",\"数据库自增\",\"号段模式\",\"Redis 生成ID\",\"雪花算法（SnowFlake）\",\"百度 Uidgenerator\",\"美团 Leaf\",\"滴滴 TinyID\"]},{\"header\":\"1、UUID\",\"slug\":\"_1、uuid\",\"contents\":[\"UUID （Universally Unique Identifier），通用唯一识别码的缩写。UUID是由一组32位数的16进制数字所构成，所以UUID理论上的总数为 16^32=2^128，约等于 3.4 x 10^38。也就是说若每纳秒产生1兆个UUID，要花100亿年才会将所有UUID用完。\",\"生成的UUID是由 8-4-4-4-12格式的数据组成，其中32个字符和4个连字符' - '，一般我们使用的时候会将连字符删除 uuid.toString().replaceAll(\\\"-\\\",\\\"\\\")。\",\"目前UUID的产生方式有5种版本，每个版本的算法不同，应用范围也不同。\",\"版本1：基于时间的UUID 这个一般是通过当前时间，随机数，和本地Mac地址来计算出来，可以通过 org.apache.logging.log4j.core.util包中的 UuidUtil.getTimeBasedUuid()来使用或者其他包中工具。由于使用了MAC地址，因此能够确保唯一性，但是同时也暴露了MAC地址，私密性不够好。\",\"版本2 ：DCE（Distributed Computing Environment） DCE安全的UUID 安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\",\"版本3：基于名字的UUID（MD5）- 版本3 基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\",\"版本4：随机UUID -根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但是重复的可能性可以忽略不计，因此该版本也是被经常使用的版本。JDK中使用的就是这个版本。\",\"版本5：基于名字的UUID（SHA1） - 版本5 和基于名字的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\",\"Java中 JDK自带的 UUID产生方式就是版本4根据随机数生成的 UUID 和版本3基于名字的 UUID，有兴趣的可以去看看它的源码。\",\"public static void main(String[] args) { //获取一个版本4根据随机字节数组的UUID。 UUID uuid = UUID.randomUUID(); System.out.println(uuid.toString().replaceAll(\\\"-\\\",\\\"\\\")); //获取一个版本3(基于名称)根据指定的字节数组的UUID。 byte[] nbyte = {1, 2, 3}; UUID uuidFromBytes = UUID.nameUUIDFromBytes(nbyte); System.out.println(uuidFromBytes.toString().replaceAll(\\\"-\\\",\\\"\\\")); } \",\"优点：属于本地解决方案，无网络消耗\",\"缺点：\",\"不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\",\"MAC 地址提供了唯一性的保证，但也带来安全风险，最糟的是它是字符串形式，占用空间大，查询性能低，无法保证趋势递增\",\"ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用： \",\"MySQL官方有明确的建议主键要尽量越短越好，36个字符长度的UUID不符合要求\",\"对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能\"]},{\"header\":\"2、数据库自增\",\"slug\":\"_2、数据库自增\",\"contents\":[\"这种方式也是我们用的最多的方式，通常使用数据库自增，不同数据库自增命令可能不同，以MySQL为例，AUTO_INCREMENT可以使主键自增。\",\"优点：\",\"单体项目实现简单，命令即可设置，成本小，有DBA专业维护\",\"生成的ID有序，可以实现一些对ID有特殊要求的业务。\",\"缺点：\",\"不同数据库语法或实现不同，数据库迁移的时候需要处理\",\"在单个数据库或读写分离或一主多从多情况下，只有一个主库可以生成ID，有单点故障的风险\",\"在性能达不到要求的情况下比较难以扩展\",\"数据迁移或者系统数据合并比较麻烦\",\"分库分表时会比较麻烦\",\"ID发号性能瓶颈限制在单台MySQL的读写性能\",\"下面代码即可获取数据库自增ID\",\"@Repository public class IdDaoImpl implements IdDao { @Autowired private JdbcTemplate jdbcTemplate; @Override public Long getAutoincrementId(String bizType) { //使用REPLACE关键词，如果没有就新增，如果有就先删除再新增 //就能够获取自增ID final String sql = \\\"REPLACE INTO `sequence_id` (`biz_type`) VALUES (?);\\\"; // 创建自增key的持有器 KeyHolder keyHolder = new GeneratedKeyHolder(); int row = jdbcTemplate.update(connection -> { PreparedStatement ps = connection.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS); ps.setString(1, bizType); return ps; }, keyHolder); //获取主键ID if (row > 0) { BigInteger id = (BigInteger) keyHolder.getKeyList().get(0).get(\\\"GENERATED_KEY\\\"); return id.longValue(); } throw new DistributedIdException(\\\"获取数据库自增ID失败\\\"); } } \"]},{\"header\":\"3、号段模式\",\"slug\":\"_3、号段模式\",\"contents\":[\"这种模式针对数据库自增的优化方案，也是现在生成分布式 ID 的一种方法。实现思路是，会从数据库获取一个号段范围，比如 [1,1000]，生成 1 到 1000 的自增 ID 加载到内存中。\",\"对于MySQL性能问题，可用如下方案解决：在分布式系统中我们可以多部署几台机器，每台机器设置不同的初始值，且步长和机器数相等。比如有两台机器。设置步长step为2，TicketServer1的初始值为1（1，3，5，7，9，11…）、TicketServer2的初始值为2（2，4，6，8，10…）。这是Flickr团队在2010年撰文介绍的一种主键生成策略（Ticket Servers: Distributed Unique Primary Keys on the Cheap ）。如下所示，为了实现上述方案分别设置两台机器对应的参数，TicketServer1从1开始发号，TicketServer2从2开始发号，两台机器每次发号之后都递增2。\",\"TicketServer1: auto-increment-increment = 2 auto-increment-offset = 1 TicketServer2: auto-increment-increment = 2 auto-increment-offset = 2 \",\"假设我们要部署N台机器，步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\",\"这需要用到一张表，表结构如下：\",\"CREATE TABLE `segment_id_info` ( `id` bigint unsigned NOT NULL AUTO_INCREMENT COMMENT '自增主键', `biz_type` varchar(63) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '' COMMENT '业务类型，唯一', `begin_id` bigint NOT NULL DEFAULT '0' COMMENT '开始id，仅记录初始值，无其他含义。初始化时begin_id和max_id应相同', `max_id` bigint NOT NULL DEFAULT '0' COMMENT '当前最大id', `step` int DEFAULT '0' COMMENT '步长', `delta` int NOT NULL DEFAULT '1' COMMENT '每次id增量', `remainder` int NOT NULL DEFAULT '0' COMMENT '余数', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间', `version` bigint NOT NULL DEFAULT '0' COMMENT '版本号', PRIMARY KEY (`id`) USING BTREE, UNIQUE KEY `uniq_biz_type` (`biz_type`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb3 ROW_FORMAT=DYNAMIC COMMENT='号段ID信息表'; \",\"可根据自身数据进行表结构调整，一般需要的字段有：\",\"biz_type：不同业务类型；\",\"max_id ：当前最大的 id；\",\"step：代表号段的步长；\",\"version ：版本号，每次更新都+1，可以理解为乐观锁\",\"步骤如下：\",\"查询当前的max_id信息，也就是整个号段对象\",\"如果为空，那就进行初始化，初始化完成后，在进行查询\",\"计算新的max_id： new_max_id = max_id + step\",\"更新数据库中的max_id，同时版本号+1\",\"如果更新成功，则可用号段获取成功，新增可用号段为 max_id,new_max_id\",\"如果更新失败，则号段可能被其他线程获取到，数据不安全，回到步骤1，重试\",\"具体实现类：\",\"@Service(\\\"segment\\\") public class SegmentIdGen implements IdGen { private final static Logger log = LoggerFactory.getLogger(SegmentIdGen.class); /** * 在Id使用完成之前，进行异步加载下一个号段的百分比 * 当前号段的id使用了百分之多少的时候，就开始加载下一个号段 */ private static final int LOADING_PERCENT = 20; /** * 重试的次数 */ private static final int RETRY = 3; @Autowired private SegmentIdInfoDao segmentIdInfoDao; @Override @Transactional public SegmentId getNextSegmentId(String bizType) { //获取下一个号段的时候，可能存在version冲突，需要进行重试 for (int i = 0; i < RETRY; i++) { SegmentIdInfo segmentIdInfo = segmentIdInfoDao.queryByBizType(bizType); if (segmentIdInfo == null) { //如果没有查询到数据，进行初始化操作 int row = segmentIdInfoDao.initBizType(bizType); if (row < 1) { throw new DistributedIdException(\\\"初始化当前业务类型失败\\\"); } //初始化成功，再次进行查询 segmentIdInfo = segmentIdInfoDao.queryByBizType(bizType); } //最新的maxId Long newMaxId = segmentIdInfo.getMaxId() + segmentIdInfo.getStep(); //原来的maxId Long oldMaxId = segmentIdInfo.getMaxId(); //修改maxId int row = segmentIdInfoDao.updateMaxId(segmentIdInfo.getId(), newMaxId, oldMaxId, segmentIdInfo.getVersion(), segmentIdInfo.getBizType()); //判断是否修改成功 if (row == 1) { segmentIdInfo.setMaxId(newMaxId); SegmentId segmentId = convert(segmentIdInfo); log.info(\\\"获取下一个号段成功，SegmentIdInfo:{},current:{}\\\", segmentIdInfo, segmentId); return segmentId; } else { log.info(\\\"获取下一个号段冲突，SegmentIdInfo:{}\\\", segmentIdInfo); } } throw new DistributedIdException(\\\"获取下一个号段冲突，获取失败\\\"); } /** * 把数据库号段数据转为客户端所需要使用的号段ID结果对象 * * @param info * @return */ private SegmentId convert(SegmentIdInfo info) { SegmentId segmentId = new SegmentId(); segmentId.setCurrentId(new AtomicLong(info.getMaxId() - info.getStep())); segmentId.setMaxId(info.getMaxId()); segmentId.setDelta(info.getDelta() == null ? 1 : info.getDelta()); segmentId.setRemainder(info.getRemainder() == null ? 0 : info.getRemainder()); //1000~2000 加载百分比20 1000+1000*20/100 = 1200 segmentId.setLoadingId(segmentId.getCurrentId().get() + info.getStep() * LOADING_PERCENT / 100); return segmentId; } } \",\"数据层实现：\",\"@Repository public class SegmentIdInfoDaoImpl implements SegmentIdInfoDao { @Autowired private JdbcTemplate jdbcTemplate; /** * 根据bizType获取数据库中的号段ID对象 * @param bizType * @return */ @Override public SegmentIdInfo queryByBizType(String bizType) { String sql = \\\"SELECT * FROM `segment_id_info` WHERE biz_type = ?\\\"; List<SegmentIdInfo> list = jdbcTemplate.query(sql, new Object[]{bizType}, new SegmentIdInfoRowMapper()); if (list == null || list.isEmpty()) { return null; } return list.get(0); } /** * 初始化号段 步长为1000 * @param bizType * @return */ @Override public int initBizType(String bizType) { String sql = \\\"INSERT INTO `segment_id_info` \\\" + \\\"( `biz_type`, `begin_id`, `max_id`, `step`, `delta`, `remainder` ) \\\" + \\\"VALUES ( ?, 0, 0, 1000, ( SELECT @@auto_increment_increment ), \\\" + \\\"( SELECT @@auto_increment_offset - 1 ))\\\"; return jdbcTemplate.update(sql, bizType); } /** * 根据id、oldMaxId，version，bizType更新最新的maxId * @param id * @param newMaxId * @param oldMaxId * @param version * @param bizType * @return */ @Override public int updateMaxId(Long id, Long newMaxId, Long oldMaxId, Long version, String bizType) { String sql = \\\"UPDATE `segment_id_info` \\\" + \\\"SET `max_id` = ?,\\\" + \\\"update_time = now( ),\\\" + \\\"version = version + 1 \\\" + \\\"WHERE \\\" + \\\"id = ? \\\" + \\\"AND max_id = ? \\\" + \\\"AND version = ? \\\" + \\\"AND biz_type = ?\\\"; return jdbcTemplate.update(sql, newMaxId, id, oldMaxId, version, bizType); } private class SegmentIdInfoRowMapper implements RowMapper<SegmentIdInfo> { @Override public SegmentIdInfo mapRow(ResultSet resultSet, int i) throws SQLException { SegmentIdInfo segmentIdInfo = new SegmentIdInfo(); segmentIdInfo.setId(resultSet.getLong(\\\"id\\\")); segmentIdInfo.setBizType(resultSet.getString(\\\"biz_type\\\")); segmentIdInfo.setBeginId(resultSet.getLong(\\\"begin_id\\\")); segmentIdInfo.setMaxId(resultSet.getLong(\\\"max_id\\\")); segmentIdInfo.setStep(resultSet.getInt(\\\"step\\\")); segmentIdInfo.setDelta(resultSet.getInt(\\\"delta\\\")); segmentIdInfo.setRemainder(resultSet.getInt(\\\"remainder\\\")); segmentIdInfo.setCreateTime(resultSet.getDate(\\\"create_time\\\")); segmentIdInfo.setUpdateTime(resultSet.getDate(\\\"update_time\\\")); segmentIdInfo.setVersion(resultSet.getLong(\\\"version\\\")); return segmentIdInfo; } } } \",\"优点：有比较成熟的方案，像百度Uidgenerator，美团Leaf\",\"缺点：\",\"系统水平扩展比较困难，比如定义好了步长和机器台数之后，如果要添加机器该怎么做？假设现在只有一台机器发号是1,2,3,4,5（步长是1），这个时候需要扩容机器一台。可以这样做：把第二台机器的初始值设置得比第一台超过很多，比如14（假设在扩容时间之内第一台不可能发到14），同时设置步长为2，那么这台机器下发的号码都是14以后的偶数。然后摘掉第一台，把ID值保留为奇数，比如7，然后修改第一台的步长为2。让它符合我们定义的号段标准，对于这个例子来说就是让第一台以后只能产生奇数。扩容方案看起来复杂吗？貌似还好，现在想象一下如果我们线上有100台机器，这个时候要扩容该怎么做？简直是噩梦。所以系统水平扩展方案复杂难以实现。\",\"ID没有了单调递增的特性，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍。\",\"数据库压力还是很大，每次获取ID都得读写一次数据库，只能靠堆机器来提高性能。\"]},{\"header\":\"4、Redis实现\",\"slug\":\"_4、redis实现\",\"contents\":[\"Redis 分布式 ID 实现主要是通过提供像 INCR 和 INCRBY 这样的自增原子命令。由于 Redis 单线程的特点，可以保证 ID 的唯一性和有序性。\",\"这种实现方式，如果并发请求量上来后，就需要集群。不过集群后，又要和传统数据库一样，设置分段和步长。\",\"具体实现：\",\"public class RedisIdWorker { private static final long BEGIN_TIMESTAMP = 1640995200L; private static final int COUNT_BITS = 32; @Resource private StringRedisTemplate stringRedisTemplate; /** * redis 生成器 * @param keyPrefix id前缀 * @return */ public long nextId(String keyPrefix){ //1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; //生成序列号 //获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(\\\"yyyy:MM:dd\\\")); //这样可用根据日期去统计订单量 自动拆箱回产生空指针，但事实上这里并不会，redis发现没有，会自动生成 long increment = stringRedisTemplate.opsForValue().increment(\\\"icr:\\\" + keyPrefix + date); //拼接返回 // 这里拼接返回的是long类型，字符串拼接返回的是字符串 //这里需要使用 位运算 时间戳向左移动32位 在高位, 系列号采用或运算去填充 return timestamp << COUNT_BITS | increment; } } \",\"优点：Redis 性能相对比较好，而且可以保证唯一性和有序性；\",\"缺点：需要依赖 Redis 来实现，系统需要引入 Redis 组件。\"]},{\"header\":\"5、雪花算法（SnowFlake）\",\"slug\":\"_5、雪花算法-snowflake\",\"contents\":[\"Snowflake，雪花算法是由Twitter开源的分布式ID生成算法，以划分命名空间的方式将 64-bit位分割成多个部分，每个部分代表不同的含义。而 Java中64bit的整数是Long类型，所以在 Java 中 SnowFlake 算法生成的 ID 就是 long 来存储的。\",\"第1位占用1bit，其值始终是0，可看做是符号位不使用。\",\"第2位开始的41位是时间戳，41-bit位可表示2^41个数，每个数代表毫秒，那么雪花算法可用的时间年限是(1L<<41)/(1000L360024*365)=69 年的时间。\",\"中间的10-bit位可表示机器数，即2^10 = 1024台机器，但是一般情况下我们不会部署这么台机器。如果我们对IDC（互联网数据中心）有需求，还可以将 10-bit 分 5-bit 给 IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，具体的划分可以根据自身需求定义。\",\"最后12-bit位是自增序列，可表示2^12 = 4096个数。\",\"这样的划分之后相当于在一毫秒一个数据中心的一台机器上可产生4096个有序的不重复的ID。但是我们 IDC 和机器数肯定不止一个，所以毫秒内能生成的有序ID数是翻倍的。\",\"下面是Java版本的雪花算法的实现\",\"/** * Twitter的Snowflake算法 * <p> * 协议格式 1： 41位时间戳 2:5位数据中心标识 3：5位机器标识 4:12位序列号 * <p> * 1111111111111111111111111111111 11111 11111 111111111111 */ public class Snowflake { //起始时间戳，可以修改为服务器第一次启动的时间 //一旦服务已经开始使用，起始时间戳就不能改变了，理论上可以使用69年 private final static long START_TIME = 1484754361114L; /** * 每一个部分占用的位数 */ private final static long SEQUENCE_BIT = 12;//序列号占用的位数 private final static long MACHINE_BIT = 5;//序机器标识 占用的位数 private final static long DATA_CENTER_BIT = 5;//数据中心标识占用的位数 /** * 每一个部分的最大值 11111111111111111 1111111100000 000000000011111 */ private final static long MAX_DATA_CENTER_ID = ~(-1L << DATA_CENTER_BIT); private final static long MAX_MACHINE_ID = ~(-1L << MACHINE_BIT); private final static long MAX_SEQUENCE = ~(-1L << SEQUENCE_BIT); /** * 每一部分向左位移数 1111111111111111111111111111111 11111 11111 111111111111 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATA_CENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTAMP_LEFT = DATA_CENTER_LEFT + DATA_CENTER_BIT; private long dataCenterId;//数据中心ID private long machineId;//数据中心ID private long sequence = 0L; private long lastTimestamp = -1L; /** * 分布式部署的时候，数据节点标识和机器标识作为联合键，必须唯一的 * * @param dataCenterId 数据中心标识ID * @param machineId 机器标识ID */ public Snowflake(long dataCenterId, long machineId) { if (dataCenterId > MAX_DATA_CENTER_ID || dataCenterId < 0) { throw new DistributedIdException(\\\"数据中心ID不合法\\\"); } if (machineId > MAX_MACHINE_ID || machineId < 0) { throw new DistributedIdException(\\\"机器标识ID不合法\\\"); } this.dataCenterId = dataCenterId; this.machineId = machineId; } /** * 获取下一个ID * * @return */ public synchronized long nextId() { long currentTmestamp = getNowTimestamp(); if (currentTmestamp < lastTimestamp) { throw new RuntimeException(\\\"时钟错误，拒绝生成ID\\\"); } if (currentTmestamp == lastTimestamp) { //相同毫秒内，序列号自增 sequence = (sequence + 1) & MAX_SEQUENCE; //同一毫秒的序列号已经达到最大 if (sequence == 0L) { currentTmestamp = getNexMill(); } } else { //不同毫秒内，序列号置为0 sequence = 0L; } lastTimestamp = currentTmestamp; return (currentTmestamp - START_TIME) << TIMESTAMP_LEFT //时间戳的部分 | dataCenterId << DATA_CENTER_LEFT //数据中心的部分 | machineId << MACHINE_LEFT //机器标识的部分 | sequence; //序列号的部分 } /** * 保证获取到的毫秒值是在最后一次分发ID的毫秒值之后lastTimestamp * 当某一个毫秒，序列号用完了之后，等待到下一个毫秒，在进行序列号的使用 * * @return */ private long getNexMill() { long timestamp = this.getNowTimestamp(); //不断的遍历，直到获取到lastTimestamp下一个毫秒值 while (timestamp <= lastTimestamp) { //进行时间回拨 timestamp = this.getNowTimestamp(); } return timestamp; } /** * 获取当前毫秒值 * @return */ private long getNowTimestamp() { return System.currentTimeMillis(); } /** * 使用当前计算机的MAC生成数据中心标识ID * * @param maxDataCenterId * @return */ private static long getDataCenterId(long maxDataCenterId) { long id = 0L; try { InetAddress ip = InetAddress.getLocalHost(); NetworkInterface network = NetworkInterface.getByInetAddress(ip); if (network == null) { id = 1L; } else { byte[] mac = network.getHardwareAddress(); if (mac != null) { id = ((0x000000FF & (long) mac[mac.length - 1]) | (0x0000FF00 & (((long) mac[mac.length - 2]) << 8))) >> 6; id = id % (maxDataCenterId + 1); } } } catch (Exception e) { e.printStackTrace(); } return id; } /** * 根据当前计算机的进程PID生成机器识别ID * @param dataCenterId * @param maxMachineId * @return */ private static long getMachineId(long dataCenterId, long maxMachineId) { StringBuilder sb = new StringBuilder(); sb.append(dataCenterId); //获取JVM进程的PID String name = ManagementFactory.getRuntimeMXBean().getName(); if (name != null) { sb.append(name.split(\\\"@\\\")[0]); } /** * MAC+PID 的hashcode 获取16个低位 */ int id = sb.toString().hashCode() & 0xffff; return id % (maxMachineId + 1); } public Snowflake() { dataCenterId = getDataCenterId(MAX_DATA_CENTER_ID); machineId = getMachineId(dataCenterId, MAX_MACHINE_ID); } //public static void main(String[] args) { // //指定数据中心和机器识别id // Snowflake snowflake = new Snowflake(2, 3); // System.out.println(\\\"指定数据中心和机器识别ID来生成ID\\\"); // for (int i = 0; i < 10; i++) { // System.out.println(snowflake.nextId()); // } // // //默认快速使用方式 // snowflake = new Snowflake(); // System.out.println(\\\"快速使用方式来生成ID\\\"); // for (int i = 0; i < 10; i++) { // System.out.println(snowflake.nextId()); // } //} } \",\"优点：\",\"毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。\",\"不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。\",\"可以根据自身业务特性分配bit位，非常灵活。\",\"缺点：\",\"强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。\"]},{\"header\":\"6、百度 Uidgenerator\",\"slug\":\"_6、百度-uidgenerator\",\"contents\":[\"UidGenerator是Java实现的, 基于Snowflake算法的唯一ID生成器。UidGenerator以组件形式工作在应用项目中, 支持自定义workerId位数和初始化策略, 从而适用于docker等虚拟化环境下实例自动重启、漂移等场景。 在实现上, UidGenerator通过借用未来时间来解决sequence天然存在的并发限制; 采用RingBuffer来缓存已生成的UID, 并行化UID的生产和消费, 同时对CacheLine补齐，避免了由RingBuffer带来的硬件级「伪共享」问题. 最终单机QPS可达600万。\",\"官方地址：https://github.com/baidu/uid-generator\",\"uid-generator提供了两种生成器: DefaultUidGenerator、CachedUidGenerator。如对UID生成性能有要求, 请使用CachedUidGenerator\",\"CachedUidGenerator\",\"RingBuffer环形数组，数组每个元素成为一个slot。RingBuffer容量，默认为Snowflake算法中sequence最大值，且为2^N。可通过boostPower配置进行扩容，以提高RingBuffer 读写吞吐量。\",\"Tail指针、Cursor指针用于环形数组上读写slot：\",\"Tail指针 表示Producer生产的最大序号(此序号从0开始，持续递增)。Tail不能超过Cursor，即生产者不能覆盖未消费的slot。当Tail已赶上curosr，此时可通过rejectedPutBufferHandler指定PutRejectPolicy\",\"Cursor指针 表示Consumer消费到的最小序号(序号序列与Producer序列相同)。Cursor不能超过Tail，即不能消费未生产的slot。当Cursor已赶上tail，此时可通过rejectedTakeBufferHandler指定TakeRejectPolicy\",\"CachedUidGenerator采用了双RingBuffer，Uid-RingBuffer用于存储Uid、Flag-RingBuffer用于存储Uid状态(是否可填充、是否可消费)\",\"由于数组元素在内存中是连续分配的，可最大程度利用CPU cache以提升性能。但同时会带来「伪共享」FalseSharing问题，为此在Tail、Cursor指针、Flag-RingBuffer中采用了CacheLine 补齐方式。\",\"RingBuffer填充时机\",\"初始化预填充 RingBuffer初始化时，预先填充满整个RingBuffer.\",\"即时填充 Take消费时，即时检查剩余可用slot量(tail - cursor)，如小于设定阈值，则补全空闲slots。阈值可通过paddingFactor来进行配置，请参考Quick Start中CachedUidGenerator配置\",\"周期填充 通过Schedule线程，定时补全空闲slots。可通过scheduleInterval配置，以应用定时填充功能，并指定Schedule时间间隔\"]},{\"header\":\"1、导入依赖\",\"slug\":\"_1、导入依赖\",\"contents\":[\"<!-- https://mvnrepository.com/artifact/com.xfvape.uid/uid-generator --> <dependency> <groupId>com.xfvape.uid</groupId> <artifactId>uid-generator</artifactId> <version>0.0.4-RELEASE</version> </dependency> \"]},{\"header\":\"2、创建表WORKER_NODE\",\"slug\":\"_2、创建表worker-node\",\"contents\":[\"运行sql脚本以导入表WORKER_NODE, 脚本如下:\",\"DROP DATABASE IF EXISTS `xxxx`; CREATE DATABASE `xxxx` ; use `xxxx`; DROP TABLE IF EXISTS WORKER_NODE; CREATE TABLE WORKER_NODE ( ID BIGINT NOT NULL AUTO_INCREMENT COMMENT 'auto increment id', HOST_NAME VARCHAR(64) NOT NULL COMMENT 'host name', PORT VARCHAR(64) NOT NULL COMMENT 'port', TYPE INT NOT NULL COMMENT 'node type: ACTUAL or CONTAINER', LAUNCH_DATE DATE NOT NULL COMMENT 'launch date', MODIFIED TIMESTAMP NOT NULL COMMENT 'modified time', CREATED TIMESTAMP NOT NULL COMMENT 'created time', PRIMARY KEY(ID) ) COMMENT='DB WorkerID Assigner for UID Generator',ENGINE = INNODB; \"]},{\"header\":\"3、maven依赖\",\"slug\":\"_3、maven依赖\",\"contents\":[\" <dependencies> <dependency> <groupId>com.xiaobear.distributedid</groupId> <artifactId>distributedid-core</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <scope>runtime</scope> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> <version>2.1.0</version> </dependency> <!--必须放在最后--> <!-- https://mvnrepository.com/artifact/com.xfvape.uid/uid-generator --> <dependency> <groupId>com.xfvape.uid</groupId> <artifactId>uid-generator</artifactId> <version>0.0.4-RELEASE</version> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>log4j-over-slf4j</artifactId> </exclusion> <exclusion> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> </exclusion> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </exclusion> <exclusion> <groupId>org.mybatis</groupId> <artifactId>mybatis-spring</artifactId> </exclusion> <exclusion> <groupId>org.mybatis</groupId> <artifactId>mybatis</artifactId> </exclusion> </exclusions> </dependency> </dependencies> \"]},{\"header\":\"4、配置文件\",\"slug\":\"_4、配置文件\",\"contents\":[\"spring: datasource: type: com.zaxxer.hikari.HikariDataSource url: jdbc:mysql://127.0.0.1:3306/distributedid?serverTimezone=Asia/Shanghai&characterEncoding=utf8&useUnicode=true&useSSL=false&autoReconnect=true username: root password: 密码 driver-class-name: com.mysql.cj.jdbc.Driver server: port: 8088 mybatis: type-aliases-package: com.xiaobear.distributedid.core.domain mapper-locations: classpath*:mapper/*.xml \"]},{\"header\":\"5、主启动类\",\"slug\":\"_5、主启动类\",\"contents\":[\"@SpringBootApplication @MapperScan(\\\"com.xiaobear.distributedid.mapper\\\") public class UidgeneratorApplication { public static void main(String[] args) { SpringApplication.run(UidgeneratorApplication.class); System.out.println(\\\"百度生成ID 启动成功\\\"); } } \"]},{\"header\":\"6、业务类\",\"slug\":\"_6、业务类\",\"contents\":[]},{\"header\":\"1、实体类\",\"slug\":\"_1、实体类\",\"contents\":[\"public class WorkerNode { private Long id; private String hostName; private String port; private Integer type; private Date launchDate; private Date modified; private Date created; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getHostName() { return hostName; } public void setHostName(String hostName) { this.hostName = hostName; } public String getPort() { return port; } public void setPort(String port) { this.port = port; } public Integer getType() { return type; } public void setType(Integer type) { this.type = type; } public Date getLaunchDate() { return launchDate; } public void setLaunchDate(Date launchDate) { this.launchDate = launchDate; } public Date getModified() { return modified; } public void setModified(Date modified) { this.modified = modified; } public Date getCreated() { return created; } public void setCreated(Date created) { this.created = created; } @Override public String toString() { return \\\"WorkerNode{\\\" + \\\"id=\\\" + id + \\\", hostName='\\\" + hostName + '\\\\'' + \\\", port='\\\" + port + '\\\\'' + \\\", type=\\\" + type + \\\", launchDate=\\\" + launchDate + \\\", modified=\\\" + modified + \\\", created=\\\" + created + '}'; } } \"]},{\"header\":\"2、数据层接口\",\"slug\":\"_2、数据层接口\",\"contents\":[\"@Mapper public interface WorkerNodeMapper { /** * 添加对象 * @param workerNodeEntity * @return */ int addWorkerNode(WorkerNode workerNodeEntity); /** * 通过host port 获取ID * @param host * @param port * @return */ WorkerNode getWorkerNodeByHostPort(@Param(\\\"host\\\") String host, @Param(\\\"port\\\") String port); } \",\"mapper.xml\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <!DOCTYPE mapper PUBLIC \\\"-//mybatis.org//DTD Mapper 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\\\"> <mapper namespace=\\\"com.xiaobear.distributedid.mapper.WorkerNodeMapper\\\"> <resultMap id=\\\"workerNodeRes\\\" type=\\\"com.xiaobear.distributedid.core.domain.WorkerNode\\\"> <id column=\\\"ID\\\" jdbcType=\\\"BIGINT\\\" property=\\\"id\\\"/> <result column=\\\"HOST_NAME\\\" jdbcType=\\\"VARCHAR\\\" property=\\\"hostName\\\"/> <result column=\\\"PORT\\\" jdbcType=\\\"VARCHAR\\\" property=\\\"port\\\"/> <result column=\\\"TYPE\\\" jdbcType=\\\"INTEGER\\\" property=\\\"type\\\"/> <result column=\\\"LAUNCH_DATE\\\" jdbcType=\\\"DATE\\\" property=\\\"launchDate\\\"/> <result column=\\\"MODIFIED\\\" jdbcType=\\\"TIMESTAMP\\\" property=\\\"modified\\\"/> <result column=\\\"CREATED\\\" jdbcType=\\\"TIMESTAMP\\\" property=\\\"created\\\"/> </resultMap> <insert id=\\\"addWorkerNode\\\" useGeneratedKeys=\\\"true\\\" keyProperty=\\\"id\\\" parameterType=\\\"com.xiaobear.distributedid.core.domain.WorkerNode\\\"> INSERT INTO WORKER_NODE (HOST_NAME, PORT, TYPE, LAUNCH_DATE, MODIFIED, CREATED) VALUES ( #{hostName}, #{port}, #{type}, #{launchDate}, NOW(), NOW()) </insert> <select id=\\\"getWorkerNodeByHostPort\\\" resultMap=\\\"workerNodeRes\\\"> SELECT ID, HOST_NAME, PORT, TYPE, LAUNCH_DATE, MODIFIED, CREATED FROM WORKER_NODE WHERE HOST_NAME = #{host} AND PORT = #{port} </select> </mapper> \"]},{\"header\":\"3、实现层接口\",\"slug\":\"_3、实现层接口\",\"contents\":[\"public interface IWorkerNodeService { /** * 获取ID * @return */ public long genUid(); } \"]},{\"header\":\"4、实现层实现类\",\"slug\":\"_4、实现层实现类\",\"contents\":[\"@Service public class WorkerNodeServiceImpl implements IWorkerNodeService { @Resource private UidGenerator uidGenerator; /** * 获取百度生成ID * @return */ @Override public long genUid() { return uidGenerator.getUID(); } } \"]},{\"header\":\"5、控制层接口\",\"slug\":\"_5、控制层接口\",\"contents\":[\"@RestController public class WorkerNodeController { @Resource private IWorkerNodeService workerNodeService; /** *使用百度 Uidgenerator获取ID * @return */ @GetMapping(\\\"/Uidgenerator\\\") public RestResponse getIdByBaiDuUid(){ long id = workerNodeService.genUid(); return RestResponse.success(id); } } \"]},{\"header\":\"7、测试\",\"slug\":\"_7、测试\",\"contents\":[\"http://localhost:8088/Uidgenerator\",\"{\\\"code\\\":200,\\\"msg\\\":\\\"成功\\\",\\\"result\\\":7067247636209745920} \"]},{\"header\":\"8、uid-generator核心对象装配为spring的bean\",\"slug\":\"_8、uid-generator核心对象装配为spring的bean\",\"contents\":[\"1、重写WorkerIdAssigner接口\",\"public class DisposableWorkerIdAssigner implements WorkerIdAssigner{ @Resource private WorkerNodeMapper workerNodeMapper; @Override @Transactional(rollbackFor = Exception.class) public long assignWorkerId() { WorkerNode workerNode = buildWorkerNode(); workerNodeMapper.addWorkerNode(workerNode); return workerNode.getId(); } private WorkerNode buildWorkerNode() { WorkerNode workNode = new WorkerNode(); if (DockerUtils.isDocker()) { workNode.setType(WorkerNodeType.CONTAINER.value()); workNode.setHostName(DockerUtils.getDockerHost()); workNode.setPort(DockerUtils.getDockerPort()); } else { workNode.setType(WorkerNodeType.ACTUAL.value()); workNode.setHostName(NetUtils.getLocalAddress()); workNode.setPort(System.currentTimeMillis() + \\\"-\\\" + RandomUtils.nextInt(100000)); } workNode.setLaunchDate(new Date()); return workNode; } } \",\"2、自动转配bean\",\"@Configuration public class WorkerNodeConfig { @Bean(name = \\\"disposableWorkerIdAssigner\\\") public DisposableWorkerIdAssigner disposableWorkerIdAssigner(){ return new DisposableWorkerIdAssigner(); } @Bean(name = \\\"cachedUidGenerator\\\") public UidGenerator uidGenerator(DisposableWorkerIdAssigner disposableWorkerIdAssigner){ CachedUidGenerator cachedUidGenerator = new CachedUidGenerator(); cachedUidGenerator.setWorkerIdAssigner(disposableWorkerIdAssigner); return cachedUidGenerator; } } \",\"3、使用，在实现类指定生成ID的bean\",\"@Service public class WorkerNodeServiceImpl implements IWorkerNodeService { @Resource(name = \\\"cachedUidGenerator\\\") private UidGenerator uidGenerator; /** * 获取百度生成ID * @return */ @Override public long genUid() { return uidGenerator.getUID(); } } \"]},{\"header\":\"7、美团 Leaf\",\"slug\":\"_7、美团-leaf\",\"contents\":[\"Leaf这个名字是来自德国哲学家、数学家莱布尼茨的一句话： >There are no two identical leaves in the world > “世界上没有两片相同的树叶”\",\"综合对比上述几种方案，每种方案都不完全符合我们的要求。所以Leaf分别在上述第二种和第三种方案上做了相应的优化，实现了Leaf-segment和Leaf-snowflake方案。\"]},{\"header\":\"Leaf-segment数据库方案\",\"slug\":\"leaf-segment数据库方案\",\"contents\":[\"第一种Leaf-segment方案，在使用数据库的方案上，做了如下改变： - 原方案每次获取ID都得读写一次数据库，造成数据库压力大。改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。 - 各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。\",\"数据库表设计如下：\",\"+-------------+--------------+------+-----+-------------------+-----------------------------+ | Field | Type | Null | Key | Default | Extra | +-------------+--------------+------+-----+-------------------+-----------------------------+ | biz_tag | varchar(128) | NO | PRI | | | | max_id | bigint(20) | NO | | 1 | | | step | int(11) | NO | | NULL | | | desc | varchar(256) | YES | | NULL | | | update_time | timestamp | NO | | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP | +-------------+--------------+------+-----+-------------------+-----------------------------+ \",\"重要字段说明：biz_tag用来区分业务，max_id表示该biz_tag目前所被分配的ID号段的最大值，step表示每次分配的号段长度。原来获取ID每次都需要写数据库，现在只需要把step设置得足够大，比如1000。那么只有当1000个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从1减小到了1/step，大致架构如下图所示：\",\"test_tag在第一台Leaf机器上是11000的号段，当这个号段用完时，会去加载另一个长度为step=1000的号段，假设另外两台号段都没有更新，这个时候第一台机器新加载的号段就应该是30014000。同时数据库对应的biz_tag这条数据的max_id会从3000被更新成4000，更新号段的SQL语句如下：\",\"Begin UPDATE table SET max_id=max_id+step WHERE biz_tag=xxx SELECT tag, max_id, step FROM table WHERE biz_tag=xxx Commit \",\"这种模式有以下优缺点：\",\"优点：\",\"Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景。\",\"ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求。\",\"容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。\",\"可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。\",\"缺点：\",\"ID号码不够随机，能够泄露发号数量的信息，不太安全。\",\"TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺。\",\"DB宕机会造成整个系统不可用。\"]},{\"header\":\"双buffer优化\",\"slug\":\"双buffer优化\",\"contents\":[\"对于第二个缺点，Leaf-segment做了一些优化，简单的说就是：\",\"Leaf 取号段的时机是在号段消耗完的时候进行的，也就意味着号段临界点的ID下发时间取决于下一次从DB取回号段的时间，并且在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。如果请求DB的网络和DB的性能稳定，这种情况对系统的影响是不大的，但是假如取DB的时候网络发生抖动，或者DB发生慢查询就会导致整个系统的响应时间变慢。\",\"为此，我们希望DB取号段的过程能够做到无阻塞，不需要在DB取号段的时候阻塞请求线程，即当号段消费到某个点时就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。这样做就可以很大程度上的降低系统的TP999指标。详细实现如下图所示：\",\"采用双buffer的方式，Leaf服务内部有两个号段缓存区segment。当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。\",\"每个biz-tag都有消费速度监控，通常推荐segment长度设置为服务高峰期发号QPS的600倍（10分钟），这样即使DB宕机，Leaf仍能持续发号10-20分钟不受影响。\",\"每次请求来临时都会判断下个号段的状态，从而更新此号段，所以偶尔的网络抖动不会影响下个号段的更新。\"]},{\"header\":\"Leaf高可用容灾\",\"slug\":\"leaf高可用容灾\",\"contents\":[\"对于第三点“DB可用性”问题，我们目前采用一主两从的方式，同时分机房部署，Master和Slave之间采用半同步方式[5]同步数据。同时使用公司Atlas数据库中间件（已开源，改名为DBProxy）做主从切换。当然这种方案在一些情况会退化成异步模式，甚至在非常极端情况下仍然会造成数据不一致的情况，但是出现的概率非常小。如果你的系统要保证100%的数据强一致，可以选择使用“类Paxos算法”实现的强一致MySQL方案，如MySQL 5.7前段时间刚刚GA的MySQL Group Replication。但是运维成本和精力都会相应的增加，根据实际情况选型即可。\",\"同时Leaf服务分IDC部署，内部的服务化框架是“MTthrift RPC”。服务调用的时候，根据负载均衡算法会优先调用同机房的Leaf服务。在该IDC内Leaf服务不可用的时候才会选择其他机房的Leaf服务。同时服务治理平台OCTO还提供了针对服务的过载保护、一键截流、动态流量分配等对服务的保护措施。\"]},{\"header\":\"Leaf-snowflake方案\",\"slug\":\"leaf-snowflake方案\",\"contents\":[\"Leaf-segment方案可以生成趋势递增的ID，同时ID号是可计算的，不适用于订单ID生成场景，比如竞对在两天中午12点分别下单，通过订单id号相减就能大致计算出公司一天的订单量，这个是不能忍受的。面对这一问题，我们提供了 Leaf-snowflake方案。\",\"Leaf-snowflake方案完全沿用snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号。对于workerID的分配，当服务集群数量较小的情况下，完全可以手动配置。Leaf服务规模较大，动手配置成本太高。所以使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID。Leaf-snowflake是按照下面几个步骤启动的：\",\"启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。\",\"如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。\",\"如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。\"]},{\"header\":\"弱依赖ZooKeeper\",\"slug\":\"弱依赖zookeeper\",\"contents\":[\"除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。这样做到了对三方组件的弱依赖。一定程度上提高了SLA。\"]},{\"header\":\"解决时钟问题\",\"slug\":\"解决时钟问题\",\"contents\":[\"因为这种方案依赖时间，如果机器的时钟发生了回拨，那么就会有可能生成重复的ID号，需要解决时钟回退的问题。\",\"参见上图整个启动流程图，服务启动时首先检查自己是否写过ZooKeeper leaf_forever节点：\",\"若写过，则用自身系统时间与leaf_forever/${self}节点记录时间做比较，若小于leaf_forever/${self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警。\",\"若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize。\",\"若abs( 系统时间-sum(time)/nodeSize ) < 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约。\",\"否则认为本机系统时间发生大步长偏移，启动失败并报警。\",\"每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}。\",\"由于强依赖时钟，对时间的要求比较敏感，在机器工作时NTP同步也会造成秒级别的回退，建议可以直接关闭NTP同步。要么在时钟回拨的时候直接不提供服务直接返回ERROR_CODE，等时钟追上即可。或者做一层重试，然后上报报警系统，更或者是发现有时钟回拨之后自动摘除本身节点并报警，如下：\",\" //发生了回拨，此刻时间小于上次发号时间 if (timestamp < lastTimestamp) { long offset = lastTimestamp - timestamp; if (offset <= 5) { try { //时间偏差大小小于5ms，则等待两倍时间 wait(offset << 1);//wait timestamp = timeGen(); if (timestamp < lastTimestamp) { //还是小于，抛异常并上报 throwClockBackwardsEx(timestamp); } } catch (InterruptedException e) { throw e; } } else { //throw throwClockBackwardsEx(timestamp); } } //分配ID \",\"从上线情况来看，在2017年闰秒出现那一次出现过部分机器回拨，由于Leaf-snowflake的策略保证，成功避免了对业务造成的影响。\"]},{\"header\":\"实现\",\"slug\":\"实现\",\"contents\":[\"官方教程：https://github.com/Meituan-Dianping/Leaf/blob/feature/spring-boot-starter/README_CN.md\",\"从官方可以看出，并没有提供仓库依赖和jar下载，需要自己拉取项目进行打包\"]},{\"header\":\"8、滴滴 TinyID\",\"slug\":\"_8、滴滴-tinyid\",\"contents\":[\"Tinyid是用Java开发的一款分布式id生成系统，基于数据库号段算法实现。Tinyid扩展了leaf-segment算法，支持了多数据库和tinyid-client\",\"Tinyid也是基于号段算法实现，系统实现图如下：\",\"优点：方便集成，有成熟的方案和解决实现\",\"缺点：依赖 DB的稳定性，需要采用集群主从备份的方式提高 DB的可用性\",\"滴滴TinyID wiki：https://github.com/didi/tinyid/wiki\"]}]},\"/architecture/distributed/message-queue.html\":{\"title\":\"分布式消息队列\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"今天我们来学习分布式消息队列，分布式消息队列的知识结构如下图。\",\"主要介绍以下内容：\",\"同步架构和异步架构的区别。异步架构的主要组成部分：消息生产者、消息消费者、分布式消息队列。异步架构的两种主要模型：点对点模型和发布订阅模型。\",\"分布式消息队列异步架构的好处：异步处理实现快速响应；消费者易于伸缩；高并发访问压力的时削峰填谷，减轻访问高峰的系统负载压力；隔离失败任务，消费者处理任务失败，不会影响主业务流程；业务逻辑解耦，系统易于开发和维护。\",\"异步架构的挑战：消息无序，竞态条件，系统复杂度提高。\",\"使用分布式消息队列异步架构的反模式：消息队列阻塞式调用，生产者消费者显式依赖，缺乏坏消息处理机制。\",\"常用的消息队列产品：RabbitMQ，ActiveMQ，RocketMQ，Kafka 等。\"]},{\"header\":\"同步调用与异步调用\",\"slug\":\"同步调用与异步调用\",\"contents\":[]},{\"header\":\"同步调用\",\"slug\":\"同步调用\",\"contents\":[\"先看一下什么是同步调用。所谓的同步调用，就是说从请求的发起一直到最终的处理完成期间，请求的调用方一直在同步阻塞，等待调用的处理完成。下图所示的例子中，客户端代码 ClientCode，需要执行发送邮件 sendEmail 这样一个操作，它会调用 EmailService 进行发送，而 EmailService 会调用 SmtpEmailAdapter 类来进行处理，这个类会调用远程的一个服务，通过 SMTP 和 TCP 协议发送请求。\",\"而远程服务器收到消息以后会对消息进行一系列的操作，然后将邮件发送出去，再进行返回。Adapter 收到返回后，再返回给 EmailService。EmailService 收到返回后再把返回结果返回给 ClientCode。\",\"ClientCode 在 sendEmail 发出请求后，就一直都阻塞在这里，等待最终调用结果的返回，是成功还是失败。因为这个过程是阻塞等待的，所以这个过程是同步调用。\"]},{\"header\":\"异步调用\",\"slug\":\"异步调用\",\"contents\":[\"与同步调用相反的是异步调用。异步调用过程，如下图所示，用户 ClientCode 调用 EmailService 以后，EmailService 会把这个调用请求发送给消息队列，然后就立即返回了。ClientCode 收到返回以后继续向下处理，不会继续阻塞等待，实际上消息发送到 Queue 后，还没有被处理。可以看到后面的消息消费，其实要比 EmailService 返回可能还要晚一点，EmailService 返回以后消息才会被消费处理。\",\"QueueConsumer 消息队列的消费者，从消息队列中取出这个消息，再把这个消息发送给 SmtpAdapter，也就是调用 SmtpAdapter，处理逻辑跟同步调用一样。SmtpAdapter 通过 SMTP 的通讯协议，把消息发送给远程的一个服务器，进行邮件发送，通过 RemoteServer 进行处理，处理完了收到返回，再把返回结果通知消息队列 Queue。\",\"在这个过程中，客户端的调用，也就是应用程序的调用，和业务逻辑真正发送邮件的操作是不同步的。在进行邮件发送操作的处理过程中，客户端的代码已经返回了，它可以继续进行自己的后续操作，而不需要等待邮件的发送，这就叫做异步调用。\"]},{\"header\":\"消息队列构建异步调用架构\",\"slug\":\"消息队列构建异步调用架构\",\"contents\":[\"使用异步调用架构的主要手段，就是通过消息队列构建。架构图如下图所示。\",\"消息的生产者将消息发送到消息队列以后，由消息的消费者从消息队列中获取消息，然后进行业务逻辑的处理，消息的生产者和消费者是异步处理的，彼此不会等待阻塞，所以叫做异步架构。\",\"使用消息队列构建一个异步调用架构，你需要了解 3 个角色：一是消息的生产者，二是消息队列，三是消息的消费者。\"]},{\"header\":\"消息生产者\",\"slug\":\"消息生产者\",\"contents\":[\"消息的生产者是客户端应用程序代码的一部分，用来初始化异步调用处理流程。在消息队列的处理中，生产者的职责非常少，它要做的就是创建一个合法的消息，并把这个消息发送到消息队列中，由应用开发者决定生产者的代码在哪里执行，什么时候发送消息。\"]},{\"header\":\"消息队列\",\"slug\":\"消息队列\",\"contents\":[\"消息队列异步架构的另一个重要组成部分就是消息队列，消息队列是消息发送的目的地，也是发给消费者过程中的一个缓冲。消息队列实现的方法有好多种，可以用共享文件夹，也可以用关系数据库或者 NoSQL 系统，当然最主要的还是使用专门的分布式消息队列服务器。\"]},{\"header\":\"消息消费者\",\"slug\":\"消息消费者\",\"contents\":[\"业务架构的第三个重要角色就是消息的消费者。消息的消费者从消息队列中接收并处理消息，也是由应用开发者实现的，但是一个异步处理的组件。消息的消费者不需要知道生产者存在，它只依赖消息队列中的消息。消息的消费者通常部署在独立的服务器上，和消息的生产者完全隔离，并且可以通过添加硬件的方式进行伸缩。\"]},{\"header\":\"点对点模型\",\"slug\":\"点对点模型\",\"contents\":[\"知道以上 3 种主要角色之后，使用消息队列构建异步的调用架构，还需要知道两种模型：点对点模型和发布订阅模型。\",\"首先来看点对点模型。消费者和生产者只需要知道消息队列的名字，生产者发送消息到消息队列中，而消息队列的另一端是多个消费者竞争消费消息，每个到达消息队列的消息只会被路由到一个消费者中去，所以消费者看到的是全部消息的一个子集。如下图所示，消息的生产者有多个，消息的消费者也有多个，多个生产者将消息发送到消息队列中，而有多个消费者去消息队列中对消息进行竞争性的消费。每条消息只会被一个消费者消费，每个消费者只会消费消息队列中的一部分消息。\"]},{\"header\":\"发布订阅模型\",\"slug\":\"发布订阅模型\",\"contents\":[\"再来看发布订阅模型。在发布订阅模型中，消息可能被发送到不止一个消费者，生产者发送消息到一个主题，而不是队列中。消息被发布到主题后，就会被克隆给每一个订阅它的消费者，每个消费者接收一份消息复制到自己的私有队列。消费者可以独立于其他消费者使用自己订阅的消息，消费者之间不会竞争消息。常用的分布式消息队列都支持发布订阅模型，也就是说消息的发布订阅模型是分布式消息队列的一个功能特性。\"]},{\"header\":\"两种模型对比\",\"slug\":\"两种模型对比\",\"contents\":[\"两种模型结合对比来看，通常使用点对点模型的，是一些耗时较长的、逻辑相对独立的业务，比如前面讲到的发送邮件这样一个操作。因为发送邮件比较耗时，而且应用程序其实并不太关心邮件发送是否成功，发送邮件的逻辑也相对比较独立，所以它只需要把邮件消息丢到消息队列中就可以返回了。消费者也不需要关心是哪个生产者去发送的邮件，它只需要把邮件消息内容取出来以后进行消费，通过远程服务器将邮件发送出去就可以了。而且每个邮件只需要被发送一次。所以消息只被一个消费者消费就可以了。\",\"相对应的另一些情况，比如新用户注册，就适合使用发布订阅模型。一个新用户注册成功以后，需要给用户发送一封激活邮件，发送一条欢迎短信，还需要将用户注册数据写入数据库，甚至需要将新用户信息发送给关联企业的系统，比如淘宝新用户信息发送给支付宝，这样允许用户可以一次注册就能登录使用多个关联产品。那么对于新用户注册这样一个消息，就需要使用按主题发布的方式，也就是发布订阅模型这种方式。一个新用户注册，会把注册消息发送给一个主题，多种消费者可以订阅这个主题，比如发送邮件的消费者、发送短信的消费者、将注册信息写入数据库的消费者，跨系统同步消息的消费者等。\"]},{\"header\":\"消息队列的好处\",\"slug\":\"消息队列的好处\",\"contents\":[\"通过前面的例子，可以看出消息队列有这样的一些优点，包括异步处理、易伸缩、使峰值变平缓、失败隔离及自我修复、解耦。\"]},{\"header\":\"异步处理\",\"slug\":\"异步处理\",\"contents\":[\"第一个好处是实现异步处理，提升处理性能。对一些比较耗时的操作，我们可以把处理过程通过消息队列进行异步处理。这样做一个显而易见的好处就是，可以推迟耗时操作的处理，使耗时操作异步化，而不必阻塞客户端的程序，客户端的程序在得到处理结果之前就可以继续执行，从而提高客户端程序的处理性能。\"]},{\"header\":\"易伸缩\",\"slug\":\"易伸缩\",\"contents\":[\"第二个好处，它可以让系统获得更好的伸缩性。因为耗时的任务可以通过分布式消息队列向多台消费者服务器并行发送消息，然后在很多台消费者服务器上并行处理消息，也就是说可以在多台物理服务器上运行消费者。那么当负载上升的时候，可以很容易地添加更多的机器成为消费者。\",\"如下图所示，用户上传文件后，通过发布消息的方式，通知后端的消费者获取数据、读取文件，进行异步的文件处理操作。那么当前端发布更多文件的时候，或者处理逻辑比较复杂的时候，就可以通过添加后端的消费者服务器，增强系统的处理能力。\"]},{\"header\":\"使峰值平缓\",\"slug\":\"使峰值平缓\",\"contents\":[\"使用消息队列的第三个好处是可以平衡流量峰值，削峰填谷。使用消息队列，即便是访问流量持续的增长，系统依然可以持续地接收请求。这种情况下，虽然生产者发布消息的速度比消费者消费消息的速度快，但是可以持续地将消息纳入到消息队列中，用消息队列作为消息的缓冲，因此短时间内，发布者不会受到消费处理能力的影响。\",\"如下图所示，因为消息的生产者是直接面向用户请求的，而用户的请求访问压力是不均衡的，比如淘宝每天的访问高峰是在上午 10 点左右，而新浪微博则可能在某个明星半夜发一条微博后突然出现访问高峰。\",\"在访问高峰，用户的并发访问数可能超过了系统的处理能力，所以在高峰期就可能会导致系统负载过大，响应速度变慢，更严重的可能会导致系统崩溃。这种情况下，通过消息队列将用户请求的消息纳入到消息队列中，通过消息队列缓冲消费者处理消息的速度。\",\"如图中所示，消息的生产者负载有高峰有低谷，但是到了消费者这里，只会按照自己的最佳处理能力去消费消息。高峰期它会把消息缓冲在消息队列中，而在低谷期它也还是使用自己最大的处理能力去获取消息，将前面缓冲起来、来不及及时处理的消息处理掉。那么，通过这种手段可以实现系统负载削峰填谷，也就是说将访问的高峰削掉，而将访问的低谷填平，使系统处在一个最佳的处理状态之下，不会对系统的负载产生太大的冲击。\"]},{\"header\":\"失败隔离及自我修复\",\"slug\":\"失败隔离及自我修复\",\"contents\":[\"消息队列的第四个好处是失败隔离和自我修复。因为发布者不直接依赖消费者，所以分布式消息队列可以将消费者系统产生的错误异常与生产者系统隔离开来，生产者不受消费者失败的影响。 当在消息消费过程中出现处理逻辑失败的时候，这个错误只会影响到消费者自身，而不会传递给消息的生产者，也就是应用程序可以按照原来的处理逻辑继续执行。\",\"这也就意味着在任何时候都可以对后端的服务器执行维护和发布操作。我们可以重启、添加或删除服务器，而不影响生产者的可用性，这样简化了部署和服务器管理的难度。\"]},{\"header\":\"解耦\",\"slug\":\"解耦\",\"contents\":[\"第五个好处，如下图所示，使用分布式消息队列，可以使生产者和消费者的代码实现解耦合，也就是说可以多个生产者发布消息，多个消费者处理消息，共同完成完整的业务处理逻辑，但是它们却不需要直接进行交互调用，没有代码的依赖耦合。在传统的同步调用中，调用者代码必须要依赖被调用者的代码，也就是生产者代码必须要依赖消费者的处理逻辑代码，代码需要直接的耦合，而使用消息队列，这两部分的代码不需要进行任何的耦合。耦合程度越低的代码越容易维护，也越容易进行扩展。\",\"比如前面提到的新用户注册的例子，如果用传统同步调用的方式，那么发邮件、发短信、写数据库、通知关联系统这些代码会和用户注册代码直接耦合起来，整个代码看起来就是完成用户注册逻辑后，后面必然跟着发邮件、发短信这些代码。如果要新增一个功能，比如将监控用户注册情况，将注册信息发送到业务监控系统，就必须要修改前面的代码，至少增加一行代码，发送注册信息到监控系统，我们知道，任何代码的修改都可能会引起 bug。\",\"而使用分布式消息队列实现生产者和消费者解耦合以后，用户注册完成，不需要调用任何后续处理代码，只需要将注册消息发送到分布式消息队列就可以了。如果要增加新功能，只需要写个新功能的消费者程序，在分布式消息队列中，订阅用户注册主题就可以了，不需要修改原来任何一行代码。\",\"解耦的特点对于团队的工作分工也很有帮助。从消息生产者的视角看，它只需要构建消息，将消息放入消息队列中，开发就完成了；而从消费者的开发视角看，它只需要从消息队列中获取消息，然后进行逻辑处理。它们彼此之间不进行任何耦合。消息的生产者不关心放入消息队列中下一步会发生什么，而消费者也不需要知道消息从哪里来。这两部分程序的开发者也可以不关心彼此的工作进展，他们开发的代码也不需要集成在一起，只要约定好消息格式，就可以各自开发了。\"]},{\"header\":\"消息队列相关挑战\",\"slug\":\"消息队列相关挑战\",\"contents\":[\"了解到上面的 5 点好处，你会认为消息队列是完美的吗？其实不是的，接下来讲分布式消息队列遇到的挑战：消息无序、消息重新入队列、竞态条件、复杂度风险。\"]},{\"header\":\"消息无序\",\"slug\":\"消息无序\",\"contents\":[\"先来说消息无序。因为生产者和消费者是异步处理的。虽然消息队列本身会保证先创建的消息在前面，但是消费者却并不能保证先创建的消息先消费掉。\",\"如下图所示，生产者会创建两个消息，一个是创建用户，另一个是欢迎邮件。消费者应该先消费创建用户，然后再消费欢迎邮件，业务逻辑有先后顺序。但是由于消费者可能是在并行执行，两个消费者分别获得了创建用户和发送欢迎邮件两个消息。那么，有可能欢迎邮件被消费者先处理完了，而创建用户的消费者还没有来得及处理这条消息，就可能会导致欢迎邮件在创建用户之前就已经发出去了。\",\"一个简单的解决办法就是将消息处理的顺序设计到异步流程中，也就是创建用户的消费者在处理消息后，再发送一个欢迎邮件的消息到消息队列中。这样就可以保证邮件发送一定在创建用户之后，从而满足业务逻辑的顺序性要求。\"]},{\"header\":\"消息重新入队列\",\"slug\":\"消息重新入队列\",\"contents\":[\"另一个挑战是消息重新入队列，重复消费。有些分布式消息队列产品支持将某个消费者处理失败的消息重新放入到消息队列中，被其它的消费者重新处理。但是重新放入到消息队列中的消息有可能是被处理完成了的，也就是表面看起来处理失败，实际上已经处理完成，这种情况在软件运行过程中并不鲜见。那么这种情况下就会导致同一条消息被多次消费。\",\"解决这个问题的主要手段是将消息处理设计成幂等性，也就是说消费者可以对同一条消息进行多次处理计算，而不会影响最终的结果。有些操作天然就是幂等的，比如将商品价格设置为 50 元，不管设置多少次，都是 50 元，不会影响最终结果。而有些操作，比如发送邮件，发送两次和发送一次结果肯定是不同的，对于这类非天然幂等的操作，需要进行特别设计，才能实现最终效果上的幂等。\"]},{\"header\":\"竞态条件\",\"slug\":\"竞态条件\",\"contents\":[\"第三个挑战是竞态条件。所谓竞态条件就是指在程序并发执行的时候，不同的执行顺序会导致不同的结果，主要是因为对共享资源的访问顺序不同导致的结果不同。我们在编程中通过多线程实现程序的并发执行，消息队列可以在分布式的环境下实现架构层面的并发执行，并发执行就可能会导致对资源的争用。在编程中我们通常使用锁的机制进行并发的控制，以避免竞态、顺序执行。在消息队列的异步架构中也需要对共享资源的并发访问进行控制，以避免竞态条件的出现。\"]},{\"header\":\"复杂度风险\",\"slug\":\"复杂度风险\",\"contents\":[\"消息队列的第四个挑战是复杂度风险。消息队列使系统的架构和处理流程更加复杂，带来了更多的复杂性问题，从而也对架构师的系统架构设计能力和架构把控能力提出了更高的挑战和要求。\"]},{\"header\":\"消息队列的反模式\",\"slug\":\"消息队列的反模式\",\"contents\":[\"所谓模式就是指可多次复用的解决方案。当解决方案一次又一次地被证明是成功的，我们就称它为“模式”。后面有同类问题出现的时候，我们就使用相同的解决方案去处理，也就是所谓的模式复用。\",\"但是如果解决方案被认为是错误的，它们经常会带来问题，就称之为“反模式”。典型的反模式一开始用起来不错，但是时间越长问题越多。熟悉反模式，你就能在未来避免它们，就像对常见的设计缺陷产生免疫一样。\",\"接下来介绍消息队列常见的几种反模式。\"]},{\"header\":\"阻塞式调用\",\"slug\":\"阻塞式调用\",\"contents\":[\"有些分布式消息队列产品允许生产者阻塞，也就是生产者发送消息以后，阻塞等待消息队列处理结果，等消费者处理完成返回处理结果以后，继续向下执行。这样就使消息队列成为一个同步的调用模式。使用同步模式看起来在某些场合下是比较合理的，因为可以等待执行结果，拿到结果后继续处理，但是这种方式使得消息队列异步架构的各种好处都丧失了。\"]},{\"header\":\"耦合生产者和消费者\",\"slug\":\"耦合生产者和消费者\",\"contents\":[\"另一种反模式是耦合生产者和消费者。虽然消息队列将生产和消费者解耦合了，但是不恰当的设计依然会使生产者和消费者产生耦合。比如说在消息中包含处理逻辑，也就是说在消息中约定消费者应该如何进行处理。或者是说使用特定的序列化协议编码消息。那么消费者必须要按照特定的序列化格式，才能解码消息。这些情况都使得生产者和消费者产生了不必要的耦合。\"]},{\"header\":\"缺少坏消息处理\",\"slug\":\"缺少坏消息处理\",\"contents\":[\"还有一种反模式是缺少坏消息的处理。使用消息队列的时候，不能总是假定消息永远正确。对于引发消费者崩溃的消息，应该丢弃而不是重新处理。因为如果导致消费者失败的原因是消息本身，那么每次重新处理都会导致消费者失败，最后导致整个消费者服务器集群都崩溃，系统什么也干不了。\"]},{\"header\":\"常用消息队列产品\",\"slug\":\"常用消息队列产品\",\"contents\":[\"目前业界常用的消息队列产品，主要有：RabbitMQ 、ActiveMQ、RocketMQ 、Kafka。\",\"RabbitMQ 的主要特点是性能好，社区活跃，但是 RabbitMQ 用 Erlang 开发，我们的应用很少用 Erlang，所以不便于二次开发和维护。\",\"ActiveMQ 影响比较广泛，可以跨平台，使用 Java 开发，对 Java 开发者比较友好。\",\"RocketMQ 是阿里推出的一个开源产品，也是使用 Java 开发，性能比较好，可靠性也比较高。\",\"Kafka 是 Linkedin 出品的，专门针对分布式场景进行了优化，因此分布式的伸缩性会比较好。\",\"目前看来，Kafka 因为最初就是针对互联网的分布式、高可用应用场景而设计的，并且在大数据领域得到广泛支持，资料文档更加完善，因此在互联网企业得到更多的应用。\",\"分享一个技术产品选型的小技巧，技术决策时可作为参考。当在几个相似的技术产品中进行选型决策，并且拿不定主意、感觉都差不多的时候，一个办法就是利用搜索引擎搜索一下这些产品的名字。搜索结果最多的产品，一般是最热门，文档资料最多，遇到问题有更大概率找到答案的，最有发展前景不会半途而废没人维护的。利用这个技巧，我们看一下消息队列（MQ）的产品选型，Kafka 在百度中的搜索结果数量是其它三个 MQ 产品的搜索结果数量之和，那么如果你拿不定主意，选择 Kafka 至少不会是最糟糕的选择。\"]},{\"header\":\"总结\",\"slug\":\"总结\",\"contents\":[\"使用异步调用的架构方法，就是使用消息队列，将生产者和消费者进行隔离。主要的架构模型有两种，一种是点对点模型，一种是发布订阅模型。其中点对点模型，一个消息只会被一个消费者消费；而发布订阅模型，一个消息可以被多个消费者订阅。\",\"消息队列实现的异步架构可以在架构上带来更多的好处。它可以实现业务逻辑的异步处理，从而获得更好性能特性；可以使系统具有更好的伸缩性；可以平衡用户访问流量，实现削峰填谷；还可以隔离失败，并进行自我修复；以及对生产者和消费者进行解耦，使系统拥有更好的扩展和维护能力。\",\"同时我们需要关注异步消息队列架构带来的挑战。第一个是消息无序，第二个是消息重复处理，第三个是竞态条件，还有一个是系统的复杂度的增加。\",\"总之，消息队列实际上可以带来很多架构上的好处，但是不正确地使用消息队列可能会丧失这些好处。\"]}]},\"/architecture/distributed/session.html\":{\"title\":\"分布式会话解决方案\",\"contents\":[{\"header\":\"1、传统Session\",\"slug\":\"_1、传统session\",\"contents\":[]},{\"header\":\"1、传统session的概述\",\"slug\":\"_1、传统session的概述\",\"contents\":[]},{\"header\":\"1、认证过程\",\"slug\":\"_1、认证过程\",\"contents\":[\"1、用户向服务器发送用户名和密码。\",\"2、服务器验证通过后，在当前对话（session）里面保存相关数据，比如用户角色、登录时间等等。\",\"3、服务器向用户返回一个 session_id，写入用户的 Cookie。\",\"4、用户随后的每一次请求，都会通过 Cookie，将 session_id 传回服务器。\",\"5、服务器收到 session_id，找到前期保存的数据，由此得知用户的身份。\"]},{\"header\":\"2、存在的问题\",\"slug\":\"_2、存在的问题\",\"contents\":[\"扩展性不好，向上图中，存在两个服务器时，session不能共享\"]},{\"header\":\"3、解决方案\",\"slug\":\"_3、解决方案\",\"contents\":[]},{\"header\":\"1、session复制\",\"slug\":\"_1、session复制\",\"contents\":[\"将服务器1的session，复制到服务器2，同样将服务器2的session也复制到服务器1，这样两台服务器的session就一致了。像tomcat等web容器都支持session复制的功能，在同一个局域网内，一台服务器的session会广播给其他服务器。\",\"缺点：同一个网段内服务器太多，每个服务器都会去复制session，会造成服务器内存浪费。\"]},{\"header\":\"2、session黏性\",\"slug\":\"_2、session黏性\",\"contents\":[]},{\"header\":\"3、Redis解决分布式session\",\"slug\":\"_3、redis解决分布式session\",\"contents\":[\"解决方案如下：统一将用户信息存入redis中，从redis中去获取登录的用户信息，这样就可解决分布式session了\"]},{\"header\":\"2、传统session实战\",\"slug\":\"_2、传统session实战\",\"contents\":[]},{\"header\":\"1、session服务搭建\",\"slug\":\"_1、session服务搭建\",\"contents\":[\"新建一个SpringBoot项目，编写controller类，代码如下👇\",\"@RestController @RequestMapping(\\\"/user\\\") public class UserController { /** * 登录接口 * @param userName * @param password * @param session * @return */ @GetMapping(\\\"/login\\\") public String login(@RequestParam String userName, @RequestParam String password, HttpSession session){ session.setAttribute(\\\"user\\\", userName); return \\\"登录成功\\\"; } /** * 获取用户信息接口 * @param session * @return */ @GetMapping(\\\"/info\\\") public String getInfo(HttpSession session){ return \\\"登录用户为：\\\" + session.getAttribute(\\\"user\\\"); } } \"]},{\"header\":\"2、测试\",\"slug\":\"_2、测试\",\"contents\":[]},{\"header\":\"1、访问登录接口\",\"slug\":\"_1、访问登录接口\",\"contents\":[\"接口路径：http://localhost:8081/user/login?userName=javaxiaobear&password=123456\"]},{\"header\":\"2、访问获取用户接口\",\"slug\":\"_2、访问获取用户接口\",\"contents\":[\"接口路径：http://localhost:8081/user/info\"]},{\"header\":\"3、为什么第二次访问，就获取了到了用户信息呢？\",\"slug\":\"_3、为什么第二次访问-就获取了到了用户信息呢\",\"contents\":[\"第一次访问登录接口，后端代码中我们有设置session，设置后，前端浏览器获取到了，就会把session的值set-cookie中，当第二次请求info接口时，会携带cookie访问到后端，通过cookie查询session，然后返回给客户端\"]},{\"header\":\"3、cookie的特性\",\"slug\":\"_3、cookie的特性\",\"contents\":[]},{\"header\":\"1、cookie不能跨域\",\"slug\":\"_1、cookie不能跨域\",\"contents\":[\"作为开发人员，我们都知道，localhost和127.0.0.1其实没啥区别，接下来我们访问下：http://127.0.0.1:8081/user/info，前提是localhost是成功登录的情况下\",\"发现获取不到用户信息\"]},{\"header\":\"2、cookie存在tomcat中\",\"slug\":\"_2、cookie存在tomcat中\",\"contents\":[\"我们模拟两台Java服务器，具体操作如下：配置是-Dserver.port=8082\",\"在8081服务器登录的情况下，访问：http://localhost:8082/user/info，发现获取不到登录用户，这是因为cookie是存在tomcat服务器中，获取不到！\"]},{\"header\":\"2、Spring-Session\",\"slug\":\"_2、spring-session\",\"contents\":[\"Spring-Session 提供了对Redis、MongoDB、MySQL 等常用存储的支持，Spring-Session 提供与 HttpSession 的透明整合，这意味着开发人员可以使用 Spring-Session 支持的实现方式，切换 HttpSession 至 Spring-Session\"]},{\"header\":\"1、引入依赖\",\"slug\":\"_1、引入依赖\",\"contents\":[\" <!--redis依赖配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-redis</artifactId> </dependency> <dependency> <groupId>org.springframework.session</groupId> <artifactId>spring-session-data-redis</artifactId> </dependency> \"]},{\"header\":\"2、配置\",\"slug\":\"_2、配置\",\"contents\":[\"server: port: 8081 spring: data: redis: port: 6379 host: 127.0.0.1 password: 123456 session: store-type: redis timeout: 3000 \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"重新启动系统，依次访问：\",\"http://localhost:8081/user/login?userName=javaxiaobear&password=123456\",\"http://localhost:8081/user/info\",\"http://localhost:8082/user/info\",\"发现访问8082服务器时，也访问成功\"]},{\"header\":\"3、JWT\",\"slug\":\"_3、jwt\",\"contents\":[\"Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。\",\"官网：https://github.com/auth0/java-jwt\"]},{\"header\":\"1、实战\",\"slug\":\"_1、实战\",\"contents\":[]},{\"header\":\"1、引入依赖\",\"slug\":\"_1、引入依赖-1\",\"contents\":[\"<dependency> <groupId>com.auth0</groupId> <artifactId>java-jwt</artifactId> <version>4.4.0</version> </dependency> \"]},{\"header\":\"2、编写controller类\",\"slug\":\"_2、编写controller类\",\"contents\":[\"private static final String JWT_KEY = \\\"javaxiaobear\\\"; @GetMapping(\\\"/loginWithJwt\\\") public String loginWithJwt(@RequestParam String userName, @RequestParam String password){ Algorithm algorithm = Algorithm.HMAC256(JWT_KEY); String token = JWT.create() .withClaim(\\\"user\\\", userName) .sign(algorithm); return token; } @GetMapping(\\\"/infoWithJwt\\\") public String infoWithJwt(@RequestHeader String token){ Algorithm algorithm = Algorithm.HMAC256(JWT_KEY); JWTVerifier verifier = JWT.require(algorithm) // reusable verifier instance .build(); DecodedJWT verify = verifier.verify(token); return verify.getClaim(\\\"user\\\").asString(); } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试-1\",\"contents\":[\"登录接口：http://localhost:8081/user/loginWithJwt?userName=javaxiaobear&password=123456\",\"获取登录用户信息：localhost:8081/user/infoWithJwt\",\"响应：javaxiaobear\",\"还有一种方法把获取到的token解析出来，https://jwt.io/专门解码的网站\"]},{\"header\":\"2、分析\",\"slug\":\"_2、分析\",\"contents\":[]},{\"header\":\"1、JWT的构成\",\"slug\":\"_1、jwt的构成\",\"contents\":[\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoiamF2YXhpYW9iZWFyIn0.TOlrBtRrnJl_FfJ8YUnWLLHOIV6HKYzD1d0MBRlu9nU \",\"token存在两个点，也就是分成了三段，第一部分我们称它为头部（header),第二部分我们称其为载荷（payload, 类似于飞机上承载的物品)，第三部分是签证（signature).\"]},{\"header\":\"header\",\"slug\":\"header\",\"contents\":[\"jwt的头部承载两部分信息：\",\"声明类型，这里是jwt\",\"声明加密的算法 通常直接使用 HMAC SHA256\",\"完整的头部就像下面这样的JSON：\",\"{ 'typ': 'JWT', 'alg': 'HS256' } \",\"然后将头部进行base64加密（该加密是可以对称解密的),构成了第一部分.\"]},{\"header\":\"payload\",\"slug\":\"payload\",\"contents\":[\"载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分\",\"标准中注册的声明\",\"公共的声明\",\"私有的声明\",\"标准中注册的声明 (建议但不强制使用) ：\",\"iss: jwt签发者\",\"sub: jwt所面向的用户\",\"aud: 接收jwt的一方\",\"exp: jwt的过期时间，这个过期时间必须要大于签发时间\",\"nbf: 定义在什么时间之前，该jwt都是不可用的.\",\"iat: jwt的签发时间\",\"jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。\",\"公共的声明 ：公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密.\",\"私有的声明 ：私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。\",\"定义一个payload:\",\"{ \\\"sub\\\": \\\"1234567890\\\", \\\"name\\\": \\\"John Doe\\\", \\\"admin\\\": true }\",\"然后将其进行base64加密，得到Jwt的第二部分。\"]},{\"header\":\"signature\",\"slug\":\"signature\",\"contents\":[\"jwt的第三部分是一个签证信息，这个签证信息由三部分组成：\",\"header (base64后的)\",\"payload (base64后的)\",\"secret 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。\",\"// javascript var encodedString = base64UrlEncode(header) + '.' + base64UrlEncode(payload); var signature = HMACSHA256(encodedString, 'secret'); \",\"将这三部分用.连接成一个完整的字符串,构成了最终的jwt 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。\",\"好了，本文就到这里了！如果觉得内容不错的话，希望大家可以帮忙点赞转发一波，这是对我最大的鼓励，感谢🙏🏻\",\"资料获取👇 最后面就是领取暗号，公众号回复即可！\"]}]},\"/architecture/distributed/task.html\":{\"title\":\"分布式任务调度\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[]},{\"header\":\"1、任务调度场景\",\"slug\":\"_1、任务调度场景\",\"contents\":[\"报表\",\"爬虫\",\"日/月结单\",\"数据归档\"]},{\"header\":\"2、调度框架\",\"slug\":\"_2、调度框架\",\"contents\":[\"非分布式：Spring Boot自带的@Scheduled\",\"分布式：QuartZ、Elasticjob、XXL-JOB、阿里云的SchedulerX、PowerJob\"]},{\"header\":\"3、对比\",\"slug\":\"_3、对比\",\"contents\":[\"QuartZ\",\"xxl-job\",\"SchedulerX 2.0\",\"PowerJob\",\"CRON\",\"CRON\",\"CRON、固定频率、固定延迟、OpenAPI\",\"CRON、固定频率、固定延迟、OpenAPI\",\"内置Java\",\"内置Java、GLUE Java、Shell、Python等脚本\",\"内置Java、外置Java（FatJar）、Shell、Python等脚本\",\"内置Java、外置Java（容器）、Shell、Python等脚本\",\"无\",\"静态分片\",\"MapReduce 动态分片\",\"MapReduce 动态分片\",\"不支持\",\"支持\",\"支持\",\"支持\",\"不支持\",\"支持\",\"不支持\",\"支持\",\"基于数据库锁，有性能瓶颈\",\"基于数据库锁，有性能瓶颈\",\"不详\",\"无锁化设计，性能强劲无上限\",\"无\",\"邮件\",\"短信\",\"邮件，提供接口允许开发者扩展\",\"关系型数据库（MySQL、Oracle...）\",\"MySQL\",\"人民币\",\"任意 Spring Data Jpa支持的关系型数据库（MySQL、Oracle...）\",\"不支持\",\"不支持\",\"支持\",\"支持\"]},{\"header\":\"4、如何选择\",\"slug\":\"_4、如何选择\",\"contents\":[\"如何选择哪一个分布式任务调度平台\",\"XXL-Job和Elastic-Job都具有广泛的用户基础和完善的技术文档，都可以满足定时任务的基本功能需求\",\"xxl-job侧重在业务实现简单和管理方便，容易学习，失败与路由策略丰富, 推荐使用在用户基数相对较少，服务器的数量在一定的范围内的场景下使用\",\"elastic-job关注的点在数据，添加了弹性扩容和数据分片的思路，更方便利用分布式服务器的资源, 但是学习难度较大，推荐在数据量庞大，服务器数量多的时候使用\"]},{\"header\":\"2、非分布式任务调度@Scheduled\",\"slug\":\"_2、非分布式任务调度-scheduled\",\"contents\":[\"@Scheduled注解是Spring Boot提供的用于定时任务控制的注解，主要用于控制任务在某个指定时间执行，或者每隔一段时间执行，默认是在单线程中执行的\"]},{\"header\":\"1、注解源码\",\"slug\":\"_1、注解源码\",\"contents\":[\"@Target({ElementType.METHOD, ElementType.ANNOTATION_TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Repeatable(Schedules.class) @Reflective public @interface Scheduled { String CRON_DISABLED = \\\"-\\\"; String cron() default \\\"\\\"; String zone() default \\\"\\\"; long fixedDelay() default -1L; String fixedDelayString() default \\\"\\\"; long fixedRate() default -1L; String fixedRateString() default \\\"\\\"; long initialDelay() default -1L; String initialDelayString() default \\\"\\\"; TimeUnit timeUnit() default TimeUnit.MILLISECONDS; } \"]},{\"header\":\"2、参数说明\",\"slug\":\"_2、参数说明\",\"contents\":[\"参数\",\"说明\",\"示例\",\"cron\",\"任务执行的cron表达式\",\"0/1 * * * * ?\",\"zone\",\"cron表达时解析使用的时区,默认为服务器的本地时区,使用java.util.TimeZone#getTimeZone(String)方法解析\",\"GMT-8:00\",\"fixedDelay\",\"上一次任务执行结束到下一次执行开始的间隔时间,单位为ms\",\"2000\",\"fixedDelayString\",\"上一次任务执行结束到下一次执行开始的间隔时间,使用java.time.Duration#parse解析\",\"PT15M\",\"fixedRate\",\"以固定间隔执行任务，即上一次任务执行开始到下一次执行开始的间隔时间,单位为ms,若在调度任务执行时,上一次任务还未执行完毕,会加入worker队列,等待上一次执行完成后立即执行下一次任务\",\"2000\",\"fixedRateString\",\"与fixedRate逻辑一致,只是使用java.time.Duration#parse解析\",\"PT15M\",\"initialDelay\",\"首次任务执行的延迟时间\",\"2000\",\"initialDelayString\",\"首次任务执行的延迟时间,使用java.time.Duration#parse解析\",\"PT15M\",\"timeUnit\"]},{\"header\":\"3、详解说明\",\"slug\":\"_3、详解说明\",\"contents\":[]},{\"header\":\"1、cron 参数\",\"slug\":\"_1、cron-参数\",\"contents\":[\"表达式格式：@Scheduled(cron = \\\"{秒数} {分钟} {小时} {日期} {月份} {星期}\\\")\",\"注意： cron表达式可分为6或7个占位符，但在spring自带的定时任务中，cron只支持6个参数，若使用7个参数就会报错\",\"Caused by: java.lang.IllegalStateException: Encountered invalid @Scheduled method 'test': Cron expression must consist of 6 fields (found 7 in \\\"*/5 * * * * * *\\\") \",\"代码示例：\",\"/** * cron 表达式 每隔5秒执行一次 */ @Scheduled(cron = \\\"*/5 * * * * *\\\") public void test(){ log.info(\\\"小熊学Java是最全Java学习网站！\\\"); } \",\"结果\",\"2023-06-28T15:20:40.009+08:00 INFO 10904 --- [ scheduling-1] com.javaxiaobear.job.MyScheduledJob : 小熊学Java是最全Java学习网站！ 2023-06-28T15:20:45.014+08:00 INFO 10904 --- [ scheduling-1] com.javaxiaobear.job.MyScheduledJob : 小熊学Java是最全Java学习网站！ 2023-06-28T15:20:50.004+08:00 INFO 10904 --- [ scheduling-1] com.javaxiaobear.job.MyScheduledJob : 小熊学Java是最全Java学习网站！ 2023-06-28T15:20:55.010+08:00 INFO 10904 --- [ scheduling-1] com.javaxiaobear.job.MyScheduledJob : 小熊学Java是最全Java学习网站！ 2023-06-28T15:21:00.002+08:00 INFO 10904 --- [ scheduling-1] com.javaxiaobear.job.MyScheduledJob : 小熊学Java是最全Java学习网站！ \",\"更多的cron表达式，自行测试哈，这里不做过多演示\",\"“30 * * * * ?” 每半分钟触发任务 “30 10 * * * ?” 每小时的10分30秒触发任务 “30 10 1 * * ?” 每天1点10分30秒触发任务 “30 10 1 20 * ?” 每月20号1点10分30秒触发任务 “30 10 1 20 10 ? *” 每年10月20号1点10分30秒触发任务 “30 10 1 20 10 ? 2011” 2011年10月20号1点10分30秒触发任务 “30 10 1 ? 10 * 2011” 2011年10月每天1点10分30秒触发任务 “30 10 1 ? 10 SUN 2011” 2011年10月每周日1点10分30秒触发任务 “15,30,45 * * * * ?” 每15秒，30秒，45秒时触发任务 “15-45 * * * * ?” 15到45秒内，每秒都触发任务 “15/5 * * * * ?” 每分钟的每15秒开始触发，每隔5秒触发一次 “15-30/5 * * * * ?” 每分钟的15秒到30秒之间开始触发，每隔5秒触发一次 “0 0/3 * * * ?” 每小时的第0分0秒开始，每三分钟触发一次 “0 15 10 ? * MON-FRI” 星期一到星期五的10点15分0秒触发任务 “0 15 10 L * ?” 每个月最后一天的10点15分0秒触发任务 “0 15 10 LW * ?” 每个月最后一个工作日的10点15分0秒触发任务 “0 15 10 ? * 5L” 每个月最后一个星期四的10点15分0秒触发任务 “0 15 10 ? * 5#3” 每个月第三周的星期四的10点15分0秒触发任务 \"]},{\"header\":\"2、fixedDelay 参数\",\"slug\":\"_2、fixeddelay-参数\",\"contents\":[\"fixedDelay 上一次任务执行结束到下一次执行开始的间隔时间,单位为ms\",\"代码示例：\",\"/** * fixedDelay 上一次任务执行结束到下一次执行开始的间隔时间,单位为ms * 每隔2秒执行一次 */ @Scheduled(fixedDelay = 2000) public void testFixedDelay (){ log.info(\\\"小熊学Java是最全Java学习网站！\\\"); } \",\"结果展示：\"]},{\"header\":\"3、fixedRate参数\",\"slug\":\"_3、fixedrate参数\",\"contents\":[\"以固定间隔执行任务，即上一次任务执行开始到下一次执行开始的间隔时间,单位为ms,若在调度任务执行时,上一次任务还未执行完毕,会加入worker队列,等待上一次执行完成后立即执行下一次任务\",\"/** * 以固定间隔执行任务，即上一次任务执行开始到下一次执行开始的间隔时间,单位为ms,若 * 在调度任务执行时,上一次任务还未执行完毕,会加入worker队列,等待上一次执行完成后立即执行下一次任务 * 每隔2秒执行一次 */ @Scheduled(fixedRate = 2000) public void testFixedRate () throws InterruptedException { Thread.sleep(3000); log.info(\\\"小熊学Java是最全Java学习网站！\\\"); } \",\"日志每3秒执行一次，这也是因为@Scheduled是在单线程中执行的\"]},{\"header\":\"4、initialDelay参数\",\"slug\":\"_4、initialdelay参数\",\"contents\":[\"首次任务执行的延迟时间\",\"/** * initialDelay 首次任务执行的延迟时间 * 每隔2秒执行一次 */ @Scheduled(fixedRate = 2000, initialDelay = 3000) public void testInitialDelay (){ log.info(\\\"小熊学Java是最全Java学习网站！\\\"); } \",\"首次延迟的时间是3秒，之后每2秒执行一次\"]},{\"header\":\"4、@Scheduled多线程\",\"slug\":\"_4、-scheduled多线程\",\"contents\":[]},{\"header\":\"1、场景演示\",\"slug\":\"_1、场景演示\",\"contents\":[\"执行以下两个方法\",\"@Scheduled(fixedRate = 2000) public void test1 () throws InterruptedException { Thread.sleep(3000); log.info(\\\"小熊学Java 是最全Java学习网站！---test1\\\"); } @Scheduled(fixedRate = 2000) public void test2 () throws InterruptedException { Thread.sleep(3000); log.info(\\\"小熊学Java是最全Java学习网站！---test2\\\"); } \",\"从执行结果中可以看出，test1方法和test2方法交替输出日志，并没有同时执行\",\"org.springframework.scheduling.config.ScheduledTaskRegistrar源码发现\",\"protected void scheduleTasks() { //如果为空，则以单线程执行 if (this.taskScheduler == null) { this.localExecutor = Executors.newSingleThreadScheduledExecutor(); this.taskScheduler = new ConcurrentTaskScheduler(this.localExecutor); } Iterator var1; if (this.triggerTasks != null) { var1 = this.triggerTasks.iterator(); while(var1.hasNext()) { TriggerTask task = (TriggerTask)var1.next(); this.addScheduledTask(this.scheduleTriggerTask(task)); } } if (this.cronTasks != null) { var1 = this.cronTasks.iterator(); while(var1.hasNext()) { CronTask task = (CronTask)var1.next(); this.addScheduledTask(this.scheduleCronTask(task)); } } IntervalTask task; if (this.fixedRateTasks != null) { var1 = this.fixedRateTasks.iterator(); while(var1.hasNext()) { task = (IntervalTask)var1.next(); if (task instanceof FixedRateTask) { FixedRateTask fixedRateTask = (FixedRateTask)task; this.addScheduledTask(this.scheduleFixedRateTask(fixedRateTask)); } else { this.addScheduledTask(this.scheduleFixedRateTask(new FixedRateTask(task))); } } } if (this.fixedDelayTasks != null) { var1 = this.fixedDelayTasks.iterator(); while(var1.hasNext()) { task = (IntervalTask)var1.next(); if (task instanceof FixedDelayTask) { FixedDelayTask fixedDelayTask = (FixedDelayTask)task; this.addScheduledTask(this.scheduleFixedDelayTask(fixedDelayTask)); } else { this.addScheduledTask(this.scheduleFixedDelayTask(new FixedDelayTask(task))); } } } } \",\"当未手动指定taskScheduler时，会通过Executors.newSingleThreadScheduledExecutor()创建默认的单线程线程池，且该线程池的拒绝策略为AbortPolicy，这种策略在线程池无可用线程时丢弃任务，并抛出异常RejectedExecutionException。\"]},{\"header\":\"2、多线程配置\",\"slug\":\"_2、多线程配置\",\"contents\":[]},{\"header\":\"1、配置bean\",\"slug\":\"_1、配置bean\",\"contents\":[\"在启动类中，配置bean，代码如下：\",\"/** * 配置线程池 * @return */ @Bean public TaskScheduler config(){ ThreadPoolTaskScheduler taskScheduler = new ThreadPoolTaskScheduler(); //线程池大小为10 taskScheduler.setPoolSize(10); return taskScheduler; } \"]},{\"header\":\"2、配置类\",\"slug\":\"_2、配置类\",\"contents\":[\"@Configuration public class SchedulerConfig implements SchedulingConfigurer { @Override public void configureTasks(ScheduledTaskRegistrar taskRegistrar) { //Scheduler指定线程池 taskRegistrar.setScheduler(Executors.newScheduledThreadPool(10)); } } \",\"运行结果跟上面一样，每3秒同时执行\"]},{\"header\":\"5、Async异步执行\",\"slug\":\"_5、async异步执行\",\"contents\":[\"异步调用指程序在顺序执行时，不等待异步调用的语句返回结果就执行后面的程序。\"]},{\"header\":\"1、简单使用\",\"slug\":\"_1、简单使用\",\"contents\":[\"Spring Boot的异步任务，只需在方法上添加异步注解，同时开启异步任务\",\"@Component @Slf4j public class AsyncTask { @Async public void test(){ log.info(\\\"小熊学Java 是最棒的！！！\\\"); } } \",\"方法调用\",\"@Resource private AsyncTask asyncTask; @Scheduled(fixedRate = 2000) public void test1 () throws InterruptedException { asyncTask.test(); Thread.sleep(3000); log.info(\\\"小熊学Java 是最全Java学习网站！---test1\\\"); } \",\"结果输出\"]},{\"header\":\"2、异步失效\",\"slug\":\"_2、异步失效\",\"contents\":[]},{\"header\":\"1、异步方法和调用异步方法在同一个类中\",\"slug\":\"_1、异步方法和调用异步方法在同一个类中\",\"contents\":[\"有时候，经常看到编写异步任务，都是这样写的\",\"@Scheduled(fixedRate = 2000) public void test1 () throws InterruptedException { asyncTask.test(); Thread.sleep(3000); log.info(\\\"小熊学Java 是最全Java学习网站！---test1\\\"); } @Async public void test(){ log.info(\\\"小熊学Java 是最棒的！！！\\\"); } \",\"这样写是不会生效的，由于@Async的AdviceMode默认为PROXY，所以当调用方和被调用方是在同一个类中，无法产生切面，@Async没有被Spring容器管理，可以查看源码，具体详情可参考这篇：https://juejin.cn/post/6976893903223914527#heading-5\"]},{\"header\":\"3、分布式任务调度QuartZ\",\"slug\":\"_3、分布式任务调度quartz\",\"contents\":[]},{\"header\":\"1、QuartZ是什么\",\"slug\":\"_1、quartz是什么\",\"contents\":[\"Quartz是一个功能强大的开源任务调度库，几乎可以集成到任何Java应用程序中，无论是超小型的独立应用还是超大型电子商务系统。\",\"它常用于企业级应用中：\",\"Driving Process Workflow：当新订单下达，可以安排一个30分钟内触发的任务，检查订单状态。 System Maintenance：安排每个工作日晚上11点将数据库内容转储到文件的任务。 Providing reminder services：提供提醒服务。\"]},{\"header\":\"2、Quartz入门\",\"slug\":\"_2、quartz入门\",\"contents\":[]},{\"header\":\"1、引入依赖\",\"slug\":\"_1、引入依赖\",\"contents\":[\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-quartz</artifactId> </dependency> \"]},{\"header\":\"2、编写任务\",\"slug\":\"_2、编写任务\",\"contents\":[\"@Slf4j public class MyQuartzJob extends QuartzJobBean { @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException { log.info(\\\"quartz 初体验！\\\"); } } \"]},{\"header\":\"3、quartz配置\",\"slug\":\"_3、quartz配置\",\"contents\":[\"@Configuration public class QuartzConfig { @Bean public JobDetail jobDetail(){ return JobBuilder.newJob(MyQuartzJob.class) .withIdentity(\\\"job1\\\", \\\"group1\\\") .storeDurably() .build(); } @Bean public Trigger trigger(){ return TriggerBuilder.newTrigger() .forJob(jobDetail()) .withIdentity(\\\"trigger1\\\", \\\"group1\\\") .startNow() .build(); } } \"]},{\"header\":\"4、运行\",\"slug\":\"_4、运行\",\"contents\":[\"启动Spring Boot程序\",\"但是发现任务只运行了一次，如果想跟之前一样运行,则需要修改配置\",\"@Bean public Trigger trigger(){ return TriggerBuilder.newTrigger() .forJob(jobDetail()) .withIdentity(\\\"trigger1\\\", \\\"group1\\\") .startNow() //配置cron表达式 .withSchedule(CronScheduleBuilder.cronSchedule(\\\"*/5 * * ? * * *\\\")) .build(); } \",\"注意，编写cron表达式的时候，天（月的多少号）和天（周几）不能同时都为*，当有一个为*时，另一个要是？，不然就会报以下错误\",\"Support for specifying both a day-of-week AND a day-of-month parameter is not implemented \"]},{\"header\":\"4、分布式任务调度XXL-JOB\",\"slug\":\"_4、分布式任务调度xxl-job\",\"contents\":[]},{\"header\":\"1、概述\",\"slug\":\"_1、概述-1\",\"contents\":[\"XXL-JOB是一个分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现已开放源代码并接入多家公司线上产品线，开箱即用。\",\"官方文档：https://www.xuxueli.com/xxl-job/#二、快速入门\",\"官网地址：https://www.xuxueli.com/xxl-job/\",\"GitHub地址：https://github.com/xuxueli/xxl-job/\",\"xxl-job的设计思想\",\"将调度行为抽象形成“调度中心”公共平台，而平台自身并不承担业务逻辑，“调度中心”负责发起调度请求。\",\"将任务抽象成分散的JobHandler，交由“执行器”统一管理\",\"“执行器”负责接收调度请求并执行对应的JobHandler中业务逻辑。\",\"因此，“调度”和“任务”两部分可以相互解耦，提高系统整体稳定性和扩展性\",\"架构图(图片来源是xxl-job官网)\",\"调度中心 \",\"负责管理调度的信息，按照调度的配置来发出调度请求\",\"支持可视化、简单的动态管理调度信息，包括新建、删除、更新等，这些操作都会实时生效，同时也支持监控调度结果以及执行日志。\",\"执行器 \",\"负责接收请求并且执行任务的逻辑。任务模块专注于任务的执行操作等等，使得开发和维护更加的简单与高效\",\"XXL-Job具有哪些特性 \",\"调度中心HA（中心式）：调度采用了中心式进行设计，“调度中心”支持集群部署，可保证调度中心HA\",\"执行器HA（分布式）：任务分布式的执行，任务执行器支持集群部署，可保证任务执行HA\",\"触发策略：有Cron触发、固定间隔触发、固定延时触发、API事件触发、人工触发、父子任务触发\",\"路由策略：执行器在集群部署的时候提供了丰富的路由策略，如：第一个、最后一个、轮询、随机、一致性HASH、最不经常使用LFU、最久未使用LRU、故障转移等等\",\"故障转移：如果执行器集群的一台机器发生故障，会自动切换到一台正常的执行器发送任务调度\",\"Rolling实时日志的监控：支持rolling方式查看输入的完整执行日志\",\"脚本任务：支持GLUE模式开发和运行脚本任务，包括Shell、python、node.js、php等等类型脚本\"]},{\"header\":\"2、搭建调度中心\",\"slug\":\"_2、搭建调度中心\",\"contents\":[]},{\"header\":\"1、下载源码\",\"slug\":\"_1、下载源码\",\"contents\":[\"下载源码导入idea，源码地址：https://gitee.com/xuxueli0323/xxl-job.git\",\"doc：xxl-job的文档资料，包括了数据库的脚本（后面要用到）\",\"xxl-job-core：公共jar包依赖\",\"xxl-job-admin：调度中心，项目源码，是Springboot项目，可以直接启动\",\"xxl-job-executor-samples：执行器，是Sample实例项目，里面的Springboot工程可以直接启动，也可以在该项目的基础上进行开发，也可以将现有的项目改造成为执行器项目\"]},{\"header\":\"2、数据库\",\"slug\":\"_2、数据库\",\"contents\":[\"数据库文件在源码doc/db目录下\",\"xxl_job的数据库里有如下几个表 \",\"xxl_job_group：执行器信息表，用于维护任务执行器的信息\",\"xxl_job_info：调度扩展信息表，主要是用于保存xxl-job的调度任务的扩展信息，比如说像任务分组、任务名、机器的地址等等\",\"xxl_job_lock：任务调度锁表\",\"xxl_job_log：日志表，主要是用在保存xxl-job任务调度历史信息，像调度结果、执行结果、调度入参等等\",\"xxl_job_log_report：日志报表，会存储xxl-job任务调度的日志报表，会在调度中心里的报表功能里使用到\",\"xxl_job_logglue：任务的GLUE日志，用于保存GLUE日志的更新历史变化，支持GLUE版本的回溯功能\",\"xxl_job_registry：执行器的注册表，用在维护在线的执行器与调度中心的地址信息\",\"xxl_job_user：系统的用户表\"]},{\"header\":\"3、调度中心配置：\",\"slug\":\"_3、调度中心配置\",\"contents\":[\"调度中心配置文件地址：\",\"/xxl-job/xxl-job-admin/src/main/resources/application.properties \",\"配置数据库连接\",\"### xxl-job, datasource spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai spring.datasource.username=root spring.datasource.password=xdclass.net spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver \",\"配置 xxl.job.accessToken（后续要配置客户端接入配置token）\",\"xxl.job.accessToken=javaxiaobear.cn \"]},{\"header\":\"4、启动项目\",\"slug\":\"_4、启动项目\",\"contents\":[\"调度中心访问地址：http://localhost:8080/xxl-job-admin (该地址执行器将会使用到，作为回调地址)\",\"默认登录账号 “admin/123456”, 登录后运行界面如下图所示。\"]},{\"header\":\"5、UI界面介绍\",\"slug\":\"_5、ui界面介绍\",\"contents\":[]},{\"header\":\"1、运行报表：以图形化来展示了整体的任务执行情况\",\"slug\":\"_1、运行报表-以图形化来展示了整体的任务执行情况\",\"contents\":[\"任务数量：能够看到调度中心运行的任务数量\",\"调度次数：调度中心所触发的调度次数\",\"执行器数量：在整个调度中心中，在线的执行器数量有多少\"]},{\"header\":\"2、任务管理（配置执行任务）\",\"slug\":\"_2、任务管理-配置执行任务\",\"contents\":[\"示例执行器：所用到的执行器\",\"任务描述：概述该任务是做什么的\",\"路由策略： \",\"第一个：选择第一个机器\",\"最后一个：选择最后一个机器\",\"轮询：依次选择执行\",\"随机：随机选择在线的机器\",\"一致性HASH：每个任务按照Hash算法固定选择某一台机器，并且所有的任务均匀散列在不同的机器上\",\"最不经常使用：使用频率最低的机器优先被使用\",\"最近最久未使用：最久未使用的机器优先被选举\",\"故障转移：按照顺序依次进行心跳检测，第一个心跳检测成功的机器选定为目标的执行器并且会发起任务调度\",\"忙碌转移：按照顺序来依次进行空闲检测，第一个空闲检测成功的机器会被选定为目标群机器，并且会发起任务调度\",\"分片广播：广播触发对于集群中的所有机器执行任务，同时会系统会自动传递分片的参数\",\"Cron：执行规则\",\"调度过期策略：调度中心错过调度时间的补偿处理策略，包括：忽略、立即补偿触发一次等\",\"JobHandler：定义执行器的名字\",\"阻塞处理策略： \",\"单机串行：新的调度任务在进入到执行器之后，该调度任务进入FIFO队列，并以串行的方式去进行\",\"丢弃后续调度：新的调度任务在进入到执行器之后，如果存在相同的且正在运行的调度任务，本次的调度任务请求就会被丢弃掉，并且标记为失败\",\"覆盖之前的调度：新的调度任务在进入到执行器之后，如果存在相同的且正在运行的调度任务，就会终止掉当前正在运行的调度任务，并且清空队列，运行新的调度任务。\",\"子任务ID：输入子任务的任务id，可填写多个\",\"任务超时时间：添加任务超时的时候，单位s，设置时间大于0的时候就会生效\",\"失败重试次数：设置失败重试的次数，设置时间大于0的时候就会生效\",\"负责人：填写该任务调度的负责人\",\"报警邮件：出现报警，则发送邮件\"]},{\"header\":\"3、调度日志\",\"slug\":\"_3、调度日志\",\"contents\":[\"这里是查看调度的日志，根据日志来查看任务具体的执行情况是怎样的\"]},{\"header\":\"4、执行器管理\",\"slug\":\"_4、执行器管理\",\"contents\":[\"这里是配置执行器，等待执行器启动的时候都会被调度中心监听加入到地址列表\"]},{\"header\":\"5、用户管理\",\"slug\":\"_5、用户管理\",\"contents\":[\"可以对用户的一些操作\"]},{\"header\":\"3、整合xxl_job\",\"slug\":\"_3、整合xxl-job\",\"contents\":[]},{\"header\":\"1、项目搭建\",\"slug\":\"_1、项目搭建\",\"contents\":[]},{\"header\":\"1、引入xxl_job依赖\",\"slug\":\"_1、引入xxl-job依赖\",\"contents\":[\"<!-- http://repo1.maven.org/maven2/com/xuxueli/xxl-job-core/ --> <dependency> <groupId>com.xuxueli</groupId> <artifactId>xxl-job-core</artifactId> <version>2.3.1</version> </dependency> \"]},{\"header\":\"2、配置yaml\",\"slug\":\"_2、配置yaml\",\"contents\":[\"xxl: job: admin: addresses: http://127.0.0.1:8080/xxl-job-admin # 执行器的名字 executor: appname: javaxiaobear-xxl-job-test accessToken: default_token server: port: 8081 \"]},{\"header\":\"3、编写配置类\",\"slug\":\"_3、编写配置类\",\"contents\":[\"@Configuration @Slf4j public class XxlJobConfig { @Value(\\\"${xxl.job.admin.addresses}\\\") private String adminAddresses; @Value(\\\"${xxl.job.executor.appname}\\\") private String appName; @Value(\\\"${xxl.job.accessToken}\\\") private String accessToken; //旧版的有bug //@Bean(initMethod = \\\"start\\\", destroyMethod = \\\"destroy\\\") @Bean public XxlJobSpringExecutor xxlJobExecutor() { log.info(\\\">>>>>>>>>>> xxl-job config init.\\\"); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppname(appName); // xxlJobSpringExecutor.setIp(ip); // xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); // xxlJobSpringExecutor.setLogPath(logPath); // xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } } \"]},{\"header\":\"2、第一个XXL-Job分布式调度任务\",\"slug\":\"_2、第一个xxl-job分布式调度任务\",\"contents\":[]},{\"header\":\"1、界面新增一个任务\",\"slug\":\"_1、界面新增一个任务\",\"contents\":[]},{\"header\":\"2、新增一个执行器\",\"slug\":\"_2、新增一个执行器\",\"contents\":[\"这里是因为在配置文件里面用的是自定义的执行器，所以我们需要新增，当然，你可以用默认的执行器\"]},{\"header\":\"3、代码配置handler\",\"slug\":\"_3、代码配置handler\",\"contents\":[\"@Slf4j @Component public class MyXxlJobHandler { @XxlJob(\\\"myXxlJobHandler\\\") public ReturnT<String> execute(String param){ log.info(\\\"小熊学Java 任务方法触发成功\\\"); return ReturnT.SUCCESS; } } \"]},{\"header\":\"4、重新启动项目\",\"slug\":\"_4、重新启动项目\",\"contents\":[]},{\"header\":\"4、路由策略\",\"slug\":\"_4、路由策略\",\"contents\":[\"第一个：选择第一个机器\",\"最后一个：选择最后一个机器\",\"轮询：依次选择执行，流量均摊（推荐）\",\"随机：随机选择在线的机器\",\"一致性HASH：每个任务按照Hash算法固定选择某一台机器，并且所有的任务均匀散列在不同的机器上\",\"最不经常使用：使用频率最低的机器优先被使用\",\"最近最久未使用：最久未使用的机器优先被选举\",\"故障转移：按照顺序依次进行心跳检测，第一个心跳检测成功的机器选定为目标的执行器并且会发起任务调度\",\"忙碌转移：按照顺序来依次进行空闲检测，第一个空闲检测成功的机器会被选定为目标群机器，并且会发起任务调度\",\"分片广播：广播触发对于集群中的所有机器执行任务，同时会系统会自动传递分片的参数\"]},{\"header\":\"5、分片广播\",\"slug\":\"_5、分片广播\",\"contents\":[]},{\"header\":\"1、场景\",\"slug\":\"_1、场景\",\"contents\":[\"需求\",\"有一个任务需要处理100W条数据，每条数据的业务逻辑处理要0.1s\",\"对于普通任务来说，只有一个线程来处理 可能需要10万秒才能处理完，业务则严重受影响\",\"案例：双十一大促，给1000万用户发营销短信\",\"什么是分片任务\",\"执行器集群部署，如果任务的路由策略选择【分片广播】，一次任务调度将会【广播触发】对应集群中所有执行器执行一次任务，同时系统自动传递分片参数，执行器可根据分片参数开发分片任务\",\"需要处理的海量数据，以执行器为划分，每个执行器分配一定的任务数，并行执行\",\"XXL-Job支持动态扩容执行器集群，从而动态增加分片数量，到达更快处理任务\",\"分片的值是调度中心分配的\",\"// 当前分片数，从0开始，即执行器的序号 int shardIndex = XxlJobHelper.getShardIndex(); //总分片数，执行器集群总机器数量 int shardTotal = XxlJobHelper.getShardTotal(); \",\"解决思路\",\"分片广播\",\"也可以启动多个job，使用同个jobHandler,通过命令行参数控制\",\"如果将100W数据均匀分给集群里的10台机器同时处理，\",\"每台机器耗时，1万秒即可，耗时会大大缩短，也能充分利用集群资源\",\"在xxl-job里，可以配置执行器集群有10个机器，那么分片总数是10，分片序号0~9 分别对应那10台机器。\",\"分片方式\",\"id % 分片总数 余数是0 的，在第1个执行器上执行\",\"id % 分片总数 余数是1 的，在第2个执行器上执行\",\"id % 分片总数 余数是2 的，在第3个执行器上执行\",\"...\",\"id % 分片总数 余数是9 的，在第10个执行器上执行\"]},{\"header\":\"2、代码编写\",\"slug\":\"_2、代码编写\",\"contents\":[\"/** * 100个用户，分片处理 */ @XxlJob(\\\"myShardingJobHandler\\\") public void shardingJobHandler(){ // 当前分片数，从0开始，即执行器的序号 int shardIndex = XxlJobHelper.getShardIndex(); //总分片数，执行器集群总机器数量 int shardTotal = XxlJobHelper.getShardTotal(); XxlJobHelper.log(\\\"分片参数：当前分片序号 = {}, 总分片数 = {}\\\", shardIndex, shardTotal); List<Integer> allUserIds = getAllUserIds(); allUserIds.forEach(obj -> { if (obj % shardTotal == shardIndex) { log.info(\\\"第 {} 片, 命中分片开始处理用户id={}\\\",shardIndex,obj); } }); } private List<Integer> getAllUserIds() { List<Integer> ids = new ArrayList<>(); for (int i = 0; i < 100; i++) { ids.add(i); } return ids; } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"新建一个分片任务\",\"新建一个服务器实例，直接copy一份\",\"启动系统，点击运行一次\"]},{\"header\":\"6、阻塞策略\",\"slug\":\"_6、阻塞策略\",\"contents\":[\"单机串行：新的调度任务在进入到执行器之后，该调度任务进入FIFO队列，并以串行的方式去进行\",\"丢弃后续调度：新的调度任务在进入到执行器之后，如果存在相同的且正在运行的调度任务，本次的调度任务请求就会被丢弃掉，并且标记为失败\",\"覆盖之前的调度：新的调度任务在进入到执行器之后，如果存在相同的且正在运行的调度任务，就会终止掉当前正在运行的调度任务，并且清空队列，运行新的调度任务。\"]}]},\"/architecture/microservices/design_experience.html\":{\"title\":\"5条设计经验\",\"contents\":[{\"header\":\"1、微服务架构的新挑战\",\"slug\":\"_1、微服务架构的新挑战\",\"contents\":[\"在著名软件著作《人月神话》中提到，软件世界没有“银弹”，这句话当然适用于架构领域，随着从单体架构过渡到微服务架构，因为将原有系统打散，给系统增加了许多不稳定因素。\",\"单体架构向微服务架构转变\",\"下面我从网络、性能、运维成本、组织架构与集成测试五个方面分别进行阐述。\",\"第一点，跨进程通信带来的新问题。 以往单体应用是在单机中进行进程内通信，通信稳定性相当好。但是打散为分布式系统后，变为进程间通信，往往这个过程还伴随着跨设备的网络访问，架构师在设计时必须考虑上下游系统因为网络因素无法通信的情况，要假设网络是不可靠的，并设计微服务在网络异常时也能进行符合预期的异常处理。以支付模块为例，用户支付成功后系统自动调用短信服务向用户手机发送“订单支付成功”的消息，此时架构师就必须假设短信服务在服务或者网络不可用时不会影响到订单业务的正常执行。\",\"微服务间跨进程RESTful调用\",\"第二点，较高的响应延迟。 相比传统单体架构进程内通信，跨进程、跨网络的微服务通信在网络传输与消息序列化带来的延迟是不可被忽略的，尤其是在五个以上微服务间消息调用时，网络延迟对于实时系统的影响是很大的。早些年我和军事院校合作了一个雷达仿真训练的系统，因为要模拟“导弹打飞机”的场景，在计算飞行轨道时1毫秒的响应增加都可以会影响到最终的结果，显然这类系统采用分布式设计就不再合适。\",\"雷达指挥训练系统流程\",\"第三点，运维成本会直线上升。 早期单体应用因为结构简单，规模也较小，发版时通常面对几台服务器部署几个Jar/War文件就可以了。同时，应用的交付周期也是以周甚至月为单位，此时硬件设备成本与运维人员技术要求都比较低，采用手动部署即可满足要求。而对于微服务架构而言，每一个服务都是可独立运行、独立部署、独立维护的业务单元，再加上互联网时代用户需求的不断变化以及市场的不稳定因素，运维人员每天面对成百上千台服务器发布几十次已是家常便饭，传统手动部署显然已经无法满足互联网的快速变化。\",\"京东 JDOS 自动化运维架构图\",\"第四点，组织架构层面的调整。 微服务不但是一种架构风格，同样也是一种软件组织模型，以往软件公司会以职能划分研发、测试、运维部门进行独立管理考核，而在微服务的实施过程中，是以业务模块进行团队划分，每一个团队是内聚的，要求可以独立完成从调研到发版的全流程，尽量减少对外界的依赖。如何将传统的职能团队调整为按业务划分的研发团队，同样是对管理者的巨大挑战，要知道人的思想比架构更难改变。\",\"独立的全生命周期研发团队\",\"第五点，服务间的集成测试变得举步维艰。 传统单体架构集成测试是将不同的模块按业务流程进行组合，在进程内验证每一种可能性下其模块间协作是否符合预期即可。但对于微服务而言，系统被拆解为很多独立运行的单元，服务间采用接口进行网络通信。要获取准确的测试结果，必须搭建完整的微服务环境，光这一项就需要花费大量的人力物力。同时，因为微服务是跨网络通信，网络延迟、超时、带宽、数据量等因素都将影响最终结果，测试结果易产生偏差。\"]},{\"header\":\"2、微服务最佳实践\",\"slug\":\"_2、微服务最佳实践\",\"contents\":[\"刚刚我们总结了引入微服务架构的一些新挑战，下面我将结合自己多年的微服务落地经验，总结出五点微服务架构最佳实践，希望能对你日后的工作提供帮助。\"]},{\"header\":\"第一点，微服务的划分原则\",\"slug\":\"第一点-微服务的划分原则\",\"contents\":[\"将已有系统拆分为多个微服务，本就没有统一的标准。举个例子，一个初创电商公司，要开发一套电商系统，将“促销活动”单独剥离出来作为“促销服务”是没有问题的。但是如果在“淘宝”“京东”这种体量的电商平台，“促销服务”就显得粒度太粗了。可以继续拆解为“价格服务”“优惠券服务”“京豆服务”等更细粒度的小服务，每个服务有专门团队负责维护。\",\"京东商城的微服务业务划分\",\"因此，在微服务拆分过程中，我们通常会从业务场景、团队能力、组织架构等多种因素综合考虑，这特别考验架构师的业务能力。一般来说，我们总结出几点通用原则：\",\"单一职责原则。 每一个微服务只做好一件事，体现出“高内聚、低耦合”，尽量减少对外界环境的依赖。比如，在公司创业之初，完全可将订单与仓储服务进行合并。因为订单与仓储在业务与数据上紧密相关，如果强行拆分会导致出现跨进程通信带来的数据一致性难题。随着业务的发展，仓储的业务职责扩展，派生出许多与订单无紧密联系的功能，到时再将其剥离形成独立的“仓储服务”。\",\"服务依赖原则。 避免服务间的循环引用，在设计时就要对服务进行分级，区分核心服务与非核心服务。比如订单服务与短信服务，显然短信服务是非核心服务，服务间调用要遵循“核心服务”到“非核心服务”的方向，不允许出现反向调用。同时，对于核心服务要做好保护，避免非核心服务出现问题影响核心服务的正常运行。\",\"Two Pizza Team原则。 就是说让团队保持在两个比萨能让队员吃饱的小规模的概念。团队要小到让每个成员都能做出显著的贡献，并且相互依赖，有共同目标，以及统一的成功标准。一个微服务团队应涵盖从设计到发布运维的完整生命周期，使团队内部便可以解决大部分任务，从人数上4~6人是比较理想的规模。\"]},{\"header\":\"第二点，为每一个微服务模块明确使命。\",\"slug\":\"第二点-为每一个微服务模块明确使命。\",\"contents\":[\"这里推荐一套标准的微服务叙述模板，集中体现“只做好一件事”的原则。\",\"模板 XX 微服务用来 在出现痛点场景的情况下 解决现有的 XX 问题 从而达到了 XXX 的效果 体现了微服务的价值示例 商品检索微服务用来 在商品数据全量多维度组合查询的情况下 解决了 MySQL 数据库全表扫描查询慢的问题 从而让查询响应降低到 50ms 以下 有效提升了用户体验\",\"通过这种描述，服务的职责与边界就十分明确，团队便以此为目标确认职责。\",\"在实施过程中因为我们是以解决问题为目标，切分时可能会比较细碎。经过漫长时间沉淀，系统中出现了类似于“商品检索服务”“订单检索服务”“商铺检索服务”等多个小服务，这时可以对这些服务形成聚合生成新的“通用检索服务”，以此来控制微服务的整体规模。反之，对于庞大的服务，可以考虑拆分为多个小服务进行细粒度的管理。总之，拆与合是伴随着公司业务的演进而变化的，一切以解决问题为准。\"]},{\"header\":\"第三点，微服务确保独立的数据存储。\",\"slug\":\"第三点-微服务确保独立的数据存储。\",\"contents\":[\"数据是任何系统最重要的资产。以往单体应用通常会选择 MySQL 这种关系型数据库作为数据的唯一存储，这样做的好处是涉及多表操作时，利用数据库自带的事务机制便可最大程度保证数据完整性。但这样做却存在诸多问题，以下图为例，不同的微服务对数据存储的需求也是不同的，订单服务需要 MySQL 数据保存订单与订单明细；新闻服务需要Elasticsearch提供全文检索支持；朋友圈需要图数据库表达现实世界人际关系；文件存储服务则需要分布式文件系统。如果将所有数据都揉在 MySQL 中使用会变得十分蹩脚，好的做法是为每一个微服务提供符合自身业务特性的数据库。\",\"独立的数据存储\",\"但理想很丰满现实很骨感，在分库后涉及跨库操作会变的难以处理。比如，订单依赖会员数据，原本单库处理时一条 SQL 语句便可实现。\",\"SELECT order.* , member.* FROM order,member WHERE order.member_id = member.member_id \",\"但在微服务架构下，因为数据库绝不允许其他团队访问，关联查询只能变为 API 调用形式，程序实现层面比单库复杂不少。\",\"通过 RESTful 通信实现数据关联\",\"与之类似，如果涉及多表写入时一致性问题更复杂。\",\"BEGIN ; 写入表A; 写入表B; COMMIT; \",\"在拆分为服务后，数据被分散到多库，为保证异构多库的数据一致性是所有分布式应用的巨大挑战，至今没有完美的解决方案，这块内容我在后面课程单独做一个专题进行讲解。\"]},{\"header\":\"第四点，服务间通信优先采用聚合器模式。\",\"slug\":\"第四点-服务间通信优先采用聚合器模式。\",\"contents\":[\"在微服务间通信时存在两种消息传递模式：链式模式与聚合器模式。下图所展示的是链式模式，请求按业务流程在各个服务间流转，最终处理完成返回客户端。\",\"链式模式\",\"因为请求是按业务流程传递，很容易能被开发人员理解，链式模式成为最常用的服务间通信模式。但链式模式采用串联模式，调用的整体成功率等于单个服务成功率的乘积，假设每个服务可靠性为 90%，一个业务在 4 个服务执行后的最终成功率只有 90%*90%*90%*90%≈66%，有将近一半的请求会处理失败，这是无法接受的。此外，链式模式因默认采用同步方式传输，在服务处理完成前应用会一直处于阻塞状态，当调用链较长时，系统整体性能会严重下滑。\",\"聚合器模式则是通过服务作为入口，组装其他服务的调用。以下图为例，因为“订单流程服务”是将其他服务进行聚合操作，所以称其为聚合器模式。以“订单流程服务”为例，将“订单”“支付”“库存”服务进行聚合，一个服务实现了下单、支付、减库存的完整流程。\",\"聚合器模式\",\"采用聚合器模式后，业务流程与编排集中在“订单流程服务”中，可对整体业务进行有效编排，支付与扣库存可以并行调用，可以有效提高系统的性能。\"]},{\"header\":\"第五点，不要强行“微服务”化。\",\"slug\":\"第五点-不要强行-微服务-化。\",\"contents\":[\"在以前公司任职时因为业务需要，要开发一个工单系统让售后人员在线为客户提供售后服务，但当时因为公司产品已经非常成熟，每天产生的工单只有几十笔，如果做成单体应用配合 Nginx 反向代理，从存储到应用便能满足需求。此任务分配给一位“年轻”的架构师，可能是为了证明实力，他采用了全套微服务技术来“炫技”，并在会议上侃侃而谈。结果老板反问他“你这么设计有必要么，为一个工单系统投入超过10台服务器成本你考虑了吗?” ，当时那场景别提多尴尬了。\",\"其实在我看来，微服务也不过是一种方案，没必要盲从。它也没有违背架构的基本规律：架构是解决当前需求和痛点而演进的。在满足需要的前提下，选择合适的而不是选择最好的，合理降低成本才是好架构师该考虑的事情。以上微服务的经验都是我在实际工作中总结归纳出来的，如有不足的地方欢迎同学们在评论中给予补充。\"]},{\"header\":\"3、微服务架构的适用场景\",\"slug\":\"_3、微服务架构的适用场景\",\"contents\":[\"首先咱们梳理下适合做微服务的场景：\",\"新规划的大型业务系统， 这肯定是最适合引入微服务架构的情况了， 微服务强调“高内聚，低耦合”，每一个团队负责一个服务，这就意味着从根本上和传统的整体性应用有本质不同，从规划阶段采用微服务架构是再好不过的。\",\"敏捷的小团队系统，公司在大型项目微服务实践前，往往这类边缘化的小项目会起到“试验田”的作用， 引入快速迭代、持续交付等模式，积累适合本公司特点的微服务实践经验，再将这些经验扩大到其他大型项目中。\",\"历史的大型留存业务系统，之前多年我一直在金融软件领域工作，在银行内部许多系统已经使用超过10年时间，成百上千个模块错综复杂维护愈发困难，无论架构、框架乃至技术人员都需要更新迭代，但都不可能一次大动手术，这时微服务的“微”就体现出来，重构时可以将某一个部分剥离为微服务独立运行，确保无误后再继续剥离出下一个服务，通过抽丝剥茧一般的剥离，逐步将原有大系统剥离为若干子服务，虽然过程十分痛苦，但这是必须做的事情。\",\"下面咱们再来列举几个不适合引入微服务的场景：\",\"微型项目，前面提到的工单系统，系统压力很小，需求变化也不大，利用单体架构便可以很好解决，使用分布式架构反而增加了架构复杂度，让系统更容易不稳定。\",\"对数据响应要求高的系统，就像前面文中提到的“导弹打飞机”的科研项目，对实时性要求极高，那显然是不合适的。\"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[\"针对引入微服务架构后带来的诸多新问题进行了分析，之后给出了多条微服务架构最佳实践，最后总结了微服务架构的使用场景。\",\"这里给你留一道思考题：微服务架构作为一种设计风格，必须要依托具体的技术与开发框架才可以实施落地，那你能列举出具体包含哪些技术吗？\"]}]},\"/architecture/microservices/microservices_design.html\":{\"title\":\"架构核心技术之微服务架构\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"本文我们来学习微服务的架构设计\",\"主要包括如下内容。\",\"单体系统的困难：编译部署困难、数据库连接耗尽、服务复用困难、新增业务困难。\",\"微服务框架：Dubbo 和 Spring Cloud，微服务的架构策略。\",\"微服务模式：事件溯源、查询与命令职责分离 CQRS、断路器、超时。\",\"微服务最佳实践。\"]},{\"header\":\"单体系统的困难\",\"slug\":\"单体系统的困难\",\"contents\":[\"在微服务出现之前，互联网应用系统主要是单体系统，也就是说一个网站的整个系统由一个应用构成。如果是 Java，就打包成一个 war 包，一个 war 包包含整个应用系统，系统更新的时候，即使只是更新其中极小的一部分，也要重新打包整个 war 包，发布整个系统。\",\"这样的单体系统面临的挑战主要是什么呢？\"]},{\"header\":\"编译、部署困难\",\"slug\":\"编译、部署困难\",\"contents\":[\"随着网站的业务不断发展，系统会变得越来越庞大，最后变成一个巨无霸的系统。\",\"在我曾经工作过的公司，单个应用可能有几个 G 大，这对于网站开发工程师来说，开发编译和部署都是非常困难的。在开发的过程中，即使只改了庞大系统中的一行代码，也必须把完整的网站系统重新打包，才能做测试。这会经历漫长的编译过程：出去抽一支烟回来一看，在编译；又去喝了一杯水回来，还在编译；再去趟厕所，回来还在编译。好不容易编译结束了，如果某个配置项错误导致编译失败，又得重来一次，浪费大半天的时间。这样的单体系统对于开发部署和测试都是非常困难的。\"]},{\"header\":\"代码分支管理困难\",\"slug\":\"代码分支管理困难\",\"contents\":[\"因为单体应用非常庞大，所以代码模块也是由多个团队共同维护的。但最后还是要编译成一个单体应用，统一发布。这就要求把各个团队的代码 merge 在一起，这个过程很容易发生代码冲突。而 merge 的时候又是网站要进行发布的时候，发布过程本来就复杂，再加上代码 merge 带来的问题，各种情况纠缠在一起，极易出错。所以，在单体应用时代每一次网站发布都需要搞到深更半夜。\"]},{\"header\":\"数据库连接耗尽\",\"slug\":\"数据库连接耗尽\",\"contents\":[\"对于一个巨型的应用而言。因为有大量的用户进行访问，所以必须把应用部署到大规模的服务器集群上。然后每个应用都需要与数据库建立连接，大量的应用服务器连接到数据库，会对数据库的连接产生巨大的压力，某些情况下甚至会耗尽数据库的连接。\"]},{\"header\":\"新增业务困难\",\"slug\":\"新增业务困难\",\"contents\":[\"巨无霸单体应用的另一个挑战是新增业务困难。因为所有的业务都耦合在一个单一的大系统里，通常随着时间的发展，这个系统会变得非常的复杂，里面的各种结构也非常乱，想要维护这样一个系统是非常困难和复杂的。很多工程师入职公司半年，都还不能熟悉业务，因为业务太过庞大和复杂，经常会出各种错误。所以就会出现这种现象：熟悉系统的老员们工忙得要死，加班加点干活，不熟悉系统的新员工们一帮忙就出乱，跟着加班加点干活。整个公司热火朝天地干活，但最后还是常常出故障，新的功能迟迟不能上线。\"]},{\"header\":\"发布困难\",\"slug\":\"发布困难\",\"contents\":[\"因为一个 war 包包含了所有的代码，进行新版本发布的时候，发布代码跟自己的开发的代码一点关系没有，但是因为 war 包包含了自己的代码，为了以防万一，也不得不跟着发布值班。结果真正更新代码功能的只有几个人，而整个部门都要跟着加班。常常出现，到了深夜，有代码更新的同事汗流浃背进行代码冲突处理和修复发布 bug，没有代码更新的同事陪着聊天、打瞌睡、打游戏，这种情况。\"]},{\"header\":\"微服务架构\",\"slug\":\"微服务架构\",\"contents\":[\"解决上述问题的主要手段是将一个单体的巨无霸系统，根据模块以及复用的粒度进行拆分，拆分成多个可以独立部署的分布式服务。应用通过远程访问调用的方式，使用这些服务，构成一个系统。但是由于它的核心服务是在其他的服务器上分布部署的，本身的业务逻辑可以变得比较简单，这样就把一个巨无霸系统单体应用拆成了若干个可复用的服务，利用较少的逻辑代码就可以组成一个应用系统。\"]},{\"header\":\"SOA 架构\",\"slug\":\"soa-架构\",\"contents\":[\"这样的设计思路其实并不是在互联网时代才出现的。在早期的时候，就有人提出了 SOA 面向服务的体系架构。如下图所示，在面向服务的体系架构里面，服务的提供者向注册中心注册自己的服务，而服务的使用者向注册中心去发现服务。发现服务以后，根据服务注册中心提供的访问接口和访问路径对服务发起请求，由服务的提供者完成请求返回结果给调用者。现在的微服务或者分布式服务，其实也是 SOA 架构的一种实现。但是在早期的 SOA 架构实践中，服务的注册与服务的调用都非常复杂，服务调用效率也比较低。\"]},{\"header\":\"微服务架构\",\"slug\":\"微服务架构-1\",\"contents\":[\"后来在互联网时代的微服务中，人们简化了 SOA 架构中的调用规范和服务规范，形成了我们现在所熟悉的分布式微服务架构。\",\"如下图，所谓的微服务架构就是将一个单体的巨无霸系统拆分成一组可复用的服务，基于这些服务构成的应用系统。图中左边是早期的单体应用系统架构，里面的各个模块互相调用、耦合，所有的系统和模块打包在一起，最后组成一个庞大的巨无霸系统。右边是微服务架构，根据服务的粒度和可复用的级别，对服务进行拆分，以独立部署服务的方式，对外提供服务调用。而应用系统也按照用途和场景的不同，依赖这些可复用的服务，进行逻辑组合，构建成自己的业务系统。\",\"通过这样一种方式，系统变得比较简单，复用级别也比较高，同时也解决了前面提出的单体巨无霸的几个重要问题。因为每一个服务或是应用系统，代码都比较简单，所以编译和部署、开发和测试，都比较简单和快速。而且这些服务都是独立维护和部署的，它的代码分支也是独立的，不会和其他的代码分支一起进行管理，减少了代码冲突的可能性。发布的时候，也是每个服务独立发布，只要做好服务的版本控制和接口兼容，应用系统不需要跟随服务一起更新发布。\",\"在微服务体系中，连接数据库的是具体的服务，应用系统不需要自己去连接数据库，只需要调用组合服务，对服务进行编排。所以对数据库的连接也相对比以前更少一些。最主要的是当需要开发新业务的时候，使用这种方式不需要对原有的单体系统进行各种重构和代码修改，只需要开发一个新的业务系统，组合调用现有的微服务，就可以组合出来一个新的产品功能，可以快速开发新产品。\"]},{\"header\":\"Dubbo\",\"slug\":\"dubbo\",\"contents\":[\"目前一些典型的微服务框架本身的架构是如何设计的？\",\"先看 Dubbo 架构。Dubbo 是阿里开源的，比较早也比较有影响力的一个分布式微服务框架。如下图所示，在 Dubbo 架构中，最核心的模块有 3 个部分，一个是服务的提供者，一个是服务的消费者，还有一个是服务的注册中心。\",\"服务的提供者顾名思义就是微服务的具体提供者，通过微服务容器对外提供服务。而服务的消费者就是应用系统或是其他的微服务。\",\"应用系统通过组合多个微服务，构成自己的业务逻辑，实现自己的产品功能。具体过程是服务的提供者程序在 Dubbo 的服务容器中启动，服务管理容器向服务注册中心进行注册，声明服务提供者所要提供的接口参数和规范，并且注册自己所在服务器的 IP 地址和端口，如下图所示。\",\"而服务的消费者如果想要调用某个服务，只需依赖服务提供者的接口进行编程。而服务接口通过 Dubbo 框架的代理访问机制，调用 Dubbo 的服务框架客户端，服务框架客户端会根据服务接口声明，去注册中心查找对应的服务提供者启动在哪些服务器上，并且将这个服务器列表返回给客户端。客户端根据某种负载均衡策略，选择某一个服务器通过远程通讯模块发送具体的服务调用请求。\",\"服务调用请求，通过 Dubbo 底层自己的远程通讯模块，也就是 RPC 调用方式，将请求发送到服务的提供者服务器，服务提供者服务器收到请求以后，将该请求发送给服务提供者程序，完成服务的执行，并将服务执行处理结果通过远程调用通讯模块 RPC 返回给服务消费者客户端，服务消费者客户端将结果返回给服务调用程序，从而完成远程服务的调用，获得服务处理的结果。\",\"Dubbo 使用 Java 进行开发，并且通过服务接口的方式对消费者提供服务，所以它的服务调用方式比较简单，可以透明地进行远程微服务调用。服务消费者程序，可以无感知地进行远程微服务调用，对开发者相对比较友好。\"]},{\"header\":\"Spring Cloud\",\"slug\":\"spring-cloud\",\"contents\":[\"另一种目前比较热门的微服务框架是 Spring Cloud。Spring Cloud 微服务框架组件跟 Dubbo 类似，也是由服务的消费者、服务的提供者和注册中心组成。如下图所示，Spring cloud 的服务提供者通过 Spring Boot 启动，然后向服务注册中心 Eureka Server 进行注册，而服务的消费者通过一个 Zuul 网关访问 Eureka Server 进行服务的发现，获得自己想要调用的远程服务对应的服务地址。获得地址以后，通过 HTTP 的方式向远程的服务提供者发起调用请求。服务提供者完成服务处理后，将处理结果通过 HTTP 返回。从而实现了远程的微服务调用。\",\"Spring Cloud 还包含了一组服务调用监控组件，主要是 Hystrix，通过 Hystrix 可以监控服务调用，还在此基础上实现了熔断、降级、超时管理等一系列高可用策略。\"]},{\"header\":\"微服务架构策略\",\"slug\":\"微服务架构策略\",\"contents\":[\"对微服务架构而言，技术现在其实比较成熟。使用什么样的技术去实现一个微服务，本身并没有太多的困难。构建一个微服务架构最困难的还是服务治理，也就是业务划分。策略要点如图所示。\",\"一个微服务包含的功能有哪些？服务的边界是什么？服务之间的依赖关系如何？这些关键的问题决定了服务的复用程度，维护的难易程度，开发的便利程度。所以设计微服务架构的时候，首先要关注的是业务，业务要先行，理顺业务模块之间的边界和依赖，做好服务治理和调用依赖管理。\",\"微服务技术是微服务架构的手段，而不是目的。微服务最主要的目的还是实现服务治理——如何划分和管理服务。首先要有独立的功能模块，然后才有分布式的服务。也就是说在软件设计的时候，软件功能模块之间的依赖关系就要清晰、合理、规范、便于维护、便于扩展，便于实现新的功能。服务之间的依赖关系要清晰、参数要简单、耦合关系要少。设计好这样的模块化结构以后，将这些设计好的模块，拆分成独立的微服务进行部署和调用，就可以构建一个良好的微服务系统。如果模块本身就是混乱的、耦合严重的、边界不清晰的、关系复杂的，那么，把它们拆分成独立的微服务进行部署，只会使事情变得更加复杂。\",\"所以进行微服务架构设计之初，就要先做好业务模块的设计和规划。同时，对于那些业务耦合比较严重、逻辑复杂多变的系统，进行微服务重构的时候，也要特别谨慎。如果做不好模块的划分和耦合管理。那么，宁可晚一点进行微服务架构重构，也不要仓促上马，以免最后带来巨大的损失。要使用微服务架构的时候，一定要搞清楚实施微服务的目的究竟是什么，是为了业务复用，是为了开发边界清晰，是为了分布式集群提升性能，还是仅仅想要使用微服务？目的一定要清楚。\",\"跟其他技术不同，微服务具有强业务属性，业务如果本身结构混乱，目标不清晰，仓促使用微服务，可能会使整个系统变得更加复杂和难以控制。所以在使用微服务前，最重要的是要先明确自己的需求：我们到底想用微服务达到什么样的目的？需求清晰了，再去考虑具体的方案和技术。这也是使用大多数技术的时候应有的方法和思路。\",\"如下图所示，最重要的是需求。在日常工作中，我们要根据需求去考虑具体的价值，再根据价值构建我们的设计原则，根据原则寻找最佳实践，最后根据实践去选择最合适的工具。按这样的方式去选择技术做架构设计才是比较成熟和高效的。如果相反，先找到一个工具，然后用工具硬往上套需求，只会导致技术也没用好，业务也没做好，所有人都疲惫不堪，事情变得一团糟，最后还可能反过来怪技术没用。\"]},{\"header\":\"微服务的使用模式\",\"slug\":\"微服务的使用模式\",\"contents\":[\"下面来看可供参考的几种微服务的使用模式。\"]},{\"header\":\"事件溯源\",\"slug\":\"事件溯源\",\"contents\":[\"第一是事件溯源，因为微服务的调用过程会比较复杂，调用链路可能会比较长。如果某个微服务调用出错，如何进行管理和监控？使用事件溯源这种模式是一种解决办法。\",\"所谓的事件溯源是指将用户的请求处理过程，每一次的状态变化都记录到事件日志中，并按照时间序列进行持久化的存储，也就是说，把所有的变更操作都按日志的方式，按时间化序列进行记录。\",\"使用事件溯源的好处有如下两点。\",\"可以精确地复现用户的状态变化。\",\"用户执行了哪些操作，使它成为现在这样一种状况，然后通过事件溯源的方式，追溯以往的操作和动作，从而进行复核和审计。当用户投诉的时候，当状态不一致的时候，可以通过事件溯源中的日志进行审计和查找。\",\"可以有效监控用户的状态变化，并在此基础上实现分布式的事务。\",\"我们传统的事务使用数据库事务进行实现，可以将多个数据库操作统一提交，或者统一回滚，保持数据的一致性，但是在分布式状况下，对数据的操作是分布在多个独立部署的服务进行处理。这个时候就无法使用数据库的事务进行管理。\",\"那么，如何在这种情况下实行分布式系统的事务？\",\"事件溯源是一种办法。因为事件溯源将所有的数据变更都按日志的方式记录起来，所以如果日志不完整，我们就知道事务不完整，可以对事务进行重组或者补偿操作，从而使数据变得一致。\"]},{\"header\":\"命令与查询职责隔离（CQRS）\",\"slug\":\"命令与查询职责隔离-cqrs\",\"contents\":[\"这种模式在服务接口层面将查询操作（也就是读操作）和命令操作（也就是写操作）隔离开来，在服务层实现读写分离。\",\"使用 CQRS 模式，主要的好处是可以有更清晰的领域模型，根据操作的方式不同，使用不同的领域模型。还可以分别进行读写优化，从而实现更好的性能。\",\"我们知道在读操作中主要使用的优化方式是缓存操作。那么，我们可以将接口层面的查询操作即读操作，尽量多地通过缓存来返回。而写操作也就是命令操作，主要的性能优化方式是使用消息队列。那么，我们可以将数据的更新操作，尽量通过消息队列，通过异步化的方式进行处理，以改善性能。\",\"因为使用 CQRS 查询和命令分离的方式，我们可以在接口层面上使用不同的优化手段。查询操作不会修改数据库，那么所有来自于查询接口的服务，可以统一连接到只读数据库中，防止误操作破坏数据，可以更好地保护数据，同时使用 CQRS，还可以更好地实现刚才的事件溯源机制。因为查询操作是无须进行事件溯源的，所有的事件溯源都可以统一设置在命令服务接口上。\"]},{\"header\":\"断路器\",\"slug\":\"断路器\",\"contents\":[\"使用微服务的时候，你还需要关注一个事情：服务的不可用。\",\"当某个服务实例出现故障的时候，它的响应延迟或者失败率增加的时候，继续调用这个服务实例会导致请求者阻塞。请求阻塞以后会导致资源消耗增加，最后可能会导致请求者也失败和崩溃，进而出现服务的级联崩溃，也就是服务请求者的请求者也失败，最后会导致整个系统全部失败，即雪崩现象。\",\"在这种情况下，可以使用断路器对故障服务进行隔离。断路器有三种状态：关闭、打开、半开。当服务出现故障的时候，通过断路器阻断对故障服务实例的调用，避免它的故障扩散开来。在 Spring Cloud 中可以使用 Hystrix 实现断路器。\"]},{\"header\":\"超时\",\"slug\":\"超时\",\"contents\":[\"还有一件需要关注的事情是：微服务调用的超时机制如何设置。\",\"如果使用统一的超时设置，那么当下游调用者超时的时候，上游调用者一定也已经超时了，因为服务调用是阻塞的。所以，下游调用的超时一定会反应在上游调用者上。因此在设置超时的时候，要设置上游调用者的超时时间大于下游调用者的超时时间之和，相同的超时设置是没有意义的，如下图所示。\",\"​\"]},{\"header\":\"总结回顾\",\"slug\":\"总结回顾\",\"contents\":[\"首先，之所以要使用微服务，是因为传统的单体巨无霸系统带来的挑战和困难，包括编译和部署的困难、连接的困难、打包代码冲突的困难，以及复用的困难、新增业务的困难。\",\"而具体的微服务框架基本上都是由三个核心部分组成的：服务的提供者、服务的调用者和服务的注册中心。服务的提供者向注册中心注册自己的服务，而服务的调用者通过注册中心发现服务，并进行远程调用。\",\"另外，很多微服务架构中还包括一个监控者的角色，通过监控者进行服务的管理和流量的控制。\",\"使用微服务最重要的是做好业务的模块化设计，模块之间要低耦合，高聚合，模块之间的依赖关系要清晰简单。只有这样的模块化设计，才能够构建出良好的微服务架构。如果系统本身就是一团遭，强行将它们拆分在不同的微服务里，只会使系统变得更加混乱。\",\"使用微服务的时候，有几个重要的使用模式，需要关注：一个是事件溯源，一个是命令与查询隔离，还有一个是断路器以及关于超时如何进行设置。\"]},{\"header\":\"福利资源\",\"slug\":\"福利资源\",\"contents\":[\"海量数据高并发场景，构建Go+ES8企业级搜索微服务：https://www.aliyundrive.com/s/ib2BeM5W3Du\"]}]},\"/dev-necessary/activation/idea.html\":{\"title\":\"IDEA系列激活图文教程\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"本教程只用于个人学习分享，请勿用于商业用途，商业用途请支持正版！\"]},{\"header\":\"1、安装\",\"slug\":\"_1、安装\",\"contents\":[\"这里就不在多说明了，已经有IDEA开发工具的教程，大家移步查看即可：IDEA下载与安装\"]},{\"header\":\"2、激活补丁\",\"slug\":\"_2、激活补丁\",\"contents\":[\"破解补丁我放置在了网盘中，提供了多个备用链接，以防下载失效。\",\"补丁下载成功后，记得 先解压, 解压后的目录如下, 本文后面所需补丁都在下面标注的这个文件夹中：\",\"install-current-user.vbs：为当前用户安装\",\"install-all-users.vbs：为所有用户安装\",\"注意：如果执行脚本被安全软件提示有风险拦截，允许执行即可。\",\"双击install-current-user.vbs即可\",\"会提示安装补丁需要等待数秒。点击【确定】按钮后，过程大概 30 秒左右，如看到弹框提示 Done 时，表示激活破解成功：\"]},{\"header\":\"3、脚本执行作用\",\"slug\":\"_3、脚本执行作用\",\"contents\":[\"Windows 用户执行脚本后，脚本会自动在环境变量 -> 用户变量下添加了 IDEA_VM_OPTIONS 变量，变量值为 /jetbra 文件夹下的 .vmoptions 参数文件绝对路径，如下所示：\",\"然后，脚本自动在 idea.vmoptions 文件中引用了破解补丁 ：\",\"提示： 细心的小伙伴应该也发现了，本文的破解方式与文章开头《第二种 IDEA 破解方法》的区别在于，这种方式提供了自动化脚本，脚本免去了手动引入补丁的繁琐步骤，一切都由脚本来完成了。\",\"Mac / Linux 用户执行脚本后，脚本会自动在当期用户环境变量文件中添加了相关参数文件，Mac / Linux 需重启系统，以确保环境变量生效。\",\"小伙伴们也可自行检查一下，如果没有自动添加这些参数，说明脚本执行没有成功。\"]},{\"header\":\"4、重启IDEA\",\"slug\":\"_4、重启idea\",\"contents\":[]},{\"header\":\"5、填写激活码激活\",\"slug\":\"_5、填写激活码激活\",\"contents\":[\"重新打开 IDEA，复制下面的激活码：\",\"6G5NXCPJZB-eyJsaWNlbnNlSWQiOiI2RzVOWENQSlpCIiwibGljZW5zZWVOYW1lIjoic2lnbnVwIHNjb290ZXIiLCJhc3NpZ25lZU5hbWUiOiIiLCJhc3NpZ25lZUVtYWlsIjoiIiwibGljZW5zZVJlc3RyaWN0aW9uIjoiIiwiY2hlY2tDb25jdXJyZW50VXNlIjpmYWxzZSwicHJvZHVjdHMiOlt7ImNvZGUiOiJQU0kiLCJmYWxsYmFja0RhdGUiOiIyMDI1LTA4LTAxIiwicGFpZFVwVG8iOiIyMDI1LTA4LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBEQiIsImZhbGxiYWNrRGF0ZSI6IjIwMjUtMDgtMDEiLCJwYWlkVXBUbyI6IjIwMjUtMDgtMDEiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiSUkiLCJmYWxsYmFja0RhdGUiOiIyMDI1LTA4LTAxIiwicGFpZFVwVG8iOiIyMDI1LTA4LTAxIiwiZXh0ZW5kZWQiOmZhbHNlfSx7ImNvZGUiOiJQUEMiLCJmYWxsYmFja0RhdGUiOiIyMDI1LTA4LTAxIiwicGFpZFVwVG8iOiIyMDI1LTA4LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBHTyIsImZhbGxiYWNrRGF0ZSI6IjIwMjUtMDgtMDEiLCJwYWlkVXBUbyI6IjIwMjUtMDgtMDEiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiUFNXIiwiZmFsbGJhY2tEYXRlIjoiMjAyNS0wOC0wMSIsInBhaWRVcFRvIjoiMjAyNS0wOC0wMSIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQV1MiLCJmYWxsYmFja0RhdGUiOiIyMDI1LTA4LTAxIiwicGFpZFVwVG8iOiIyMDI1LTA4LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBQUyIsImZhbGxiYWNrRGF0ZSI6IjIwMjUtMDgtMDEiLCJwYWlkVXBUbyI6IjIwMjUtMDgtMDEiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiUFJCIiwiZmFsbGJhY2tEYXRlIjoiMjAyNS0wOC0wMSIsInBhaWRVcFRvIjoiMjAyNS0wOC0wMSIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQQ1dNUCIsImZhbGxiYWNrRGF0ZSI6IjIwMjUtMDgtMDEiLCJwYWlkVXBUbyI6IjIwMjUtMDgtMDEiLCJleHRlbmRlZCI6dHJ1ZX1dLCJtZXRhZGF0YSI6IjAxMjAyMjA5MDJQU0FOMDAwMDA1IiwiaGFzaCI6IlRSSUFMOi0xMDc4MzkwNTY4IiwiZ3JhY2VQZXJpb2REYXlzIjo3LCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-SnRVlQQR1/9nxZ2AXsQ0seYwU5OjaiUMXrnQIIdNRvykzqQ0Q+vjXlmO7iAUwhwlsyfoMrLuvmLYwoD7fV8Mpz9Gs2gsTR8DfSHuAdvZlFENlIuFoIqyO8BneM9paD0yLxiqxy/WWuOqW6c1v9ubbfdT6z9UnzSUjPKlsjXfq9J2gcDALrv9E0RPTOZqKfnsg7PF0wNQ0/d00dy1k3zI+zJyTRpDxkCaGgijlY/LZ/wqd/kRfcbQuRzdJ/JXa3nj26rACqykKXaBH5thuvkTyySOpZwZMJVJyW7B7ro/hkFCljZug3K+bTw5VwySzJtDcQ9tDYuu0zSAeXrcv2qrOg==-MIIETDCCAjSgAwIBAgIBDTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTIwMTAxOTA5MDU1M1oXDTIyMTAyMTA5MDU1M1owHzEdMBsGA1UEAwwUcHJvZDJ5LWZyb20tMjAyMDEwMTkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCUlaUFc1wf+CfY9wzFWEL2euKQ5nswqb57V8QZG7d7RoR6rwYUIXseTOAFq210oMEe++LCjzKDuqwDfsyhgDNTgZBPAaC4vUU2oy+XR+Fq8nBixWIsH668HeOnRK6RRhsr0rJzRB95aZ3EAPzBuQ2qPaNGm17pAX0Rd6MPRgjp75IWwI9eA6aMEdPQEVN7uyOtM5zSsjoj79Lbu1fjShOnQZuJcsV8tqnayeFkNzv2LTOlofU/Tbx502Ro073gGjoeRzNvrynAP03pL486P3KCAyiNPhDs2z8/COMrxRlZW5mfzo0xsK0dQGNH3UoG/9RVwHG4eS8LFpMTR9oetHZBAgMBAAGjgZkwgZYwCQYDVR0TBAIwADAdBgNVHQ4EFgQUJNoRIpb1hUHAk0foMSNM9MCEAv8wSAYDVR0jBEEwP4AUo562SGdCEjZBvW3gubSgUouX8bOhHKQaMBgxFjAUBgNVBAMMDUpldFByb2ZpbGUgQ0GCCQDSbLGDsoN54TATBgNVHSUEDDAKBggrBgEFBQcDATALBgNVHQ8EBAMCBaAwDQYJKoZIhvcNAQELBQADggIBABqRoNGxAQct9dQUFK8xqhiZaYPd30TlmCmSAaGJ0eBpvkVeqA2jGYhAQRqFiAlFC63JKvWvRZO1iRuWCEfUMkdqQ9VQPXziE/BlsOIgrL6RlJfuFcEZ8TK3syIfIGQZNCxYhLLUuet2HE6LJYPQ5c0jH4kDooRpcVZ4rBxNwddpctUO2te9UU5/FjhioZQsPvd92qOTsV+8Cyl2fvNhNKD1Uu9ff5AkVIQn4JU23ozdB/R5oUlebwaTE6WZNBs+TA/qPj+5/we9NH71WRB0hqUoLI2AKKyiPw++FtN4Su1vsdDlrAzDj9ILjpjJKA1ImuVcG329/WTYIKysZ1CWK3zATg9BeCUPAV1pQy8ToXOq+RSYen6winZ2OO93eyHv2Iw5kbn1dqfBw1BuTE29V2FJKicJSu8iEOpfoafwJISXmz1wnnWL3V/0NxTulfWsXugOoLfv0ZIBP1xH9kmf22jjQ2JiHhQZP7ZDsreRrOeIQ/c4yR8IQvMLfC0WKQqrHu5ZzXTH4NO3CwGWSlTY74kE91zXB5mwWAx1jig+UXYc2w4RkVhy0//lOmVya/PEepuuTTI4+UJwC7qbVlh5zfhj8oTNUXgN0AOc+Q0/WFPl1aw5VV/VrO8FCoB15lFVlpKaQ1Yh+DVU8ke+rt9Th0BCHXe0uZOEmH0nOnH/0onD \"]},{\"header\":\"6、激活补丁下载地址：\",\"slug\":\"_6、激活补丁下载地址\",\"contents\":[\"小熊学Java JetBrains激活脚本：https://www.aliyundrive.com/s/d382meWkQL4\",\"失效请联系\",\"(左边是公众号，右边是个人微信)\"]}]},\"/dev-necessary/activation/navicat.html\":{\"title\":\"Navicat 16激活图文教程\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"本教程只用于个人学习分享，请勿用于商业用途，商业用途请支持正版！\"]},{\"header\":\"1、安装\",\"slug\":\"_1、安装\",\"contents\":[\"安装就没啥好说的了，一直点下一步吧！\"]},{\"header\":\"2、激活\",\"slug\":\"_2、激活\",\"contents\":[\"激活补丁下载地址：https://www.aliyundrive.com/s/X3VgdEdzpHK\",\"安装完成后，先不打开软件，注意：关闭网络，关闭所有杀毒软件，WIN10/11 系统需关闭 Windows Defender 的实时保护\",\"使用管理员身份打开注册文件，默认Patch目录【C:\\\\Program Files\\\\PremiumSoft\\\\Navicat Premium 16】\"]},{\"header\":\"1、设置安装路径，点击 Patch\",\"slug\":\"_1、设置安装路径-点击-patch\",\"contents\":[\"这里是显示默认的路径，你选择你自己安装的路径即可\"]},{\"header\":\"2、生成许可证\",\"slug\":\"_2、生成许可证\",\"contents\":[\"在 KeyGen 中选择软件，版本，语言后，点击 Generate，生成许可证\",\"同时打开 Navicat Premium，点击注册\",\"将生成的许可证复制到下图位置\"]},{\"header\":\"3、手动激活\",\"slug\":\"_3、手动激活\",\"contents\":[\"点击激活后，会出现服务器不可用提示，点击手动激活\",\"点击手动激活之后，只会出现激活码，将激活码复制到 Request Code 框中\",\"点击 Generate Activation Code，会在 Activation Code 框中生成激活码\",\"将 Activation Code 框中的激活码复制到左边的激活码框中，点击激活\",\"点击激活之后，就会出现激活成功，永久许可证\"]}]},\"/dev-necessary/activation/xmind.html\":{\"title\":\"Xmind激活教程\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"本教程只用于个人学习分享，请勿用于商业用途，商业用途请支持正版！\"]},{\"header\":\"1、安装\",\"slug\":\"_1、安装\",\"contents\":[\"安装就不说了哈，自行从官网下载安装即可!\"]},{\"header\":\"2、激活\",\"slug\":\"_2、激活\",\"contents\":[]},{\"header\":\"1、下载激活补丁\",\"slug\":\"_1、下载激活补丁\",\"contents\":[\"下载地址：Xmind激活 https://www.aliyundrive.com/s/w3cxPDWD4SU\"]},{\"header\":\"2、激活\",\"slug\":\"_2、激活-1\",\"contents\":[\"关闭Xmind软件，打开Xmind的安装目录，默认在你电脑的C盘：AppData\\\\Local\\\\Programs\\\\Xmind\",\"将winmm.dll文件放到安装目录下即可，重新打开，就是pro版本了\"]}]},\"/dev-necessary/install/docker.html\":{\"title\":\"Docker安装\",\"contents\":[{\"header\":\"常用命令\",\"slug\":\"常用命令\",\"contents\":[\"查询镜像\",\"docker search mysql \",\"拉取镜像\",\"docker pull 镜像名称[:version] \",\"删除镜像\",\"# 删除一个镜像 docker rmi 镜像名称/id # 删除多个镜像 docker rmi 镜像名称1/id1 镜像名称2/id2 ... # 删除所有镜像 docker rmi `docker images ‐q` \",\"创建容器\",\"docker run [options] image command [ARG...] \",\"options选项: ‐i、‐t、‐d、‐‐name ‐i：交互式容器 ‐t：tty，终端 ‐d:后台运行，并且打印容器id\",\"--restart=always：docker 的容器自动在开机启动\",\"创建的容器名称不能重复\",\"进入容器\",\"#方式一 docker attach 容器名称/id (ps:exit,容器停止) #方式二 docker exec ‐it 容器名称/id /bin/bash （ps:exit,容器不会停止） \",\"查看容器\",\"docker ps：查看正在运行的容器 docker ps ‐a：查看运行过的容器（历史） docker ps ‐l：最后一次运行的容器 \",\"停止/启动容器\",\"docker start 容器名称/id docker stop 容器名称/id \",\"获取容器/镜像的元数据\",\"#查看容器/镜像全部信息： docker inspect 容器/镜像 #查看容器/镜像部分信息： docker inspect ‐f='{{.NetworkSettings.IPAddress}}' 容器/镜像 ‐f：可通过‐‐format代替 \",\"删除容器\",\"#删除一个容器： docker rm 容器名称/id #删除多个容器： docker rm 容器名称1/id1 容器名称2/id2 ... #删除所有容器 docker rm `docker ps ‐a ‐q` \",\"PS：无法删除正在运行的容器\",\"查看容器日志\",\"docker logs 容器名称/id \",\"文件拷贝\",\"docker cp 需要拷贝的文件或目录 容器名称:容器目录 例如：docker cp 1.txt c2:/root \",\"目录挂载\",\"我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。\",\"创建容器 添加-v参数 后边为 宿主机目录:容器目录\",\"docker run ‐id ‐‐name=c4 ‐v /opt/:/usr/local/myhtml centos \",\"如果你共享的是多级的目录，可能会出现权限不足的提示\",\"这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 --privileged=true 来解决挂载的目录没有权限的问题\",\"docker run ‐id ‐‐privileged=true ‐‐name=c4 ‐v /opt/:/usr/local/myhtml centos \",\"创建容器时没有添加参数 --restart=always ，导致的后果是：当 Docker 重启时，容器未能自动启动。\",\"docker container update --restart=always 容器名字 \"]},{\"header\":\"1、docker安装MySQL\",\"slug\":\"_1、docker安装mysql\",\"contents\":[\"查看mysql可用的版本\",\"访问 MySQL 镜像库地址：https://hub.docker.com/_/mysql?tab=tags 。\",\"可以通过 Sort by 查看其他版本的 MySQL，默认是最新版本 mysql:latest 。\",\"拉取 MySQL 镜像\",\"docker pull mysql:latest \",\"查看本地镜像\",\"docker images \",\"运行容器\",\"docker run -itd --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql \",\"-p 3306:3306 ：映射容器服务的 3306 端口到宿主机的 3306 端口，外部主机可以直接通过 宿主机ip:3306 访问到 MySQL 的服务。\",\"MYSQL_ROOT_PASSWORD=123456：设置 MySQL 服务 root 用户的密码。\",\"查看是否启动成功\",\"docker ps \",\"MySQL 配置\",\"vim /mydata/mysql/conf/my.cnf \",\"[client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake skip-name-resolve \",\"重启mysql\",\"docker restart mysql \",\"进入容器\",\"docker exec -it mysql bash \",\"登录mysql\",\"mysql -u root -p ALTER USER 'root'@'localhost' IDENTIFIED BY '123456'; \",\"添加远程登录用户\",\"CREATE USER 'root'@'% IDENTIFIED WITH mysql_native_password BY '123456'; alter user 'root'@'%' identified with mysql_native_password by '123456'; \",\"授权使用\",\"GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION; \"]},{\"header\":\"2、docker安装Redis\",\"slug\":\"_2、docker安装redis\",\"contents\":[\"查看可用的 Redis 版本\",\"访问 Redis 镜像库地址： https://hub.docker.com/_/redis?tab=tags。\",\"取最新版的 Redis 镜像\",\"docker pull redis \",\"查看本地镜像\",\"docker images \",\"运行容器\",\"docker run -itd --name redis -p 6379:6379 redis \",\"-p 6379:6379：映射容器服务的 6379 端口到宿主机的 6379 端口。外部可以直接通过宿主机ip:6379 访问到 Redis 的服务\",\"查看运行状态\",\"docker ps \",\"redis-cli 连接测试使用 redis 服务\",\"docker exec -it redis /bin/bash redis- \"]},{\"header\":\"3、docker安装Elasticsearch\",\"slug\":\"_3、docker安装elasticsearch\",\"contents\":[\"官网镜像：https://hub.docker.com/_/elasticsearch/tags\",\"1、取最新版的 elasticsearch镜像\",\"docker pull docker.elastic.co/elasticsearch/elasticsearch:7.15.1 #存储和检索数据 \",\"2、查看本地镜像\",\"docker images \",\"3、运行容器\",\"docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\\\ -e \\\"discovery.type=single-node\\\" \\\\ -e ES_JAVA_OPTS=\\\"-Xms64m -Xmx512m\\\" \\\\ -v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\\\ -v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\\\ -v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\\\ -d docker.elastic.co/elasticsearch/elasticsearch:7.15.1 \",\"-p 9200:9200 -p 9300:9300：这里是外部访问端口9200，elasticsearch集群内部端口9300\",\"-e \\\"discovery.type=single-node\\\"：以单节点方式启动\",\"-e ES_JAVA_OPTS=\\\"-Xms64m -Xmx512m\\\"：设置内存，不然elasticsearch会全部占用，导致死机\",\"-v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\\\：这里以它为例，进行目录挂载\",\"-d docker.elastic.co/elasticsearch/elasticsearch:7.15.1：启动容器的名称\",\"4、查看运行状态\",\"docker ps \",\"5、发现突然就挂了，查看日志\",\"docker logs elasticsearch \",\"ElasticsearchException[failed to bind service]; nested: AccessDeniedException[/usr/share/elasticsearch/data/nodes]; Likely root cause: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/data/nodes at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:90) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:398) at java.base/java.nio.file.Files.createDirectory(Files.java:700) at java.base/java.nio.file.Files.createAndCheckIsDirectory(Files.java:807) at java.base/java.nio.file.Files.createDirectories(Files.java:793) at org.elasticsearch.env.NodeEnvironment.lambda$new$0(NodeEnvironment.java:265) at org.elasticsearch.env.NodeEnvironment$NodeLock.<init>(NodeEnvironment.java:202) at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:262) at org.elasticsearch.node.Node.<init>(Node.java:383) at org.elasticsearch.node.Node.<init>(Node.java:288) at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:219) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:219) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:399) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:167) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:158) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:114) at org.elasticsearch.cli.Command.main(Command.java:79) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:123) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:81) For complete error details, refer to the log at /usr/share/elasticsearch/logs/elasticsearch.log \",\"ps：没有权限访问config目录\",\"6、添加权限\",\"chmod -R 777 /mydata/elasticsearch/ 保证权限 \",\"-R：遍历改目录下的所有目录\",\"7、重新启动\",\"docker ps -a \",\"docker start 385 \",\"8、查看运行状态\",\"docker ps \",\"9、外网访问 http://虚拟机ip:9200/\",\"{ \\\"name\\\" : \\\"3859b80b5c1a\\\", \\\"cluster_name\\\" : \\\"elasticsearch\\\", \\\"cluster_uuid\\\" : \\\"dSKyudV8Qleuc_CXEIYvkA\\\", \\\"version\\\" : { \\\"number\\\" : \\\"7.15.1\\\", \\\"build_flavor\\\" : \\\"default\\\", \\\"build_type\\\" : \\\"docker\\\", \\\"build_hash\\\" : \\\"83c34f456ae29d60e94d886e455e6a3409bba9ed\\\", \\\"build_date\\\" : \\\"2021-10-07T21:56:19.031608185Z\\\", \\\"build_snapshot\\\" : false, \\\"lucene_version\\\" : \\\"8.9.0\\\", \\\"minimum_wire_compatibility_version\\\" : \\\"6.8.0\\\", \\\"minimum_index_compatibility_version\\\" : \\\"6.0.0-beta1\\\" }, \\\"tagline\\\" : \\\"You Know, for Search\\\" } \",\"11、运行kibana\",\" \"]},{\"header\":\"4、docker安装Kibana\",\"slug\":\"_4、docker安装kibana\",\"contents\":[\"取 kibana镜像\",\"docker pull docker.elastic.co/kibana/kibana:7.15.1 \",\"运行kibana\",\"docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.222.129:9200 -p 5601:5601 \\\\ -d docker.elastic.co/kibana/kibana:7.15.1 \",\"ELASTICSEARCH_HOSTS：elasticsearch的安装服务器ip\",\"访问访问：http://ip:5601/\"]},{\"header\":\"5、docker安装Nginx\",\"slug\":\"_5、docker安装nginx\",\"contents\":[\"1、启动一个nginx的实例，如果不存在实例，则会安装再启动\",\"docker run -p 80:80 --name nginx -d nginx \",\"2、将容器内的配置文件拷贝到当前目录，后面有个点\",\"docker container cp nginx:/etc/nginx . \",\"3、修改文件名称，把这个conf 移动到/mydata/nginx 下\",\"mv nginx conf \",\"4、终止原容器\",\"docker stop nginx \",\"5、删除容器\",\"docker rm nginx \",\"6、创建新的nginx\",\"docker run -p 80:80 --name nginx \\\\ -v /mydata/nginx/html:/usr/share/nginx/html \\\\ -v /mydata/nginx/logs:/var/log/nginx \\\\ -v /mydata/nginx/conf:/etc/nginx \\\\ -d nginx \",\"给nginx 的html 下面放的所有资源可以直接访问\"]},{\"header\":\"6、docker安装Tomcat\",\"slug\":\"_6、docker安装tomcat\",\"contents\":[]},{\"header\":\"1、正常安装\",\"slug\":\"_1、正常安装\",\"contents\":[\"1、搜索镜像\",\"docker search tomcat \",\"2、拉取镜像\",\"docker pull tomcat \",\"3、查看镜像\",\"docker images \",\"4、创建容器示例\",\"docker run -it -p 8080:8080 tomcat \",\"-p 小写，主机端口:docker容器端口\",\"-P 大写，随机分配端口\",\"i:交互\",\"t:终端\",\"d:后台\",\"PS：宿主机8080映射到容器内的8080端口\"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"访问http://[ip]:8080/\"]},{\"header\":\"解决\",\"slug\":\"解决\",\"contents\":[\"可能没有映射端口或者没有关闭防火墙\",\"查看tomcat文件，把webapps.dist目录换成webapps\",\"先后台启动tomcat\",\"docker run -d -p 8080:8080 tomcat \",\"进入容器内部\",\"docker exec -it tomcat /bin/bash \",\"查看文件目录，webapps.dist目录换成webapps\",\"root@90c2f8c553b0:/usr/local/tomcat# ls BUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.dist CONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work root@90c2f8c553b0:/usr/local/tomcat# rm -f webapps rm: cannot remove 'webapps': Is a directory root@90c2f8c553b0:/usr/local/tomcat# rm -r webapps root@90c2f8c553b0:/usr/local/tomcat# mv webapps.dist webapps root@90c2f8c553b0:/usr/local/tomcat# ls BUILDING.txt LICENSE README.md RUNNING.txt conf logs temp work CONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps root@90c2f8c553b0:/usr/local/tomcat# \",\"重新访问：http://[ip]:8080/\"]},{\"header\":\"2、免修改版\",\"slug\":\"_2、免修改版\",\"contents\":[]},{\"header\":\"拉取镜像\",\"slug\":\"拉取镜像\",\"contents\":[\"docker pull billygoo/tomcat8-jdk8 \"]},{\"header\":\"创建容器\",\"slug\":\"创建容器\",\"contents\":[\"docker run -d -p 8080:8080 --name mytomcat8 billygoo/tomcat8-jdk8 \"]},{\"header\":\"8、docker安装Nacos\",\"slug\":\"_8、docker安装nacos\",\"contents\":[\"查看可用的版本\",\"docker search nacos \",\"拉取最新的版本\",\"# 下载镜像 https://hub.docker.com/ docker 官方搜索查看有哪些 nacos 镜像版本 docker pull nacos/nacos-server \",\"启动容器，以单例模式启动\",\"docker run -d -p 8848:8848 --env MODE=standalone --name nacos nacos/nacos-server \",\"访问：http://192.168.130.135:8848/nacos\",\"如果出现404，则是没有开放对应的端口\",\"#查看开放端口 firewall-cmd --list-port #防火墙开放8848端口 firewall-cmd --zone=public --add-port=8848/tcp --permanent #重启防火墙 firewall-cmd --reload \",\"为Nacos配置数据库，不然未启动30天后配置就消失了\",\"修改容器的配置文件\",\"#执行如下命令进入配置nacos docker exec -it nacos bash #修改conf/application.properties文件，可以修改可以覆盖如下 #注意如果mysql也是使用docker启动的，需要指定mysql在docker中的ip，这里我只需要一个主数据库配置 server.contextPath=/nacos server.servlet.contextPath=/nacos server.port=8848 spring.datasource.platform=mysql db.num=1 db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true db.user=root db.password=123456 nacos.cmdb.dumpTaskInterval=3600 nacos.cmdb.eventTaskInterval=10 nacos.cmdb.labelTaskInterval=300 nacos.cmdb.loadDataAtStart=false management.metrics.export.elastic.enabled=false management.metrics.export.influx.enabled=false server.tomcat.accesslog.enabled=true server.tomcat.accesslog.pattern=%h %l %u %t \\\"%r\\\" %s %b %D %{User-Agent}i nacos.security.ignore.urls=/,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/v1/auth/login,/v1/console/health/**,/v1/cs/**,/v1/ns/**,/v1/cmdb/**,/actuator/**,/v1/console/server/** nacos.naming.distro.taskDispatchThreadCount=1 nacos.naming.distro.taskDispatchPeriod=200 nacos.naming.distro.batchSyncKeyCount=1000 nacos.naming.distro.initDataRatio=0.9 nacos.naming.distro.syncRetryDelay=5000 nacos.naming.data.warmup=true nacos.naming.expireInstance=true \",\"记得重启容器\",\"docker restart nacos \",\"启动时配置\",\"docker run -d \\\\ -e TZ=\\\"Asia/Shanghai\\\" \\\\ -e MODE=standalone \\\\ -e SPRING_DATASOURCE_PLATFORM=mysql \\\\ -e MYSQL_MASTER_SERVICE_HOST=192.168.101.11 \\\\ -e MYSQL_MASTER_SERVICE_PORT=3306 \\\\ -e MYSQL_MASTER_SERVICE_USER=root \\\\ -e MYSQL_MASTER_SERVICE_PASSWORD=root \\\\ -e MYSQL_MASTER_SERVICE_DB_NAME=nacos-config \\\\ -e MYSQL_SLAVE_SERVICE_HOST=192.168.101.11 \\\\ -p 8848:8848 \\\\ --expose=8848 \\\\ --name nacos \\\\ --restart=always \\\\ -v /root/nacos/standalone-logs/:/home/nacos/logs \\\\ nacos/nacos-server:1.1.0 \",\"具体配置参数参考官方文档，这里有个注意的是 MYSQL_SLAVE_SERVICE_HOST 也需要配置\"]},{\"header\":\"7、docker安装Elasticsearch-head\",\"slug\":\"_7、docker安装elasticsearch-head\",\"contents\":[\"官网镜像：https://hub.docker.com/r/mobz/elasticsearch-head/tags\",\"搜索镜像\",\"docker search elasticsearch-head \",\"拉取镜像\",\"docker pull mobz/elasticsearch-head:5 \",\"运行容器\",\"docker run --name es-head -d -p 9100:9100 mobz/elasticsearch-head:5 \",\"访问是否正常：http://192.168.130.135:9100/\"]},{\"header\":\"9、docker安装Seata\",\"slug\":\"_9、docker安装seata\",\"contents\":[\"docker镜像仓库：https://hub.docker.com/r/seataio/seata-server/tags\",\"搜索镜像\",\"docker search seataio/seata-server:1.5.0 \",\"拉取镜像\",\"docker pull seataio/seata-server:1.5.0 \",\"运行\",\"docker run -itd --name seata-server -p 8091:8091 -p 7091:7091 seataio/seata-server:1.5.0 \"]}]},\"/dev-necessary/install/Linux-offline.html\":{\"title\":\"Linux离线安装\",\"contents\":[{\"header\":\"1、linux离线安装gcc\",\"slug\":\"_1、linux离线安装gcc\",\"contents\":[\"检查是否存在环境\",\"rpm -qa | grep gcc-c++ \",\"离线下载gcc安装包\",\"下载地址：http://mirrors.aliyun.com/centos/7/os/x86_64/Packages/\",\"mpfr-3.1.1-4.el7.x86_64.rpm libmpc-1.0.1-3.el7.x86_64.rpm kernel-headers-3.10.0-123.el7.x86_64.rpm glibc-headers-2.17-55.el7.x86_64.rpm glibc-devel-2.17-55.el7.x86_64.rpm cpp-4.8.2-16.el7.x86_64.rpm gcc-4.8.2-16.el7.x86_64.rpm \",\"上传至服务器\",\"进入目录统一安装\",\"rpm -Uvh *.rpm --nodeps --force \",\"验证是否安装成功\",\"gcc -v g++ -v \"]},{\"header\":\"2、linux离线安装perl\",\"slug\":\"_2、linux离线安装perl\",\"contents\":[\"下载安装包，需先安装gcc环境\",\"解压安装包，目录可指定，不指定，就解压在该目录下\",\"tar -zxvf perl-5.26.1.tar.gz -C /[指定的目录] \",\"检查平台环境\",\"./Configure -des -Dprefix=/[指定的目录]/perl \",\"编译\",\"make \",\"安装\",\"make install \"]},{\"header\":\"3、linux离线安装svn\",\"slug\":\"_3、linux离线安装svn\",\"contents\":[\"依赖包：\",\"主要安装包： subversion-1.14.0.tar.gz http://subversion.apache.org/download/\",\"SVN基础依赖包： apr-1.7.0.tar.gz http://apr.apache.org/download.cgi\",\"SVN基础依赖包： apr-util-1.6.1.tar.gz http://apr.apache.org/download.cgi\",\"SVN基础依赖包： zlib-1.2.11.tar.xz http://linux.softpedia.com/get/Programming/Libraries/zlib-159.shtml\",\"数据库依赖包： sqlite-autoconf-3320300.tar.gz http://sqlite.org/download.html\",\"apr-util依赖包： expat-2.0.1.tar.gz http://www.linuxfromscratch.org/blfs/view/6.3/general/expat.html\"]},{\"header\":\"1、上传至相应目录，解压\",\"slug\":\"_1、上传至相应目录-解压\",\"contents\":[\"cd /home/ mkdir svn 压缩包上传到svn，解压 tar -xzvf subversion-1.14.0.tar.gz tar -xzvf apr-1.7.0.tar.gz tar -xzvf apr-util-1.6.1.tar.gz tar -xzvf zlib-1.2.11.tar.xz tar -xzvf sqlite-autoconf-3320300.tar.gz tar -xzvf expat-2.0.1.tar.gz \"]},{\"header\":\"2、安装apr\",\"slug\":\"_2、安装apr\",\"contents\":[\"进入apr-1.7.0文件夹下\",\"cd apr-1.7.0 \",\"执行安装命令\",\"./configure prefix=/home/svn/apr-1.7.0 make make install \"]},{\"header\":\"3、安装apr-util\",\"slug\":\"_3、安装apr-util\",\"contents\":[\"安装expat库\",\"cd expat-2.0.1 ./configure make make install \",\"进入apr-util-1.6.1文件夹下\",\"cd apr-util-1.6.1 \",\"安装\",\"./configure prefix=/home/svn/apr-util-1.6.1 --with-apr=/usr/local/apr make make install \"]},{\"header\":\"4、安装zlib\",\"slug\":\"_4、安装zlib\",\"contents\":[\"进入zlib-1.2.11文件夹下\",\"cd zlib-1.2.11 \",\"安装命令\",\"./configure make make install \"]},{\"header\":\"5、将sqlite-autoconf-3320300改名为sqlite-amalgamation并移动到subversion-1.14.0文件夹下\",\"slug\":\"_5、将sqlite-autoconf-3320300改名为sqlite-amalgamation并移动到subversion-1-14-0文件夹下\",\"contents\":[\"mv sqlite-autoconf-3320300 ./subversion-1.14.0/sqlite-amalgamation \"]},{\"header\":\"6、安装subversion\",\"slug\":\"_6、安装subversion\",\"contents\":[\"进入subversion-1.14.0文件夹下\",\"cd subversion-1.14.0 \",\"安装命令\",\"./configure prefix=/home/svn/subversion-1.14.0 --with-apr=/home/svn/apr-1.7.0 --with-apr-util=/home/svn/apr-util-1.6.1 --with-zlib=/home/svn/zlib-1.2.11 --with-lz4=internal --with-utf8proc=internal make make install \",\"安装完成，添加环境变量path\",\"vi .bash_profile PATH=/home/svn/subversion-1.14.0/bin:$PATH #生效配置文件 source .bash_profile \"]},{\"header\":\"7、配置SVN仓库\",\"slug\":\"_7、配置svn仓库\",\"contents\":[\"创建库目录\",\"mkdir -p myproject \",\"添加配置文件\",\"svnadmin create /home/svn/myproject \",\"进入配置文件所在目录\",\"cd myproject/conf vi subversion.conf \",\"修改配置文件\",\"#将文件中以下内容前的删除并顶格 [general] anon-access = none #使非授权用户无法访问 auth-access = write #使授权用户有写权限 password-db = passwd #指明密码文件路径 authz-db =authz #访问控制文件 realm = repos #认证命名空间，subversion会在认证提示里显示，并且作为凭证缓存的关键字 \",\"修改passwd文件用于创建用户\",\"vi passwd \",\"[users] admin =admin # 用户名1 = 密码1 user =user # 用户名2 = 密码2 \",\"修改authz文件用于管理用户权限\",\" vi authz 按格式输入权限内容 [库名:目录] 用户名1 = 权限 用户名2 = 权限 例如： [myproject:/] admin = rw user =r \",\"启动服务\",\"svnserve -d --listen-port 3690 -r /home/svn \",\"查看svn服务\",\"ps - ef|grep svn \",\"防火墙端口\",\"firewall-cmd --zone=public --add-port=3690/tcp --permanent #重启防火墙 systemctl restart firewalld \",\"访问svn服务\",\"svn://svn服务器IP地址/myproject\"]},{\"header\":\"4、linux离线源码安装Redis\",\"slug\":\"_4、linux离线源码安装redis\",\"contents\":[\"预先安装gcc和make这两个软件\",\"yum install -y gcc make\",\"下载地址：https://download.redis.io/releases/redis-6.0.0.tar.gz\",\"下载redis压缩包\",\"压缩包上传到/usr/local/redis目录\",\"解压\",\"tar -zxvf redis-6.0.0.tar.gz -C /usr/local/redis \",\"切换到/usr/local/redis/redis-6.0.0目录\",\"cd /usr/local/redis/redis-6.0.0 \",\"编译\",\"make \",\"安装到redis目录下\",\"make install PREFIX=/usr/local/redis \",\"修改redis.conf配置文件\",\"vim /usr/local/redis/redis-6.0.0/redis.conf \",\"#bind 127.0.0.1 #允许所有IP访问 port 6379 #端口号为6379 protected-mode no #关闭保护模式，不然远程还是连接不了 daemonize yes #设为后台运行 #requirepass 123456 #简化开发，没有设置密码 \",\"启动\",\"把解压后的redis-6.0.0/redis.conf复制到/usr/local/redis/bin目录下\",\"cp redis.conf /usr/local/redis/bin/ \",\"后台启动redis, 先cd /usr/local/redis/bin 进入\",\"./redis-server redis.conf \",\"查看进程\",\"ps -ef|grep redis 查看本机监听端口 netstat -tunlp|grep redis \",\"常用命令\",\"强行终止redis\",\"pkill redis-server \",\"开启redis\",\"./redis-server ./redis.conf \",\"（可选）设置redis开机启动\",\"#文件不存在，则会创建此文件 vim /etc/systemd/system/redis.service [Unit] Description=redis-server After=network.target [Service] Type=forking ExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target \",\"其他命令\",\"systemctl常用命令 systemctl daemon-reload #此命令用于重新加载修改后的启动脚本 systemctl start redis.service #启动redis服务 systemctl enable redis.service #设置开机自启动 systemctl disable redis.service #停止开机自启动 systemctl status redis.service #查看服务当前状态 systemctl restart redis.service #重新启动服务 systemctl list-units --type=service #查看所有已启动的服务 \",\"firewall-cmd --zone=public --add-port=6379/tcp --permanent ----添加6379端口 firewall-cmd --reload ----重启防火墙 firewall-cmd --list-port -----查看所有开放端口号 firewall-cmd --query-port=6379/tcp -----查看指定端口是否开放 \"]},{\"header\":\"5、linux离线安装libreoffice\",\"slug\":\"_5、linux离线安装libreoffice\",\"contents\":[]},{\"header\":\"1、下载安装包\",\"slug\":\"_1、下载安装包\",\"contents\":[\"下载地址：https://www.libreoffice.org/download/download/\",\"例如：LibreOffice_7.1.0.2_Linux_x86-64_rpm.tar.gz\"]},{\"header\":\"2、上传至服务器目录\",\"slug\":\"_2、上传至服务器目录\",\"contents\":[]},{\"header\":\"3、解压文件\",\"slug\":\"_3、解压文件\",\"contents\":[\"tar -zxvf LibreOffice_7.1.0.2_Linux_x86-64_rpm.tar.gz \"]},{\"header\":\"4、进入文件 RPMS 目录下\",\"slug\":\"_4、进入文件rpms目录下\",\"contents\":[\"cd /opt/libreoffice7.1/LibreOffice_7.1.0.2_Linux_x86-64_rpm/RPMS \"]},{\"header\":\"5、安装 rpm 文件\",\"slug\":\"_5、安装rpm文件\",\"contents\":[\"rpm -Uivh *.rpm --nodeps \"]},{\"header\":\"6、测试是否安装成功\",\"slug\":\"_6、测试是否安装成功\",\"contents\":[\"这里指的是启动服务成功\",\"/usr/bin/libreoffice7.1 --headless --accept=\\\"socket,host=0.0.0.0,port=8100;urp;\\\" --nofirststartwizard #速度有点慢 \",\"[root@docker RPMS]# /usr/bin/libreoffice7.1 --headless --accept=\\\"socket,host=0.0.0.0,port=8100;urp;\\\" --nofirststartwizard /opt/libreoffice7.1/program/oosplash: error while loading shared libraries: libXinerama.so.1: cannot open shared object file: No such file or directory \",\"PS：因是刚搭建好的系统，又是离线安装，缺少了相关的依赖\",\"这里整理了一份完整的离线依赖列表，需全部安装才可启动成功。\",\"avahi-libs-0.6.31-20.el7.x86_64.rpm cairo-1.15.12-4.el7.x86_64.rpm cups-libs-1.6.3-51.el7.x86_64.rpm fontconfig-2.13.0-4.3.el7.x86_64.rpm libglvnd-1.0.1-0.8.git5baa1e5.el7.x86_64.rpm libglvnd-egl-1.0.1-0.8.git5baa1e5.el7.x86_64.rpm libglvnd-glx-1.0.1-0.8.git5baa1e5.el7.x86_64.rpm libICE-1.0.9-9.el7.x86_64.rpm libSM-1.2.2-2.el7.x86_64.rpm libX11-1.6.7-2.el7.x86_64.rpm libXau-1.0.8-2.1.el7.x86_64.rpm libxcb-1.13-1.el7.x86_64.rpm libXext-1.3.3-3.el7.x86_64.rpm libXinerama-1.1.3-2.1.el7.x86_64.rpm libXrender-0.9.10-1.el7.x86_64.rpm libpng15-1.5.30-7.el8.x86_64.rpm \",\"链接：https://pan.baidu.com/s/1bp5uivZyNQZUo9BezIsaUA\",\"提取码：6666\",\"再接着执行命令，若没有报错，即依赖已全部装上\",\"/usr/bin/libreoffice7.1 --headless --accept=\\\"socket,host=0.0.0.0,port=8100;urp;\\\" --nofirststartwizard \"]},{\"header\":\"6、linux离线安装Nginx\",\"slug\":\"_6、linux离线安装nginx\",\"contents\":[\"源码需安装依赖库：gcc环境、pcre、zlib、openssl\",\"gcc：nginx 编译时依赖 gcc 环境\",\"pcre：nginx 支持重写功能\",\"zlib：zlib 库提供了很多压缩和解压缩的方式，nginx 使用 zlib 对 http 包内容进行 gzip 压缩\",\"openssl： 安全套接字层密码库，用于通信加密，如不需要https访问，可不安装\",\"安装编译工具及库文件\",\"yum -y install make zlib zlib-devel gcc-c++ libtool pcre-devel openssl openssl-devel \",\"源码下载包地址：https://nginx.org/en/download.html\",\"解压\",\"tar -zvxf nginx-1.20.2.tar.gz \",\"进入安装包目录\",\"cd nginx-1.20.2 \",\"检查平台环境，编译、安装\",\"./confiure #检查平台环境 make #编译 make install #安装 \",\"检查版本型号\",\"/usr/local/nginx/sbin/nginx -v \",\"启动nginx\",\"/usr/local/nginx/sbin/nginx \",\"其他命令\",\"/usr/local/nginx/sbin/nginx -s reload # 重新载入配置文件 /usr/local/nginx/sbin/nginx -s reopen # 重启 Nginx /usr/local/nginx/sbin/nginx -s stop # 停止 Nginx \",\"遇到的问题：\",\"部署的路径没有权限访问\",\"切换至root用户或\",\"非Root用户启动\",\"增加权限\",\"chmod 755 nginx chmod u+s nginx \",\"修改配置\",\"vi /usr/local/nginx/conf/nginx.conf \",\"user nobody; //取消注释 error_log /home/user/ nginx /log/error.log; pid /home/user/nginx /nginx.pid; \",\"PS：在 linux 下，只有以 root 启动的进程才能监听小于 1024 的端口。nginx 如果设置了监听 80 或 443 端口，则一定得以 root 帐号启动。如果只是测试，则可将端口设成 8080 之类大于 1024 的端口。\"]},{\"header\":\"7、linux离线安装MySQL\",\"slug\":\"_7、linux离线安装mysql\",\"contents\":[]},{\"header\":\"1、RPM安装\",\"slug\":\"_1、rpm安装\",\"contents\":[\"检测系统是否自带安装 MySQL\",\"rpm -qa | grep mysql \",\"卸载\",\"rpm -e mysql // 普通删除模式 rpm -e --nodeps mysql // 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除 \",\"下载MySql的RPM文件\",\"https://cdn.mysql.com/Downloads/MySQL-5.7/mysql-5.7.27-1.el7.x86_64.rpm-bundle.tar\",\"解压\",\"tar -xf mysql-5.7.27-1.el7.x86_64.rpm-bundle.tar -C /home/mysql/ \",\"安装\",\"rpm -Uvh *.rpm --nodeps –force \",\"启动\",\"systemctl start mysqld #查看启动状态 service mysqld status \",\"查看root密码\",\"grep 'temporary password' /var/log/mysqld.log \"]}]},\"/dev-necessary/install/Linux.html\":{\"title\":\"Linux安装\",\"contents\":[{\"header\":\"1、linux安装 Docker\",\"slug\":\"_1、linux安装-docker\",\"contents\":[\"Docker 安装文档：https://docs.docker.com/install/linux/docker-ce/centos/\",\"卸载系统之前的docker\",\"sudo yum remove docker \\\\ docker-client \\\\ docker-client-latest \\\\ docker-common \\\\ docker-latest \\\\ docker-latest-logrotate \\\\ docker-logrotate \\\\ docker-engine \",\"安装Docker-CE\",\"安装必须的依赖\",\"sudo yum install -y yum-utils \\\\ device-mapper-persistent-data \\\\ lvm2 \",\"设置docker repo 的yum 位置\",\"sudo yum-config-manager \\\\ --add-repo \\\\ https://download.docker.com/linux/centos/docker-ce.repo \",\"安装docker，以及docker-cli\",\"sudo yum install docker-ce docker-ce-cli containerd.io \",\"启动docker\",\"sudo systemctl start docker \",\"查看docker版本\",\"docker -v \",\"查看docker 镜像列表\",\"docker images \",\"设置开机自启动\",\"sudo systemctl enable docker \",\"测试docker 常用命令，注意切换到root 用户下\",\"https://docs.docker.com/engine/reference/commandline/docker/\",\"配置docker 镜像加速\",\"阿里云，容器镜像服务 针对Docker 客户端版本大于1.10.0 的用户 您可以通过修改daemon 配置文件/etc/docker/daemon.json 来使用加速器\"]},{\"header\":\"常见错误：\",\"slug\":\"常见错误\",\"contents\":[]},{\"header\":\"1、镜像名重复或已存在\",\"slug\":\"_1、镜像名重复或已存在\",\"contents\":[\"docker: Error response from daemon: Conflict. The container name \\\"/mysql\\\" is already in use by container \\\"9f52d88a12eadbfb81707191fe2c2090f5dcfbd1f6a7461d97e32ad947c2feab\\\". You have to remove (or rename) that container to be able to reuse that name. \",\"解法方法：\",\"先查看镜像\",\"docker ps -a \",\"[root@xiaobear ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9f52d88a12ea mysql \\\"docker-entrypoint.s…\\\" 7 minutes ago Exited (1) 7 minutes ago mysql \",\"移除CONTAINER ID名\",\"docker rm 9f52d88a12ea \",\"再创建新的容器\"]},{\"header\":\"2、linux安装jdk\",\"slug\":\"_2、linux安装jdk\",\"contents\":[\"1、先去官网下载好对应的linux版本的压缩包，https://www.oracle.com/java/technologies/downloads/#java8\",\"2、将压缩包通过ftp进行上传至服务器\",\"3、进入目录，进行解压\",\"cd /压缩包目录 tar -zxvf jdk-8u131-linux-x64.tar.gz \",\"4、进入环境变量文件\",\"vi /etc/profile \",\"5、增加环境变量\",\"//JAVA_HOME=解压后的jdk路径(改成自己的) export JAVA_HOME=/usr/local/java/jdk1.8.0_181 export PATH=$JAVA_HOME/bin:$PATH \",\"6、配置文件生效\",\"source /etc/profile \",\"7、查看是否安装成功\",\"java -version \"]},{\"header\":\"3、linux安装 Nacos\",\"slug\":\"_3、linux安装-nacos\",\"contents\":[\"1、从官网地址下载最新安装包：https://github.com/alibaba/nacos/releases\",\"nacos-server-2.0.3.tar.gz \",\"2、查看官方安装教程：https://nacos.io/zh-cn/docs/quick-start.html\",\"3、解压nacos\",\"tar -zvxf nacos-server-2.0.3.tar.gz \",\"4、更改配置\",\"server.port=8848 #*************** Network Related Configurations ***************# ### If prefer hostname over ip for Nacos server addresses in cluster.conf: # nacos.inetutils.prefer-hostname-over-ip=false ### Specify local server's IP: # nacos.inetutils.ip-address= #*************** Config Module Related Configurations ***************# ### If use MySQL as datasource: spring.datasource.platform=mysql ### Count of DB: db.num=1 ### Connect URL of DB: db.url.0=jdbc:mysql://127.0.0.1:3306/ry-config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC db.user=root db.password=123456 ### Connection pool configuration: hikariCP db.pool.config.connectionTimeout=30000 db.pool.config.validationTimeout=10000 db.pool.config.maximumPoolSize=20 db.pool.config.minimumIdle=2 \",\"5、启动命令(standalone代表着单机模式运行，非集群模式)\",\"cd nacos/bin sh startup.sh -m standalone \",\"6、关闭服务器\",\"sh shutdown.sh \",\"备注：\",\"2.0以上版本记得多开放端口\",\"Nacos2.0版本相比1.X新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port)基础上，进行一定偏移量自动生成。\",\"端口\",\"与主端口的偏移量\",\"描述\",\"9848\",\"1000\",\"客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求\",\"9849\",\"1001\",\"服务端gRPC请求服务端端口，用于服务间同步等\"]},{\"header\":\"4、linux安装 maven\",\"slug\":\"_4、linux安装-maven\",\"contents\":[\"去官网下载对应的版本：https://maven.apache.org/download.cgi\",\"将下载好的 maven压缩文件置于指定的目录下：cd /root/tools/\",\"解压\",\"tar -zvxf apache-maven-3.8.4-bin.tar.gz \",\"配置环境变量，vi /etc/profile\",\"# 添加配置 export MAVEN_HOME=/root/tools/apache-maven-3.8.4 export PATH=$MAVEN_HOME/bin:$PATH \",\"重载配置文件\",\"source /etc/profile \",\"查看是否安装成功\",\"mvn -v \",\"配置settings，找到maven的安装路径下的conf/settings.xml文件\",\"<!--配置仓库地址--> <localRepository>/root/tools/apache-maven-3.8.4/repo</localRepository> <!--配置镜像仓库地址--> <mirror> <id>alimaven</id> <name>aliyun maven</name> <url>http://maven.aliyun.com/nexus/content/groups/public/</url> <mirrorOf>central</mirrorOf> </mirror> \",\"查看是否配置成功，执行命令，是否从镜像仓库下载\",\"mvn help:system \"]},{\"header\":\"5、linux安装Nginx\",\"slug\":\"_5、linux安装nginx\",\"contents\":[]},{\"header\":\"1、yum安装\",\"slug\":\"_1、yum安装\",\"contents\":[\"安装命令\",\"yum -y install nginx # 安装 nginx yum remove nginx # 卸载 nginx \",\"配置文件路径，/etc/nginx目录下\",\"配置nginx\",\"systemctl enable nginx # 设置开机启动 service nginx start # 启动 nginx 服务 service nginx stop # 停止 nginx 服务 service nginx restart # 重启 nginx 服务 service nginx reload # 重新加载配置，一般是在修改过 nginx 配置文件时使用。 \"]},{\"header\":\"6、linux安装MySQL\",\"slug\":\"_6、linux安装mysql\",\"contents\":[\"检测系统是否自带安装 MySQL\",\"rpm -qa | grep mysql \",\"卸载\",\"rpm -e mysql // 普通删除模式 rpm -e --nodeps mysql // 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除 \"]},{\"header\":\"7、Linux安装RabbitMQ\",\"slug\":\"_7、linux安装rabbitmq\",\"contents\":[\"我使用的Linux搭建，搭建流程可参考官网：https://www.rabbitmq.com/install-rpm.html\",\"查看系统版本号，Rabbitmq 对 Erlang 有版本要求，不能使用太旧的Erlang版本：https://www.rabbitmq.com/which-erlang.html\",\"Erlang下载地址：https://github.com/rabbitmq/erlang-rpm/releases\"]},{\"header\":\"1、下载rpm包\",\"slug\":\"_1、下载rpm包\",\"contents\":[\"erlang-23.3.4.8-1.el7.x86_64.rpm\",\"rabbitmq-server-3.10.5-1.el8.noarch.rpm\"]},{\"header\":\"2、上传至服务器\",\"slug\":\"_2、上传至服务器\",\"contents\":[\"#创建存放目录 mkdir -p /usr/rabbitmq \"]},{\"header\":\"3、安装文件\",\"slug\":\"_3、安装文件\",\"contents\":[\"[root@xiaobear rabbitmq]# rpm -Uvih erlang-25.0.2-1.el8.x86_64.rpm 警告：erlang-25.0.2-1.el8.x86_64.rpm: 头V4 RSA/SHA256 Signature, 密钥 ID cc4bbe5b: NOKEY 错误：依赖检测失败： libcrypto.so.1.1()(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libcrypto.so.1.1(OPENSSL_1_1_0)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libcrypto.so.1.1(OPENSSL_1_1_1)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libstdc++.so.6(CXXABI_1.3.9)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libtinfo.so.6()(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libz.so.1(ZLIB_1.2.7.1)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 \",\"PS：这是因为下载版本的问题，el8下载的是8的版本，改回下载7的就可以了\",\"我下载的版本：erlang-23.3.4.8-1.el7.x86_64.rpm\",\"[root@xiaobear rabbitmq]# rpm -Uivh erlang-23.3.4.8-1.el7.x86_64.rpm 警告：erlang-23.3.4.8-1.el7.x86_64.rpm: 头V4 RSA/SHA256 Signature, 密钥 ID cc4bbe5b: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:erlang-23.3.4.8-1.el7 ################################# [100%] [root@xiaobear rabbitmq]# yum install socat -y \",\"查看版本\",\"# 测试 erl -version \",\"rabbitmq 在安装过程中需要依赖socat这个插件，需要先安装\",\"[root@xiaobear rabbitmq]# rpm -ivh rabbitmq-server-3.10.5-1.el8.noarch.rpm 警告：rabbitmq-server-3.10.5-1.el8.noarch.rpm: 头V4 RSA/SHA512 Signature, 密钥 ID 6026dfca: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:rabbitmq-server-3.10.5-1.el8 ################################# [100%] [root@xiaobear rabbitmq]# \"]},{\"header\":\"4、常用命令\",\"slug\":\"_4、常用命令\",\"contents\":[\"#开机启动 chkconfig rabbitmq-server on会转发到下面命令 systemctl enable rabbitmq-server.service # 启动服务 systemctl start rabbitmq-server # 查看服务状态，running表示启动成功 systemctl status rabbitmq-server.service # 开机自启动 systemctl enable rabbitmq-server # 停止服务 systemctl stop rabbitmq-server \"]},{\"header\":\"5、安装Web管理插件\",\"slug\":\"_5、安装web管理插件\",\"contents\":[\"rabbitmq-plugins enable rabbitmq_management #安装完成后，重启服务 systemctl restart rabbitmq-server \"]},{\"header\":\"6、访问web页面\",\"slug\":\"_6、访问web页面\",\"contents\":[\"访问地址：服务器 IP+端口号（默认15672）,若没有反应，请开放端口，执行下面命令\",\"# 防火墙开放15672端口 firewall-cmd --zone=public --add-port=15672/tcp --permanent firewall-cmd --reload \",\"注意：\",\"在对应服务器（阿里云，腾讯云等）的安全组中开放15672端口（rabbitmq默认端口号），5672端口后续程序需要使用也要开放\",\"rabbitmq有一个默认账号和密码都是：guest默认情况只能在 localhost本计下访问，所以需要添加一个远程登录的用户\"]},{\"header\":\"7、添加用户\",\"slug\":\"_7、添加用户\",\"contents\":[\"创建账号\",\"rabbitmqctl add_user admin admin123 \",\"分配角色\",\"rabbitmqctl set_user_tags admin administrator \",\"用户操作权限分四种级别：\",\"administrator：可以登录控制台、查看所有信息、可以对 rabbitmq进行管理\",\"monitoring：监控者 登录控制台，查看所有信息\",\"policymaker：策略制定者 登录控制台，指定策略\",\"managment 普通管理员 登录控制台\",\"设置权限\",\"set_permissions [-p <vhostpath>] <user> <conf> <write> <read>\",\"用户 user_admin 具有/vhost1 这个 virtual host 中所有资源的配置、写、读权限当前用户和角色\",\"rabbitmqctl set_permissions -p \\\"/\\\" admin \\\".*\\\" \\\".*\\\" \\\".*\\\" \",\"再次访问登录，即可成功\"]},{\"header\":\"8、重置命令\",\"slug\":\"_8、重置命令\",\"contents\":[\"#关闭应用的命令 rabbitmqctl stop_app #清除的命令 rabbitmqctl reset #重新启动命令 rabbitmqctl start_app \"]},{\"header\":\"9、其他命令\",\"slug\":\"_9、其他命令\",\"contents\":[\"# 添加账号、密码 rabbitmqctl add_user # 设置账号为管理员 rabbitmqctl set_user_tags 账号 administrator # 修改账号密码 rabbitmqctl change_password Username Newpassword # 查看用户清单 rabbitmqctl list_users # 添加账号查看资源的权限 rabbitmqctl set_permissions -p / 用户名 \\\".*\\\"\\\".*\\\"\\\".*\\\" \"]},{\"header\":\"8、Linux安装Tomcat\",\"slug\":\"_8、linux安装tomcat\",\"contents\":[\"下载tomcat安装包，链接\",\"解压缩安装包，安装Tomcat\",\"tar -zxvf apache-tomcat-9.0.68.tar.gz \",\"进入Tomcat服务bin目录\",\"cd apache-tomcat-9.0.68/bin \",\"启动Tomcat\",\"[root@docker bin]# bash startup.sh Using CATALINA_BASE: /usr/xiaobear-study-tool/apache-tomcat-9.0.68 Using CATALINA_HOME: /usr/xiaobear-study-tool/apache-tomcat-9.0.68 Using CATALINA_TMPDIR: /usr/xiaobear-study-tool/apache-tomcat-9.0.68/temp Using JRE_HOME: /root/jre1.8.0_301 Using CLASSPATH: /usr/xiaobear-study-tool/apache-tomcat-9.0.68/bin/bootstrap.jar:/usr/xiaobear-study-tool/apache-tomcat-9.0.68/bin/tomcat-juli.jar Using CATALINA_OPTS: Tomcat started. \",\"访问tomcat，是否启动成功，http://ip:8080\",\"若是没反应，则检查是否开启端口号，防火墙，防火墙专题\"]},{\"header\":\"9、Linux安装CAT链路追踪\",\"slug\":\"_9、linux安装cat链路追踪\",\"contents\":[\"下载CAT源码, 如GIT方式过慢， 可用Download Zip 方式打包下载\",\"构建CAT服务war包 可以导入IDEA工程进行编译， 或者直接用MAVEN进行编译，这里编译的目录是：cat-home 将MAVEN加入到系统PATH， 执行mvn命令：\",\"mvn clean install -Dmaven.test.skip=true \",\"创建数据库\",\"先创建CAT数据库， 采用utf8mb4字符集， 再导入{CAT_SRC}/script/目录下的CatApplication.sql脚本。\",\"创建目录，因为cat需要/data的全部权限，运行盘下的/data/appdatas/cat和/data/applogs/cat有读写权限\",\"mkdir /data/appdatas/cat/ chmod -R 777 /data/appdatas/cat/ \",\"将打包好的war包传入tomcat的webapp下\",\"创建客户端的配置/data/appdatas/cat/client.xml (客户端使用)\",\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?> <config mode=\\\"client\\\"> <servers> <server ip=\\\"127.0.0.1\\\" port=\\\"2280\\\" http-port=\\\"8080\\\"/> </servers> </config> \",\"创建服务端的配置/data/appdatas/cat/datasources.xml (服务端使用)\",\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?> <data-sources> <data-source id=\\\"cat\\\"> <maximum-pool-size>3</maximum-pool-size> <connection-timeout>1s</connection-timeout> <idle-timeout>10m</idle-timeout> <statement-cache-size>1000</statement-cache-size> <properties> <driver>com.mysql.jdbc.Driver</driver> <url><![CDATA[jdbc:mysql://127.0.0.1:3306/cat]]></url> <!-- 请替换为真实数据库URL及Port --> <user>root</user> <!-- 请替换为真实数据库用户名 --> <password>123456</password> <!-- 请替换为真实数据库密码 --> <connectionProperties><![CDATA[useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&socketTimeout=120000]]></connectionProperties> </properties> </data-source> </data-sources> \",\"Tomcat配置，修改tomcat conf 目录下 server.xml， 检查好端口没有被其他程序占用。\",\"<Connector port=\\\"8080\\\" protocol=\\\"HTTP/1.1\\\" URIEncoding=\\\"utf-8\\\" connectionTimeout=\\\"20000\\\" redirectPort=\\\"8443\\\" /> <!-- 增加 URIEncoding=\\\"utf-8\\\" --> \",\"如需内存不足，需作调整\",\"CATALINA_OPTS=\\\"-Xms1024m -Xmx2048m -Xss1024K -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=1024m\\\" \",\"启动\",\"进入tomcat bin目录下\",\"bash startup.sh \",\"访问cat客户端\",\"打开控制台地址： http://ip:8080/cat/s/config?op=routerConfigUpdate\",\"默认用户名：admin 默认密码：admin\",\"正常可以看到后台：\"]}]},\"/dev-necessary/install/windows.html\":{\"title\":\"Windows安装\",\"contents\":[{\"header\":\"1、安装mysql(非安装版本)\",\"slug\":\"_1、安装mysql-非安装版本\",\"contents\":[\"1、将下载的zip文件解压至相应目录，例如C:\\\\web\\\\mysql-8.0.11\",\"2、配置mysql的配置文件，在解压的文件下C:\\\\web\\\\mysql-8.0.11创建my.ini配置文件\",\"[mysqld] # 设置 3307 端口 port=3307 # 设置 mysql 的安装目录 basedir=D:\\\\Java-tool\\\\mysql\\\\mysql-8.0.26-winx64-3307 # 设置 mysql 数据库的数据的存放目录 datadir=D:\\\\Java-tool\\\\mysql\\\\mysql-8.0.26-winx64-3307\\\\data # 允许最大连接数 max_connections=200 # 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统 max_connect_errors=10 # 服务端使用的字符集默认为 UTF8 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # 默认使用“mysql_native_password”插件认证 default_authentication_plugin=mysql_native_password sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION' [mysql] # 设置 mysql 客户端默认字符集 default-character-set=utf8 [client] # 设置 mysql 客户端连接服务端时默认使用的端口 port=3307 default-character-set=utf8 \",\"3、以管理员身份运行cmd，切换目录\",\"cd C:\\\\web\\\\mysql-8.0.26\\\\bin \",\"4、初始化数据库\",\"mysqld --initialize --console \",\"执行完成后，会输出 root 用户的初始默认密码,如\",\"2021-12-17T01:33:30.597838Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: q(rGq1!u2P)8 \",\"随机密码：q(rGq1!u2P)8\",\"5、安装mysql\",\"mysqld install \",\"6、启动mysql\",\"net start mysql \",\"7、登录mysql\",\"mysql -u root -p \",\"输入密码即可❗\",\"忘记密码教程：\"]},{\"header\":\"2、安装JDK\",\"slug\":\"_2、安装jdk\",\"contents\":[\"1、下载JDk\",\"🔗下载地址，例如：🇳🇦 jdk-8u311-windows-x64.exe\",\"2、双击默认安装，可自行更改安装地址\",\"3、配置环境变量\",\"配置环境变量：右击“我的电脑”-->\\\"属性\\\"-->\\\"高级系统设置\\\"-->\\\"环境变量\\\"\",\"#1、JAVA_HOME环境变量\",\"配置方法：在系统变量里点击新建，变量名填写JAVA_HOME，变量值填写JDK的安装路径。（根据自己的安装路径填写）\",\"JAVA_HOME C:\\\\Program Files\\\\Java\\\\jdk1.8.0_251 \",\"#2、CLASSPATH环境变量\",\"配置方法： 新建CLASSPATH变量，变量值为：.;%JAVA_HOME%\\\\lib\\\\dt.jar;%JAVA_HOME%\\\\lib\\\\tools.jar;。CLASSPATH变量名字，可以大写也可以小写。注意不要忘记前面的点和中间的分号。且要在英文输入的状态下的分号和逗号。\",\"CLASSPATH .;%JAVA_HOME%\\\\lib\\\\dt.jar;%JAVA_HOME%\\\\lib\\\\tools.jar; \",\"#3、path环境变量\",\"在系统变量里找到Path变量，这是系统自带的，不用新建。双击Path，由于原来的变量值已经存在，故应在已有的变量后加上;%JAVA_HOME%\\\\bin;%JAVA_HOME%\\\\jre\\\\bin。注意前面的分号。\",\"4、测试\",\"检验是否配置成功 运行cmd 分别输入java，javac， java -version （java 和 -version 之间有空格）。 \"]},{\"header\":\"3、安装maven\",\"slug\":\"_3、安装maven\",\"contents\":[\"1、maven下载\",\"Maven 下载地址：http://maven.apache.org/download.cgi\",\"2、解压至目录\",\"E:\\\\Maven\\\\apache-maven-3.3.9\",\"3、添加环境变量\",\"添加环境变量MAVEN_HOME\",\"右键 \\\"计算机\\\"，选择 \\\"属性\\\"，之后点击 \\\"高级系统设置\\\"，点击\\\"环境变量\\\"，来设置环境变量，有以下系统变量需要配置： 新建系统变量 MAVEN_HOME，变量值：E:\\\\Maven\\\\apache-maven-3.3.9 \",\"编辑Path\",\"编辑系统变量 Path，添加变量值：;%MAVEN_HOME%\\\\bin \",\"4、测试\",\"检验是否配置成功 运行cmd 输入mvn -v。 \"]},{\"header\":\"4、安装Redis\",\"slug\":\"_4、安装redis\",\"contents\":[\"1、官网下载\",\"redis下载地址，解压至相应目录\",\"2、进入解压目录，启动redis\",\"redis-server redis.windows.conf \",\"3、安装服务至windows\",\"redis-server --service-install redis.windows.conf \",\"4、安装后的启动服务\",\"redis-server --service-start \",\"5、常用命令\",\"卸载服务：redis-server --service-uninstall 开启服务：redis-server --service-start 停止服务：redis-server --service-stop \"]},{\"header\":\"5、安装Oracle\",\"slug\":\"_5、安装oracle\",\"contents\":[\"下载安装包\",\"Oracle 11g 官网下载地址：点击下载\",\"解压\",\"点击setup.exe文件安装\",\"配置更新安全\",\"这里可以不填写电子邮件，点击是即可\",\"安装选项，选择第一个【创建和配置数据库】，然后单击【下一步】按钮\",\"系统类，选择桌面类\",\"典型安装\",\"选择数据安装目录，填写管理员密码\",\"接下来自动安装数据库，大概5分钟左右安装完毕\",\"将文件复制到相应的文件夹并安装 Oracle 组件和服务。完成所需的时间需要几分钟，请耐心等待\",\"等待完成\"]}]},\"/dev-necessary/common/TreeStructure.html\":{\"title\":\"\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"平时开发中我们经常遇到需要构建树形结构返回给前端，比如菜单结构、部门列表、文件结构、地区等具有层级关系的，都可以构建，我们一般想到的就是利用递归来循环构建；现在，就我个人解决的方法如下：\",\"原始递归\",\"利用Java 8 Stream流进行处理（原理还是递归）\",\"Stream流升级构建\"]},{\"header\":\"场景构建\",\"slug\":\"场景构建\",\"contents\":[\"public class TreeSelect implements Serializable { /** 节点ID */ private Long id; /** 节点名称 */ private String label; /** 父ID */ private Long parentId; /** 子节点 */ private List<TreeSelect> children; public TreeSelect() { } public TreeSelect(Long id, String label, Long parentId) { this.id = id; this.label = label; this.parentId = parentId; } public TreeSelect(TreeSelect treeSelect) { this.id = treeSelect.getId(); this.label = treeSelect.getLabel(); this.children = treeSelect.getChildren(); } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getLabel() { return label; } public void setLabel(String label) { this.label = label; } public List<TreeSelect> getChildren() { return children; } public void setChildren(List<TreeSelect> children) { this.children = children; } } \"]},{\"header\":\"1、原始递归构建树\",\"slug\":\"_1、原始递归构建树\",\"contents\":[\" /** * 构建前端所需要树结构 * * @param trees 列表 * @return 树结构列表 */ public static List<TreeSelect> buildDeptTree(List<TreeSelect> trees) { List<TreeSelect> returnList = new ArrayList<TreeSelect>(); List<Long> tempList = new ArrayList<Long>(); for (TreeSelect dept : trees) { tempList.add(dept.getId()); } for (Iterator<TreeSelect> iterator = trees.iterator(); iterator.hasNext();) { TreeSelect treeSelect = (TreeSelect) iterator.next(); // 如果是顶级节点, 遍历该父节点的所有子节点 if (!tempList.contains(treeSelect.getParentId())) { recursionFn(trees, treeSelect); returnList.add(treeSelect); } } if (returnList.isEmpty()) { returnList = trees; } return returnList; } /** * 递归列表 */ private static void recursionFn(List<TreeSelect> list, TreeSelect t) { // 得到子节点列表 List<TreeSelect> childList = getChildList(list, t); t.setChildren(childList); for (TreeSelect tChild : childList) { if (hasChild(list, tChild)) { recursionFn(list, tChild); } } } /** * 得到子节点列表 */ private static List<TreeSelect> getChildList(List<TreeSelect> list, TreeSelect t) { List<TreeSelect> tlist = new ArrayList<TreeSelect>(); for (TreeSelect n : list) { if (StringUtils.isNotNull(n.getParentId()) && n.getParentId().longValue() == t.getId().longValue()) { tlist.add(n); } } return tlist; } /** * 判断是否有子节点 */ private static boolean hasChild(List<TreeSelect> list, TreeSelect t) { return getChildList(list, t).size() > 0; } \"]},{\"header\":\"2、利用Java 8 Stream流进行处理（原理还是递归）\",\"slug\":\"_2、利用java-8-stream流进行处理-原理还是递归\",\"contents\":[\" public static List<TreeSelect> buildDeptTreeByStream(List<TreeSelect> trees){ //获取parentId = 0的根节点 List<TreeSelect> list = trees.stream().filter(item -> item.getParentId() == 0L).collect(Collectors.toList()); //根据parentId进行分组 Map<Long, List<TreeSelect>> map = trees.stream().collect(Collectors.groupingBy(TreeSelect::getParentId)); recursionFnTree(list, map); return list; } /** * 递归遍历节点 * @param list * @param map */ public static void recursionFnTree(List<TreeSelect> list, Map<Long, List<TreeSelect>> map){ for (TreeSelect treeSelect : list) { List<TreeSelect> childList = map.get(treeSelect.getId()); treeSelect.setChildren(childList); if (null != childList && 0 < childList.size()){ recursionFnTree(childList,map); } } } \"]},{\"header\":\"3、Stream流升级构建\",\"slug\":\"_3、stream流升级构建\",\"contents\":[\"//获取父节点 List<TreeSelect> collect = trees.stream().filter(m -> m.getParentId() == 0).map( (m) -> { m.setChildren(getChildrenList(m, trees)); return m; } ).collect(Collectors.toList()); /** * 获取子节点列表 * @param tree * @param list * @return */ public static List<TreeSelect> getChildrenList(TreeSelect tree, List<TreeSelect> list){ List<TreeSelect> children = list.stream().filter(item -> Objects.equals(item.getParentId(), tree.getId())).map( (item) -> { item.setChildren(getChildrenList(item, list)); return item; } ).collect(Collectors.toList()); return children; } \",\"PS：个人还是比较倾向用Stream流构建树形结构，节省代码量还通俗易懂！！！Stream在实际开发过程中，运用得体的话，既能节省代码量，还能提高效率，但是复杂的流式处理数据也会让代码看起来不易理解！\"]}]},\"/dev-necessary/problem/Linux.html\":{\"title\":\"Linux常见的问题\",\"contents\":[{\"header\":\"1、修改jar里的文件\",\"slug\":\"_1、修改jar里的文件\",\"contents\":[\"使用jar tvf jar名称 | grep 目标文件名 查询出目标文件在war包中的目录\",\"使用jar xvf jar名称 目标文件名(copy上面查出的全路径) 将目标文件及所在war包中的目录解压到当前路径\",\"修改目标文件的内容，或者将要新的目标文件替换掉提取出来的目标文件\",\"使用jar uvf jar名称 目标文件名（和步骤（2）中的目标文件名相同） 将新目标文件替换到 jar包中\"]},{\"header\":\"1、具体\",\"slug\":\"_1、具体\",\"contents\":[\"这里以blog.jar为例进行操作\",\"1、首先，查找你需要修改的文件\",\"jar tvf blog-0.0.1-SNAPSHOT.jar | grep _fragments.html \",\"2、解压文件到当前目录\",\"jar -xvf blog-0.0.1-SNAPSHOT.jar BOOT-INF/classes/templates/_fragments.html \",\"3、如果你有替换的文件就直接替换，没有则修改你需要修改的部分\",\"vim _fragments.html cp 文件 目标文件 \",\"4、将修改的新文件替换到jar包中\",\"jar -uvf blog-0.0.1-SNAPSHOT.jar BOOT-INF/classes/templates/_fragments.html \"]},{\"header\":\"2、关于centos 8下载依赖报错问题\",\"slug\":\"_2、关于centos-8下载依赖报错问题\",\"contents\":[\"Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist \",\"原因\",\"在2022年1月31日，CentOS团队终于从官方镜像中移除CentOS 8的所有包。\",\"CentOS 8已于2021年12月31日寿终正非，但软件包仍在官方镜像上保留了一段时间。现在他们被转移到https://vault.centos.org\",\"解决方法\",\"如果你仍然需要运行CentOS 8，你可以在/etc/yum.repos.d中更新一下源。使用vault.centos.org代替mirror.centos.org。\",\"sed -i -e \\\"s|mirrorlist=|#mirrorlist=|g\\\" /etc/yum.repos.d/CentOS-* sed -i -e \\\"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\\\" /etc/yum.repos.d/CentOS-* \"]},{\"header\":\"3、启动centos 没有ip，重启网卡失败，服务状态 Failed to start LSB: Bring up/down networking\",\"slug\":\"_3、启动centos-没有ip-重启网卡失败-服务状态-failed-to-start-lsb-bring-up-down-networking\",\"contents\":[\"解决方式：禁用NetworkManager\",\"systemctl stop NetworkManager\",\"systemctl disable NetworkManager\",\"systemctl restart network\"]}]},\"/dev-necessary/problem/MySQL.html\":{\"title\":\"MySQL常见的问题\",\"contents\":[{\"header\":\"1、mysql忘记密码解决方法\",\"slug\":\"_1、mysql忘记密码解决方法\",\"contents\":[\"1、停止mysql服务\",\"net stop mysql \",\"2、以管理员身份运行cmd\",\"进入mysql的bin目录下，开启跳过密码验证登录的mysql服务\",\"mysqld --console --skip-grant-tables --shared-memory \",\"这个界面不要关闭\",\"3、以管理员身份打开第二个CMD，进入mysql的bin目录下\",\"#第一步： mysql #刷新权限 flush privileges; #第二步，MySQL 5.7.6及更高版本 ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass'; #MySQL 5.7.5及更早版本 SET PASSWORD FOR 'root'@'localhost' = PASSWORD('MyNewPass'); \",\"4、关闭第一个CMD程序，启动mysql服务\",\"#启动mysql服务 net start mysql #登录 mysql -u root -p \"]},{\"header\":\"2、mysql数据库编码问题\",\"slug\":\"_2、mysql数据库编码问题\",\"contents\":[\"修改表的编码\",\"alter table 表名 character set utf8mb4; \",\"修改表中所有字段编码\",\"alter table 表名 convert to character set utf8mb4; \"]},{\"header\":\"3、MYSQL安装出现问题（The service already exists）\",\"slug\":\"_3、mysql安装出现问题-the-service-already-exists\",\"contents\":[\"以管理员运行cmd\",\"查询mysql服务\",\"sc query mysql \",\"删除mysql\",\"sc delete mysql \"]},{\"header\":\"4、mysql命令报“不是内部或外部命令”\",\"slug\":\"_4、mysql命令报-不是内部或外部命令\",\"contents\":[\"如果输入mysql命令报“不是内部或外部命令”，把mysql安装目录的bin目录配置到环境变量path中。如下：\"]},{\"header\":\"5、错误ERROR ：没有选择数据库就操作表格和数据\",\"slug\":\"_5、错误error-没有选择数据库就操作表格和数据\",\"contents\":[\"ERROR 1046 (3D000): No database selected\",\"解决方案一：就是使用“USE 数据库名;”语句，这样接下来的语句就默认针对这个数据库进行操作\",\"解决方案二：就是所有的表对象前面都加上“数据库.”\"]},{\"header\":\"6、命令行客户端的字符集问题\",\"slug\":\"_6、命令行客户端的字符集问题\",\"contents\":[\"mysql> INSERT INTO t_stu VALUES(1,'张三','男'); ERROR 1366 (HY000): Incorrect string value: '\\\\xD5\\\\xC5\\\\xC8\\\\xFD' for column 'sname' at row 1 \",\"原因：服务器端认为你的客户端的字符集是utf-8，而实际上你的客户端的字符集是GBK。\",\"查看所有字符集：SHOW VARIABLES LIKE 'character_set_%';\",\"解决方案，设置当前连接的客户端字符集 “SET NAMES GBK;”\"]},{\"header\":\"7、修改数据库和表的字符编码\",\"slug\":\"_7、修改数据库和表的字符编码\",\"contents\":[\"修改编码：\",\"（1)先停止服务，（2）修改my.ini文件（3）重新启动服务\",\"说明：\",\"如果是在修改my.ini之前建的库和表，那么库和表的编码还是原来的Latin1，要么删了重建，要么使用alter语句修改编码。\",\"mysql> create database 0728db charset Latin1; Query OK, 1 row affected (0.00 sec) \",\"mysql> use 0728db; Database changed \",\"mysql> create table student (id int , name varchar(20)) charset Latin1; Query OK, 0 rows affected (0.02 sec) mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec) \",\"mysql> alter table student charset utf8; #修改表字符编码为UTF8 Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) CHARACTER SET latin1 DEFAULT NULL, #字段仍然是latin1编码 PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) mysql> alter table student modify name varchar(20) charset utf8; #修改字段字符编码为UTF8 Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) \",\"mysql> show create database 0728db;; +--------+-----------------------------------------------------------------+ |Database| Create Database | +------+-------------------------------------------------------------------+ |0728db| CREATE DATABASE `0728db` /*!40100 DEFAULT CHARACTER SET latin1 */ | +------+-------------------------------------------------------------------+ 1 row in set (0.00 sec) mysql> alter database 0728db charset utf8; #修改数据库的字符编码为utf8 Query OK, 1 row affected (0.00 sec) mysql> show create database 0728db; +--------+-----------------------------------------------------------------+ |Database| Create Database | +--------+-----------------------------------------------------------------+ | 0728db | CREATE DATABASE `0728db` /*!40100 DEFAULT CHARACTER SET utf8 */ | +--------+-----------------------------------------------------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"8、MySQL 8以上的版本的数据导入到5.7的编码问题\",\"slug\":\"_8、mysql-8以上的版本的数据导入到5-7的编码问题\",\"contents\":[\"提示**[ERR] 1273 - Unknown collation: ‘utf8mb4_0900_ai_ci’** ，原因是数据文件版本是SQL8，而本地数据库是SQL5.7\",\"方法一：数据库升级到8\",\"方法二：利用编辑器进行批量替换，把sql文件中所有的utf8mb4换成utf8 ,utf8mb4_0900_ai_ci换成utf8_general_ci\"]},{\"header\":\"9、使用 Group By 分组报错\",\"slug\":\"_9、使用group-by分组报错\",\"contents\":[\"报错信息：ERROR 1055 (42000): Expression #2 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'school_db.SC.Cid' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by\",\"ONLY_FULL_GROUP_BY的意思是：对于GROUP BY聚合操作，如果在SELECT中的列，没有在GROUP BY中出现，那么这个SQL是不合法的，因为列不在GROUP BY从句中，也就是说查出来的列必须在group by后面出现否则就会报错，或者这个字段出现在聚合函数里面。\",\"解决\",\"查看sql_mode参数命令，查看是否存在ONLY_FULL_GROUP_BY这个配置\",\"-- 查看ONLY_FULL_GROUP_BY 校验规则是否开启 SELECT @@GLOBAL.sql_mode; SELECT @@SESSION.sql_mode; \",\"去除校验 (1)第一种，通过命令去除\",\"-- 关闭only_full_group_by的规则校验 set @@GLOBAL.sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; set @@SESSION.sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; \",\"(2)第二种，通过修改配置文件my.ini\",\"在 [mysqld] 下面添加代码： sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION \",\"重启mysql服务\"]}]},\"/interview/cheats/hr.html\":{\"title\":\"2、HR的必备问题\",\"contents\":[{\"header\":\"1、请你自我介绍一下你自己？\",\"slug\":\"_1、请你自我介绍一下你自己\",\"contents\":[\"**回答提示：**一般人回答这个问题过于平常，只说姓名、年龄、爱好、工作经验，这些在简历上都有。\",\"其实，企业最希望知道的是求职者能否胜任工作，包括：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成\",\"功的事，主要的成就等，这些都可以和学习无关，也可以和学习有关，但要突出积极的个性和做事的能力，说得合情合理企业\",\"才会相信。企业很重视一个人的礼貌，求职者要尊重考官，在回答每个问题之后都说一句“谢谢”，企业喜欢有礼貌的求职者。\",\"样本\",\"大家好，我叫小华，毕业于XXX学校xxx专业，我先后参加了3个项目。在项目中，我通过什么技术，解决了什么样的问题。这里可一次说明；然后，目前我正在学习或研究什么技术，\"]},{\"header\":\"2、你觉得你个性上最大的优点是什么？\",\"slug\":\"_2、你觉得你个性上最大的优点是什么\",\"contents\":[\"**回答提示：**沉着冷静、条理清楚、立场坚定、顽强向上、乐于助人和关心他人、适应能力和幽默感、乐观和友爱。\"]},{\"header\":\"3、说说你最大的缺点？\",\"slug\":\"_3、说说你最大的缺点\",\"contents\":[\"**回答提示：**这个问题企业问的概率很大，通常不希望听到直接回答的缺点是什么等，如果求职者说自己小心眼、爱忌妒人、非常懒、脾气大、工作效率低，企业肯定不会录用你。\",\"绝对不要自作聪明地回答“我最大的缺点是过于追求完美”，有的人以为这样回答会显得自己比较出色，但事实上，他已经岌岌可危了。企业喜欢求职者从自己的优点说起，中间加一些小缺点，最后再把问题转回到优点上，突出优点的部分，企业喜欢聪明的求职者。\"]},{\"header\":\"4、你对加班的看法？\",\"slug\":\"_4、你对加班的看法\",\"contents\":[\"**回答提示：**实际上好多公司问这个问题，并不证明一定要加班，只是想测试你是否愿意为公司奉献。\",\"样本\",\"如果是工作需要我会义不容辞加班，我现在单身，没有任何家庭负担，可以全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班。\"]},{\"header\":\"5、你对薪资的要求？\",\"slug\":\"_5、你对薪资的要求\",\"contents\":[\"**回答提示：**如果你对薪酬的要求太低，那显然贬低自己的能力；如果你对薪酬的要求太高，那又会显得你分量过重，公司受用不起。一些雇主通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱，他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。\",\"如果你自己必须说出具体数目，请不要说一个宽泛的范围，那样你将只能得到最低限度的数字。\",\"最好给出一个具体的数字，这样表明你已经对当今的人才市场作了调查，知道像自己这样学历的雇员有什么样的价值。\",\"样本一\",\"样本二\",\"我对工资没有硬性要求，我相信贵公司在处理我的问题上会友善合理。我注重的是找对工作机会，所以只要条件公平，我则不会计较太多。\",\"我受过系统的软件编程的训练，不需要进行大量的培训，而且我本人也对编程特别感兴趣。因此，我希望公司能根据我的情况和市场标准的水平，给我合理的薪水。\"]},{\"header\":\"6、你的职业规划？\",\"slug\":\"_6、你的职业规划\",\"contents\":[\"**回答提示：**这是每一个应聘者都不希望被问到的问题，但是几乎每个人都会被问到，比较多的答案是“管理者”。但是近几年来，许多公司都已经建立了专门的技术途径。这些工作地位往往被称作“顾问”、“参议技师”或“高级软件工程师”等等。\",\"当然，说出其他一些你感兴趣的职位也是可以的，比如产品销售部经理，生产部经理等一些与你的专业有相关背景的工作。要知道，考官总是喜欢有进取心的应聘者，此时如果说“不知道”，或许就会使你丧失一个好机会。最普通的回答应该是“我准备在技术领域有所作为”或“我希望能按照公司的管理思路发展”。\"]},{\"header\":\"7、你还有什么问题要问吗？\",\"slug\":\"_7、你还有什么问题要问吗\",\"contents\":[\"**回答提示：**企业的这个问题看上去可有可无，其实很关键，企业不喜欢说“没问题”的人，因为其很注重员工的个性和创新能力。企业不喜欢求职者问个人福利之类的问题，如果有人这样问：贵公司对新入公司的员工有没有什么培训项目，我可以参加吗？或者说贵公司的晋升机制是什么样的？企业将很欢迎，因为体现出你对学习的热情和对公司的忠诚度以及你的上进心。在其中适当问一些福利也可。\"]},{\"header\":\"8、如果通过这次面试我们单位录用了你，但工作一段时间却发现你根本不适合这个职位，你怎么办？\",\"slug\":\"_8、如果通过这次面试我们单位录用了你-但工作一段时间却发现你根本不适合这个职位-你怎么办\",\"contents\":[\"**回答提示：**一段时间发现工作不适合我，有两种情况：\",\"①如果你确实热爱这个职业，那你就要不断学习，虚心向领导和同事学习业务知识和处事经验，了解这个职业的精神内涵和职业要求，力争减少差距；\",\"②你觉得这个职业可有可无，那还是趁早换个职业，去发现适合你的，你热爱的职业，那样你的发展前途也会大点，对单位和个人都有好处。\"]},{\"header\":\"9、在完成某项工作时，你认为领导要求的方式不是最好的，自己还有更好的方法，你应该怎么做？\",\"slug\":\"_9、在完成某项工作时-你认为领导要求的方式不是最好的-自己还有更好的方法-你应该怎么做\",\"contents\":[\"回答提示：\",\"原则上我会尊重和服从领导的工作安排，同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法。\",\"如果领导没有采纳我的建议，我也同样会按领导的要求认真地去完成这项工作。\",\"还有一种情况，假如领导要求的方式违背原则，我会坚决提出反对意见，如领导仍固执己见，我会毫不犹豫地再向上级领导反映。\"]},{\"header\":\"10、如果你的工作出现失误，给本公司造成经济损失，你认为该怎么办？\",\"slug\":\"_10、如果你的工作出现失误-给本公司造成经济损失-你认为该怎么办\",\"contents\":[\"回答提示：\",\"我本意是为公司努力工作，如果造成经济损失，我认为首要的问题是想方设法去弥补或挽回经济损失。如果我无能力负责，希望单位帮助解决。\",\"分清责任，各负其责，如果是我的责任，我甘愿受罚；如果是一个我负责的团队中别人的失误，也不能幸灾乐祸，作为一个团队，需要互相提携共同完成工作，安慰同事并且帮助同事查找原因总结经验。\",\"总结经验教训，一个人的一生不可能不犯错误，重要的是能从自己的或者是别人的错误中吸取经验教训，并在今后的工作中避免发生同类的错误。检讨自己的工作方法、分析问题的深度和力度是否不够，以致出现了本可以避免的错误。\"]},{\"header\":\"11、谈谈你对跳槽的看法？\",\"slug\":\"_11、谈谈你对跳槽的看法\",\"contents\":[\"回答提示：\",\"正常的“跳槽”能促进人才合理流动，应该支持。\",\"频繁的跳槽对单位和个人双方都不利，应该反对。\"]},{\"header\":\"12、工作中你难以和同事、上司相处，你该怎么办？\",\"slug\":\"_12、工作中你难以和同事、上司相处-你该怎么办\",\"contents\":[\"回答提示：\",\"我会服从领导的指挥，配合同事的工作。\",\"我会从自身找原因，仔细分析是不是自己工作做得不好让领导不满意，同事看不惯。还要看看是不是为人处世方面做得不好，\",\"如果是这样的话 我会努力改正。\",\"如果我找不到原因，我会找机会跟他们沟通，请他们指出我的不足，有问题就及时改正。\",\"作为优秀的员工，应该时刻以大局为重，即使在一段时间内，领导和同事对我不理解，我也会做好本职工作，虚心向他们学习，我相信，他们会看见我在努力，总有一天会对我微笑的。\"]},{\"header\":\"13、你对于我们公司了解多少？\",\"slug\":\"_13、你对于我们公司了解多少\",\"contents\":[\"**回答提示：**在去公司面试前上网查一下该公司主营业务。如回答：贵公司有意改变策略，加强与国外大厂的 OEM 合作，自有品牌的部分则透过海外经销商。\"]},{\"header\":\"14、请说出你选择这份工作的动机？\",\"slug\":\"_14、请说出你选择这份工作的动机\",\"contents\":[\"**回答提示：**这是想知道面试者对这份工作的热忱及理解度，并筛选因一时兴起而来应试的人，如果是无经验者，可以强调“就算职种不同，也希望有机会发挥之前的经验”。\"]},{\"header\":\"15、你最擅长的技术方向是什么？\",\"slug\":\"_15、你最擅长的技术方向是什么\",\"contents\":[\"**回答提示：**说和你要应聘的职位相关的课程，表现一下自己的热诚没有什么坏处。\"]},{\"header\":\"16、你能为我们公司带来什么呢？\",\"slug\":\"_16、你能为我们公司带来什么呢\",\"contents\":[\"**回答提示：**企业很想知道未来的员工能为企业做什么，求职者应再次重复自己的优势，然后说：“就我的能力，我可以做一个优秀的员工在组织中发挥能力，给组织带来高效率和更多的收益”。企业喜欢求职者就申请的职位表明自己的能力，比如申请营销之类的职位，可以说：“我可以开发大量的新客户，同时，对老客户做更全面周到的服务，开发老客户的新需求和消费。”等等。\"]},{\"header\":\"17、最能概括你自己的三个词是什么？\",\"slug\":\"_17、最能概括你自己的三个词是什么\",\"contents\":[\"**回答提示：**我经常用的三个词是：适应能力强，有责任心和做事有始终，结合具体例子向主考官解释\"]},{\"header\":\"18、为什么要离职?\",\"slug\":\"_18、为什么要离职\",\"contents\":[\"**回答提示：**回答这个问题时一定要小心，就算在前一个工作受到再大的委屈，对公司有多少的怨言，都千万不要表现出来，尤其要避免对公司本身主管的批评，避免面试官的负面情绪及印象。\",\"建议此时最好的回答方式是将问题归咎在自己身上，例如觉得工作没有学习发展的空间，自己想在面试工作的相关产业中多加学习，或是前一份工作与自己的生涯规划不合等等，回答的答案最好是积极正面的。\",\"样本\",\"我希望能获得一份更好的工作，如果机会来临，我会抓住。我觉得目前的工作，已经达到顶峰，即沒有升迁机会。\"]},{\"header\":\"19、对工作的期望与目标何在？\",\"slug\":\"_19、对工作的期望与目标何在\",\"contents\":[\"**回答提示：**这是面试者用来评断求职者是否对自己有一定程度的期望、对这份工作是否了解的问题。对于工作有确实学习目标的人通常学习较快，对于新工作自然较容易进入状况，这时建议你，最好针对工作的性质找出一个确实的答案，\",\"如业务员的工作可以这样回答：“我的目标是能成为一个超级业务员，将公司的产品广泛的推销出去，达到最好的业绩成效；为\",\"了达到这个目标，我一定会努力学习，而我相信以我认真负责的态度，一定可以达到这个目标。”\",\"其他类的工作也可以比照这个方式来回答，只要在目标方面稍微修改一下就可以了。\"]},{\"header\":\"20、就你申请的这个职位，你认为你还欠缺什么？\",\"slug\":\"_20、就你申请的这个职位-你认为你还欠缺什么\",\"contents\":[\"**回答提示：**企业喜欢问求职者弱点，但精明的求职者一般不直接回答。\",\"他们希望看到这样的求职者：继续重复自己的优势，然后说：“对于这个职位和我的能力来说，我相信自己是可以胜任的，只是缺乏经验，这个问题我想我可以进入公司以后以最短的时间来解决，我的学习能力很强，我相信可以很快融入公司的企业文化，进入工作状态。”企业喜欢能够巧妙地躲过难题的求职者。\"]},{\"header\":\"21、你通常如何处理別人的批评？\",\"slug\":\"_21、你通常如何处理別人的批评\",\"contents\":[\"回答提示：\",\"沈默是金，不必说什么，否则情况更糟，不过我会接受建设性的批评。\",\"我会等大家冷靜下来再讨论。\"]},{\"header\":\"22、怎样对待自己的失败？\",\"slug\":\"_22、怎样对待自己的失败\",\"contents\":[\"**回答提示：**我们大家生来都不是十全十美的，我相信我有第二个机会改正我的错误。\"]},{\"header\":\"23、什么会让你有成就感？\",\"slug\":\"_23、什么会让你有成就感\",\"contents\":[\"回答提示：\",\"为贵公司竭力效劳，尽我所能，完成一个项目。\",\"尽自己的能力，解决每一个难题\"]},{\"header\":\"24、你为什么愿意到我们公司来工作？\",\"slug\":\"_24、你为什么愿意到我们公司来工作\",\"contents\":[\"**回答提示：**对于这个问题，你要格外小心，如果你已经对该单位作了研究，你可以回答一些详细的原因，\",\"“公司本身的高技术开发环境很吸引我。”\",\"“我同公司出生在同样的时代，我希望能够进入一家与我共同成长的公司。”、\",\"“你们公司一直都稳定发展，在近几年来在市场上很有竞争力。”、\",\"“我认为贵公司能够给我提供一个与众不同的发展道路。”\",\"这都显示出你已经做了一些调查，也说明你对自己的未来有了较为具体的远景规划。\"]},{\"header\":\"25、你和别人发生过争执吗？你是怎样解决的？\",\"slug\":\"_25、你和别人发生过争执吗-你是怎样解决的\",\"contents\":[\"**回答提示：**这是面试中最险恶的问题，其实是考官布下的一个陷阱，千万不要说任何人的过错，应知成功解决矛盾是一个协作团体中成员所必备的能力。假如你工作在一个服务行业，这个问题简直成了最重要的一个环节。你是否能获得这份工作，将取决于这个问题的回答。考官希望看到你是成熟且乐于奉献的。他们通过这个问题了解你的成熟度和处世能力。在没有外界干涉的情况下，通过妥协的方式来解决才是正确答案\"]},{\"header\":\"26、对这项工作，你有哪些可预见的困难？\",\"slug\":\"_26、对这项工作-你有哪些可预见的困难\",\"contents\":[\"回答提示：\",\"不宜直接说出具体的困难，否则可能令对方怀疑应聘者不行。\",\"可以尝试迂回战术，说出应聘者对困难所持有的态度——工作中出现一些困难是正常的，也是难免的，但是只要有坚忍不拔的毅力、良好的合作精神以及事前周密而充分的准备，任何困难都是可以克服。\",\"**分析：**一般问这个问题，面试者的希望就比较大了，因为已经在谈工作细节，但常规思路中的回答，又被面试官“骗”了。当面试官询问这个问题的时候，有两个目的。\",\"第一，看看应聘者是不是在行，说出的困难是不是在这个职位中一般都不可避免的问题。\",\"第二，是想看一下应聘者解决困难的手法对不对，及公司能否提供这样的资源。而不是想了解应聘者对困难的态度\"]},{\"header\":\"27、如果我录用你，你将怎样开展工作？\",\"slug\":\"_27、如果我录用你-你将怎样开展工作\",\"contents\":[\"回答提示：\",\"如果应聘者对于应聘的职位缺乏足够的了解，最好不要直接说出自己开展工作的具体办法。\",\"可以尝试采用迂回战术来回答，如“首先听取领导的指示和要求，然后就有关情况进行了解和熟悉，接下来制定一份近期的工作计划并报领导批准，最后根据计划开展工作。\",\"**分析：**这个问题的主要目的也是了解应聘者的工作能力和计划性、条理性，而且重点想要知道细节。如果向思路中所讲的迂回战术，面试官会认为回避问题，如果引导了几次仍然是回避的话，此人绝对不会录用了。\"]},{\"header\":\"28、你工作经验欠缺，如何能胜任这项工作？\",\"slug\":\"_28、你工作经验欠缺-如何能胜任这项工作\",\"contents\":[\"常规思路：\",\"如果招聘单位对应届毕业生的应聘者提出这个问题，说明招聘公司并不真正在乎“经验”，关键看应聘者怎样回答。\",\"对这个问题的回答最好要体现出应聘者的诚恳、机智、果敢及敬业。\",\"如“作为应届毕业生，在工作经验方面的确会有所欠缺，因此在读书期间我一直利用各种机会在这个行业里做兼职。我也发现，实际工作远比书本知识丰富、复杂。但我有较强的责任心、适应能力和学习能力，而且比较勤奋，所以在兼职中均能圆满完成各项工作，从中获取的经验也令我受益非浅。请贵公司放心，学校所学及兼职的工作经验使我一定能胜任这个职位。”\",\"**点评：**这个问题思路中的答案尚可，突出自己的吃苦能力和适应性以及学习能力（不是学习成绩）为好。\"]},{\"header\":\"29、您在前一家公司的离职原因是什么？\",\"slug\":\"_29、您在前一家公司的离职原因是什么\",\"contents\":[\"回答提示：\",\"最重要的是：应聘者要使找招聘单位相信，应聘者在过往的单位的“离职原因”在此家招聘单位里不存在。\",\"避免把“离职原因”说得太详细、太具体。\",\"不能掺杂主观的负面感受，如“太辛苦”、“人际关系复杂”、“管理太混乱”、“公司不重视人才”、“公司排斥我们某某的员工”等。\",\"但也不能躲闪、回避，如“想换换环境”、“个人原因”等。\",\"不能涉及自己负面的人格特征，如不诚实、懒惰、缺乏责任感、不随和等。\",\"尽量使解释的理由为应聘者个人形象添彩。\",\"相关例子：如“我离职是因为这家公司倒闭；我在公司工作了三年多，有较深的感情；从去年始，由于市场形势突变，公司的局面急转直下；到眼下这一步我觉得很遗憾，但还要面对显示，重新寻找能发挥我能力的舞台。”同一个面试问题并非只有一个答案，而同一个答案并不是在任何面试场合都有效，关键在应聘者掌握了规律后，对面试的具体情况进行把握，有意识地揣摩面试官提出问题的心理背景，然后投其所好。\",\"**分析：**除非是薪资太低，或者是最初的工作，否则不要用薪资作为理由。“求发展”也被考官听得太多，离职理由要根据每个人的真实离职理由来设计，但是在回答时一定要表现得真诚。实在想不出来的时候，家在外地可以说是因为家中有事，须请假几个月，公司又不可能准假，所以辞职，这个答案一般面试官还能接受\"]},{\"header\":\"30、为了做好你工作份外之事，你该怎样获得他人的支持和帮助？\",\"slug\":\"_30、为了做好你工作份外之事-你该怎样获得他人的支持和帮助\",\"contents\":[\"**回答提示：**每个公司都在不断变化发展的过程中，你当然希望你的员工也是这样。你希望得到那些希望并欢迎变化的人，因为这些人明白，为了公司的发展，变化是公司日常生活中重要组成部分。这样的员工往往很容易适应公司的变化，并会对变化做出积极的响应。\"]},{\"header\":\"31、如果你在这次面试中没有被录用，你怎么打算？\",\"slug\":\"_31、如果你在这次面试中没有被录用-你怎么打算\",\"contents\":[\"**回答提示：**现在的社会是一个竞争的社会，从这次面试中也可看出这一点，有竞争就必然有优劣，有成功必定就会有失败。往往成功的背后有许多的困难和挫折，如果这次失败了也仅仅是一次而已，只有经过经验经历的积累才能塑造出一个完全的成功者。我会从以下几个方面来正确看待这次失败：\",\"要敢于面对，面对这次失败不气馁，接受已经失去了这次机会就不会回头这个现实，从心理意志和精神上体现出对这次失败的抵抗力。要有自信，相信自己经历了这次之后经过努力一定能行，能够超越自我。\",\"善于反思，对于这次面试经验要认真总结，思考剖析，能够从自身的角度找差距。正确对待自己，实事求是地评价自己，辩证的\",\"看待自己的长短得失，做一个明白人。\",\"走出阴影，要克服这一次失败带给自己的心理压力，时刻牢记自己弱点，防患于未然，加强学习，提高自身素质。\",\"认真工作，回到原单位岗位上后，要实实在在、踏踏实实地工作，三十六行、行行出状元，争取在本岗位上做出一定的成绩。\",\"再接再厉，成为国家公务员一直是我的梦想，以后如果有机会我仍然后再次参加竞争。\"]},{\"header\":\"32、谈谈你过去做过的成功案例？(工作中遇到什么问题)\",\"slug\":\"_32、谈谈你过去做过的成功案例-工作中遇到什么问题\",\"contents\":[\"**回答提示：**举一个你最有把握的例子，把来龙去脉说清楚，而不要说了很多却没有重点。切忌夸大其词，把别人的功劳到说成自己的，很多主管为了确保要用的人是最适合的，会打电话向你的前一个主管征询对你的看法及意见，所以如果说谎，是很容易穿梆的。\"]},{\"header\":\"33、如何安排自己的时间？会不会排斥加班？\",\"slug\":\"_33、如何安排自己的时间-会不会排斥加班\",\"contents\":[\"**回答提示：**基本上，如果上班工作有效率，工作量合理的话，应该不太需要加班。可是我也知道有时候很难避免加班，加上现在工作都采用责任制，所以我会调配自己的时间，全力配合。\",\"**分析：**虽然不会有人心甘情愿的加班，但依旧要表现出高配合度的诚意。\"]},{\"header\":\"34、这个职务的期许？\",\"slug\":\"_34、这个职务的期许\",\"contents\":[\"**回答提示：**希望能借此发挥我的所学及专长，同时也吸收贵公司在这方面的经验，就公司、我个人而言，缔造“双赢”的局面。\",\"**分析：**回答前不妨先询问该公司对这项职务的责任认定及归属，因为每一家公司的状况不尽相同，以免说了一堆理想抱负却发现牛头不对马嘴。\"]},{\"header\":\"35、什么选择我们这家公司？\",\"slug\":\"_35、什么选择我们这家公司\",\"contents\":[\"**回答提示：**曾经在报章杂志看过关于贵公司的报道，与自己所追求的理念有志一同。而贵公司在业界的成绩也是有目共睹的，而且对员工的教育训练、升迁等也都很有制度。\",\"**分析：**去面试前先做功课，了解一下该公司的背景，让对方觉得你真的很有心想得到这份工作，而不只是探探路。\"]},{\"header\":\"36、谈谈如何适应办公室工作的新环境？\",\"slug\":\"_36、谈谈如何适应办公室工作的新环境\",\"contents\":[\"回答提示：\",\"办公室里每个人有各自的岗位与职责，不得擅离岗位。\",\"根据领导指示和工作安排，制定工作计划，提前预备，并按计划完成。\",\"多请示并及时汇报，遇到不明白的要虚心请教。\",\"抓间隙时间，多学习，努力提高自己的政治素质和业务水平。\"]},{\"header\":\"37、工作中学习到了些什么？\",\"slug\":\"_37、工作中学习到了些什么\",\"contents\":[\"**回答提示：**这是针对转职者提出的问题，建议此时可以配合面试工作的特点作为主要依据来回答，如业务工作需要与人沟通，便可举出之前工作与人沟通的例子，经历了哪些困难，学习到哪些经验，把握这些要点做陈述，就可以轻易过关了。\"]},{\"header\":\"38、除了本公司外，还应聘了哪些公司？\",\"slug\":\"_38、除了本公司外-还应聘了哪些公司\",\"contents\":[\"**回答提示：**很奇怪，这是相当多公司会问的问题，其用意是要概略知道应徵者的求职志向，所以这并非绝对是负面答案，就算不便说出公司名称，也应回答“销售同种产品的公司”，如果应聘的其他公司是不同业界，容易让人产生无法信任的感觉。\"]},{\"header\":\"39、何时可以到职？\",\"slug\":\"_39、何时可以到职\",\"contents\":[\"**回答提示：**大多数企业会关心就职时间，最好是回答“如果被录用的话，到职日可按公司规定上班”，但如果还未辞去上一个工作、上班时间又太近，似乎有些强人所难，因为交接至少要一个月的时间，应进一步说明原因，录取公司应该会通融的。\"]}]},\"/interview/database/MySQL.html\":{\"title\":\"MySQL面试题\",\"contents\":[{\"header\":\"1、数据库三范式是什么?\",\"slug\":\"_1、数据库三范式是什么\",\"contents\":[\"第一范式（1NF）：字段具有原子性,不可再分。(所有关系型数据库系统都满足第一范式数据库表中的字段都是单一属性的，不可再分)\",\"第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。要求数据库表中的每个实例或行必须可以被惟一地区分。通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键。\",\"满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。 所以第三范式具有如下特征： \",\"每一列只有一个值\",\"每一行都能区分。\",\"每一个表都不包含其他表已经包含的非主关键字信息。\"]},{\"header\":\"2、请简述常用的索引有哪些种类?\",\"slug\":\"_2、请简述常用的索引有哪些种类\",\"contents\":[\"从 功能逻辑 上说，索引主要有 4 种，分别是普通索引、唯一索引、主键索引、全文索引。\",\"按照 物理实现方式 ，索引可以分为 2 种：聚簇索引和非聚簇索引。\",\"按照 作用字段个数 进行划分，分成单列索引和联合索引\",\"普通索引: 即针对数据库表创建索引\",\"唯一索引: 与普通索引类似，不同的就是：MySQL数据库索引列的值必须唯一，但允许有空值\",\"主键索引: 它是一种特殊的唯一索引，不允许有空值。一般是在建表的时候同时创建主键索引\",\"组合索引: 为了进一步榨取MySQL的效率，就要考虑建立组合索引。即将数据库表中的多个字段联合起来作为一个组合索引。\",\"全文索引：是目前搜索引擎使用的一种关键技术，对文本的内容进行分词、搜索。\",\"覆盖索引：查询列要被所建的索引覆盖，不必读取数据行\"]},{\"header\":\"3、MySQL数据库中索引的工作机制是什么？\",\"slug\":\"_3、mysql数据库中索引的工作机制是什么\",\"contents\":[\"数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。\",\"索引的实现通常使用B树及其变种B+树\"]},{\"header\":\"4、MySQL的基础操作命令\",\"slug\":\"_4、mysql的基础操作命令\",\"contents\":[\"MySQL 是否处于运行状态: \",\"Debian 上运行命令 service mysql status，\",\"在RedHat 上运行命令 service mysqld status\",\"开启或停止 MySQL 服务 \",\"service mysqld start 开启服务\",\"service mysqld stop 停止服务\",\"列出所有数据库:运行命令 show databases;\",\"切换到某个数据库并在上面工作:运行命令 use databasename; 进入名为 databasename 的数据库\",\"列出某个数据库内所有表: show tables;\"]},{\"header\":\"5、一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15,16,17条记录，再把Mysql重启，再insert一条记录，这条记录的ID是18还是15 ？\",\"slug\":\"_5、一张表-里面有id自增主键-当insert了17条记录之后-删除了第15-16-17条记录-再把mysql重启-再insert一条记录-这条记录的id是18还是15\",\"contents\":[\"如果表的类型为MyISAM，ID为18 \",\"因为MyISAM表会把自增主键的最大ID记录到数据文件里，重启MySQL自增主键的最大ID也不会丢失\",\"如果表的类型是InnoDB，ID是15 \",\"InnoDB表只是把自增主键的最大ID记录到内存中，所以重启数据库或者是对表进行OPTIMIZE操作，都会导致最大ID丢失\"]},{\"header\":\"6、MySQL中varchar与char的区别以及varchar(32)中的32代表的涵义？\",\"slug\":\"_6、mysql中varchar与char的区别以及varchar-32-中的32代表的涵义\",\"contents\":[\"varchar与char的区别:\",\"char是一种固定长度的类型\",\"varchar则是一种可变长度的类型.\",\"varchar(32)中32的涵义 : 最多存放32个字节\",\"int（20）中20的涵义: int(M)中的M indicates the maximum display width (最大显示宽度)for integer types. The maximum legal display width is 255.\"]},{\"header\":\"7、 SELECT * 和 SELECT 全部字段 的2种写法有何优缺点?\",\"slug\":\"_7、select-和select-全部字段的2种写法有何优缺点\",\"contents\":[\"SELECT *\",\"SELECT 全部字段\",\"解析数据字典\",\"需要\",\"不需要\",\"输出顺序\",\"建表列顺序相同\",\"指定字段顺序\",\"字段改名\",\"不需要修改\",\"需要\",\"索引优化\",\"无法优化\",\"可以建立索引进行优化\",\"可读性\",\"可读性低\",\"可读性高\"]},{\"header\":\"8、HAVNG 子句 和 WHERE的异同点?\",\"slug\":\"_8、havng-子句-和-where的异同点\",\"contents\":[\"having\",\"where\",\"语法上\",\"having用select结果别名\",\"where 用表中列名\",\"结果范围\",\"返回客户端的行数\",\"where从表读出数据的行数\",\"索引\",\"不能使用索引，只能在临时结果集操作\",\"可以使用索引\",\"聚集函数\",\"专门使用聚集函数的\",\"不能使用聚集函数\"]},{\"header\":\"9、如何区分FLOAT和DOUBLE？\",\"slug\":\"_9、如何区分float和double\",\"contents\":[\"浮点数以8位精度存储在FLOAT中，并且有四个字节\",\"浮点数存储在DOUBLE中，精度为18位，有八个字节\"]},{\"header\":\"10、一张表最多创建多少索引？\",\"slug\":\"_10、一张表最多创建多少索引\",\"contents\":[\"任何标准表最多可以创建16个索引列。\"]},{\"header\":\"11、MySQL里记录金钱用什么字段类型?\",\"slug\":\"_11、mysql里记录金钱用什么字段类型\",\"contents\":[\"NUMERIC和DECIMAL类型被Mysql实现为同样的类型，这在SQL92标准允许。他们被用于保存值，该值的准确精度是极其重要的值，例如与金钱有关的数据。当声明一个类是这些类型之一时，精度和规模的能被(并且通常是)指定。\",\"PS：salary DECIMAL(9,2)\",\"在这个例子中，9(precision)代表将被用于存储值的总的小数位数，而2(scale)代表将被用于存储小数点后的位数。\",\"因此，在这种情况下，能被存储在salary列中的值的范围是从-9999999.99到9999999.99。在ANSI/ISO SQL92中，句法DECIMAL(p)等价于DECIMAL(p,0)。\",\"同样，句法DECIMAL等价于DECIMAL(p,0)，这里实现被允许决定值p。Mysql当前不支持DECIMAL/NUMERIC数据类型的这些变种形式的任何一种。\"]},{\"header\":\"12、MySQL 中有哪几种锁？\",\"slug\":\"_12、mysql-中有哪几种锁\",\"contents\":[\"表级锁： 开销小， 加锁快； 不会出现死锁； 锁定粒度大， 发生锁冲突的概率最高， 并发度最低。行级锁： 开销大， 加锁慢； 会出现死锁； 锁定粒度最小， 发生锁冲突的概率最低， 并发度也最高。页面锁： 开销和加锁时间界于表锁和行锁之间； 会出现死锁； 锁定粒度界于表锁和行锁之间， 并发度一般。\"]},{\"header\":\"13、MySQL 中有哪些不同的表格？\",\"slug\":\"_13、mysql-中有哪些不同的表格\",\"contents\":[\"MyISAM\",\"Heap\",\"Merge\",\"INNODB\",\"MISAM\",\"MyISAM是Mysql的默认存储引擎。\"]},{\"header\":\"14、Mysql如何优化DISTINCT？\",\"slug\":\"_14、mysql如何优化distinct\",\"contents\":[\"DISTINCT在所有列上转换为GROUP BY，并与ORDER BY子句结合使用。\"]},{\"header\":\"15、CHAR 和VARCHAR 的区别？\",\"slug\":\"_15、char-和varchar-的区别\",\"contents\":[\"CHAR 和 VARCHAR 类型在存储和检索方面有所不同\",\"CHAR 列长度固定为创建表时声明的长度， 长度值范围是 1 到 255 当 CHAR 值被存储时， 它们被用空格填充到特定长度， 检索 CHAR 值时需删除尾随空格。\"]},{\"header\":\"16、MySQL 数据库作发布系统的存储，一天五万条以上的增量， 预计运维三年,怎么优化？\",\"slug\":\"_16、mysql-数据库作发布系统的存储-一天五万条以上的增量-预计运维三年-怎么优化\",\"contents\":[\"设计良好的数据库结构， 允许部分数据冗余， 尽量避免 join 查询， 提高效率。\",\"选择合适的表字段数据类型和存储引擎， 适当的添加索引。\",\"MySQL 库主从读写分离。\",\"找规律分表， 减少单表中的数据量提高查询速度。\",\"添加缓存机制， 比如 memcached， apc等。\",\"不经常改动的页面， 生成静态页面。\",\"书写高效率的 SQL。比如 SELECT * FROM TABEL 改为 SELECT field_1, field_2, field_3 FROM TABLE.\"]},{\"header\":\"17、limit 1000000加载很慢的话，你是怎么解决的呢？\",\"slug\":\"_17、limit-1000000加载很慢的话-你是怎么解决的呢\",\"contents\":[\"方案一：如果id是连续的，可以这样，返回上次查询的最大记录(偏移量)，再往下limit\",\"select id，name from employee where id>1000000 limit 10. \"]},{\"header\":\"方案二：在业务允许的情况下限制页数：\",\"slug\":\"方案二-在业务允许的情况下限制页数\",\"contents\":[\"建议跟业务讨论，有没有必要查这么后的分页啦。因为绝大多数用户都不会往后翻太多页。\"]},{\"header\":\"方案三：order by + 索引（id为索引）\",\"slug\":\"方案三-order-by-索引-id为索引\",\"contents\":[\"select id，name from employee order by id limit 1000000，10 \"]},{\"header\":\"方案四：利用延迟关联或者子查询优化超多分页场景。（先快速定位需要获取的id段，然后再关联）\",\"slug\":\"方案四-利用延迟关联或者子查询优化超多分页场景。-先快速定位需要获取的id段-然后再关联\",\"contents\":[\"SELECT a.* FROM employee a, (select id from employee where 条件 LIMIT 1000000,10 ) b where a.id=b. \"]},{\"header\":\"18、实践中如何优化 MySQL？\",\"slug\":\"_18、实践中如何优化-mysql\",\"contents\":[\"SQL 语句及索引的优化\",\"数据库表结构的优化\",\"系统配置的优化\",\"硬件的优化\"]},{\"header\":\"19、优化数据库的方法？\",\"slug\":\"_19、优化数据库的方法\",\"contents\":[\"选取最适用的字段属性，尽可能减少定义字段宽度，尽量把字段设置 NOTNULL， 例如’ 省份’、’ 性别’ 最好适用 ENUM\",\"使用连接(JOIN)来代替子查询\",\"适用联合(UNION)来代替手动创建的临时表\",\"事务处理\",\"分库分表\",\"读写分离\",\"锁定表、优化事务处理\",\"适用外键， 优化锁定表\",\"建立索引\",\"优化查询语句\"]},{\"header\":\"20、InnoDB与MyISAM的区别\",\"slug\":\"_20、innodb与myisam的区别\",\"contents\":[\"InnoDB支持事务，MyISAM不支持事务\",\"InnoDB支持外键，MyISAM不支持外键\",\"InnoDB 支持 MVCC(多版本并发控制)，MyISAM 不支持\",\"select count(*) from table时，MyISAM更快，因为它有一个变量保存了整个表的总行数，可以直接读取，InnoDB就需要全表扫描。\",\"Innodb不支持全文索引，而MyISAM支持全文索引（5.7以后的InnoDB也支持全文索引）\",\"InnoDB支持表、行级锁，而MyISAM支持表级锁。\",\"InnoDB表必须有主键，而MyISAM可以没有主键\",\"Innodb表需要更多的内存和存储，而MyISAM可被压缩，存储空间较小，。\",\"Innodb按主键大小有序插入，MyISAM记录插入顺序是，按记录插入顺序保存。\",\"InnoDB 存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全，与 MyISAM 比 InnoDB 写的效率差一些，并且会占用更多的磁盘空间以保留数据和索引\"]},{\"header\":\"21、MySQL遇到过死锁问题吗，你是如何解决的？\",\"slug\":\"_21、mysql遇到过死锁问题吗-你是如何解决的\",\"contents\":[\"查看死锁日志show engine innodb status;\",\"找出死锁Sql\",\"分析sql加锁情况\",\"模拟死锁案发\",\"分析死锁日志\",\"分析死锁结果\"]},{\"header\":\"22、创建索引的原则\",\"slug\":\"_22、创建索引的原则\",\"contents\":[\"最左前缀匹配原则\",\"频繁作为查询条件的字段才去创建索引\",\"频繁更新的字段不适合创建索引\",\"索引列不能参与计算，不能有函数操作\",\"优先考虑扩展索引，而不是新建索引，避免不必要的索引\",\"在order by或者group by子句中，创建索引需要注意顺序\",\"区分度低的数据列不适合做索引列(如性别）\",\"定义有外键的数据列一定要建立索引。\",\"对于定义为text、image数据类型的列不要建立索引。\",\"删除不再使用或者很少使用的索引\"]},{\"header\":\"23、创建索引的方式有哪些？\",\"slug\":\"_23、创建索引的方式有哪些\",\"contents\":[\"在创建表的时候创建索引\",\"CREATE TABLE table_name [col_name data_type] [UNIQUE | FULLTEXT | SPATIAL] [INDEX | KEY] [index_name] (col_name [length]) [ASC | DESC] \",\"UNIQUE 、 FULLTEXT 和 SPATIAL 为可选参数，分别表示唯一索引、全文索引和空间索引；\",\"INDEX 与 KEY 为同义词，两者的作用相同，用来指定创建索引；\",\"index_name 指定索引的名称，为可选参数，如果不指定，那么MySQL默认col_name为索引名；\",\"col_name 为需要创建索引的字段列，该列必须从数据表中定义的多个列中选择；\",\"length 为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度；\",\"ASC 或 DESC 指定升序或者降序的索引值存储。\",\"使用ALTER TABLE命令添加索引\",\"alter table table_name ADD index index_name(column); \",\"使用CREATE INDEX命令创建\",\"create index index_name ON table_name(column); \"]},{\"header\":\"24、如何删除百万级别以上的数据？\",\"slug\":\"_24、如何删除百万级别以上的数据\",\"contents\":[\"可以删除百万数据的时候可以先删除索引\",\"然后批量删除其中无用数据\",\"删除完成后重新创建索引\"]},{\"header\":\"25、索引的优缺点？\",\"slug\":\"_25、索引的优缺点\",\"contents\":[\"优点：\",\"类似大学图书馆建书目索引，提高数据检索的效率，降低 数据库的IO成本 ，这也是创建索引最主要的原因\",\"通过创建唯一索引，可以保证数据库表中每一行 数据的唯一性\",\"在实现数据的参考完整性方面，可以 加速表和表之间的连接 。换句话说，对于有依赖关系的子表和父表联合查询时，可以提高查询速度\",\"在使用分组和排序子句进行数据查询时，可以显著 减少查询中分组和排序的时 间 ，降低了CPU的消耗\",\"缺点：\",\"创建索引和维护索引要 耗费时间 ，并且随着数据量的增加，所耗费的时间也会增加\",\"索引需要占 磁盘空间 ，除了数据表占数据空间之外，每一个索引还要占一定的物理空间， 存储在磁盘上 ，如果有大量的索引，索引文件就可能比数据文件更快达到最大文件尺寸\",\"虽然索引大大提高了查询速度，同时却会 降低更新表的速度 。当对表中的数据进行增加、删除和修改的时候，索引也要动态地维护，这样就降低了数据的维护速度\"]},{\"header\":\"26、哪些情况适合创建索引？\",\"slug\":\"_26、哪些情况适合创建索引\",\"contents\":[]},{\"header\":\"1、字段的数值有唯一性的限制\",\"slug\":\"_1、字段的数值有唯一性的限制\",\"contents\":[\"业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。（来源：Alibaba）\",\"说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的。\"]},{\"header\":\"2、频繁作为WHERE查询条件的字段\",\"slug\":\"_2、频繁作为where查询条件的字段\",\"contents\":[\"某个字段在SELECT语句的 WHERE 条件中经常被使用到，那么就需要给这个字段创建索引了。尤其是在数据量大的情况下，创建普通索引就可以大幅提升数据查询的效率。\",\"例如：比如student_info数据表（含100万条数据），假设我们想要查询 student_id=123110 的用户信息。\"]},{\"header\":\"3、经常 GROUP BY 和 ORDER BY 的列\",\"slug\":\"_3、经常-group-by-和-order-by-的列\",\"contents\":[\"索引就是让数据按照某种顺序进行存储或检索，因此当我们使用 GROUP BY 对数据进行分组查询，或者使用 ORDER BY 对数据进行排序的时候，就需要 对分组或者排序的字段进行索引 。如果待排序的列有多个，那么可以在这些列上建立组合索引\"]},{\"header\":\"4、UPDATE、DELETE 的 WHERE 条件列\",\"slug\":\"_4、update、delete-的-where-条件列\",\"contents\":[\"对数据按照某个条件进行查询后再进行 UPDATE 或 DELETE 的操作，如果对 WHERE 字段创建了索引，就能大幅提升效率。原理是因为我们需要先根据 WHERE 条件列检索出来这条记录，然后再对它进行更新或删除。如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更新不需要对索引进行维护。\"]},{\"header\":\"5、DISTINCT 字段需要创建索引\",\"slug\":\"_5、distinct-字段需要创建索引\",\"contents\":[\"有时候我们需要对某个字段进行去重，使用 DISTINCT，那么对这个字段创建索引，也会提升查询效率\"]},{\"header\":\"6、多表 JOIN 连接操作时，创建索引注意事项\",\"slug\":\"_6、多表-join-连接操作时-创建索引注意事项\",\"contents\":[\"连接表的数量尽量不要超过 3 张 ，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。\",\"对 WHERE 条件创建索引 ，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下，没有 WHERE 条件过滤是非常可怕的。\",\"对用于连接的字段创建索引 ，并且该字段在多张表中的 类型必须一致 。比如 course_id 在 student_info 表和 course 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型\"]},{\"header\":\"7、使用列的类型小的创建索引\",\"slug\":\"_7、使用列的类型小的创建索引\",\"contents\":[]},{\"header\":\"8、使用字符串前缀创建索引\",\"slug\":\"_8、使用字符串前缀创建索引\",\"contents\":[\"创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引\",\"create table shop(address varchar(120) not null); alter table shop add index(address(12)); \",\"问题是，截取多少呢？截取得多了，达不到节省索引存储空间的目的；截取得少了，重复内容太多，字段的散列度(选择性)会降低。怎么计算不同的长度的选择性呢？\",\"先看一下字段在全部数据中的选择度：\",\"select count(distinct address) / count(*) from shop; \",\"通过不同长度去计算，与全表的选择性对比：\",\"公式：\",\"count(distinct left(列名, 索引长度))/count(*) \",\"例如：\",\"select count(distinct left(address,10)) / count(*) as sub10, -- 截取前10个字符的选择度 count(distinct left(address,15)) / count(*) as sub11, -- 截取前15个字符的选择度 count(distinct left(address,20)) / count(*) as sub12, -- 截取前20个字符的选择度 count(distinct left(address,25)) / count(*) as sub13 -- 截取前25个字符的选择度 from shop; \",\"引申另一个问题：索引列前缀对排序的影响\",\"拓展：Alibaba《Java开发手册》\",\"【 强制 】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\",\"说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会 高达 90% 以上 ，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。\"]},{\"header\":\"9、 区分度高(散列性高)的列适合作为索引\",\"slug\":\"_9、-区分度高-散列性高-的列适合作为索引\",\"contents\":[]},{\"header\":\"10、使用最频繁的列放到联合索引的左侧\",\"slug\":\"_10、使用最频繁的列放到联合索引的左侧\",\"contents\":[\"这样也可以较少的建立一些索引。同时，由于\\\"最左前缀原则\\\"，可以增加联合索引的使用率。\"]},{\"header\":\"11、在多个字段都要创建索引的情况下，联合索引优于单值索引\",\"slug\":\"_11、在多个字段都要创建索引的情况下-联合索引优于单值索引\",\"contents\":[]},{\"header\":\"27、哪些情况下创建索引，但是没有生效？\",\"slug\":\"_27、哪些情况下创建索引-但是没有生效\",\"contents\":[\"查询条件包含or，可能导致索引失效\",\"如何字段类型是字符串，where时一定用引号括起来，否则索引失效\",\"like通配符可能导致索引失效。\",\"联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。\",\"在索引列上使用mysql的内置函数，索引失效。\",\"对索引列运算（如，+、-、*、/），索引失效。\",\"索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。\",\"索引字段上使用is null， is not null，可能导致索引失效。\",\"左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。\",\"mysql估计使用全表扫描要比使用索引快,则不使用索引。\"]},{\"header\":\"28、数据库索引的原理，为什么要用B+树，为什么不用二叉树？\",\"slug\":\"_28、数据库索引的原理-为什么要用b-树-为什么不用二叉树\",\"contents\":[\"为什么不是一般二叉树？\",\"如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找树来说，查找效率更稳定，总体的查找速度也更快。\",\"为什么不是平衡二叉树呢？\",\"我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果是B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数就降下来啦，查询效率就快啦。\",\"那为什么不是B树而是B+树呢？\",\"B+树非叶子节点上是不存储数据的，仅存储键值，而B树节点中不仅存储键值，也会存储数据。innodb中页的默认大小是16KB，如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的IO次数有会再次减少，数据查询的效率也会更快。\",\"B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链表连着的。那么B+树使得范围查找，排序查找，分组查找以及去重查找变得异常简单。\"]},{\"header\":\"29、如何选择合适的分布式主键？\",\"slug\":\"_29、如何选择合适的分布式主键\",\"contents\":[\"数据库自增长序列或字段\",\"UUID\",\"Redis生成ID\",\"Twitter的snowflake算法\",\"利用zookeeper生成唯一ID\",\"MongoDB的ObjectId\"]},{\"header\":\"30、在高并发情况下，如何做到安全的修改同一行数据？\",\"slug\":\"_30、在高并发情况下-如何做到安全的修改同一行数据\",\"contents\":[\"这种情况下，采用加锁的方式进行安全修改\"]},{\"header\":\"使用悲观锁\",\"slug\":\"使用悲观锁\",\"contents\":[\"悲观锁思想就是，当前线程要进来修改数据时，别的线程都得拒之门外~ 比如，可以使用select…for update ~\",\"select * from User where name=‘jay’ for update \",\"以上这条sql语句会锁定了User表中所有符合检索条件（name=‘jay’）的记录。本次事务提交之前，别的线程都无法修改这些记录。\"]},{\"header\":\"使用乐观锁\",\"slug\":\"使用乐观锁\",\"contents\":[\"乐观锁思想就是，有线程过来，先放过去修改，如果看到别的线程没修改过，就可以修改成功，如果别的线程修改过，就修改失败或者重试。实现方式：乐观锁一般会使用版本号机制或CAS算法实现。\"]},{\"header\":\"31、数据库自增主键可能遇到什么问题\",\"slug\":\"_31、数据库自增主键可能遇到什么问题\",\"contents\":[\"使用自增主键对数据库做分库分表，可能出现诸如主键重复等的问题。解决方案的话，简单点的话可以考虑使用UUID解决，复杂的可以考虑前面提到的分布式主键方案\",\"自增主键会产生表锁，从而引发问题\",\"自增主键可能用完问题。\"]},{\"header\":\"32、MVCC你了解吗？\",\"slug\":\"_32、mvcc你了解吗\",\"contents\":[\"多版本并发控制技术的英文全称是 Multiversion Concurrency Control，简称 MVCC。\",\"多版本并发控制（MVCC） 是通过保存数据在某个时间点的快照来实现并发控制的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。\",\"简单来说，多版本并发控制 的思想就是保存数据的历史版本，通过对数据行的多个版本管理来实现数据库的并发控制。这样我们就可以通过比较版本号决定数据是否显示出来，读取数据的时候不需要加锁也可以保证事务的隔离效果。\",\"MVCC需要关注这几个知识点：\",\"事务版本号\",\"表的隐藏列\",\"undo log\",\"read view\"]},{\"header\":\"33、说一下大表查询的优化方案\",\"slug\":\"_33、说一下大表查询的优化方案\",\"contents\":[\"优化shema、sql语句+索引；\",\"可以考虑加缓存，memcached, redis，或者JVM本地缓存；\",\"主从复制，读写分离；\",\"分库分表；\"]},{\"header\":\"34、Blob和text有什么区别？\",\"slug\":\"_34、blob和text有什么区别\",\"contents\":[\"Blob用于存储二进制数据，而Text用于存储大字符串。\",\"Blob值被视为二进制字符串（字节字符串）,它们没有字符集，并且排序和比较基于列值中的字节的数值。\",\"text值被视为非二进制字符串（字符字符串）。它们有一个字符集，并根据字符集的排序规则对值进行排序和比较。\"]},{\"header\":\"35、组合索引是什么？为什么需要注意组合索引中的顺序？\",\"slug\":\"_35、组合索引是什么-为什么需要注意组合索引中的顺序\",\"contents\":[\"组合索引，用户可以在多个列上建立索引,这种索引叫做组合索引。\",\"因为InnoDB引擎中的索引策略的最左原则，所以需要注意组合索引中的顺序。\"]},{\"header\":\"36、为什么要使用视图？什么是视图？\",\"slug\":\"_36、为什么要使用视图-什么是视图\",\"contents\":[\"为什么要使用视图？\",\"为了提高复杂SQL语句的复用性和表操作的安全性，MySQL数据库管理系统提供了视图特性。\",\"什么是视图？\",\"视图是一个虚拟的表，是一个表中的数据经过某种筛选后的显示方式，视图由一个预定义的查询select语句组成。\"]},{\"header\":\"37、视图有哪些特点？哪些使用场景？\",\"slug\":\"_37、视图有哪些特点-哪些使用场景\",\"contents\":[\"视图特点：\",\"视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。\",\"视图是由基本表(实表)产生的表(虚表)。\",\"视图的建立和删除不影响基本表。\",\"对视图内容的更新(添加，删除和修改)直接影响基本表。\",\"当视图来自多个基本表时，不允许添加和删除数据。\",\"视图用途： 简化sql查询，提高开发效率，兼容老的表结构。\",\"视图的常见使用场景：\",\"重用SQL语句；\",\"简化复杂的SQL操作。\",\"使用表的组成部分而不是整个表；\",\"保护数据\",\"更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。\"]},{\"header\":\"38、视图的优点，缺点，讲一下？\",\"slug\":\"_38、视图的优点-缺点-讲一下\",\"contents\":[\"查询简单化。视图能简化用户的操作\",\"数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护\",\"逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性\"]},{\"header\":\"39、count(1)、count(*) 与 count(列名) 的区别？\",\"slug\":\"_39、count-1-、count-与-count-列名-的区别\",\"contents\":[\"count(*)：包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL\",\"count(1)：包括了忽略所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL\",\"count(列名)：只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是指空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计。\"]},{\"header\":\"40、什么是存储过程？有哪些优缺点？\",\"slug\":\"_40、什么是存储过程-有哪些优缺点\",\"contents\":[\"存储过程，就是一些编译好了的SQL语句，这些SQL语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后给这些代码块取一个名字，在用到这个功能的时候调用即可。\",\"优点：\",\"存储过程是一个预编译的代码块，执行效率比较高\",\"存储过程在服务器端运行，减少客户端的压力\",\"允许模块化程序设计，只需要创建一次过程，以后在程序中就可以调用该过程任意次，类似方法的复用\",\"一个存储过程替代大量T_SQL语句 ，可以降低网络通信量，提高通信速率\",\"可以一定程度上确保数据安全\",\"缺点：\",\"调试麻烦\",\"可移植性不灵活\",\"重新编译问题\"]},{\"header\":\"41、什么是触发器？触发器的使用场景有哪些？\",\"slug\":\"_41、什么是触发器-触发器的使用场景有哪些\",\"contents\":[\"触发器，指一段代码，当触发某个事件时，自动执行这些代码。\",\"使用场景：\",\"可以通过数据库中的相关表实现级联更改。\",\"实时监控某张表中的某个字段的更改而需要做出相应的处理。\",\"例如可以生成某些业务的编号。\",\"注意不要滥用，否则会造成数据库及应用程序的维护困难。\"]},{\"header\":\"42、MySQL中都有哪些触发器？\",\"slug\":\"_42、mysql中都有哪些触发器\",\"contents\":[\"MySQL 数据库中有六种触发器：\",\"Before Insert\",\"After Insert\",\"Before Update\",\"After Update\",\"Before Delete\",\"After Delete\"]},{\"header\":\"43、drop、delete与truncate的区别\",\"slug\":\"_43、drop、delete与truncate的区别\",\"contents\":[\"delete\",\"truncate\",\"drop\",\"类型\",\"DML\",\"DDL\",\"DDL\",\"回滚\",\"可回滚\",\"不可回滚\",\"不可回滚\",\"删除内容\",\"表结构还在，删除表的全部或者一部分数据行\",\"表结构还在，删除表中的所有数据\",\"从数据库中删除表，所有的数据行，索引和权限也会被删除\",\"删除速度\",\"删除速度慢，逐行删除\",\"删除速度快\",\"删除速度最快\"]},{\"header\":\"44、列值为NULL时，查询是否会用到索引？\",\"slug\":\"_44、列值为null时-查询是否会用到索引\",\"contents\":[\"列值为NULL也是可以走索引的\",\"计划对列进行索引，应尽量避免把它设置为可空，因为这会让 MySQL 难以优化引用了可空列的查询，同时增加了引擎的复杂度\"]},{\"header\":\"45、如果要存储用户的密码散列，应该使用什么字段进行存储？\",\"slug\":\"_45、如果要存储用户的密码散列-应该使用什么字段进行存储\",\"contents\":[\"密码散列，盐，用户身份证号等固定长度的字符串，应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。\"]},{\"header\":\"46、Innodb的事务实现原理？\",\"slug\":\"_46、innodb的事务实现原理\",\"contents\":[\"原子性：是使用 undo log来实现的，如果事务执行过程中出错或者用户执行了rollback，系统通过undo log日志返回事务开始的状态。\",\"持久性：使用 redo log来实现，只要redo log日志持久化了，当系统崩溃，即可通过redo log把数据恢复。\",\"隔离性：通过锁以及MVCC,使事务相互隔离开。\",\"一致性：通过回滚、恢复，以及并发情况下的隔离性，从而实现一致性。\"]},{\"header\":\"47、MySQL中TEXT数据类型的最大长度\",\"slug\":\"_47、mysql中text数据类型的最大长度\",\"contents\":[\"TINYTEXT：256 bytes\",\"TEXT：65,535 bytes(64kb)\",\"MEDIUMTEXT：16,777,215 bytes(16MB)\",\"LONGTEXT：4,294,967,295 bytes(4GB)\"]},{\"header\":\"48、MySQL 事务隔离级别\",\"slug\":\"_48、mysql-事务隔离级别\",\"contents\":[\"未提交读 - 读到其它事务未提交的数据（最新的版本）\",\"错误现象：有脏读、不可重复读、幻读现象\",\"提交读（RC） - 读到其它事务已提交的数据（最新已提交的版本）\",\"错误现象：有不可重复读、幻读现象\",\"使用场景：希望看到最新的有效值\",\"可重复读（RR） - 在事务范围内，多次读能够保证一致性（快照建立时最新已提交版本）\",\"错误现象：有幻读现象，可以用加锁避免\",\"使用场景：事务内要求更强的一致性，但看到的未必是最新的有效值\",\"串行读 - 在事务范围内，仅有读读可以并发，读写或写写会阻塞其它事务，用这种办法保证更强的一致性\",\"错误现象：无\"]},{\"header\":\"1、脏读现象\",\"slug\":\"_1、脏读现象\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level read uncommitted;\",\"start transaction;\",\"select * from account;/两个账户都为 1000/\",\"start transaction;\",\"update account set balance = 2000 where accountNo=1;\",\"select * from account;/1号账户2000, 2号账户1000/\",\"tx2 未提交的情况下，tx1 仍然读取到了它的更改，此时与第一次读取的数据不一致，则是产生了脏读\"]},{\"header\":\"2、不可重复读现象\",\"slug\":\"_2、不可重复读现象\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level read committed;\",\"start transaction;\",\"select * from account; /两个账户都为 1000/\",\"update account set balance = 2000 where accountNo=1;\",\"select * from account; /1号账户2000, 2号账户1000/\",\"tx1 在同一事务内，两次读取的结果不一致，当然，此时 tx2 的事务已提交\"]},{\"header\":\"3、幻读现象\",\"slug\":\"_3、幻读现象\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction;\",\"select * from account; /存在 1,2 两个账户/\",\"insert into account values(3, 1000);\",\"select * from account; /发现还是只有 1,2 两个账户/\",\"insert into account values(3, 5000); /* ERROR 1062 (23000): Duplicate entry '3' for key 'PRIMARY' */\",\"tx1 查询时并没有发现 3 号账户，执行插入时却发现主键冲突异常，就好像出现了幻觉一样\"]},{\"header\":\"4、使用 for update 避免幻读现象\",\"slug\":\"_4、使用-for-update-避免幻读现象\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction;\",\"select * from account; /存在 1,2 两个账户/\",\"select * from account where accountNo=3 for update;\",\"insert into account values(3, 1000); /* 阻塞 */\",\"insert into account values(3, 5000);\",\"在 for update 这行语句执行时，虽然此时 3 号账户尚不存在，但 MySQL 在 repeatable read 隔离级别下会用间隙锁，锁住 2 号记录与正无穷大之间的间隙\",\"此时 tx2 想插入 3 号记录就不行了，被间隙锁挡住了\"]},{\"header\":\"5、串行读避免幻读\",\"slug\":\"_5、串行读避免幻读\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level serializable;\",\"start transaction;\",\"select * from account; /* 存在 1,2 两个账户 */\",\"insert into account values(3, 1000); /* 阻塞 */\",\"insert into account values(3, 5000);\",\"串行读隔离级别下，普通的 select 也会加共享读锁，其它事务的查询可以并发，但增删改就只能阻塞了\"]},{\"header\":\"49、快照读与当前读\",\"slug\":\"_49、快照读与当前读\",\"contents\":[]},{\"header\":\"1、当前读\",\"slug\":\"_1、当前读\",\"contents\":[\"当前读，即读取最新提交的数据，查询时需要加锁，以下情况都属于当前读\",\"select … for update\",\"insert、update、delete，都会按最新提交的数据进行操作\"]},{\"header\":\"2、快照读\",\"slug\":\"_2、快照读\",\"contents\":[\"快照读，读取某一个快照建立时（可以理解为某一时间点）的数据，快照读主要体现在 select 时，不同隔离级别下，select 的行为不同\",\"在 Serializable 隔离级别下 - 普通 select 也变成当前读\",\"在 RC 隔离级别下 - 每次 select 都会建立新的快照\",\"在 RR 隔离级别下 \",\"事务启动后，首次 select 会建立快照\",\"如果事务启动选择了 with consistent snapshot，事务启动时就建立快照\",\"基于旧数据的修改操作，会重新建立快照\",\"快照读本质上读取的是历史数据（原理是回滚段），属于无锁查询\"]},{\"header\":\"1、RR 下，快照建立时机 - 第一次 select 时\",\"slug\":\"_1、rr-下-快照建立时机-第一次-select-时\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction;\",\"select * from account; /* 此时建立快照，两个账户为 1000 */\",\"update account set balance = 2000 where accountNo=1;\",\"select * from account; /* 两个账户仍为 1000 */\",\"快照一旦建立，以后的查询都基于此快照，因此 tx1 中第二次 select 仍然得到 1 号账户余额为 1000\",\"如果 tx2 的 update 先执行\",\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction;\",\"update account set balance = 2000 where accountNo=1;\",\"select * from account; /* 此时建立快照，1号余额已经为2000 */\"]},{\"header\":\"2、RR 下，快照建立时机 - 事务启动时\",\"slug\":\"_2、rr-下-快照建立时机-事务启动时\",\"contents\":[\"如果希望事务启动时就建立快照，可以添加 with consistent snapshot 选项\",\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction with consistent snapshot; /* 此时建立快照，两个账户为 1000 */\",\"update account set balance = 2000 where accountNo=1;\",\"select * from account; /* 两个账户仍为 1000 */\"]},{\"header\":\"3、RR 下，快照建立时机 - 修改数据时\",\"slug\":\"_3、rr-下-快照建立时机-修改数据时\",\"contents\":[\"tx1\",\"tx2\",\"set session transaction isolation level repeatable read;\",\"start transaction;\",\"select * from account; /* 此时建立快照，两个账户为 1000 */\",\"update account set balance=balance+1000 where accountNo=1;\",\"update account set balance=balance+1000 where accountNo=1;\",\"select * from account; /* 1号余额为3000 */\",\"tx1 内的修改必须重新建立快照，否则，就会发生丢失更新的问题\"]},{\"header\":\"50、InnoDB vs MyISAM\",\"slug\":\"_50、innodb-vs-myisam\",\"contents\":[]},{\"header\":\"1、InnoDB\",\"slug\":\"_1、innodb\",\"contents\":[\"索引分为聚簇索引与二级索引\",\"聚簇索引：主键值作为索引数据，叶子节点还包含了所有字段数据，索引和数据是存储在一起的\",\"二级索引：除主键外的其它字段建立的索引称为二级索引。被索引的字段值作为索引数据，叶子节点还包含了主键值\",\"支持事务\",\"通过 undo log 支持事务回滚、当前读（多版本查询）\",\"通过 redo log 实现持久性\",\"通过两阶段提交实现一致性\",\"通过当前读、锁实现隔离性\",\"支持行锁、间隙锁\",\"支持外键\",\"InnoDB 索引特点\",\"聚簇索引：主键值作为索引数据，叶子节点还包含了所有字段数据，索引和数据是存储在一起的\",\"主键即 7369、7499、7521 等\",\"二级索引：除主键外的其它字段建立的索引称为二级索引。被索引的字段值作为索引数据，叶子节点还包含了主键值\",\"上图中 800、950、1100 这些是工资字段的值，根据它们建立了二级索引\",\"上图中，如果执行查询 select empno, ename, sal from emp where sal = 800，这时候可以利用二级索引定位到 800 这个工资，同时还能知道主键值 7369\",\"但 select 字句中还出现了 ename 字段，在二级索引中不存在，因此需要根据主键值 7369 查询聚簇索引来获取 ename 的信息，这个过程俗称回表\"]},{\"header\":\"2、MyISAM\",\"slug\":\"_2、myisam\",\"contents\":[\"索引只有一种\",\"被索引字段值作为索引数据，叶子节点还包含了该记录数据页地址，数据和索引是分开存储的\",\"不支持事务，没有 undo log 和 redo log\",\"仅支持表锁\",\"不支持外键\",\"会保存表的总行数\",\"MyISAM 索引特点\",\"被索引字段值作为索引数据，叶子节点还包含了该记录数据页地址，数据和索引是分开存储的\"]},{\"header\":\"51、查询语句执行流程\",\"slug\":\"_51、查询语句执行流程\",\"contents\":[\"执行 SQL 语句 select * from user where id = 1 时发生了什么\",\"连接器：负责建立连接、检查权限、连接超时时间由 wait_timeout 控制，默认 8 小时\",\"查询缓存：会将 SQL 和查询结果以键值对方式进行缓存，修改操作会以表单位导致缓存失效\",\"分析器：词法、语法分析\",\"优化器：决定用哪个索引，决定表的连接顺序等\",\"执行器：根据存储引擎类型，调用存储引擎接口\",\"存储引擎：数据的读写接口，索引、表都在此层实现\"]},{\"header\":\"52、undo log 与 redo log\",\"slug\":\"_52、undo-log-与-redo-log\",\"contents\":[]},{\"header\":\"1、undo log\",\"slug\":\"_1、undo-log\",\"contents\":[\"回滚数据，以行为单位，记录数据每次的变更，一行记录有多个版本并存\",\"多版本并发控制，即快照读（也称为一致性读），让查询操作可以去访问历史版本\",\"每个事务会按照开始时间，分配一个单调递增的事务编号 trx id\",\"每次事务的改动都会以行为单位记入回滚日志，包括当时的事务编号，改动的值等\",\"查询操作，事务编号大于自己的数据是不可见的，事务编号小于等于自己的数据才是可见的\",\"例如图中红色事务看不到 trx id=102 以及 trx id=101 的数据，只有 trx id=99 的数据对它可见\"]},{\"header\":\"2、redo log\",\"slug\":\"_2、redo-log\",\"contents\":[\"redo log 的作用主要是实现 ACID 中的持久性，保证提交的数据不丢失\",\"它记录了事务提交的变更操作，服务器意外宕机重启时，利用 redo log 进行回放，重新执行已提交的变更操作\",\"事务提交时，首先将变更写入 redo log，事务就视为成功。至于数据页（表、索引）上的变更，可以放在后面慢慢做 \",\"数据页上的变更宕机丢失也没事，因为 redo log 里已经记录了\",\"数据页在磁盘上位置随机，写入速度慢，redo log 的写入是顺序的速度快\",\"它由两部分组成，内存中的 redo log buffer，磁盘上的 redo log file\",\"redo log file 由一组文件组成，当写满了会循环覆盖较旧的日志，这意味着不能无限依赖 redo log，更早的数据恢复需要 binlog\",\"buffer 和 file 两部分组成意味着，写入了文件才真正安全，同步策略由参数 innodb_flush_log_at_trx_commit 控制 \",\"0 - 每隔 1s 将日志 write and flush 到磁盘\",\"1 - 每次事务提交将日志 write and flush（默认值）\",\"2 - 每次事务提交将日志 write，每隔 1s flush 到磁盘，意味着 write 意味着写入操作系统缓存，如果 MySQL 挂了，而操作系统没挂，那么数据不会丢失\"]},{\"header\":\"53、MySQL锁\",\"slug\":\"_53、mysql锁\",\"contents\":[]},{\"header\":\"1、全局锁\",\"slug\":\"_1、全局锁\",\"contents\":[\"用作全量备份时，保证表与表之间的数据一致性\",\"如果不加任何包含，数据备份时就可能产生不一致的情况，如下图所示\",\"全局锁的语法：\",\"flush tables with read lock; \",\"使用全局读锁锁定所有数据库的所有表。这时会阻塞其它所有 DML 以及 DDL 操作，这样可以避免备份过程中的数据不一致。接下来可以执行备份，最后用 unlock tables 来解锁\",\"注意\",\"但 flush tables 属于比较重的操作，可以使用 --single-transaction 参数来完成不加锁的一致性备份（仅针对 InnoDB 引擎的表）\",\"mysqldump --single-transaction -uroot -p test > 1.sql \"]},{\"header\":\"2、表级锁\",\"slug\":\"_2、表级锁\",\"contents\":[]},{\"header\":\"1、表锁\",\"slug\":\"_1、表锁\",\"contents\":[\"语法：加锁 lock tables 表名 read/write，解锁 unlock tables\",\"缺点：粒度较粗，在 InnoDB 引擎很少使用\"]},{\"header\":\"2、 元数据锁\",\"slug\":\"_2、-元数据锁\",\"contents\":[\"即 metadata-lock（MDL），主要是为了避免 DML 与 DDL 冲突，DML 的元数据锁之间不互斥\",\"加元数据锁的几种情况\",\"lock tables read/write，类型为 SHARED_READ_ONLY 和 SHARED_NO_READ_WRITE\",\"alter table，类型为 EXCLUSIVE，与其它 MDL 都互斥\",\"select，select … lock in share mode，类型为 SHARED_READ\",\"insert，update，delete，select for update，类型为 SHARED_WRITE\",\"查看元数据锁（适用于 MySQL 8.0 以上版本）\",\"select object_type,object_schema,object_name,lock_type,lock_duration from performance_schema.metadata_locks;\"]},{\"header\":\"3、IS（意向共享） 与 IX（意向排他）\",\"slug\":\"_3、is-意向共享-与-ix-意向排他\",\"contents\":[\"主要是避免 DML 与表锁冲突，DML 主要目的是加行锁，为了让表锁不用检查每行数据是否加锁，加意向锁（表级）来减少表锁的判断，意向锁之间不会互斥\",\"加意向表锁的几种情况 \",\"select … lock in share mode 会加 IS 锁\",\"insert，update，delete， select … for update 会加 IX 锁\",\"查看意向表锁（适用于 MySQL 8.0 以上版本） \",\"select object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks;\"]},{\"header\":\"3、行级锁\",\"slug\":\"_3、行级锁\",\"contents\":[\"种类\",\"行锁 – 在 RC 下，锁住的是行，防止其他事务对此行 update 或 delete\",\"间隙锁 – 在 RR 下，锁住的是间隙，防止其他事务在这个间隙 insert 产生幻读\",\"临键锁 – 在 RR 下，锁住的是前面间隙+行，特定条件下可优化为行锁\",\"查看行级锁\",\"select object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks where object_name='表名';\",\"注意\",\"它们锁定的其实都是索引上的行与间隙，根据索引的有序性来确定间隙\",\"测试数据\",\"create table t (id int primary key, name varchar(10),age int, key (name)); insert into t values(1, 'zhangsan',18); insert into t values(2, 'lisi',20); insert into t values(3, 'wangwu',21); insert into t values(4, 'zhangsan', 17); insert into t values(8,'zhang',18); insert into t values(12,'zhang',20); \",\"说明\",\"1,2,3,4 之间其实并不可能有间隙\",\"4 与 8 之间有间隙\",\"8 与 12 之间有间隙\",\"12 与正无穷大之间有间隙\",\"其实我们的例子中还有负无穷大与 1 之间的间隙，想避免负数可以通过建表时选择数据类型为 unsigned int\",\"间隙锁例子\",\"事务1：\",\"begin; select * from t where id = 9 for update; /* 锁住的是 8 与 12 之间的间隙 */ \",\"事务2：\",\"update t set age=100 where id = 8; /* 不会阻塞 */ update t set age=100 where id = 12; /* 不会阻塞 */ insert into t values(10,'aaa',18); /* 会阻塞 */ \",\"临键锁和记录锁例子\",\"事务1：\",\"begin; select * from t where id >= 8 for update; \",\"临键锁锁定的是左开右闭的区间，与上条查询条件相关的区间有 (4,8]，(8,12]，(12,+∞)\",\"临键锁在某些条件下可以被优化为记录锁，例如 (4,8] 被优化为只针对 8 的记录锁，前面的区间不会锁住\",\"事务2：\",\"insert into t values(7,'aaa',18); /* 不会阻塞 */ update t set age=100 where id = 8; /* 会阻塞 */ insert into t values(10,'aaa',18); /* 会阻塞 */ update t set age=100 where id = 12; /* 会阻塞 */ insert into t values(13,'aaa',18); /* 会阻塞 */ \"]}]},\"/interview/database/\":{\"title\":\"数据库\",\"contents\":[]},\"/interview/computer/network.html\":{\"title\":\"计算机网络 43道面试题\",\"contents\":[{\"header\":\"1、OSI 七层网络模型，你了解吗？具体功能有哪些？\",\"slug\":\"_1、osi-七层网络模型-你了解吗-具体功能有哪些\",\"contents\":[\"网络的七层架构从下到上主要包括物理层、数据链路层、网络层、传输层、会话层、表示层和应用层；\",\"物理层：主要定义物理设备标准，它的主要作用是传输比特流，具体做法是在发送端将1 、0 转化为电流强弱来进行传输，在到达目的地后再将电流强弱转化为l 、0 ， 也就是我们常说的模数转换与数模转换，这一层的数据叫作比特\",\"数据链路层：主要用于对数据包中的MAC 地址进行解析和封装。这一层的数据叫作数据帧。在这一层工作的设备是网卡、网桥、交换机\",\"网络层：主要用于对数据包中的 IP 地址进行封装和解析，这一层的数据叫作数据包。在这一层工作的设备有路由器、交换机、防火墙等\",\"传输层：定义了传输数据的协议和端口号，主要用于数据的分段、传输和重组。在这一层工作的协议有TCP 和UDP 等。 \",\"TCP 是传输控制协议，传输效率低，可靠性强，用于传输对可靠性要求高、数据量大的数据，比如支付宝转账使用的就是 TCP;\",\"UDP 是用户数据报协议，与TCP 的特性恰恰相反，用于传输可靠性要求不高、数据量小的数据，例如抖音等视频服务就使用了UDP 。\",\"会话层：在传输层的基础上建立连接和管理会话，具体包括登录验证、断点续传、数据粘包与分包等。在设备之间需要互相识别的可以是IP ， 也可以是MAC 或者主机名。\",\"表示层：主要对接收的数据进行解释、加密、解密、压缩、解压缩等，即把计算机能够识别的内容转换成人能够识别的内容（图片、声音、文字等）。\",\"应用层：基于网络构建具体应用，例如FTP 文件上传下载服务、Telnet 服务、HTTP 服务、DNS 服务、SNMP 邮件服务等\"]},{\"header\":\"2、TCP/IP四层模型？\",\"slug\":\"_2、tcp-ip四层模型\",\"contents\":[\"TCP/IP 不是指TCP 和IP 这两个协议的合称， 而是指因特网的整个TCP/IP 协议簇。\",\"网络接口层：定义了主机间网络连通的协议，具体包括Echernet 、FDDI 、ATM 等通信协议\",\"网络层：主要用于数据的传输、路由及地址的解析，以保障主机可以把数据发送给任何网络上的目标。数据经过网络传输，发送的顺序和到达的顺序可能发生变化。在网络层使用IP ( Internet Protocol ）和地址解析协议（ ARP ）\",\"传输层：使源端和目的端机器上的对等实体可以基于会话相互通信。在这一层定义了两个端到端的协议TCP 和UDP 。TCP 是面向连接的协议，提供可靠的报文传输和对上层应用的连接服务，除了基本的数据传输，它还有可靠性保证、流量控制、多路复用、优先权和安全性控制等功能。UDP 是面向无连接的不可靠传输的协议，主要用于不需要TCP 的排序和流量控制等功能的应用程序\",\"应用层：负责具体应用层协议的定义，包括\",\"Telnet( TELecommunications NETwork ，虚拟终端协议）\",\"FTP (File Transfer Protocol ,文件传输协议）\",\"SMTP ( Simple Mail Transfer Protoco l ，电子邮件传输协议）\",\"DNS (Domain Name Serv i ce ，域名服务）\",\"NNTP (Net News Transfer Protoco l ,网上新闻传输协议）\",\"HTTP ( HyperText Transfer Protoco l ，超文本传输协议）\"]},{\"header\":\"3、说一下TCP的三次握手？\",\"slug\":\"_3、说一下tcp的三次握手\",\"contents\":[\"官方回答：\",\"客户端发送SYN ( seq=x ）报文给服务器端，进入SYN_SEND 状态。\",\"服务器端收到SYN 报文， 回应一个SYN (seq =y ） 和ACK ( ack=x+ I ）报文，进入SYN RECV 状态。\",\"客户端收到服务器端的SYN 报文， 回应一个ACK ( ack=y+ 1 ）报文，进入Established 状态\"]},{\"header\":\"4、为什么要三次握手？两次行不行？四次呢？\",\"slug\":\"_4、为什么要三次握手-两次行不行-四次呢\",\"contents\":[\"非官方解释三次握手\",\"第一次握手：客户端发送网络包，服务端收到了。服务端得出结论：客户端的发送能力、服务端的接收能力是正常的。\",\"第二次握手：服务端发包，客户端收到了。客户端得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。\",\"第三次握手：客户端发包，服务端收到了。服务端得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。\",\"三次握手能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号\",\"不使用「两次握手」和「四次握手」的原因：\",\"「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；\",\"「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。\"]},{\"header\":\"5、为什么建立连接是三次握手，关闭连接确是四次挥手呢？\",\"slug\":\"_5、为什么建立连接是三次握手-关闭连接确是四次挥手呢\",\"contents\":[\"TCP 在建立连接时要进行三次握手，在断开连接时要进行四次挥手，这是由于TCP的半关闭造成的。因为TCP 连接是全双工的（即数据可在两个方向上同时传递），所以在进行关闭时对每个方向都要单独进行关闭，这种单方向的关闭叫作半关闭。在二方完 成它的数据发送任务时，就发送一个FIN 来向另一方通告将要终止这个方向的连接。\"]},{\"header\":\"6、TCP四次挥手的过程？\",\"slug\":\"_6、tcp四次挥手的过程\",\"contents\":[\"客户端应用进程调用断开连接的请求，向服务器端发送一个终止标志位FIN = 1，seq=u 的消息，表示在客户端关闭链路前要发送的数据已经安全发送完毕，可以开始关闭链路操作，并请求服务器端确认关闭客户端到服务器的链路操作。此时客户端处于 FIN-WAIT-l 状态。\",\"服务器在收到这个FIN 消息后返回一个ACK=l,ack=u+ 1 ,seq=v 的消息给客户端，表示接收到客户端断开链路的操作请求，这时TCP 服务器端进程通知高层应用进程释放客户端到服务器端的链路服务器处于CLOSE - WAIT 状态，即半关闭状态。客户端在收到消息后处于FIN - WAIT- 2 状态\",\"服务器端将关闭链路前需要发送给客户端的消息发送给客户端，在等待该数据发送完成后， 发送一个终止标志位FIN = l ,ACK= l ,seq=w,a ck= u+ 1 的消息给客户端，表示关闭链路前服务器需要向客户端发送的消息已经发送完毕，请求客户端确认关闭从服务器到客户端的链路操作， 此时服务器端处于LAS 下A C K 状态，等待客户端最终断开链路。\",\"客户端在接收到这个最终FI N 消息后，发送一个ACK = l ,seq=u + l ,ack=w+ I 的消息给服务器端，表示接收到服务器端的断开连接请求并准备断开服务器端到客户端的链路。此时客户端处于TIM-WA IT 状态， TCP 连接还没有释放，然后经过等待计时器 ( 2MSL ）设置的时间后，客户端将进入CLOSE 状态。\"]},{\"header\":\"7、如果已经建立了连接，但是客户端突然出现故障了怎么办？\",\"slug\":\"_7、如果已经建立了连接-但是客户端突然出现故障了怎么办\",\"contents\":[\"TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。\"]},{\"header\":\"8、为什么客户端最后还要等待2MSL？\",\"slug\":\"_8、为什么客户端最后还要等待2msl\",\"contents\":[\"MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。\",\"保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器\",\"防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文\"]},{\"header\":\"9、HTTP的原理？\",\"slug\":\"_9、http的原理\",\"contents\":[\"HTTP 是一个无状态的协议。无状态是指客户机（Web 浏览器）和服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息\",\"HTTP 遵循请求(Request)/应答(Response)模型。客户机（浏览器）向服务器发送请求，服务器处理请求并返回适当的应答。所有HTTP 连接都被构造成一套请求和应答。\"]},{\"header\":\"10、如何理解HTTP协议是无状态的？\",\"slug\":\"_10、如何理解http协议是无状态的\",\"contents\":[\"当浏览器第一次发送请求给服务器时，服务器响应了；如果同个浏览器发起第二次请求给服务器时，它还是会响应，但是呢，服务器不知道你就是刚才的那个浏览器。简言之，服务器不会去记住你是谁，所以是无状态协议.\"]},{\"header\":\"11、从浏览器地址栏输入url到显示主页的过程？\",\"slug\":\"_11、从浏览器地址栏输入url到显示主页的过程\",\"contents\":[\"浏览器通过域名查找对应的ip地址\",\"浏览器与服务器通过三次握手建议TCP连接\",\"浏览器向服务器发送一个HTTP请求\",\"服务器处理请求（处理请求参数、cookie，生成HTML响应）\",\"服务器返回一个HTML响应\",\"浏览器解析渲染页面\",\"TCP四次挥手，结束\"]},{\"header\":\"12、HTTP的传输过程？\",\"slug\":\"_12、http的传输过程\",\"contents\":[\"地址解析： 地址解析通过域名系统DNS 解析服务器域名从而获得主机的IP 地址。例如， 用客户端的浏览器请求http 😕/localhost.com: 8080/index . htm ， 则可从中分解出协议名、主机名、端口、对象路径等部分结果如下。 \",\"协议名： HTTP 。\",\"主机名： localhost.com\",\"端口： 8080\",\"对象路径： /index. html\",\"封装HTTP 数据包： 解析协议名、主机名、端口、对象路径等并结合本机自己的信息封装成一个HTTP 请求数据包\",\"封装TCP 包： 将HTTP 请求数据包进一步封装成TCP 数据包。\",\"建立TCP 连接： 基于TCP 的三次握手机制建立TCP 连接。\",\"客户端发送请求： 在建立连接后，客户端发送一个请求给服务器\",\"服务器响应：服务器在接收到请求后，结合业务逻辑进行数据处理，然后向客户端返回相应的响应信息。在响应信息中包含状态行、协议版本号、成功或错误的代码、消息体等内容\",\"服务器关闭TCP 连接： 服务器在向浏览器发送请求响应数据后关闭TCP 连接。但如果浏览器或者服务器在消息头中加入了Connection: keep - alive ，则TCP 连接在请求响应数据发送后仍然保持连接状态，在下一次请求中浏览器可以继续使用相同的连接发送请求。采用keep-alive 方式不但减少了请求响应的时间，还节约了网络带宽和系统资源。\"]},{\"header\":\"13、说下HTTP/1.0，1.1，2.0的区别？\",\"slug\":\"_13、说下http-1-0-1-1-2-0的区别\",\"contents\":[\"简化版区别：\",\"HTTP/1.0：默认是短连接，可以强制开启，通过加入Connection: keep - alive\",\"默认使用短连接，每次请求都需要建立一个TCP连接\",\"HTTP/1.1：默认为长连接\",\"最主要的改进就是引入了持久连接。所谓的持久连接即TCP连接默认不关闭，可以被多个请求复用\",\"引入了管道机制（pipelining），即在同一个TCP连接里面，客户端可以同时发送多个请求。这样就进一步改进了HTTP协议的效率\",\"HTTP/2.0：多路复用\",\"多路复用：在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应\",\"做了Header压缩、服务端推送等优化\"]},{\"header\":\"14、说一下TCP与UDP的区别？\",\"slug\":\"_14、说一下tcp与udp的区别\",\"contents\":[\"区别\",\"TCP\",\"UDP\",\"可靠性传输\",\"可靠\",\"不可靠\",\"连接\",\"面向连接\",\"无连接\",\"传输数据有序性\",\"有序\",\"无序\",\"传输速度\",\"相对UDP较慢\",\"较快\",\"流量控制和拥塞控制\",\"有\",\"没有\",\"字节\",\"首部有20字节\",\"首部有8字节\",\"报文格式\",\"面向字节流\",\"面向报文\",\"适用场景\",\"网页、邮件\",\"语音广播\"]},{\"header\":\"15、HTTP 如何实现长连接？在什么时候会超时？\",\"slug\":\"_15、http-如何实现长连接-在什么时候会超时\",\"contents\":[\"通过在头部（请求和响应头）设置Connection字段指定为keep-alive，HTTP/1.0协议支持，但是是默认关闭的，从HTTP/1.1以后，连接默认都是长连接。\",\"HTTP一般会有httpd守护进程，里面可以设置keep-alive timeout，当tcp连接闲置超过这个时间就会关闭，也可以在HTTP的header里面设置超时时间；TCP 的keep-alive包含三个参数，支持在系统内核的net.ipv4里面设置；当 TCP 连接之后，闲置了tcp_keepalive_time，则会发生侦测包，如果没有收到对方的ACK，那么会每隔 tcp_keepalive_intvl再发一次，直到发送了tcp_keepalive_probes，就会丢弃该连接。\",\"1. tcp_keepalive_intvl = 15 2. tcp_keepalive_probes = 5 3. tcp_keepalive_time = 1800 \"]},{\"header\":\"16、说说HTTP常用的状态码及其含义？\",\"slug\":\"_16、说说http常用的状态码及其含义\",\"contents\":[\"状态码\",\"类别\",\"1XX\",\"信息性状态码\",\"2XX\",\"成功类状态码\",\"3XX\",\"重定向状态码\",\"4XX\",\"客户端错误状态码\",\"5XX\",\"服务端错误状态码\"]},{\"header\":\"17、什么是HTTPS？\",\"slug\":\"_17、什么是https\",\"contents\":[\"HTTPS 是以安全为目标的HTTP 通道，它在HTTP 中加入SSL 层以提高数据传输的安全性。\",\"HTTP 被用于在Web 浏览器和网站服务器之间传递信息，但以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web 浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此HTTP 不适合传输一些敏感信息，比如身份证号码、密码等。为了数据传输的安全， HTTPS 在HTTP 的基础上加入了SSL 协议，SSL 依靠证书来验证服务器的身份，并对浏览器和服务器之间的通信进行数据加密，以保障数据传输的安全性，其端口一般是443 。\"]},{\"header\":\"18、HTTPS的加密流程是怎样的？\",\"slug\":\"_18、https的加密流程是怎样的\",\"contents\":[\"发起请求： 客户端在通过TCP 和服务器建立连接之后（ 443 端口），发出一个请求证书的消息给服务器，在该请求消息里包含自己可实现的算法列表和其他需要的消息。\",\"证书返回：服务器端在收到消息后回应客户端并返回证书，在证书中包含服务器信息、域名、申请证书的公司、公钥、数据加密算法等。\",\"证书验证： 客户端在收到证书后，判断证书签发机构是否正确，并使用该签发机构的公钥确认签名是否有效， 客户端还会确保在证书中列出的域名就是它正在连接的域名。如果客户端确认证书有效，则生成对称密钥，并使用公钥将对称密钥加密。\",\"密钥交换： 客户端将加密后的对称密钥发送给服务器，服务器在接收到对称密钥后使用私钥解密\",\"数据传输： 经过上述步骤，客户端和服务器就完成了密钥对的交换， 在之后的数据传输过程中， 客户端和服务端就可以基于对称加密（ 加密和解密使用相同密钥的加密算法）对数据加密后在网络上传输，保证了网络数据传输的安全性。\"]},{\"header\":\"19、HTTP 与 HTTPS 的区别？\",\"slug\":\"_19、http-与-https-的区别\",\"contents\":[\"HTTP\",\"HTTPS\",\"安全性\",\"不安全\",\"安全\",\"默认端口\",\"80\",\"443\",\"资源消耗\",\"较少\",\"较多\",\"是否需要证书\",\"不需要\",\"需要\",\"报文是否加密\",\"明文\",\"密文\"]},{\"header\":\"20、HTTP 常用的请求方式，区别和用途？\",\"slug\":\"_20、http-常用的请求方式-区别和用途\",\"contents\":[\"请求方式\",\"用途\",\"GET\",\"对服务资源获取的简单请求\",\"POST\",\"用于发送包含用户提交数据的请求\",\"PUT\",\"向服务提交数据，以修改数据\",\"DELETE\",\"删除服务器上的某些资源\",\"HEAD\",\"请求页面的首部，获取资源的元信息\",\"CONNECT\",\"HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器\",\"OPTIONS\",\"允许客户端查看服务器的性能\",\"TRACE\",\"回显服务器收到的请求，主要用于测试或诊断\",\"PATCH\",\"是对 PUT 方法的补充，用来对已知资源进行局部更新\"]},{\"header\":\"21、TCP的粘包和拆包？\",\"slug\":\"_21、tcp的粘包和拆包\",\"contents\":[\"TCP是面向流，没有界限的一串数据。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题\",\"为什么会产生粘包和拆包呢?\",\"要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包；\",\"接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包；\",\"要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包；\",\"待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。即TCP报文长度-TCP头部长度>MSS\",\"如何解决？\",\"发送端将每个数据包封装为固定长度\",\"在数据尾部增加特殊字符进行分割\",\"将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小\"]},{\"header\":\"22、说一下TCP的流量控制？\",\"slug\":\"_22、说一下tcp的流量控制\",\"contents\":[\"TCP三次握手，发送端和接收端进入到ESTABLISHED状态，它们即可以愉快地传输数据啦。\",\"但是发送端不能疯狂地向接收端发送数据，因为接收端接收不过来的话，接收方只能把处理不过来的数据存在缓存区里。如果缓存区都满了，发送方还在疯狂发送数据的话，接收方只能把收到的数据包丢掉，这就浪费了网络资源啦\",\"TCP 提供一种机制可以让发送端根据接收端的实际接收能力控制发送的数据量，这就是流量控制\"]},{\"header\":\"23、IP地址有哪些分类？\",\"slug\":\"_23、ip地址有哪些分类\",\"contents\":[\"一般可以这么认为，IP地址=网络号+主机号。\",\"网络号：它标志主机所连接的网络地址表示属于互联网的哪一个网络。\",\"主机号：它标志主机地址表示其属于该网络中的哪一台主机\",\"IP地址分为A，B，C，D，E五大类：\",\"A类地址(1~126)：以0开头，网络号占前8位，主机号占后面24位。\",\"B类地址(128~191)：以10开头，网络号占前16位，主机号占后面16位。\",\"C类地址(192~223)：以110开头，网络号占前24位，主机号占后面8位。\",\"D类地址(224~239)：以1110开头，保留位多播地址。\",\"E类地址(240~255)：以11110开头，保留位为将来使用\"]},{\"header\":\"24、说下ARP 协议的工作过程？\",\"slug\":\"_24、说下arp-协议的工作过程\",\"contents\":[\"ARP 协议协议，Address Resolution Protocol，地址解析协议，它是用于实现IP地址到MAC地址的映射\",\"当源主机和目的主机要进行通信时\",\"当源主机需要将一个数据包要发送到目的主机时，会首先检查自己的ARP列表，是否存在该IP地址对应的MAC地址；如果有，就直接将数据包发送到这个MAC地址\",\"如果没有，就向本地网段发起一个ARP请求的广播包，查询此目的主机对应的MAC地址。此ARP请求的数据包里，包括源主机的IP地址、硬件地址、以及目的主机的IP地址\",\"网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。如果不相同，就会忽略此数据包；如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中，如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个 ARP响应数据包，告诉对方自己是它需要查找的MAC地址\",\"源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败\"]},{\"header\":\"25、TCP 和 UDP 分别对应的常见应用层协议有哪些？\",\"slug\":\"_25、tcp-和-udp-分别对应的常见应用层协议有哪些\",\"contents\":[\"基于TCP的应用层协议有：HTTP、FTP、SMTP、TELNET、SSH\",\"HTTP：HyperText Transfer Protocol（超文本传输协议），默认端口80\",\"FTP: File Transfer Protocol (文件传输协议), 默认端口(20用于传输数据，21用于传输控制信息)\",\"SMTP: Simple Mail Transfer Protocol (简单邮件传输协议) ,默认端口25\",\"TELNET: Teletype over the Network (网络电传), 默认端口23\",\"SSH：Secure Shell（安全外壳协议），默认端口 22\",\"基于UDP的应用层协议：DNS、TFTP、SNMP\",\"DNS : Domain Name Service (域名服务),默认端口 53\",\"TFTP: Trivial File Transfer Protocol (简单文件传输协议)，默认端口69\",\"SNMP：Simple Network Management Protocol（简单网络管理协议），通过UDP端口161接收，只有Trap信息采用UDP端口162。\"]},{\"header\":\"26、 URI和URL的区别？\",\"slug\":\"_26、-uri和url的区别\",\"contents\":[\"URI，全称是Uniform Resource Identifier)，中文翻译是统一资源标志符，主要作用是唯一标识一个资源。\",\"URL，全称是Uniform Resource Location)，中文翻译是统一资源定位符，主要作用是提供资源的路径\",\"URI像是身份证，可以唯一标识一个人，而URL更像一个住址，可以通过URL找到这个人\"]},{\"header\":\"27、说一下TCP的拥塞控制？\",\"slug\":\"_27、说一下tcp的拥塞控制\",\"contents\":[\"官方回答：TCP拥塞控制是传输控制协议（英语：Transmission Control Protocol，缩写TCP）避免网络拥塞的算法，是互联网上主要的一个拥塞控制措施。它使用一套基于线增积减模式的多样化网络拥塞控制方法（包括慢启动和拥塞窗口等模式）来控制拥塞。\",\"拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。\",\"为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念\",\"拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。\",\"拥塞窗口 cwnd 变化的规则：\",\"只要网络中没有出现拥塞，cwnd 就会增大；\",\"但网络中出现了拥塞，cwnd 就减少\",\"其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了拥塞\"]},{\"header\":\"28、TCP拥塞控制的算法有哪些？\",\"slug\":\"_28、tcp拥塞控制的算法有哪些\",\"contents\":[\"慢启动\",\"拥塞避免\",\"拥塞发生\",\"快速恢复\",\"具体详情参考文章：https://xiaolincoding.com/network/3_tcp/tcp_feature.html#重传机制\"]},{\"header\":\"29、拔掉网线后，原有的TCP连接还在吗？\",\"slug\":\"_29、拔掉网线后-原有的tcp连接还在吗\",\"contents\":[\"TCP 连接在 Linux 内核中是一个名为 struct socket 的结构体，该结构体的内容包含 TCP 连接的状态等信息。当拔掉网线的时候，操作系统并不会变更该结构体的任何内容，所以 TCP 连接的状态也不会发生改变。\"]},{\"header\":\"30、TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？\",\"slug\":\"_30、tcp-keepalive-和-http-keep-alive-是一个东西吗\",\"contents\":[\"HTTP 的 Keep-Alive，是由应用层（用户态） 实现的，称为 HTTP 长连接；\",\"TCP 的 Keepalive，是由 TCP 层（内核态） 实现的，称为 TCP 保活机制；\"]},{\"header\":\"31、HTTP缓存有哪些？\",\"slug\":\"_31、http缓存有哪些\",\"contents\":[\"强制缓存：只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边\",\"协商缓存：通过服务端告知客户端是否可以使用缓存的方式，协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。\"]},{\"header\":\"32、HTTPS一定安全可靠吗？\",\"slug\":\"_32、https一定安全可靠吗\",\"contents\":[\"从协议本身来说，HTTPS协议目前是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。\"]},{\"header\":\"33、HTTPS解决了HTTP的哪些问题？\",\"slug\":\"_33、https解决了http的哪些问题\",\"contents\":[\"HTTP 由于是明文传输，所以安全上存在以下三个风险：\",\"窃听风险，比如通信链路上可以获取通信内容，用户号容易没。\",\"篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。\",\"冒充风险，比如冒充淘宝网站，用户钱容易没。\",\"HTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险：\",\"信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。\",\"校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。\",\"身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。\",\"混合加密的方式实现信息的机密性，解决了窃听的风险。\",\"摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。\",\"将服务器公钥放入到数字证书中，解决了冒充的风险。\"]},{\"header\":\"34、TCP 和 UDP 可以同时绑定相同的端口吗？\",\"slug\":\"_34、tcp-和-udp-可以同时绑定相同的端口吗\",\"contents\":[\"可以。\",\"传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。\",\"传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。\",\"当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。\"]},{\"header\":\"35、 多个 TCP 服务进程可以绑定同一个端口吗？\",\"slug\":\"_35、-多个-tcp-服务进程可以绑定同一个端口吗\",\"contents\":[\"如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”\",\"如果两个TCP服务进程绑定的IP地址不同，端口相同，则可以绑定(不包含0.0.0.0，可以代表任意地址）\"]},{\"header\":\"36、客户端的端口可以重复使用吗？\",\"slug\":\"_36、客户端的端口可以重复使用吗\",\"contents\":[\"在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。\",\"TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。\",\"所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。\"]},{\"header\":\"37、客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？\",\"slug\":\"_37、客户端-tcp-连接-time-wait-状态过多-会导致端口资源耗尽而无法建立新的连接吗\",\"contents\":[\"要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。\",\"如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。\"]},{\"header\":\"38、HTTPS TSL和TCP能同时握手吗？\",\"slug\":\"_38、https-tsl和tcp能同时握手吗\",\"contents\":[\"HTTPS 是先进行 TCP 三次握手，再进行 TLSv1.2 四次握手\",\"同时握手的前提：\",\"客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3；\",\"客户端和服务端已经完成过一次通信；\"]},{\"header\":\"39、如何在 Linux 系统中查看 TCP 状态？\",\"slug\":\"_39、如何在-linux-系统中查看-tcp-状态\",\"contents\":[\"TCP 的连接状态查看，在 Linux 可以通过 netstat -napt 命令查看\"]},{\"header\":\"40、为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？\",\"slug\":\"_40、为什么每次建立-tcp-连接时-初始化的序列号都要求不一样呢\",\"contents\":[\"为了防止历史报文被下一个相同四元组（源地址、源端口、目的地址、目的端口）的连接接收（主要方面）；\",\"了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收；\"]},{\"header\":\"41、TCP第一次握手丢失了，会发生什么？\",\"slug\":\"_41、tcp第一次握手丢失了-会发生什么\",\"contents\":[\"当客户端想和服务端建立 TCP 连接的时候，首先第一个发的就是 SYN 报文，然后进入到 SYN_SENT 状态。\",\"此时客户端迟迟收不到服务端发送的SYN+ACK报文，就会触发超时重传机制，重传 SYN 报文，而且重传的 SYN 报文的序列号都是一样的\",\"在 Linux 里，客户端的 SYN 报文最大重传次数由 tcp_syn_retries内核参数控制，这个参数是可以自定义的，默认值一般是 5。\",\"# cat /proc/sys/net/ipv4/tcp_syn_retries 5 \",\"通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，每次超时的时间是上一次的 2 倍。\",\"当第五次超时重传后，会继续等待 32 秒，如果服务端仍然没有回应 ACK，客户端就不再发送 SYN 包，然后断开 TCP 连接。\"]},{\"header\":\"42、TCP第二次握手丢失了，会发生什么？\",\"slug\":\"_42、tcp第二次握手丢失了-会发生什么\",\"contents\":[\"客户端会重传 SYN 报文，也就是第一次握手，最大重传次数由 tcp_syn_retries内核参数决定；\",\"服务端会重传 SYN-ACK 报文，也就是第二次握手，最大重传次数由 tcp_synack_retries 内核参数决定\"]},{\"header\":\"43、TCP第三次握手丢失了，会发生什么？\",\"slug\":\"_43、tcp第三次握手丢失了-会发生什么\",\"contents\":[\"第三次握手过程：客户端收到服务端的 SYN-ACK 报文后，就会给服务端回一个 ACK 报文，也就是第三次握手，此时客户端状态进入到 ESTABLISH 状态。\",\"因为这个第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。\",\"注意，ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。\"]}]},\"/interview/computer/\":{\"title\":\"计算机基础\",\"contents\":[]},\"/interview/distributed/ElasticSearch.html\":{\"title\":\"ElasticSearch 21道面试题\",\"contents\":[{\"header\":\"1、为什么要使用 Elasticsearch?\",\"slug\":\"_1、为什么要使用-elasticsearch\",\"contents\":[\"系统中的数据， 随着业务的发展， 时间的推移， 将会非常多，而业务中往往采用模糊查询进行数据的 搜索，而模糊查询会导致查询引擎放弃索引， 导致系统查询数据时都是全表扫描，在百万级别的数据库中， 查询效率是非常低下的，而我们使用 ES 做一个全文索引， 将经常查询的系统功能的某些字段，比如说电 商系统的商品表中商品名，描述、价格还有 id 这些字段我们放入 ES 索引库里，可以提高查询速度。\"]},{\"header\":\"2、Elasticsearch 的 master 选举流程？\",\"slug\":\"_2、elasticsearch-的-master-选举流程\",\"contents\":[\"Elasticsearch 的选主是 ZenDiscovery 模块负责的， 主要包含 Ping(节点之间通过这个 RPC 来发现彼此)和 Unicast (单播模块包含一个主机列表以控制哪些节点需要 ping 通)这两部分\",\"对所有可以成为 master 的节点(node.master: true)根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个(第 0 位)节点， 暂且认为它是 master 节点\",\"如果对某个节点的投票数达到一定的值(可以成为 master 节点数 n/2+1) 并且该节点自己也选举自己，那这个节点就是 master 。否则重新选举一直到满足上述条件\",\"master 节点的职责主要包括集群、节点和索引的管理， 不负责文档级别的管理； data 节点可以关闭 http功能\"]},{\"header\":\"3、Elasticsearch 集群脑裂问题？\",\"slug\":\"_3、elasticsearch-集群脑裂问题\",\"contents\":[\"所谓脑裂问题（类似于精神分裂），就是同一个集群中的不同节点，对于集群的状态有了不一样的理解。\",\"由于某些节点的失效，部分节点的网络连接会断开，并形成一个与原集群一样名字的集群，这种情况成为集群脑裂（split-brain）现象。这个问题非常危险，因为两个新形成的集群会同时索引和修改集群的数据。\",\"“脑裂”问题可能的成因：\",\"网络问题：集群间的网络延迟导致一些节点访问不到 master，认为 master 挂掉了从而选举出新的master，并对 master 上的分片和副本标红，分配新的主分片\",\"节点负载：主节点的角色既为 master 又为 data，访问量较大时可能会导致 ES 停止响应造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。\",\"内存回收：data 节点上的 ES 进程占用的内存较大，引发 JVM 的大规模内存回收，造成 ES 进程失去响应\",\"脑裂问题解决方案：\",\"减少误判： discovery.zen.ping_timeout 节点状态的响应时间， 默认为 3s，可以适当调大，如果 master在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数(如 6s ， discovery.zen.ping_timeout:6 ) ，可适当减少误判。\",\"选举触发: discovery.zen.minimum_master_nodes:1，该参数是用于控制选举行为发生的最小集群主节点数量。当备选主节点的个数大于等于该参数的值， 且备选主节点中有该参数个节点认为主节点挂了， 进行选举。官方建议为(n/2) +1，n 为主节点个数 (即有资格成为主节点的节点个数)\",\"角色分离：即 master 节点与data 节点分离，限制角色 \",\"主节点配置为： node.master: true node.data: false\",\"从节点配置为： node.master: false node.data: true\"]},{\"header\":\"4、文档索引步骤顺序是什么？\",\"slug\":\"_4、文档索引步骤顺序是什么\",\"contents\":[\"新建单个文档所需要的步骤顺序：\",\"客户端向 Node 1 发送新建、索引或者删除请求。\",\"节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。\",\"Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。\"]},{\"header\":\"5、Elasticsearch 索引文档的流程？\",\"slug\":\"_5、elasticsearch-索引文档的流程\",\"contents\":[\"协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。\",\"shard = hash(document_id) % (num_of_primary_shards) \",\"当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh；\",\"当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush。\",\"在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时。\"]},{\"header\":\"6、 Elasticsearch 更新和删除文档的流程？\",\"slug\":\"_6、-elasticsearch-更新和删除文档的流程\",\"contents\":[\"删除和更新也都是写操作，但是 Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；\",\"磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。\",\"在新的文档被创建时， Elasticsearch 会为该文档指定一个版本号， 当执行更新时， 旧版本的文档在.del文件中被标记为删除， 新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询， 但是会在结 果中被过滤掉。\"]},{\"header\":\"7、Elasticsearch 搜索的流程？\",\"slug\":\"_7、elasticsearch-搜索的流程\",\"contents\":[\"搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；\",\"在初始查询阶段时，查询会广播到索引中每一个分片拷贝(主分片或者副本分片) 。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS：在搜索的时候是会查询 Filesystem Cache 的，但是有部分数据还在 Memory Buffer，所以搜索是近实时的。\",\"每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点， 它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。\",\"接下来就是取回阶段， 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了， 协调节点返回结果给客户端。\",\"Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确， DFS Query Then Fetch 增加了一个预查询的处理，询问 Term 和 Document frequency，这个评分更准确， 但是性能会变差。\"]},{\"header\":\"8、GC 方面，在使用 Elasticsearch 时要注意什么？\",\"slug\":\"_8、gc-方面-在使用-elasticsearch-时要注意什么\",\"contents\":[\"倒排词典的索引需要常驻内存，无法 GC，需要监控 data node 上 segment memory 增长趋势。\",\"各类缓存， field cache, filter cache, indexing cache, bulk queue 等等， 要设置合理的大小， 并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他 任务吗？避免采用 clear cache 等“自欺欺人”的方式来释放内存。\",\"避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景， 可以采用 scan & scroll api 来实现。\",\"cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。\",\"想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。\"]},{\"header\":\"9、Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？\",\"slug\":\"_9、elasticsearch-在部署时-对-linux-的设置有哪些优化方法\",\"contents\":[\"64 GB 内存的机器是非常理想的， 但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反。\",\"如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\",\"如果你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点， 查询和索引性能都有提升。如果你负担得起， SSD 是一个好的选择。\",\"即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。\",\"请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化。\",\"通过设置 gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time 可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。\",\"Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。\",\"不要随意修改垃圾回收器(CMS)和各个线程池的大小。\",\"你的内存的(少于) 一半给 Lucene (但不要超过 32 GB！) ，通过 ES_HEAP_SIZE 环境变量设置。\",\"内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。\",\"Lucene 使用了大量的文件。同时， Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符， 设置一个很大的值，如 64,000。\"]},{\"header\":\"10、索引阶段性能提升方法有哪些？\",\"slug\":\"_10、索引阶段性能提升方法有哪些\",\"contents\":[\"使用批量请求并调整其大小：每次批量数据 5 – 15 MB 大是个不错的起始点。\",\"存储：使用 SSD\",\"段和合并： Elasticsearch 默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100 –200 MB/s。如果你在做批量导入， 完全不在意搜索， 你可以彻底关掉合并限流。 另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB ，这可以在一次清空触发的时候在事务日志里积累出更大的段。\",\"如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s。\",\"如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本。\"]},{\"header\":\"11、elasticsearch 了解多少，说说你们公司 es 的集群架构，索引数据大小，分片有多少，以及一些调优手段 。\",\"slug\":\"_11、elasticsearch-了解多少-说说你们公司-es-的集群架构-索引数据大小-分片有多少-以及一些调优手段-。\",\"contents\":[\"面试官：想了解应聘者之前公司接触的 ES 使用场景、规模，有没有做过比较大规模的索引设计、规划、调优。\",\"解答：如实结合自己的实践场景回答即可。\",\"比如：ES 集群架构 13 个节点，索引根据通道不同共 20+索引，根据日期，每日递增 20+，索引：10分片，每日递增 1 亿+数据，每个通道每天索引大小控制：150GB 之内。\",\"仅索引层面调优手段：\",\"设计阶段调优\",\"根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引；\",\"使用别名进行索引管理；\",\"每天凌晨定时对索引做 force_merge 操作，以释放空间；\",\"采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink操作，以缩减存储；\",\"采取 curator 进行索引的生命周期管理；\",\"仅针对需要分词的字段，合理的设置分词器；\",\"Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。……..\",\"写入调优\",\"写入前副本数设置为 0；\",\"写入前关闭 refresh_interval 设置为-1，禁用刷新机制；\",\"写入过程中：采取 bulk 批量写入；\",\"写入后恢复副本数和刷新间隔；\",\"尽量使用自动生成的 id。\",\"查询调优\",\"禁用 wildcard；\",\"禁用批量 terms（成百上千的场景）；\",\"充分利用倒排索引机制，能 keyword 类型尽量 keyword；\",\"数据量大时候，可以先基于时间敲定索引再检索；\",\"设置合理的路由机制\"]},{\"header\":\"12、Elasticsearch 对于大数据量(上亿量级) 的聚合如何实现？\",\"slug\":\"_12、elasticsearch-对于大数据量-上亿量级-的聚合如何实现\",\"contents\":[\"Elasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct 或者 unique 值的数目。它是基于 HLL 算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的 结果中的 bits 做概率估算从而得到基数。其特点是： 可配置的精度， 用来控制内存的使用(更精确 ＝ 更 多内存)；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无 论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关\"]},{\"header\":\"13、在并发情况下，Elasticsearch 如果保证读写一致？\",\"slug\":\"_13、在并发情况下-elasticsearch-如果保证读写一致\",\"contents\":[\"可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；\",\"对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用， 也可能存在因为网络等原因导致写入副本失败， 这样该副本被认为故 障，分片将会在一个不同的节点上重建。\",\"对于读操作， 可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片， 确保文档是最新版本。\"]},{\"header\":\"14、如何监控 Elasticsearch 集群状态？\",\"slug\":\"_14、如何监控-elasticsearch-集群状态\",\"contents\":[\"elasticsearch-head 插件 通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能， 也可以分析过去的集群、 索引和节点指标\"]},{\"header\":\"15、是否了解字典树？\",\"slug\":\"_15、是否了解字典树\",\"contents\":[\"常用字典数据结构如下所示:\",\"字典树又称单词查找树， Trie 树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计， 排序和保存大量的字符串 (但不仅限于字符串) ，所以经常被搜索引擎系统用于文本词频统计。\",\"它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。\",\"Trie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。\",\"它有 3 个基本性质:\",\"根节点不包含字符，除根节点外每一个节点都只包含一个字符\",\"从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串\",\"每个节点的所有子节点包含的字符都不相同。\",\"对于中文的字典树，每个节点的子节点用一个哈希表存储， 这样就不用浪费太大的空间， 而且查询速度上 可以保留哈希的复杂度 O(1)。\"]},{\"header\":\"16、 Elasticsearch 中的集群、节点、索引、文档、类型是什么？\",\"slug\":\"_16、-elasticsearch-中的集群、节点、索引、文档、类型是什么\",\"contents\":[\"集群是一个或多个节点(服务器) 的集合， 它们共同保存您的整个数据， 并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识， 默认情况下为“elasticsearch”。此名称很重要， 因为如果节点设 置为按名称加入群集，则该节点只能是群集的一部分。\",\"节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。\",\"索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间， 映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =>数据库 Elasticsearch =>索引\",\"文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构(字段) ，但是对于通用字段应该具有相同的数据类型。 MySQL => Databases => Tables => Columns / Rows Elasticsearch => Indices => Types =>具有属性的文档.\",\"类型是索引的逻辑类别/分区，其语义完全取决于用户。\"]},{\"header\":\"17、Elasticsearch 中的倒排索引是什么？\",\"slug\":\"_17、elasticsearch-中的倒排索引是什么\",\"contents\":[\"倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。 ES 中的倒排索引其实就是 lucene 的倒排索引，区别于传统的正向索引， 倒排索引会再存储数据时将关键词和数据进行关联，保存到倒排表中，然后查询时，将查询内容进行分词后在倒排表中进行查询，最后匹配数 据即可。\"]},{\"header\":\"18、Elasticsearch与数据库之间的对应关系是什么？\",\"slug\":\"_18、elasticsearch与数据库之间的对应关系是什么\",\"contents\":[\"Elasticsearch\",\"数据库\",\"索引index\",\"数据库\",\"文档\",\"表数据\",\"索引库(index)中的映射\",\"数据库(database)中的表结构(table)\",\"字段(Field)\",\"数据表的字段，对文档数据根据不同属性进行的分类标识\",\"反向索引\",\"索引\",\"查询DSL\",\"SQL\",\"get http://\",\"select * from table\",\"put http://\",\"update table set\",\"delete http://\",\"delete\"]},{\"header\":\"19、数据库修改信息如何同步ElasticSearch？\",\"slug\":\"_19、数据库修改信息如何同步elasticsearch\",\"contents\":[\"同步调用：直接在代码里写逻辑，数据在增删改查进数据库的同时，也往es里同步一份\",\"使用官方的logstash，定时查询数据库，查询到数据有变化就发送到es中\",\"利用数据库的binlog同步变化数据，然后将数据发送给es,当然也可以通过java代码监听拿到数据，再发送到es或做其他处理。\",\"MQ中间件，有数据变化的时候，就通知mq，然后监听mq实现数据同步到mq\"]},{\"header\":\"20、如何在保留不变性的前提下实现倒排索引的更新？\",\"slug\":\"_20、如何在保留不变性的前提下实现倒排索引的更新\",\"contents\":[\"倒排索引是一种数据结构，用于存储某个词汇项和出现该词汇项的文档之间的映射。在搜索引擎中，如Elasticsearch，倒排索引是实现快速全文搜索的关键。\",\"然而，对于倒排索引的更新，一种常见的做法是，不直接在现有的倒排索引上修改，而是创建一个新的倒排索引来反映数据的更新，这样就能保持不变性。这种方法通常被称为\\\"索引重建\\\"或\\\"索引刷新\\\"。这通常在后台发生，不会影响正在进行的搜索操作。\",\"以下是一个简单的步骤示例：\",\"当文档更新时，首先在内存中创建一个新的倒排索引，包含更新后的文档。\",\"创建新索引的同时，旧的索引仍然可用，可以继续响应搜索请求。\",\"一旦新的索引创建完成，搜索操作就可以切换到新的索引上。\",\"旧的索引在不再被使用后，可以被安全地删除。\",\"这种方法的优点是，它能保持不变性，这对于在分布式系统中保持数据一致性很重要。此外，它还允许系统在更新数据的同时，继续响应搜索请求，从而提供高可用性。\",\"当然，这种方法的代价是需要额外的存储空间来保存新的索引，以及可能需要更多的时间来创建新的索引。一些系统可能会使用一些优化策略，比如只对变化的部分创建新的索引，或者在低流量时进行索引重建等。\",\"在实践中，Elasticsearch和许多其他搜索引擎都使用了这种方法来处理索引的更新。同时，它们还使用了一些其他技术，如合并索引段（Merging Segments），以减少存储的使用和优化查询性能。\"]},{\"header\":\"21、ElasticSearch的主要功能及应用场景？\",\"slug\":\"_21、elasticsearch的主要功能及应用场景\",\"contents\":[\"主要功能：\",\"1）海量数据的分布式存储以及集群管理，达到了服务与数据的高可用以及水平扩展；\",\"2）近实时搜索，性能卓越。对结构化、全文、地理位置等类型数据的处理；\",\"3）海量数据的近实时分析（聚合功能）\",\"应用场景：\",\"1）网站搜索、垂直搜索、代码搜索；\",\"2）日志管理与分析、安全指标监控、应用性能监控、Web抓取舆情分析\"]},{\"header\":\"22、ElasticSearch是怎么进行全文检索的？模糊还是精准？\",\"slug\":\"_22、elasticsearch是怎么进行全文检索的-模糊还是精准\",\"contents\":[\"Elasticsearch是一个高度可扩展的开源全文搜索和分析引擎。它能够在几乎实时内存取和分析大量数据。Elasticsearch支持全文搜索，这是它的一个主要优势。\",\"在进行全文搜索时，Elasticsearch使用了称为倒排索引的数据结构，它允许在极短的时间内对大量文本进行高效搜索。倒排索引是一种映射，它将每个唯一的词汇项映射到包含它的文档。\",\"Elasticsearch的搜索可以是精确的，也可以是模糊的。这取决于你使用的查询类型和配置。\",\"精确查询：也称作Term查询，用于搜索确切的值。例如，如果你有一个\\\"status\\\"字段，并希望找到所有状态为\\\"active\\\"的文档，你可以使用Term查询。这种查询通常用于过滤结构化的数据，如数字、日期、枚举类型等。\",\"全文查询：也称作Match查询，用于执行全文搜索。Match查询会对你的查询进行分析（例如，将句子分解为单词，处理大小写等），然后在倒排索引中查找匹配项。这种查询通常用于非结构化的文本数据。\",\"模糊查询：Elasticsearch也支持模糊查询（Fuzzy Query），它可以在查询词汇项附近查找匹配项。这可以用来抵消拼写错误或者近似匹配。模糊查询使用Levenshtein编辑距离来找到相似的词汇项。\",\"Elasticsearch的查询功能非常强大，可以通过组合各种查询类型和过滤器来构建复杂的搜索条件。\"]}]},\"/interview/distributed/RabbitMQ.html\":{\"title\":\"RabbitMQ 26道面试题\",\"contents\":[{\"header\":\"1、为什么要使用MQ\",\"slug\":\"_1、为什么要使用mq\",\"contents\":[]},{\"header\":\"1、流量消峰\",\"slug\":\"_1、流量消峰\",\"contents\":[\"举个例子：如果订单系统最多能处理一万次订单，这个处理能力应付正常时段的下单时绰绰有余，正常时段我们下单一秒后就能返回结果。但是在高峰期，如果有两万次下单操作系统是处理不了的，只能限制订单超过一万后不允许用户下单。使用消息队列做缓冲，我们可以取消这个限制，把一秒内下的订单分散成一段时间来处理，这时有些用户可能在下单十几秒后才能收到下单成功的操作，但是比不能下单的体验要好。\"]},{\"header\":\"2、应用解耦\",\"slug\":\"_2、应用解耦\",\"contents\":[\"以电商应用为例，应用中有订单系统、库存系统、物流系统、支付系统。用户创建订单后，如果耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障，都会造成下单操作异常。当转变成基于消息队列的方式后，系统间调用的问题会减少很多，比如物流系统因为发生故障，需要几分钟来修复。在这几分钟的时间里，物流系统要处理的内存被缓存在消息队列中，用户的下单操作可以正常完成。当物流系统恢复后，继续处理订单信息即可，中单用户感受不到物流系统的故障，提升系统的可用性。\"]},{\"header\":\"3、异步处理\",\"slug\":\"_3、异步处理\",\"contents\":[\"有些服务间调用是异步的，例如 A 调用 B，B 需要花费很长时间执行，但是 A 需要知道 B 什么时候可以执行完，以前一般有两种方式：\",\"A 过一段时间去调用 B 的查询 api 查询\",\"A 提供一个 callback api， B 执行完之后调用 api 通知 A 服务。\",\"这两种方式都不是很优雅，使用消息总线，可以很方便解决这个问题，A 调用 B 服务后，只需要监听 B 处理完成的消息，当 B 处理完成后，会发送一条消息给 MQ，MQ 会将此消息转发给 A 服务。这样 A 服务既不用循环调用 B 的查询 api，也不用提供 callback api。同样 B 服务也不用做这些操作。A 服务还能及时的得到异步处理成功的消息。\"]},{\"header\":\"2、什么是 RabbitMQ\",\"slug\":\"_2、什么是rabbitmq\",\"contents\":[\"RabbitMQ是一个消息中间件：它接受并转发消息。你可以把它当做一个快递站点，当你要发送一个包裹时，你把你的包裹放到快递站，快递员最终会把你的快递送到收件人那里，按照这种逻辑RabbitMQ是一个快递站，一个快递员帮你传递快件。RabbitMQ与快递站的主要区别在于，它不处理快件而是接收，存储和转发消息数据。\"]},{\"header\":\"3、 RabbitMQ 各组件的功能\",\"slug\":\"_3、rabbitmq各组件的功能\",\"contents\":[\"Server：接收客户端的连接，实现AMQP实体服务。\",\"Connection：连接，应用程序与Server的网络连接，TCP连接。\",\"Channel：信道，消息读写等操作在信道中进行。客户端可以建立多个信道，每个信道代表一个会话任务。如果每一次访问 RabbitMQ 都建立一个 Connection，在消息量大的时候建立 TCP Connection 的开销将是巨大的，效率也较低。Channel 是在 connection 内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread 创建单独的 channel 进行通讯，AMQP method 包含了 channel id 帮助客户端和 message broker 识别 channel，所以 channel 之间是完全隔离的。Channel 作为轻量级的\",\"Connection极大减少了操作系统建立TCP connection的开销\",\"Message：消息，应用程序和服务器之间传送的数据，消息可以非常简单，也可以很复杂。由Properties和Body组成。Properties为外包装，可以对消息进行修饰，比如消息的优先级、延迟等高级特性；Body就是消息体内容。\",\"Virtual Host：虚拟主机，用于逻辑隔离。一个虚拟主机里面可以有若干个Exchange和Queue，同一个虚拟主机里面不能有相同名称的Exchange或Queue。\",\"Exchange：交换器，接收消息，按照路由规则将消息路由到一个或者多个队列。如果路由不到，或者返回给生产者，或者直接丢弃。RabbitMQ常用的交换器常用类型有direct、topic、fanout、headers四种，后面详细介绍。\",\"Binding：绑定，交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个RoutingKey，Binding 信息被保存到 exchange 中的查询表中，用于 message 的分发依据\",\"RoutingKey：路由键，生产者将消息发送给交换器的时候，会发送一个RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。路由键通常为一个“.”分割的字符串，例如“com.rabbitmq”。\",\"Queue：消息队列，用来保存消息，供消费者消费。\"]},{\"header\":\"3、RabbitMQ工作原理\",\"slug\":\"_3、rabbitmq工作原理\",\"contents\":[\"不得不看一下经典的图了，如下👇\",\"AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下：\",\"生产者是连接到 Server，建立一个连接，开启一个信道。\",\"生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。\",\"消费者也需要进行建立连接，开启信道等操作，便于接收消息。\",\"生产者发送消息，发送到服务端中的虚拟主机。\",\"虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。\",\"订阅了消息队列的消费者就可以获取到消息，进行消费。\"]},{\"header\":\"4、RabbitMQ 上的一个 queue 中存放的 message 是否有数量限制？\",\"slug\":\"_4、rabbitmq-上的一个-queue-中存放的-message-是否有数量限制\",\"contents\":[\"可以认为是无限制，因为限制取决于机器的内存，但是消息过多会导致处理效率的下降。\"]},{\"header\":\"5、RabbitMQ 允许发送的 message 最大可达多大？\",\"slug\":\"_5、rabbitmq-允许发送的-message-最大可达多大\",\"contents\":[\"根据 AMQP 协议规定，消息体的大小由 64-bit 的值来指定，所以你就可以知道到底能发多大的数据了\"]},{\"header\":\"6、RabbitMQ的工作模式\",\"slug\":\"_6、rabbitmq的工作模式\",\"contents\":[]},{\"header\":\"1、simple模式（即最简单的收发模式）\",\"slug\":\"_1、simple模式-即最简单的收发模式\",\"contents\":[\"生产者产生消息，将消息放入队列\",\"消息的消费者(consumer) 监听 消息队列，如果队列中有消息，就消费掉，消息被拿走后，自动从队列中删除(隐患 消息可能没有被消费者正确处理，已经从队列中消失了，造成消息的丢失，这里可以设置成手动的ack，但如果设置成手动ack，处理完后要及时发送ack消息给队列，否则会造成内存溢出)。\"]},{\"header\":\"2、Work Queues(工作队列)\",\"slug\":\"_2、work-queues-工作队列\",\"contents\":[\"消息产生者将消息放入队列，消费者可以有多个，消费者1,消费者2同时监听同一个队列,消息被消费。C1 C2共同争抢当前的消息队列内容,谁先拿到谁负责消费消息(隐患：高并发情况下,默认会产生某一个消息被多个消费者共同使用,可以设置一个开关(syncronize) 保证一条消息只能被一个消费者使用)。\"]},{\"header\":\"3、publish/subscribe发布订阅(共享资源)\",\"slug\":\"_3、publish-subscribe发布订阅-共享资源\",\"contents\":[\"每个消费者监听自己的队列；\",\"生产者将消息发给broker，由交换机将消息转发到绑定此交换机的每个队列，每个绑定交换机的队列都将接收到消息。\"]},{\"header\":\"4、routing路由模式\",\"slug\":\"_4、routing路由模式\",\"contents\":[\"消息生产者将消息发送给交换机按照路由判断，路由是字符串(info) 当前产生的消息携带路由字符(对象的方法)，交换机根据路由的key，只能匹配上路由key对应的消息队列，对应的消费者才能消费消息;\",\"根据业务功能定义路由字符串\",\"从系统的代码逻辑中获取对应的功能字符串,将消息任务扔到对应的队列中。\",\"业务场景:error 通知;EXCEPTION;错误通知的功能;传统意义的错误通知;客户通知;利用key路由,可以将程序中的错误封装成消息传入到消息队列中,开发者可以自定义消费者,实时接收错误;\"]},{\"header\":\"5、topic 主题模式(路由模式的一种)\",\"slug\":\"_5、topic-主题模式-路由模式的一种\",\"contents\":[\"星号井号代表通配符\",\"星号代表多个单词,井号代表一个单词\",\"路由功能添加模糊匹配\",\"消息产生者产生消息,把消息交给交换机\",\"交换机根据key的规则模糊匹配到对应的队列,由队列的监听消费者接收消息消费\",\"PS：（在我的理解看来就是routing查询的一种模糊匹配，就类似sql的模糊查询方式）\"]},{\"header\":\"7、如何保证RabbitMQ消息的顺序性？\",\"slug\":\"_7、如何保证rabbitmq消息的顺序性\",\"contents\":[\"拆分多个 queue(消息队列)，每个 queue(消息队列) 一个 consumer(消费者)，就是多一些 queue(消息队列)而已，确实是麻烦点；\",\"或者就一个 queue (消息队列)但是对应一个 consumer(消费者)，然后这个 consumer(消费者)内部用内存队列做排队，然后分发给底层不同的 worker 来处理。\"]},{\"header\":\"8、RabbitMQ消息丢失的情况有哪些？\",\"slug\":\"_8、rabbitmq消息丢失的情况有哪些\",\"contents\":[\"生产者发送消息RabbitMQ Server 消息丢失\",\"RabbitMQ Server中存储的消息丢失\",\"RabbitMQ Server中存储的消息分发给消费者者丢失\"]},{\"header\":\"1、生产者发送消息RabbitMQ Server 消息丢失\",\"slug\":\"_1、生产者发送消息rabbitmq-server-消息丢失\",\"contents\":[\"发送过程中存在网络问题，导致消息没有发送成功\",\"代码问题，导致消息没发送\"]},{\"header\":\"2、RabbitMQ Server中存储的消息丢失\",\"slug\":\"_2、rabbitmq-server中存储的消息丢失\",\"contents\":[\"消息没有持久化，服务器重启导致存储的消息丢失\"]},{\"header\":\"3、RabbitMQ Server到消费者消息丢失\",\"slug\":\"_3、rabbitmq-server到消费者消息丢失\",\"contents\":[\"消费端接收到相关消息之后，消费端还没来得及处理消息，消费端机器就宕机了\",\"处理消息存在异常\"]},{\"header\":\"9、RabbitMQ如何保证消息不丢失？\",\"slug\":\"_9、rabbitmq如何保证消息不丢失\",\"contents\":[\"针对上面的情况，确保消息不丢失\",\"生产者发送消息RabbitMQ Server 消息丢失解决方案：\",\"常用解决方案：发送方确认机制（publisher confirm）\",\"开启AMQP的事务处理（不推荐）\",\"RabbitMQ Server中存储的消息丢失解决方案：\",\"消息回退：通过设置 mandatory 参数可以在当消息传递过程中不可达目的地时将消息返回给生产者\",\"设置持久化：保证重启过程中，交换机和队列也是持久化的\",\"RabbitMQ Server到消费者消息丢失解决方案：\",\"手动ack确认机制\"]},{\"header\":\"1、生产者发送消息RabbitMQ Server 消息丢失解决方案\",\"slug\":\"_1、生产者发送消息rabbitmq-server-消息丢失解决方案\",\"contents\":[]},{\"header\":\"1、发布确认机制\",\"slug\":\"_1、发布确认机制\",\"contents\":[\"生产者将信道设置成 confirm 模式，一旦信道进入 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，一旦消息被投递到所有匹配的队列之后，broker就会发送一个确认给生产者(包含消息的唯一 ID)，这就使得生产者知道消息已经正确到达目的队列了，如果消息和队列是可持久化的，那么确认消息会在将消息写入磁盘之后发出，broker 回传给生产者的确认消息中 delivery-tag 域包含了确认消息的序列号，此外 broker 也可以设置basic.ack 的 multiple 域，表示到这个序列号之前的所有消息都已经得到了处理。 confirm 模式最大的好处在于它是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果 RabbitMQ 因为自身内部错误导致消息丢失，就会发送一条 nack 消息，生产者应用程序同样可以在回调方法中处理该 nack 消息。\",\"发布确认分为三种：\",\"单独发布确认：这是一种简单的确认方式，它是一种同步确认发布的方式，也就是发布一个消息之后只有它被确认发布，后续的消息才能继续发布，waitForConfirmsOrDie(long)这个方法只有在消息被确认的时候才返回，如果在指定时间范围内这个消息没有被确认那么它将抛出异常。\",\"这种确认方式有一个最大的缺点就是：**发布速度特别的慢，**因为如果没有确认发布的消息就会阻塞所有后续消息的发布，这种方式最多提供每秒不超过数百条发布消息的吞吐量。当然对于某些应用程序来说这可能已经足够了。\",\"批量发布确认：上面那种方式非常慢，与单个等待确认消息相比，先发布一批消息然后一起确认可以极大地提高吞吐量，当然这种方式的缺点就是:当发生故障导致发布出现问题时，不知道是哪个消息出现问题了，我们必须将整个批处理保存在内存中，以记录重要的信息而后重新发布消息。当然这种方案仍然是同步的，也一样阻塞消息的发布。\",\"异步发布确认：异步确认虽然编程逻辑比上两个要复杂，但是性价比最高，无论是可靠性还是效率都没得说，他是利用回调函数来达到消息可靠性传递的，这个中间件也是通过函数回调来保证是否投递成功\"]},{\"header\":\"2、开启AMQP的事务处理（不推荐）\",\"slug\":\"_2、开启amqp的事务处理-不推荐\",\"contents\":[\"为什么不推荐呢，因为它是同步的，一条消息发送之后会使发送端阻塞，以等待RabbitMQ Server的回应，之后才能继续发送下一条消息，生产者生产消息的吞吐量和性能都会大大降低，这就跟单独发布确认一样。\",\"如何使用：在生产者发送消息之前，通过channel.txSelect开启一个事务，接着发送消息， 如果消息投递server失败，进行事务回滚channel.txRollback，然后重新发送， 如果server收到消息，就提交事务channel.txCommit\"]},{\"header\":\"2、RabbitMQ Server中存储的消息丢失解决方案\",\"slug\":\"_2、rabbitmq-server中存储的消息丢失解决方案\",\"contents\":[\"第一种保证消息丢失，只能够保证发送方发送消息成功到达交换机，若此时服务器存在问题或者绑定的routingKey不正确，导致消息发送失败，那么消息最终也会丢失。\",\"采用消息回退：通过设置 mandatory 参数可以在当消息传递过程中不可达目的地时将消息返回给生产者\",\"设置持久化\"]},{\"header\":\"1、消息回退\",\"slug\":\"_1、消息回退\",\"contents\":[\"源码： mandatory参数 true:交换机无法将消息进行路由时，会将该消息返回给生产者 false：如果发现消息无法进行路由，则直接丢弃\",\"public void basicPublish(String exchange, String routingKey, boolean mandatory, BasicProperties props, byte[] body) throws IOException { this.delegate.basicPublish(exchange, routingKey, mandatory, props, body); } \",\"有了 mandatory 参数和回退消息，我们获得了对无法投递消息的感知能力，有机会在生产者的消息无法被投递时发现并处理。但有时候，我们并不知道该如何处理这些无法路由的消息，最多打个日志，然后触发报警，再来手动处理。而通过日志来处理这些无法路由的消息是很不优雅的做法，特别是当生产者所在的服务有多台机器的时候，手动复制日志会更加麻烦而且容易出错。\",\"这时需要采用备份交换机了\",\"备份交换机可以理解为 RabbitMQ 中交换机的“备胎”，当我们为某一个交换机声明一个对应的备份交换机时，\",\"就是为它创建一个备胎，当交换机接收到一条不可路由消息时，将会把这条消息转发到备份交换机中，由备份交换机来进行转发和处理，通常备份交换机的类型为 Fanout ，这样就能把所有消息都投递到与其绑定的队列中，然后我们在备份交换机下绑定一个队列，这样所有那些原交换机无法被路由的消息，就会都进入这个队列了。当然，我们还可以建立一个报警队列，用独立的消费者来进行监测和报警。\",\"具体代码请参考这篇：\"]},{\"header\":\"2、设置持久化\",\"slug\":\"_2、设置持久化\",\"contents\":[\"上面我们的角度是站在生产者的方向，但是如果服务器重启了，此时交换机和队列都不存在了，消息存在也发送不了，这时需要把交换机和队列都持久化。\",\"/** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 true 可以多个消费者消费 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); \"]},{\"header\":\"3、RabbitMQ Server到消费者消息丢失解决方案\",\"slug\":\"_3、rabbitmq-server到消费者消息丢失解决方案\",\"contents\":[\"默认消息采用的是自动应答，所以我们要想实现消息消费过程中不丢失，需要把自动应答改为手动应答\",\"//将自动应答关闭 boolean autoAck = false; channel.basicConsume(TASK_QUEUE_NAME, autoAck, deliverCallback, consumerTag -> { }); \"]},{\"header\":\"10、RabbitMQ消息基于什么传输？\",\"slug\":\"_10、rabbitmq消息基于什么传输\",\"contents\":[\"由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。\"]},{\"header\":\"11、RabbitMQ支持消息的幂等性吗？\",\"slug\":\"_11、rabbitmq支持消息的幂等性吗\",\"contents\":[\"支持。在消息生产时，MQ 内部针对每条生产者发送的消息生成一个 inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列。\",\"在消息消费时，要求消息体中必须要有一个 bizId（对于同一业务全局唯一，如支付 ID、订单 ID、帖子 ID 等）作为去重的依据，避免同一条消息被重复消费。\"]},{\"header\":\"12、RabbitMQ怎么确保消息已被消费？\",\"slug\":\"_12、rabbitmq怎么确保消息已被消费\",\"contents\":[\"消费端配置手动ACK确认机制\",\"结合数据库进行状态标记处理\"]},{\"header\":\"13、RabbitMQ支持事务消息吗？\",\"slug\":\"_13、rabbitmq支持事务消息吗\",\"contents\":[\"支持事务消息。前面在第9题中保证生产者不丢失消息，提到可以使用AMQP的事务，但是它是同步的，所以不怎么推荐使用\",\"事务的实现主要是对信道(Channel)的设置，主要方法如下： 1. channel.txSelect() 声明启动事务模式 2.channel.txCommit() 提交事务 3.channel.txRollback()回滚事务 \"]},{\"header\":\"14、RabbitMQ消息持久化的条件？\",\"slug\":\"_14、rabbitmq消息持久化的条件\",\"contents\":[\"消息持久化，当然前提是队列必须持久化\",\"声明队列必须设置持久化 durable 设置为 true.\",\"消息推送投递模式必须设置持久化，deliveryMode 设置为 2（持久）。\",\"消息已经到达持久化交换器。\",\"消息已经到达持久化队列。\"]},{\"header\":\"15、RabiitMQ消息什么情况下会变成死信消息？\",\"slug\":\"_15、rabiitmq消息什么情况下会变成死信消息\",\"contents\":[\"由于特定的原因导致queue中的某些消息无法被消费，这样的消息如果没有后续的处理，就变成了死信消息\"]},{\"header\":\"16、RabbitMQ死信消息的来源？\",\"slug\":\"_16、rabbitmq死信消息的来源\",\"contents\":[\"消息 TTL 过期\",\"队列达到最大长度(队列满了，无法再添加数据到 mq 中)\",\"消息被拒绝(basic.reject 或 basic.nack)并且 requeue=false.\"]},{\"header\":\"17、RabbitMQ死信队列的用处？\",\"slug\":\"_17、rabbitmq死信队列的用处\",\"contents\":[\"可以用于实现延迟队列\"]},{\"header\":\"18、RabbitMQ支持延迟队列吗？\",\"slug\":\"_18、rabbitmq支持延迟队列吗\",\"contents\":[\"支持。延时队列，队列内部是有序的，最重要的特性就体现在它的延时属性上，延时队列中的元素是希望在指定时间到了以后或之前取出和处理，简单来说，延时队列就是用来存放需要在指定时间被处理的元素的队列。\"]},{\"header\":\"19、RabbitMQ延迟队列的使用场景\",\"slug\":\"_19、rabbitmq延迟队列的使用场景\",\"contents\":[\"订单在十分钟之内未支付则自动取消\",\"新创建的店铺，如果在十天内都没有上传过商品，则自动发送消息提醒\",\"用户注册成功后，如果三天内没有登陆则进行短信提醒\",\"用户发起退款，如果三天内没有得到处理则通知相关运营人员\",\"预定会议后，需要在预定的时间点前十分钟通知各个与会人员参加会议\"]},{\"header\":\"20、RabbitMQ实现延迟队列的有什么条件？\",\"slug\":\"_20、rabbitmq实现延迟队列的有什么条件\",\"contents\":[\"消息设置TTL\",\"配置了死信队列\"]},{\"header\":\"21、RabbitMQ怎么实现优先级队列？\",\"slug\":\"_21、rabbitmq怎么实现优先级队列\",\"contents\":[\"控制台页面：添加一个x-max-priority\",\"生产者添加优先级，案例代码\",\"public class Product { private static final String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { try(Channel channel = RabbitMQConfig.getChannel()){ //给消息赋予一个 priority 属性 AMQP.BasicProperties basicProperties = new AMQP.BasicProperties().builder().priority(5).build(); for (int i = 1; i < 11; i++) { String msg = \\\"info\\\" + i; if(i==5){ channel.basicPublish(\\\"\\\", QUEUE_NAME, basicProperties, msg.getBytes()); }else{ channel.basicPublish(\\\"\\\", QUEUE_NAME, null, msg.getBytes()); } System.out.println(\\\"发送消息完成:\\\" + msg); } } } } \",\"消费者队列中代码添加优先级\",\"public class Consumer { private static final String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { Channel channel = RabbitMQConfig.getChannel(); //设置队列的最大优先级 最大可以设置到 255 官网推荐 1-10 如果设置太高比较吃内存和 CPU Map<String, Object> map = new HashMap<>(); map.put(\\\"x-max-priority\\\", 10); channel.queueDeclare(QUEUE_NAME, true, false, false, map); System.out.println(\\\"消费者等待启动接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, delivery) ->{ String receivedMessage = new String(delivery.getBody()); System.out.println(\\\"接收到消息:\\\"+receivedMessage); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, (consumerTag) ->{ System.out.println(\\\"消费者无法消费消息时调用，如队列被删除\\\"); }); } } \"]},{\"header\":\"22、哪些情况下推荐使用RabbitMQ的惰性队列\",\"slug\":\"_22、哪些情况下推荐使用rabbitmq的惰性队列\",\"contents\":[\"队列可能会产生消息堆积\",\"队列对性能（吞吐量）的要求不是非常高，例如TPS 1万以下的场景\",\"希望队列有稳定的生产消费性能，不受内存影响而波动\"]},{\"header\":\"23、RabbitMQ如何处理消息堆积情况？\",\"slug\":\"_23、rabbitmq如何处理消息堆积情况\",\"contents\":[\"方法：临时扩容，快速处理积压的消息\",\"先修复 consumer 的问题，确保其恢复消费速度，然后将现有的 consumer 都停掉；\",\"临时创建原先 N 倍数量的 queue ，然后写一个临时分发数据的消费者程序，将该程序部署上去消费队列中积压的数据，消费之后不做任何耗时处理，直接均匀轮询写入临时建立好的 N 倍数量的 queue 中；\",\"接着，临时征用 N 倍的机器来部署 consumer，每个 consumer 消费一个临时 queue 的数据\",\"等快速消费完积压数据之后，恢复原先部署架构 ，重新用原先的 consumer 机器消费消息。\",\"这种做法相当于临时将 queue 资源和 consumer 资源扩大 N 倍，以正常 N 倍速度消费。\"]},{\"header\":\"24、RabbitMQ如何处理消息堆积过程中丢失的数据？\",\"slug\":\"_24、rabbitmq如何处理消息堆积过程中丢失的数据\",\"contents\":[\"采用**“批量重导”**的方式，在流量低峰期，写一个程序，手动去查询丢失的那部分数据，然后将消息重新发送到mq里面，把丢失的数据重新补回来。\"]},{\"header\":\"25、RabbitMQ如何处理长时间未处理导致写满的情况？\",\"slug\":\"_25、rabbitmq如何处理长时间未处理导致写满的情况\",\"contents\":[\"如果消息积压在RabbitMQ里，并且长时间都没处理掉，导致RabbitMQ都快写满了，这种情况肯定是临时扩容方案执行太慢；这种时候只好采用 “丢弃+批量重导” 的方式来解决了。首先，临时写个程序，连接到RabbitMQ里面消费数据，消费一个丢弃一个，快速消费掉积压的消息，降低RabbitMQ的压力，然后在流量低峰期时去手动查询重导丢失的这部分数据。\"]},{\"header\":\"26、如何设计一个消息队列？\",\"slug\":\"_26、如何设计一个消息队列\",\"contents\":[\"要考虑三点：伸缩性、持久化、可用性\",\"伸缩性：需要扩容的时候可以快速扩容，增加吞吐量和容量；可以参考kafaka的设计理念，broker -> topic -> partition，每个partition放一个机器，就存一部分数据；资源不够了，给topic增加partition，然后做数据迁移，增加机器；\",\"持久化：也就是数据要不要写入磁盘，不写入吧，进程挂了，数据就丢失了，写入磁盘该如何高效写入呢？kafaka的思路：顺序读写，采用磁盘缓存（Page Cache）的策略，操作系统采用预读和后写的方式，对磁盘进行优化。 \",\"预读：磁盘顺序读取的效率是很高的（不需要寻道时间，只需要很少的旋转时间）。而在读取磁盘某块数据时，同时会顺序读取相邻地址的数据加载到PageCache，这样在读取后续连续数据时，只需要从PageCache中读取数据，相当于内存读写，速度会特别快\",\"后写：数据并不是直接写入到磁盘，而是默认先写入到Page Cache，再由Page Cache刷新到磁盘，刷新频率是由操作系统周期性的sync触发的（用户也可以手动调用sync触发刷新操作）。后写的方式大大减少对磁盘的总写入次数，提高写入效率\",\"可用性：分布式系统的高可用几乎都是通过冗余实现的，Kafka同样如此。Kafka的消息存储到partition中，每个partition在其他的broker中都存在多个副本。对外通过主partition提供读写服务，当主partition所在的broker故障时，通过HA机制，将其他Broker上的某个副本partition会重新选举成主partition，继续对外提供服务。\"]}]},\"/interview/distributed/redis.html\":{\"title\":\"Redis 53道面试题\",\"contents\":[{\"header\":\"1、什么是Redis?\",\"slug\":\"_1、什么是redis\",\"contents\":[\"Redis 是完全开源免费的， 遵守 BSD 协议， 是一个高性能的 key-value 数据库。\",\"特点：\",\"Redis 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。\",\"Redis 不仅仅支持简单的 key-value 类型的数据， 同时还提供 list， set， zset， hash 等数据结构的存储。、\",\"Redis 支持数据的备份， 即 master-slave 模式的数据备份。\"]},{\"header\":\"2、Redis的数据类型？\",\"slug\":\"_2、redis的数据类型\",\"contents\":[\"string（ 字符串）\",\"字符串最基础的数据结构。字符串类型的值实际可以是字符串（简单的字符串、复杂的字符串（例如JSON、XML））、数字 （整数、浮点数），甚至是二进制（图片、音频、视频），但是值最大不能超过512MB。\",\"字符串主要有以下几个典型使用场景：\",\"缓存功能\",\"计数\",\"共享Session\",\"限速\",\"hash（ 哈希）\",\"哈希类型是指键值本身又是一个键值对结构。\",\"哈希主要有以下典型应用场景：\",\"缓存用户信息\",\"缓存对象\",\"list（ 列表）\",\"列表（list）类型是用来存储多个有序的字符串。列表是一种比较灵活的数据结构，它可以充当栈和队列的角色\",\"列表主要有以下几种使用场景：\",\"消息队列\",\"文章列表\",\"set（ 集合）\",\"集合（set）类型也是用来保存多个的字符串元素，但和列表类型不一 样的是，集合中不允许有重复元素，并且集合中的元素是无序的。\",\"集合主要有如下使用场景：\",\"标签（tag）\",\"共同关注\",\"zset（有序集合)\",\"有序集合中的元素可以排序。但是它和列表使用索引下标作为排序依据不同的是，它给每个元素设置一个权重（score）作为排序的依据。\",\"有序集合主要应用场景：\",\"用户点赞统计\",\"用户排序\",\"HyperLogLog\",\"pub/sub\"]},{\"header\":\"3、Redis有哪些优缺点？\",\"slug\":\"_3、redis有哪些优缺点\",\"contents\":[\"优点：\",\"读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。\",\"支持数据持久化，支持AOF和RDB两种持久化方式。\",\"支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。\",\"数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。\",\"支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。\",\"缺点：\",\"数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。\",\"Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。\",\"主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。\",\"Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。\"]},{\"header\":\"4、为什么要用 Redis做缓存？\",\"slug\":\"_4、为什么要用-redis做缓存\",\"contents\":[\"主要从“高性能”和“高并发”这两点来看待这个问题。\",\"高性能：假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！\",\"高并发：直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。\"]},{\"header\":\"5、Redis为什么这么快？\",\"slug\":\"_5、redis为什么这么快\",\"contents\":[\"完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)；\",\"数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；\",\"采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；\",\"使用多路 I/O 复用模型，非阻塞 IO；\",\"使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；\"]},{\"header\":\"6、Redis的应用场景\",\"slug\":\"_6、redis的应用场景\",\"contents\":[\"计数器：可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。\",\"缓存：将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。\",\"会话缓存：可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。\",\"全页缓存（FPC）：除基本的会话token之外，Redis还提供很简便的FPC平台。以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wpredis，这个插件能帮助你以最快速度加载你曾浏览过的页面。\",\"查找表： 例如 DNS 记录就很适合使用 Redis 进行存储。查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。\",\"消息队列(发布/订阅功能)：List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息。不过最好使用 Kafka、RabbitMQ 等消息中间件。 发布/订阅的使用场景确实非常多。还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！\",\"分布式锁实现：在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。\",\"排行榜：排行榜 Redis提供了列表和有序集合数据结构，合理地使用这些数据结构可以很方便地构建各种排行榜系统。\",\"社交网络 赞/踩、粉丝、共同好友/喜好、推送、下拉刷新。\",\"Redis的应用一般会结合项目去问，以一个电商项目的用户服务为例：\",\"Token存储：用户登录成功之后，使用Redis存储Token\",\"登录失败次数计数：使用Redis计数，登录失败超过一定次数，锁定账号\",\"地址缓存：对省市区数据的缓存\",\"分布式锁：分布式环境下登录、注册等操作加分布式锁，防止一人多卖增加分布式锁\",\"……\"]},{\"header\":\"7、什么是持久化？\",\"slug\":\"_7、什么是持久化\",\"contents\":[\"持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。\"]},{\"header\":\"8、Redis 的持久化机制是什么？各自的优缺点？\",\"slug\":\"_8、redis-的持久化机制是什么-各自的优缺点\",\"contents\":[\"持久化机制：\",\"RDB（默认）Redis DataBase\",\"AOF Append Only File\",\"RDB\",\"RDB是Redis默认的持久化方式。\",\"工作机制：每隔一段时间，就把内存中的数据保存到硬盘上的指定文件中。对应产生的数据文件为dump.rdb\",\"触发RDB的方式有两种：手动触发和自动触发\",\"手动触发分别对应save和bgsave命令\",\"save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存比较大的实例会造成长时间阻塞，线上环境不建议使用。\",\"bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。\",\"以下场景会自动触发RDB持久化： 1.使用save相关配置，如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave。 2.如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 3.执行debug reload命令重新加载Redis时，也会自动触发save操作 4.默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave。\",\"优点：\",\"只有一个文件 dump.rdb，方便持久化。\",\"容灾性好，一个文件可以保存到安全的磁盘。\",\"性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能\",\"相对于数据集大时，比 AOF 的启动效率更高。\",\"缺点：\",\"数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候。如果redis要故障时要尽可能少的丢失数据，RDB没有AOF好，例如1:00进行的快照，在1:10又要进行快照的时候宕机了，这个时候就会丢失10分钟的数据。\",\"RDB每次fork出子进程来执行RDB快照生成文件时，如果文件特别大，可能会导致客户端提供服务暂停数毫秒或者几秒\",\"AOF\",\"AOF 是 以日志的形式来记录每个写操作，将每一次对数据进行修改，都把新建、修改数据的命令保存到指 定文件中。Redis 重新启\",\"动时读取这个文件，重新执行新建、修改数据的命令恢复数据。\",\"当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复\",\"优点：\",\"数据安全，AOF持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 AOF文件中一次。\",\"通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。\",\"AOF日志文件的命令通过非常可读的方式进行记录，这个非常适合做灾难性的误删除紧急恢复，如果某人不小心用flushall命令\",\"清空了所有数据，只要这个时候还没有执行rewrite，那么就可以将日志文件中的flushall删除，进行恢复\",\"缺点：\",\"对于同一份文件AOF文件比RDB数据快照要大。\",\"AOF开启后支持写的QPS会比RDB支持的写的QPS低，因为AOF一般会配置成每秒fsync操作，每秒的fsync操作还是很高的、\",\"数据恢复比较慢，不适合做冷备。\"]},{\"header\":\"9、RDB和AOF到底如何选择？\",\"slug\":\"_9、rdb和aof到底如何选择\",\"contents\":[\"如何看待数据“绝对”安全？\",\"Redis 作为内存数据库从本质上来说，如果不想牺牲性能，就不可能做到数据的“绝对”安全。 RDB 和AOF 都只是尽可能在兼顾性能的前提下降低数据丢失的风险，如果真的发生数据丢失问题，尽可能 减少损失。 在整个项目的架构体系中，Redis 大部分情况是扮演“二级缓存”角色。\",\"二级缓存适合保存的数据\",\"经常要查询，很少被修改的数据。\",\"不是非常重要，允许出现偶尔的并发问题。\",\"不会被其他应用程序修改。\",\"如果Redis 是作为缓存服务器，那么说明数据在MySQL 这样的传统关系型数据库中是有正式版本的。数据最终以MySQL 中的为准。\",\"RDB和AOF到底如何选择?\",\"不要仅仅使用RDB这样会丢失很多数据。\",\"也不要仅仅使用AOF，因为这会有两个问题，第一通过AOF做冷备没有RDB做冷备恢复的速度快；第二RDB每次简单粗暴生成数据快照，更加健壮。\",\"综合AOF和RDB两种持久化方式，用AOF来保证数据不丢失，作为恢复数据的第一选择；用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，可以使用RDB进行快速的数据恢复。\",\"官方推荐两个都用：如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是 做纯内存缓存，可以都不用\"]},{\"header\":\"10、Redis持久化数据和缓存怎么做扩容？\",\"slug\":\"_10、redis持久化数据和缓存怎么做扩容\",\"contents\":[\"如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。\",\"如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即\",\"Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。\"]},{\"header\":\"11、Redis 过期键的删除策略？\",\"slug\":\"_11、redis-过期键的删除策略\",\"contents\":[\"定时删除：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。\",\"惰性删除：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。\",\"定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)\",\"redis 有一个定时任务处理器 serverCron，负责周期性任务处理，默认 100 ms 执行一次（hz 参数控制）包括：① 处理过期 key、② hash 表 rehash、③ 更新统计结果、④ 持久化、⑤ 清理过期客户端\",\"对于处理过期 key 会：依次遍历库，在规定时间内运行如下操作\",\"① 从每个库的 expires 过期字典中随机选择 20 个 key 检查，如果过期则删除\",\"② 如果删除达到 5 个，重复 ① 步骤，没有达到，遍历至下一个库\",\"③ 规定时间没有做完，等待下一轮 serverCron 运行\",\"PS：Redis中同时使用了惰性删除和定期删除两种过期策略。\"]},{\"header\":\"12、Redis的内存淘汰策略有哪些？\",\"slug\":\"_12、redis的内存淘汰策略有哪些\",\"contents\":[\"Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。\",\"全局的键空间选择性移除\",\"noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。\",\"allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的）\",\"allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。\",\"设置过期时间的键空间选择性移除\",\"volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。\",\"volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。\",\"volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。\",\"PS：Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据\"]},{\"header\":\"13、Redis如何做内存优化？\",\"slug\":\"_13、redis如何做内存优化\",\"contents\":[\"可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应 该把这个用户的所有信息存储到一张散列表里面\"]},{\"header\":\"14、什么是事务？\",\"slug\":\"_14、什么是事务\",\"contents\":[\"事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。\"]},{\"header\":\"15、Redis事务的概念\",\"slug\":\"_15、redis事务的概念\",\"contents\":[\"Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。\"]},{\"header\":\"16、Redis事务的三个阶段\",\"slug\":\"_16、redis事务的三个阶段\",\"contents\":[\"事务开始 MULTI\",\"命令入队\",\"事务执行 EXEC\"]},{\"header\":\"17、 Redis事务相关命令\",\"slug\":\"_17、-redis事务相关命令\",\"contents\":[\"Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的\",\"Redis会将一个事务中的所有命令序列化，然后按顺序执行。\",\"redis不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。\",\"如果在一个事务中的命令出现错误，那么所有的命令都不会执行；\",\"如果在一个事务中出现运行错误，那么正确的命令会被执行。\",\"WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。\",\"MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。\",\"EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil。\",\"通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。\",\"UNWATCH命令可以取消watch对所有key的监控。\"]},{\"header\":\"18、 事务管理（ACID）概述\",\"slug\":\"_18、-事务管理-acid-概述\",\"contents\":[\"原子性（Atomicity）\",\"原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。\",\"一致性（Consistency）\",\"事务前后数据的完整性必须保持一致。\",\"隔离性（Isolation）\",\"多个事务并发执行时，一个事务的执行不应影响其他事务的执行\",\"持久性（Durability）\",\"持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响\",\"Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在_AOF_持久化模式 下，并且appendfsync选项的值为always时，事务也具有耐久性。\"]},{\"header\":\"19、Redis事务支持隔离性吗\",\"slug\":\"_19、redis事务支持隔离性吗\",\"contents\":[\"Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis的事务是总是带有隔离性的。\"]},{\"header\":\"20、主从复制了解吗？\",\"slug\":\"_20、主从复制了解吗\",\"contents\":[\"主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为 主节点(master)，后者称为 从节点(slave)。且数据的复制是 单向 的，只能由主节点到从节点。Redis 主从复制支持 主从同步 和 从从同步 两种，后者是 Redis 后续版本新增的功能，以减轻主节点的同步负担。\",\"作用：\",\"数据冗余： 主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。\",\"故障恢复： 当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复 (实际上是一种服务的冗余)。\",\"负载均衡： 在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务 （即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载。尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。\",\"高可用基石： 除了上述作用以外，主从复制还是哨兵和集群能够实施的 基础，因此说主从复制是 Redis 高可用的基础。\"]},{\"header\":\"21、Redis主从有几种常见的拓扑结构？\",\"slug\":\"_21、redis主从有几种常见的拓扑结构\",\"contents\":[\"Redis的复制拓扑结构可以支持单层或多层复制关系，根据拓扑复杂性可以分为以下三种：一主一从、一主多从、树状主从结构。\",\"一主一从结构\",\"一主一从结构是最简单的复制拓扑结构，用于主节点出现宕机时从节点提供故障转移支持。\",\"一主多从结构\",\"主多从结构（又称为星形拓扑结构）使得应用端可以利用多个从节点实现读写分离）。对于读占比较大的场景，可以把读命令发送到从节点来分担主节点压力。\",\"树状主从结构\",\"树状主从结构（又称为树状拓扑结构）使得从节点不但可以复制主节点数据，同时可以作为其他从节点的主节点继续向下层复制。通过引入复制中间层，可以有效降低主节点负载和需要传送给从节点的数据量。\"]},{\"header\":\"22、Redis的主从复制原理了解吗？\",\"slug\":\"_22、redis的主从复制原理了解吗\",\"contents\":[\"保存主节点（master）信息 这一步只是保存主节点信息，保存主节点的ip和port。\",\"主从建立连接 从节点（slave）发现新的主节点后，会尝试和主节点建立网络连接。\",\"发送ping命令 连接建立成功后从节点发送ping请求进行首次通信，主要是检测主从之间网络套接字是否可用、主节点当前是否可接受处理命令。\",\"权限验证 如果主节点要求密码验证，从节点必须正确的密码才能通过验证。\",\"同步数据集 主从复制连接正常通信后，主节点会把持有的数据全部发送给从节点。\",\"命令持续复制 接下来主节点会持续地把写命令发送给从节点，保证主从数据一致性。\"]},{\"header\":\"23、主从数据同步的方式？\",\"slug\":\"_23、主从数据同步的方式\",\"contents\":[\"Redis在2.8及以上版本使用psync命令完成主从数据同步，同步过程分为：\",\"全量复制\",\"部分复制\"]},{\"header\":\"全量复制\",\"slug\":\"全量复制\",\"contents\":[\"一般用于初次复制场景，Redis早期支持的复制功能只有全量复制，它会把主节点全部数据一次性发送给从节点，当数据量较大时，会对主从节点和网络造成很大的开销。\",\"全量复制的完整运行流程如下：\",\"发送psync命令进行数据同步，由于是第一次进行复制，从节点没有复制偏移量和主节点的运行ID，所以发送psync-1。\",\"主节点根据psync-1解析出当前为全量复制，回复+FULLRESYNC响应。\",\"从节点接收主节点的响应数据保存运行ID和偏移量offset\",\"主节点执行bgsave保存RDB文件到本地\",\"主节点发送RDB文件给从节点，从节点把接收的RDB文件保存在本地并直接作为从节点的数据文件\",\"对于从节点开始接收RDB快照到接收完成期间，主节点仍然响应读写命令，因此主节点会把这期间写命令数据保存在复制客户端缓冲区内，当从节点加载完RDB文件后，主节点再把缓冲区内的数据发送给从节点，保证主从之间数据一致性。\",\"从节点接收完主节点传送来的全部数据后会清空自身旧数据\",\"从节点清空数据后开始加载RDB文件\",\"从节点成功加载完RDB后，如果当前节点开启了AOF持久化功能， 它会立刻做bgrewriteaof操作，为了保证全量复制后AOF持久化文件立刻可用。\"]},{\"header\":\"部分复制\",\"slug\":\"部分复制\",\"contents\":[\"部分复制主要是Redis针对全量复制的过高开销做出的一种优化措施， 使用psync{runId}{offset}命令实现。当从节点（slave）正在复制主节点 （master）时，如果出现网络闪断或者命令丢失等异常情况时，从节点会向 主节点要求补发丢失的命令数据，如果主节点的复制积压缓冲区内存在这部分数据则直接发送给从节点，这样就可以保持主从节点复制的一致性。\",\"当主从节点之间网络出现中断时，如果超过repl-timeout时间，主节点会认为从节点故障并中断复制连接\",\"主从连接中断期间主节点依然响应命令，但因复制连接中断命令无法发送给从节点，不过主节点内部存在的复制积压缓冲区，依然可以保存最近一段时间的写命令数据，默认最大缓存1MB。\",\"当主从节点网络恢复后，从节点会再次连上主节点\",\"当主从连接恢复后，由于从节点之前保存了自身已复制的偏移量和主节点的运行ID。因此会把它们当作psync参数发送给主节点，要求进行部分复制操作。\",\"主节点接到psync命令后首先核对参数runId是否与自身一致，如果一 致，说明之前复制的是当前主节点；之后根据参数offset在自身复制积压缓冲区查找，如果偏移量之后的数据存在缓冲区中，则对从节点发送+CONTINUE响应，表示可以进行部分复制。\",\"主节点根据偏移量把复制积压缓冲区里的数据发送给从节点，保证主从复制进入正常状态。\"]},{\"header\":\"24、主从复制存在哪些问题呢？\",\"slug\":\"_24、主从复制存在哪些问题呢\",\"contents\":[\"主从复制虽好，但也存在一些问题：\",\"一旦主节点出现故障，需要手动将一个从节点晋升为主节点，同时需要修改应用方的主节点地址，还需要命令其他从节点去复制新的主节点，整个过程都需要人工干预。\",\"主节点的写能力受到单机的限制。\",\"主节点的存储能力受到单机的限制。\",\"第一个问题是Redis的高可用问题，第二、三个问题属于Redis的分布式问题。\"]},{\"header\":\"25、Redis为什么早期选择单线程？\",\"slug\":\"_25、redis为什么早期选择单线程\",\"contents\":[\"官方解释：https://redis.io/topics/faq\",\"官方FAQ表示，因为Redis是基于内存的操作，CPU成为Redis的瓶颈的情况很少见，Redis的瓶颈最有可能是内存的大小或者网络限制。\",\"如果想要最大程度利用CPU，可以在一台机器上启动多个Redis实例。\",\"PS：网上有这样的回答，吐槽官方的解释有些敷衍，其实就是历史原因，开发者嫌多线程麻烦，后来这个CPU的利用问题就被抛给了使用者。\",\"同时FAQ里还提到了， Redis 4.0 之后开始变成多线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 Key 的删除等等。\"]},{\"header\":\"26、Redis6.0使用多线程是怎么回事?\",\"slug\":\"_26、redis6-0使用多线程是怎么回事\",\"contents\":[\"Redis6.0的多线程是用多线程来处理数据的读写和协议解析，但是Redis执行命令还是单线程的。\",\"这样做的⽬的是因为Redis的性能瓶颈在于⽹络IO⽽⾮CPU，使⽤多线程能提升IO读写的效率，从⽽整体提⾼Redis的性能。\"]},{\"header\":\"27、Redis Sentinel（哨兵）了解吗\",\"slug\":\"_27、redis-sentinel-哨兵-了解吗\",\"contents\":[\"主从复制存在一个问题，没法完成自动故障转移。所以我们需要一个方案来完成自动故障转移，它就是Redis Sentinel（哨兵）。\",\"Redis Sentinel ，它由两部分组成，哨兵节点和数据节点：\",\"哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的 Redis 节点，不存储数据，对数据节点进行监控。\",\"数据节点：主节点和从节点都是数据节点；\",\"在复制的基础上，哨兵实现了 自动化的故障恢复 功能，下面是官方对于哨兵功能的描述：\",\"监控（Monitoring）： 哨兵会不断地检查主节点和从节点是否运作正常。\",\"自动故障转移（Automatic failover）： 当 主节点 不能正常工作时，哨兵会开始 自动故障转移操作，它会将失效主节点的其中一个 从节点升级为新的主节点，并让其他从节点改为复制新的主节点。\",\"配置提供者（Configuration provider）： 客户端在初始化时，通过连接哨兵来获得当前 Redis 服务的主节点地址。\",\"通知（Notification）： 哨兵可以将故障转移的结果发送给客户端。\",\"其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移。而配置提供者和通知功能，则需要在与客户端的交互中才能体现。\"]},{\"header\":\"28、Redis Sentinel（哨兵）实现原理知道吗？\",\"slug\":\"_28、redis-sentinel-哨兵-实现原理知道吗\",\"contents\":[\"哨兵模式是通过哨兵节点完成对数据节点的监控、下线、故障转移。\"]},{\"header\":\"定时监控\",\"slug\":\"定时监控\",\"contents\":[\"Redis Sentinel通过三个定时监控任务完成对各个节点发现和监控：\",\"每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的拓扑结构\",\"每隔2秒，每个Sentinel节点会向Redis数据节点的__sentinel__：hello 频道上发送该Sentinel节点对于主节点的判断以及当前Sentinel节点的信息\",\"每隔1秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点发送一条ping命令做一次心跳检测，来确认这些节点当前是否可达\",\"主观下线和客观下线主观下线就是哨兵节点认为某个节点有问题，客观下线就是超过一定数量的哨兵节点认为主节点有问题。\",\"主观下线 每个Sentinel节点会每隔1秒对主节点、从节点、其他Sentinel节点发送ping命令做心跳检测，当这些节点超过 down-after-milliseconds没有进行有效回复，Sentinel节点就会对该节点做失败判定，这个行为叫做主观下线。\",\"客观下线 当Sentinel主观下线的节点是主节点时，该Sentinel节点会通过sentinel is- master-down-by-addr命令向其他Sentinel节点询问对主节点的判断，当超过 quorum个数，Sentinel节点认为主节点确实有问题，这时该Sentinel节点会做出客观下线的决定\",\"领导者Sentinel节点选举Sentinel节点之间会做一个领导者选举的工作，选出一个Sentinel节点作为领导者进行故障转移的工作。Redis使用了Raft算法实现领导者选举。\",\"故障转移\",\"领导者选举出的Sentinel节点负责故障转移，过程如下：\",\"在从节点列表中选出一个节点作为新的主节点，这一步是相对复杂一些的一步\",\"Sentinel领导者节点会对第一步选出来的从节点执行slaveof no one命令让其成为主节点\",\"Sentinel领导者节点会向剩余的从节点发送命令，让它们成为新主节点的从节点\",\"Sentinel节点集合会将原来的主节点更新为从节点，并保持着对其关注，当其恢复后命令它去复制新的主节点\"]},{\"header\":\"29、领导者Sentinel节点选举了解吗？\",\"slug\":\"_29、领导者sentinel节点选举了解吗\",\"contents\":[\"Redis使用了Raft算法实 现领导者选举，大致流程如下：\",\"每个在线的Sentinel节点都有资格成为领导者，当它确认主节点主观 下线时候，会向其他Sentinel节点发送sentinel is-master-down-by-addr命令， 要求将自己设置为领导者。\",\"收到命令的Sentinel节点，如果没有同意过其他Sentinel节点的sentinel is-master-down-by-addr命令，将同意该请求，否则拒绝。\",\"如果该Sentinel节点发现自己的票数已经大于等于max（quorum， num（sentinels）/2+1），那么它将成为领导者。\",\"如果此过程没有选举出领导者，将进入下一次选举。\"]},{\"header\":\"30、新的主节点是怎样被挑选出来的？\",\"slug\":\"_30、新的主节点是怎样被挑选出来的\",\"contents\":[\"过滤：“不健康”（主观下线、断线）、5秒内没有回复过Sentinel节 点ping响应、与主节点失联超过down-after-milliseconds*10秒。\",\"选择slave-priority（从节点优先级）最高的从节点列表，如果存在则返回，不存在则继续。\",\"选择复制偏移量最大的从节点（复制的最完整），如果存在则返 回，不存在则继续。\",\"选择runid最小的从节点。\"]},{\"header\":\"31、Redis 集群了解吗？\",\"slug\":\"_31、redis-集群了解吗\",\"contents\":[\"前面说到了主从存在高可用和分布式的问题，哨兵解决了高可用的问题，而集群就是终极方案，一举解决高可用和分布式问题。\",\"数据分区： 数据分区 (或称数据分片) 是集群最核心的功能。集群将数据分散到多个节点，一方面 突破了 Redis 单机内存大小的限制，存储容量大大增加；另一方面 每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。\",\"高可用： 集群支持主从复制和主节点的 自动故障转移（与哨兵类似），当任一节点发生故障时，集群仍然可以对外提供服务。\"]},{\"header\":\"32、集群中数据如何分区？\",\"slug\":\"_32、集群中数据如何分区\",\"contents\":[\"分布式的存储中，要把数据集按照分区规则映射到多个节点，常见的数据分区规则三种：\",\"哈希取余分区\",\"一致性哈希算法分区\",\"哈希槽分区\"]},{\"header\":\"1、哈希取余分区\",\"slug\":\"_1、哈希取余分区\",\"contents\":[\"2亿条记录就是2亿个k,v，我们单机不行必须要分布式多机，假设有3台机器构成一个集群，用户每次读写操作都是根据公式： hash(key) % N个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。\",\"优点：\",\"简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。 \",\"缺点：\",\" 原来规划好的节点，进行扩容或者缩容就比较麻烦了额，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3会变成Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。 某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。 \"]},{\"header\":\"2、一致性哈希算法分区\",\"slug\":\"_2、一致性哈希算法分区\",\"contents\":[\"一致性哈希算法在1997年由麻省理工学院中提出的，设计目标是为了解决分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不OK了。\",\"作用：\",\"提出一致性Hash解决方案。目的是当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系\",\"步骤：\",\"算法构建一致性哈希环\",\"一致性哈希环\",\"​ 一致性哈希算法必然有个hash函数并按照算法产生hash值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个hash空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。\",\"它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性Hash算法是对2^32 取模，简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1 （即哈希值是一个32位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、……直到2^32-1 ，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1 在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。\",\"服务器IP节点映射\",\"节点映射\",\"将集群中各个IP节点映射到环上的某一个位置。 将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如4个节点NodeA、B、C、D，经过IP地址的哈希函数计算(hash(ip))，使用IP地址哈希后在环空间的位置如下：\",\"key落到服务器的落键规则\",\"当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。 如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。\",\"优点：\",\"1.一致性哈希算法的容错性\",\"容错性 假设Node C宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。简单说，就是C挂了，受到影响的只是B、C之间的数据，并且这些数据会转移到D进行存储。\",\"2.一致性哈希算法的扩展性\",\"扩展性 数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可，不会导致hash取余全部数据重新洗牌。\",\"缺点：\",\"一致性哈希算法的数据倾斜问题\",\"一致性Hash算法在服务节点太少时，容易因为节点分布不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题， 例如系统中只有两台服务器：\",\"总结：\",\"为了在节点数目发生改变时尽可能少的迁移数据\",\"将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会顺时针找到临近的存储节点存放。 而当有节点加入或退出时仅影响该节点在Hash环上顺时针相邻的后续节点。\",\"优点 加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。\",\"缺点 数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。\"]},{\"header\":\"3、哈希槽分区\",\"slug\":\"_3、哈希槽分区\",\"contents\":[\"哈希槽实质就是一个数组，数组[0,2^14 -1]形成hash slot空间。\",\"**作用：**解决一致性哈希算法的数据倾斜问题\",\"解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。\",\"槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。 哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。\"]},{\"header\":\"哈希槽的计算\",\"slug\":\"哈希槽的计算\",\"contents\":[\"Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key之A 、B在Node2， key之C落在Node3上\"]},{\"header\":\"33、什么是缓存击穿、缓存穿透、缓存雪崩？\",\"slug\":\"_33、什么是缓存击穿、缓存穿透、缓存雪崩\",\"contents\":[\"缓存穿透：key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。\",\"缓存击穿：key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。\",\"缓存雪崩：某⼀时刻发生大规模的缓存失效的情况，例如缓存服务宕机、缓存服务器重启、大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。\"]},{\"header\":\"缓存击穿解决方案\",\"slug\":\"缓存击穿解决方案\",\"contents\":[\"加锁更新，⽐如请求查询A，发现缓存中没有，对A这个key加锁，同时去数据库查询数据，写⼊缓存，再返回给⽤户，这样后⾯的请求就可以从缓存中拿到数据了。\",\"将过期时间组合写在value中，通过异步的⽅式不断的刷新过期时间，防⽌此类现象。\"]},{\"header\":\"1、互斥锁\",\"slug\":\"_1、互斥锁\",\"contents\":[\"所谓互斥，就是不同线程，通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写。\",\"线程1发起请求，查询缓存发现未命中，然后获取互斥锁，成功之后，则去查询数据库重建缓存数据，写入缓存，释放锁。\",\"线程2在线程1未释放锁之前发起请求，查询缓存未命中，然后获取互斥锁，发现被线程1占用了，则获取失败，休眠一会儿，再重新获取锁（直到线程1释放），最后缓存命中。\",\"存在的问题：互斥等待时间，如果1000个线程同时访问，则只有1个获取成功，其他999个都是在等待，性能会下降\"]},{\"header\":\"2、逻辑过期\",\"slug\":\"_2、逻辑过期\",\"contents\":[\"逻辑过期：原来我们存储数据到redis中的时候，存的是k:v键值对，那逻辑过期，就是手动给value增加一个expire时间\",\"KEY\",\"VALUE\",\"heima:user:1\",\"{name:\\\"Jack\\\", age:21, expire:152141223}\",\"线程1发起请求，查询缓存，发现逻辑时间已过期，则回获取互斥锁，此时线程会开启一个新线程2（用于查询数据存入缓存），先返回过期的数据\",\"线程2查询数据库后，重建缓存数据，写入缓存后，重置逻辑过期时间，最后释放锁\",\"线程3发起请求（与线程1同步），查询缓存，发现逻辑时间已过期，获取互斥锁失败，就先返回旧数据\",\"线程4查询缓存，此时线程2已经释放锁，缓存命中，逻辑过期时间未过期，则直接返回\"]},{\"header\":\"3、比较\",\"slug\":\"_3、比较\",\"contents\":[\"解决方案\",\"优点\",\"缺点\",\"互斥锁（一致性）\",\"没有额外的内存消耗、保证一致性、实现简单\",\"线程需要等待，性能受影响可能有死锁风险\",\"逻辑过期（性能）\",\"线程无需等待，性能较好\",\"不保证一致性、有额外内存消耗、实现复杂\"]},{\"header\":\"缓存穿透解决方案\",\"slug\":\"缓存穿透解决方案\",\"contents\":[]},{\"header\":\"1、缓存空对象\",\"slug\":\"_1、缓存空对象\",\"contents\":[\"当客户端请求到redis后，未命中去查询数据库，数据库查询返回为null，则缓存为null\",\"优点：\",\"实现简单，维护方便\",\"缺点：\",\"额外的内存消耗\",\"若客户端请求大量数据都是不存在的，则redis会缓存大量的null数据\",\"可能造成短期的不一致\",\"客户端请求不存在的数据后，redis缓存数据为null，并设置了超时时间，此时就新增了一条数据，则再去查询时（还在TTL内），还是为null，只有当时间失效时，才会查询到\",\"可在新增时，更新缓存，可解决短期的不一致\"]},{\"header\":\"2、布隆过滤器\",\"slug\":\"_2、布隆过滤器\",\"contents\":[\"布隆过滤器是一个 bit 向量或者说 bit 数组（超长超长，记住一定要足够长）\",\"将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。\",\"优点：\",\"内存占用较少，没有多余key\",\"缺点：\",\"实现复杂\",\"存在误判可能\",\"解决缓存穿透\",\"适用场景\",\"维护成本\",\"缓存空对象\",\"数据命中不高；数据频繁实时性高\",\"代码维护简单；需要较多的缓存空间；数据不一致\",\"布隆过滤器\",\"数据命中不高；数据相对固定实时性低\",\"代码维护复杂；缓存空间占用少\"]},{\"header\":\"缓存雪崩解决方案\",\"slug\":\"缓存雪崩解决方案\",\"contents\":[\"提高缓存可用性\",\"集群部署：通过集群来提升缓存的可用性，可以利用Redis本身的Redis Cluster或者第三方集群方案如Codis等。\",\"多级缓存：设置多级缓存，第一级缓存失效的基础上，访问二级缓存，每一级缓存的失效时间都不同。\",\"过期时间\",\"均匀过期：为了避免大量的缓存在同一时间过期，可以把不同的 key 过期时间随机生成，避免过期时间太过集中。\",\"热点数据永不过期。\",\"熔断降级\",\"服务熔断：当缓存服务器宕机或超时响应时，为了防止整个系统出现雪崩，暂时停止业务服务访问缓存系统。\",\"服务降级：当出现大量缓存失效，而且处在高并发高负荷的情况下，在业务系统内部暂时舍弃对一些非核心的接口和数据的请求，而直接返回一个提前准备好的 fallback（退路）错误处理信息。\",\"给业务添加多级缓存\"]},{\"header\":\"34、如何保证缓存和数据库数据的⼀致性？\",\"slug\":\"_34、如何保证缓存和数据库数据的一致性\",\"contents\":[\"根据CAP理论，在保证可用性和分区容错性的前提下，无法保证一致性，所以缓存和数据库的绝对一致是不可能实现的，只能尽可能保存缓存和数据库的最终一致性。\"]},{\"header\":\"选择合适的缓存更新策略\",\"slug\":\"选择合适的缓存更新策略\",\"contents\":[\"1. 删除缓存而不是更新缓存\",\"当一个线程对缓存的key进行写操作的时候，如果其它线程进来读数据库的时候，读到的就是脏数据，产生了数据不一致问题。\",\"相比较而言，删除缓存的速度比更新缓存的速度快很多，所用时间相对也少很多，读脏数据的概率也小很多。\",\"2.先更数据，后删缓存先更数据库还是先删缓存？这是一个问题。\",\"更新数据，耗时可能在删除缓存的百倍以上。在缓存中不存在对应的key，数据库又没有完成更新的时候，如果有线程进来读取数据，并写入到缓存，那么在更新成功之后，这个key就是一个脏数据。\",\"毫无疑问，先删缓存，再更数据库，缓存中key不存在的时间的时间更长，有更大的概率会产生脏数据。\",\"目前最流行的缓存读写策略cache-aside-pattern就是采用先更数据库，再删缓存的方式。\"]},{\"header\":\"缓存不一致处理\",\"slug\":\"缓存不一致处理\",\"contents\":[\"如果不是并发特别高，对缓存依赖性很强，其实一定程序的不一致是可以接受的。但是如果对一致性要求比较高，那就得想办法保证缓存和数据库中数据一致。\",\"缓存和数据库数据不一致常见的两种原因：\",\"缓存key删除失败\",\"并发导致写入了脏数据\",\"解决方案：\",\"消息队列保证key被删除\",\"数据库订阅+消息队列保证key被删除\",\"延时双删防止脏数据\",\"设置缓存过期时间兜底\"]},{\"header\":\"消息队列保证key被删除\",\"slug\":\"消息队列保证key被删除\",\"contents\":[\"可以引入消息队列，把要删除的key或者删除失败的key丢尽消息队列，利用消息队列的重试机制，重试删除对应的key。\",\"这种方案看起来不错，缺点是对业务代码有一定的侵入性。\"]},{\"header\":\"数据库订阅+消息队列保证key被删除\",\"slug\":\"数据库订阅-消息队列保证key被删除\",\"contents\":[\"可以用一个服务（比如阿里的 canal）去监听数据库的binlog，获取需要操作的数据。然后用一个公共的服务获取订阅程序传来的信息，进行缓存删除操作。\",\"这种方式降低了对业务的侵入，但其实整个系统的复杂度是提升的，适合基建完善的大厂。\"]},{\"header\":\"延时双删防止脏数据\",\"slug\":\"延时双删防止脏数据\",\"contents\":[\"还有一种情况，是在缓存不存在的时候，写入了脏数据，这种情况在先删缓存，再更数据库的缓存更新策略下发生的比较多，解决方案是延时双删。简单说，就是在第一次删除缓存之后，过了一段时间之后，再次删除缓存。\",\"这种方式的延时时间设置需要仔细考量和测试。\"]},{\"header\":\"设置缓存过期时间兜底\",\"slug\":\"设置缓存过期时间兜底\",\"contents\":[\"这是一个朴素但是有用的办法，给缓存设置一个合理的过期时间，即使发生了缓存数据不一致的问题，它也不会永远不一致下去，缓存过期的时候，自然又会恢复一致。\"]},{\"header\":\"35、缓存预热怎么做呢？\",\"slug\":\"_35、缓存预热怎么做呢\",\"contents\":[\"所谓缓存预热，就是提前把数据库里的数据刷到缓存里，通常有这些方法：\",\"1、直接写个缓存刷新页面或者接口，上线时手动操作\",\"2、数据量不大，可以在项目启动的时候自动进行加载\",\"3、定时任务刷新缓存.\"]},{\"header\":\"36、热点key重建？问题？解决？\",\"slug\":\"_36、热点key重建-问题-解决\",\"contents\":[\"开发的时候一般使用“缓存+过期时间”的策略，既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。\",\"但是有两个问题如果同时出现，可能就会出现比较大的问题：\",\"当前key是一个热点key（例如一个热门的娱乐新闻），并发量非常大。\",\"重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的 SQL、多次IO、多个依赖等。在缓存失效的瞬间，有大量线程来重建缓存，造成后端负载加大，甚至可能会让应用崩溃。\",\"怎么处理呢？\",\"要解决这个问题也不是很复杂，解决问题的要点在于：\",\"减少重建缓存的次数。\",\"数据尽可能一致。\",\"较少的潜在危险。\",\"所以一般采用如下方式：\",\"互斥锁（mutex key） 这种方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可。\",\"永远不过期 “永远不过期”包含两层意思：\",\"从缓存层面来看，确实没有设置过期时间，所以不会出现热点key过期后产生的问题，也就是“物理”不过期。\",\"从功能层面来看，为每个value设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。\"]},{\"header\":\"37、Redis报内存不足怎么处理？\",\"slug\":\"_37、redis报内存不足怎么处理\",\"contents\":[\"Redis 内存不足有这么几种处理方式：\",\"修改配置文件 redis.conf 的 maxmemory 参数，增加 Redis 可用内存\",\"也可以通过命令set maxmemory动态设置内存上限\",\"修改内存淘汰策略，及时释放内存空间\",\"使用 Redis 集群模式，进行横向扩容。\"]},{\"header\":\"38、大key问题了解吗？\",\"slug\":\"_38、大key问题了解吗\",\"contents\":[\"Redis使用过程中，有时候会出现大key的情况， 比如：\",\"单个简单的key存储的value很大，size超过10KB\",\"hash， set，zset，list 中存储过多的元素（以万为单位）\",\"大key会造成什么问题呢？\",\"客户端耗时增加，甚至超时\",\"对大key进行IO操作时，会严重占用带宽和CPU\",\"造成Redis集群中数据倾斜\",\"主动删除、被动删等，可能会导致阻塞\",\"如何找到大key?\",\"bigkeys命令：使用bigkeys命令以遍历的方式分析Redis实例中的所有Key，并返回整体统计信息与每个数据类型中Top1的大Key\",\"redis-rdb-tools：redis-rdb-tools是由Python写的用来分析Redis的rdb快照文件用的工具，它可以把rdb快照文件生成json文件或者生成报表用来分析Redis的使用详情。\",\"如何处理大key?\",\"删除大key\",\"当Redis版本大于4.0时，可使用UNLINK命令安全地删除大Key，该命令能够以非阻塞的方式，逐步地清理传入的Key。\",\"当Redis版本小于4.0时，避免使用阻塞式命令KEYS，而是建议通过SCAN命令执行增量迭代扫描key，然后判断进行删除。\",\"压缩和拆分key\",\"当vaule是string时，比较难拆分，则使用序列化、压缩算法将key的大小控制在合理范围内，但是序列化和反序列化都会带来更多时间上的消耗。\",\"当value是string，压缩之后仍然是大key，则需要进行拆分，一个大key分为不同的部分，记录每个部分的key，使用multiget等操作实现事务读取。\",\"当value是list/set等集合类型时，根据预估的数据规模来进行分片，不同的元素计算后分到不同的片。\"]},{\"header\":\"39、Redis常见性能问题和解决方案？\",\"slug\":\"_39、redis常见性能问题和解决方案\",\"contents\":[\"Master 最好不要做任何持久化工作，包括内存快照和 AOF 日志文件，特别是不要启用内存快照做持久化。\",\"如果数据比较关键，某个 Slave 开启 AOF 备份数据，策略为每秒同步一次。\",\"为了主从复制的速度和连接的稳定性，Slave 和 Master 最好在同一个局域网内。\",\"尽量避免在压力较大的主库上增加从库。\",\"Master 调用 BGREWRITEAOF 重写 AOF 文件，AOF 在重写的时候会占大量的 CPU 和内存资源，导致服务 load 过高，出现短暂服务暂停现象。\",\"为了 Master 的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关为：Master<–Slave1<–Slave2<–Slave3…，这样的结构也方便解决单点故障问题，实现 Slave 对 Master 的替换，也即，如果 Master 挂了，可以立马启用 Slave1 做 Master，其他不变。\"]},{\"header\":\"40、使用Redis 如何实现异步队列？\",\"slug\":\"_40、使用redis-如何实现异步队列\",\"contents\":[\"使用list作为队列，lpush生产消息，rpop消费消息\",\"这种方式，消费者死循环rpop从队列中消费消息。但是这样，即使队列里没有消息，也会进行rpop，会导致Redis CPU的消耗。\",\"可以通过让消费者休眠的方式的方式来处理，但是这样又会又消息的延迟问题。\",\"-使用list作为队列，lpush生产消息，brpop消费消息\",\"brpop是rpop的阻塞版本，list为空的时候，它会一直阻塞，直到list中有值或者超时。\",\"这种方式只能实现一对一的消息队列。\",\"使用Redis的pub/sub来进行消息的发布/订阅\",\"发布/订阅模式可以1：N的消息发布/订阅。发布者将消息发布到指定的频道频道（channel），订阅相应频道的客户端都能收到消息。\",\"但是这种方式不是可靠的，它不保证订阅者一定能收到消息，也不进行消息的存储。\",\"所以，一般的异步队列的实现还是交给专业的消息队列。\"]},{\"header\":\"41、Redis 如何实现延时队列?\",\"slug\":\"_41、redis-如何实现延时队列\",\"contents\":[\"使用zset，利用排序实现\",\"可以使用 zset这个结构，用设置好的时间戳作为score进行排序，使用 zadd score1 value1 ....命令就可以一直往内存中生产消息。再利用 zrangebysocre 查询符合条件的所有待处理的任务，通过循环执行队列任务即可。\"]},{\"header\":\"42、Redis和Lua脚本的使用了解吗？\",\"slug\":\"_42、redis和lua脚本的使用了解吗\",\"contents\":[\"Redis的事务功能比较简单，平时的开发中，可以利用Lua脚本来增强Redis的命令。\",\"Lua脚本能给开发人员带来这些好处：\",\"Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令。\",\"Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这 些命令常驻在Redis内存中，实现复用的效果。\",\"Lua脚本可以将多条命令一次性打包，有效地减少网络开销。\",\"比如这一段很（烂）经（大）典（街）的秒杀系统利用lua扣减Redis库存的脚本：\",\" -- 库存未预热 if (redis.call('exists', KEYS[2]) == 1) then return -9; end; -- 秒杀商品库存存在 if (redis.call('exists', KEYS[1]) == 1) then local stock = tonumber(redis.call('get', KEYS[1])); local num = tonumber(ARGV[1]); -- 剩余库存少于请求数量 if (stock < num) then return -3 end; -- 扣减库存 if (stock >= num) then redis.call('incrby', KEYS[1], 0 - num); -- 扣减成功 return 1 end; return -2; end; -- 秒杀商品库存不存在 return -1; \"]},{\"header\":\"43、Redis回收进程如何工作的？\",\"slug\":\"_43、redis回收进程如何工作的\",\"contents\":[\"一个客户端运行了新的命令，添加了新的数据。\",\"Redis检查内存使用情况，如果大于maxmemory的限制， 则根据设定好的策略进行回收。\",\"一个新的命令被执行，等等。\",\"所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。\",\"如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被 这个内存使用量超越。\"]},{\"header\":\"44、假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？\",\"slug\":\"_44、假如redis里面有1亿个key-其中有10w个key是以某个固定的已知的前缀开头的-如果将它们全部找出来\",\"contents\":[\"使用keys指令可以扫出指定模式的key列表。\",\"如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？\",\"这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。\"]},{\"header\":\"45、Redis 底层的数据结构了解吗？\",\"slug\":\"_45、redis-底层的数据结构了解吗\",\"contents\":[\"数据类型实际描述的是 value 的类型，key 都是 string，常见数据类型（value）有\",\"string（embstr、raw、int）\",\"list（quicklist，由多个 ziplist 双向链表组成）\",\"hash（ziplist、hashtable）\",\"set（intset、hashtable）\",\"sorted set（ziplist、skiplist）\",\"bitmap\",\"hyperloglog\",\"每一种类型都用 redisObject 结构体来表示，每种类型根据情况不同，有不同的编码 encoding（即底层数据结构）\",\"String\",\"如果字符串保存的是整数值，则底层编码为 int，实际使用 long 来存储 \",\"占用空间小\",\"作为数据运算时方便，不需要转换\",\"如果字符串保存的是非整数值（浮点数字或其它字符）又分两种情况 \",\"长度 <= 39 字节，使用 embstr 编码来保存，即将 redisObject 和 sdshdr 结构体保存在一起，分配内存只需一次\",\"长度 > 39 字节，使用 raw 编码来保存，即 redisObject 结构体分配一次内存，sdshdr 结构体分配一次内存，用指针相连\",\"sdshdr 称为简单动态字符串，实现上有点类似于 java 中的 StringBuilder，有如下特性 \",\"单独存储字符长度，相比 char* 获取长度效率高（char* 是 C 语言原生字符串表示）\",\"支持动态扩容，方便字符串拼接操作\",\"预留空间，减少内存分配、释放次数（< 1M 时容量是字符串实际长度 2 倍，>= 1M 时容量是原有容量 + 1M）\",\"二进制安全，例如传统 char* 以 \\\\0 作为结束字符，这样就不能保存视频、图片等二进制数据，而 sds 以长度来进行读取\",\"List\",\"3.2 开始，Redis 采用 quicklist 作为其编码方式，它是一个双向链表，节点元素是 ziplist\",\"由于是链表，内存上不连续\",\"操作头尾效率高，时间复杂度 O(1)\",\"链表中 ziplist 的大小和元素个数都可以设置，其中大小默认 8kb\",\"ziplist 用一块连续的内存存储数据，设计目标是让数据存储更紧凑，减少碎片开销，节约内存，它的结构如下\",\"zlbytes – 记录整个 ziplist 占用字节数\",\"zltail-offset – 记录尾节点偏移量\",\"zllength – 记录节点数量\",\"entry – 节点，1 ~ N 个，每个 entry 记录了前一 entry 长度，本 entry 的编码、长度、实际数据，为了节省内存，根据实际数据长度不同，用于记录长度的字节数也不同，例如前一 entry 长度是 253 时，需要用 1 个字节，但超过了 253，需要用 5 个字节\",\"zlend – 结束标记\",\"ziplist *适合存储少量元素，否则查询效率不高，并且长度可变的设计会带来连锁更新问题\",\"Hash\",\"在数据量较小时，采用 ziplist 作为其编码，当键或值长度过大（64）或个数过多（512）时，转为 hashtable 编码\",\"hashtable 编码\",\"hash 函数，Redis 5.0 采用了 SipHash 算法\",\"采用拉链法解决 key 冲突\",\"rehash 时机\",\"① 当元素数 < 1 * 桶个数时，不扩容\",\"② 当元素数 > 5 * 桶个数时，一定扩容\",\"③ 当 1 * 桶个数 <= 元素数 <= 5 * 桶个数时，如果此时没有进行 AOF 或 RDB 操作时\",\"④ 当元素数 < 桶个数 / 10 时，缩容\",\"rehash 要点\",\"① 每个字典有两个哈希表，桶个数为 2n，平时使用 ht[0]，ht[1] 开始为 null，扩容时新数组大小为元素个数 * 2\",\"② 渐进式 rehash，即不是一次将所有桶都迁移过去，每次对这张表 CRUD 仅迁移一个桶\",\"③ active rehash，server 的主循环中，每 100 ms 里留出 1s 进行主动迁移\",\"④ rehash 过程中，新增操作 ht[1] ，其它操作先操作 ht[0]，若没有，再操作 ht[1]\",\"⑤ redis 所有 CRUD 都是单线程，因此 rehash 一定是线程安全的\",\"Sorted Set\",\"在数据量较小时，采用 ziplist 作为其编码，按 score 有序，当键或值长度过大（64）或个数过多（128）时，转为 skiplist + hashtable 编码，同时采用的理由是\",\"只用 hashtable，CRUD 是 O(1)，但要执行有序操作，需要排序，带来额外时间空间复杂度\",\"只用 skiplist，虽然范围操作优点保留，但时间复杂度上升\",\"虽然同时采用了两种结构，但由于采用了指针，元素并不会占用双份内存\",\"skiplist 要点：多层链表、排序规则、 backward、level（span，forward）\",\"score 存储分数、member 存储数据、按 score 排序，如果 score 相同再按 member 排序\",\"backward 存储上一个节点指针\",\"每个节点中会存储层级信息（level），同一个节点可能会有多层，每个 level 有属性： \",\"foward 同层中下一个节点指针\",\"span 跨度，用于计算排名，不是所有跳表都实现了跨度，Redis 实现特有\",\"多层链表可以加速查询，规则为，从顶层开始\",\"大于同层右边的，继续在同层向右找\",\"相等找到了\",\"小于同层右边的或右边为 NULL，下一层，重复 1、2 步骤\",\"以查找【崔八】为例 \",\"从顶层（4）层向右找到【王五】节点，22 > 7 继续向右找，但右侧是 NULL，下一层\",\"在【王五】节点的第 3 层向右找到【孙二】节点，22 < 37，下一层\",\"在【王五】节点的第 2 层向右找到【赵六】节点，22 > 19，继续向右找到【孙二】节点，22 < 37，下一层\",\"在【赵六】节点的第 1 层向右找到【崔八】节点，22 = 22，返回\"]},{\"header\":\"46、Redis的管道命令你是怎么理解的？\",\"slug\":\"_46、redis的管道命令你是怎么理解的\",\"contents\":[\"pipeline出现的背景及是什么\",\"Redis客户端执行一条命令分4个过程：发送命令－〉命令排队－〉命令执行－〉返回结果\",\"这个过程称为Roundtriptime(简称RTT,往返时间)，Redis的原生批命令（mget和mset）有效节约了RTT，但大部分命令不支持批量操作，需要消耗N次RTT产生多次往返，比如set k1 v1,get k1,lpush key...\",\"一个命令来回一趟，徒增RTT。这个时候需要pipeline来解决这个问题。\",\"Pipeline是为了解决RTT往返回时，仅仅是将命令打包一次性发送，对整个Redis的执行不选成其它任何影响，是提升性能的利器。\"]},{\"header\":\"47、为什么Redis集群的最大槽树是16384？\",\"slug\":\"_47、为什么redis集群的最大槽树是16384\",\"contents\":[\"CRC16算法产生的hash值有16bit，该算法可以产生2^16=65536个值。换句话说值是分布在0~65535之间，有更大的65536不用为什么只用16384就够？作者在做mod运算的时候，为什么不mod65536，而选择mod16384？ HASH_SLOT = CRC16(key) mod 65536为什么没启用？\",\"作者回答：https://github.com/redis/redis/issues/2576\",\"正常的心跳数据包携带节点的完整配置，可以以幂等方式替换旧配置以更新旧配置。这意味着它们以原始形式包含节点的插槽配置，该节点使用带有 16k 插槽的 2k 空间，但使用 65k 插槽时会使用令人望而却步的 8k 空间。\",\"同时，由于其他设计权衡，Redis 集群不太可能扩展到超过 1000 个主节点。\",\"因此，16k 处于正确的范围内，可以确保每个主机有足够的插槽，最多 1000 个主机，但数量足够小，可以轻松地将插槽配置作为原始位图传播。请注意，在小簇中，位图将难以压缩，因为当 N 很小时，位图将具有插槽/N 位设置，这是位设置的很大百分比。\",\"(1)如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。\",\"在消息头中最占空间的是myslots[CLUSTER_SLOTS/8]。 当槽位为65536时，这块的大小是: 65536÷8÷1024=8kb\",\"在消息头中最占空间的是myslots[CLUSTER_SLOTS/8]。 当槽位为16384时，这块的大小是: 16384÷8÷1024=2kb\",\"因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。\",\"(2)redis的集群主节点数量基本不可能超过1000个。\",\"集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者不建议redis cluster节点数量超过1000个。 那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。\",\"(3)槽位越小，节点少的情况下，压缩比高，容易传输\",\"Redis主节点的配置信息中它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中会对bitmap进行压缩，但是如果bitmap的填充率slots / N很高的话(N表示节点数)，bitmap的压缩率就很低。 如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。\"]},{\"header\":\"48、 请谈谈实现一个Redis分布式锁要达到哪些要求或条件?\",\"slug\":\"_48、-请谈谈实现一个redis分布式锁要达到哪些要求或条件\",\"contents\":[\"独占性：任何时刻有且仅有一个线程占有\",\"高可用：Redis集群环境下，不能因为一个节点挂了而出现获取锁或释放锁失败的情况；高并发请求下，性能依旧很好\",\"防死锁：杜绝死锁，必须有超时控制机制或撤销机制，有个兜底终止跳出方案\",\"不乱抢：不能私下unlock别人的锁，只能自己加锁自己释放\",\"重入性：同一个节点的同一个线程如果获得锁之后，它可以再次获取这个锁\"]},{\"header\":\"49、手写Redis分布式锁的思路和步骤？\",\"slug\":\"_49、手写redis分布式锁的思路和步骤\",\"contents\":[\"通常开发中，秒杀下单、抢红包等等业务场景，都需要用到分布式锁。而Redis非常适合作为分布式锁使用。\",\"1、Redis分布式锁方案一：SETNX+EXPIRE (项目中一般不用，仅供学习)\",\"2、Redis分布式锁方案二：while判断并自旋重试获取锁+hset含自然过期时间+Lock接口规范+Lua脚本删除锁命令\",\"3、Redis分布式锁方案三：采用Redisson框架实现，Redisson框架实现的锁非常丰富，底层也是采用Redis实现，框架内实现了可重入锁、读写锁、信号量锁、闭锁、红锁等。\"]},{\"header\":\"50、Redis分布式锁如何续期？watchDog看门狗是什么意思？\",\"slug\":\"_50、redis分布式锁如何续期-watchdog看门狗是什么意思\",\"contents\":[\"确保redisLock过期时间大于业务执行时间的问题，自动续期问题，俗称看门狗。\",\"当我们加锁成功拿到redis分布式锁后，同时也异步启动一个线程，这个就是看门狗线程，它会判断对应key是否到过期时间的1/3，\",\"如果到了，自动执行expire key命令，对key进行续期，直到任务完成。比如过期时间是30秒，到第10秒后，会自动给key续命加到30秒，以此类推。当然，实现看门狗的任务通过lua脚本来完成\",\"//==============自动续期 if redis.call('HEXISTS',KEYS[1],ARGV[1]) == 1 then return redis.call('expire',KEYS[1],ARGV[2]) else return 0 end \"]},{\"header\":\"51、Redis的源码读过吗，请说一下RedisObject对象？\",\"slug\":\"_51、redis的源码读过吗-请说一下redisobject对象\",\"contents\":[\"Redis内部所有存储的数据都使用redisObject来封装。\",\"为了便于操作，Redis采用redisObject结构来统一五种不同的数据类型，这样所有的数据类型就都可以以相同的形式在函数间传递而不用使用特定的类型结构。同时，为了识别不同的数据类型，redisObject中定义了type和encoding字段对不同的数据类型加以区别。简单地说，redisObject就是string、hash、list、set、zset的父类，可以在函数间传递时隐藏具体的类型信息，所以作者抽象了redisObject结构来到达同样的目的。\",\"各个字段的含义\",\"4位的type表示具体的数据类型\",\"4位的encoding表示该类型的物理编码方式见下表，同一种数据类型可能有不同的编码方式。(比如String就提供了3种:int embstr raw)\",\"lru字段表示当内存超限时采用LRU算法清除内存中的对象。\",\"refcount表示对象的引用计数。\",\"ptr指针指向真正的底层数据结构的指针。\",\"比如，set age 17\"]},{\"header\":\"52、Redis跳表你了解吗？\",\"slug\":\"_52、redis跳表你了解吗\",\"contents\":[\"跳表是一种特殊的链表，特殊的点在于其可以进行二分查找。普通的链表要查找元素只能挨个遍历链表中的所有元素，而跳表则利用了空间换时间的策略，在原来有序链表的基础上面增加了多级索引，然后利用类似二分查找的思路来快速实现查找功能。跳表可以支持快速的查找，插入，删除等操作，时间复杂度为O(logn)，空间复杂度为O(n)。\",\"跳表是可以实现二分查找的有序链表！跳表 = 链表 + 多级索引\"]},{\"header\":\"53、Redis单线程如何处理并发客户端连接，为什么单线程还这么快？\",\"slug\":\"_53、redis单线程如何处理并发客户端连接-为什么单线程还这么快\",\"contents\":[\"Redis的IO多路复用是的单线程也依旧很快，性能强大\",\"Redis利用epoll来实现IO多路复用，将连接信息和事件放到队列中，一次放到文件事件分派器，事件分派器将事件分发给事件处理器。\",\"Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现。\",\"所谓 I/O 多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。这种机制的使用需要 select 、 poll 、 epoll 来配合。多个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象上等待，无需阻塞等待所有连接。当某条连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理。\",\"Redis 服务采用 Reactor 的方式来实现文件事件处理器（每一个网络连接其实都对应一个文件描述符）\",\"Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器。它的组成结构为4部分：\",\"多个套接字、\",\"IO多路复用程序、\",\"文件事件分派器、\",\"事件处理器。\",\"因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型\"]}]},\"/interview/framework/Mybatis.html\":{\"title\":\"MyBatis 33道面试题\",\"contents\":[{\"header\":\"1、什么是MyBatis?\",\"slug\":\"_1、什么是mybatis\",\"contents\":[\"Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。\",\"MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。\"]},{\"header\":\"2、Mybatis的优缺点？\",\"slug\":\"_2、mybatis的优缺点\",\"contents\":[\"优点：\",\"基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用\",\"与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接\",\"很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）\",\"提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护\",\"能够与Spring很好的集成\",\"缺点：\",\"SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求\",\"SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库\"]},{\"header\":\"3、Hibernate 和 MyBatis 的区别\",\"slug\":\"_3、hibernate-和-mybatis-的区别\",\"contents\":[\"相同点：都是对jdbc的封装，都是持久层的框架，都用于dao层的开发。\",\"不同点：\",\"映射关系 \",\"MyBatis 是一个半自动映射的框架，配置Java对象与sql语句执行结果的对应关系，多表关联关系配置简单\",\"Hibernate 是一个全表映射的框架，配置Java对象与数据库表的对应关系，多表关联关系配置复杂\",\"SQL优化和移植性 \",\"Hibernate 对SQL语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但SQL语句优化困难。\",\"MyBatis 需要手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用SQL语句操作数据库，不支持数据库无关性，但sql语句优化容易。\",\"ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单Java对象（POJO）的映射关系的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。\"]},{\"header\":\"4、为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？\",\"slug\":\"_4、为什么说mybatis是半自动orm映射工具-它与全自动的区别在哪里\",\"contents\":[\"Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。\",\"而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。\"]},{\"header\":\"5、传统JDBC开发存在什么问题？\",\"slug\":\"_5、传统jdbc开发存在什么问题\",\"contents\":[\"频繁创建数据库连接对象、释放，容易造成系统资源浪费，影响系统性能。可以使用连接池解决这个问题。但是使用jdbc需要自己实现连接池\",\"sql语句定义、参数设置、结果集处理存在硬编码。实际项目中sql语句变化的可能性较大，一旦发生变化，需要修改java代码，系统需要重新编译，重新发布。不好维护。\",\"使用preparedStatement向占有位符号传参数存在硬编码，因为sql语句的where条件不一定，可能多也可能少，修改sql还要修改代码，系统不易维护。\",\"结果集处理存在重复代码，处理麻烦。如果可以映射成Java对象会比较方便。\"]},{\"header\":\"6、JDBC编程有哪些不足之处，MyBatis是如何解决的？\",\"slug\":\"_6、jdbc编程有哪些不足之处-mybatis是如何解决的\",\"contents\":[\"针对5提到的4点不足，依次的解决方法如下：\",\"在mybatis-config.xml中配置数据链接池，使用连接池管理数据库连接。\",\"将Sql语句配置在XXXXmapper.xml文件中与java代码分离。\",\"Mybatis自动将java对象映射至sql语句。\",\"Mybatis自动将sql执行结果映射至java对象。\"]},{\"header\":\"7、MyBatis编程步骤是什么样的？\",\"slug\":\"_7、mybatis编程步骤是什么样的\",\"contents\":[\"创建SqlSessionFactory\",\"通过SqlSessionFactory创建SqlSession\",\"通过sqlsession执行数据库操作\",\"调用session.commit()提交事务\",\"调用session.close()关闭会话\"]},{\"header\":\"8、说说MyBatis的工作原理\",\"slug\":\"_8、说说mybatis的工作原理\",\"contents\":[\"读取 MyBatis 配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。\",\"加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。\",\"构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。\",\"创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。\",\"Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。\",\"MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。\",\"输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。\",\"输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。\"]},{\"header\":\"9、MyBatis的功能架构是怎样的\",\"slug\":\"_9、mybatis的功能架构是怎样的\",\"contents\":[\"我们把Mybatis的功能架构分为三层：\",\"API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。\",\"数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。\",\"基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。\"]},{\"header\":\"10、什么是DBMS\",\"slug\":\"_10、什么是dbms\",\"contents\":[\"DBMS：数据库管理系统(database management system)是一种操纵和管理数据库的大型软件，用于建立、使用和维护数库，简称dbms。它对数据库进行统一的管理和控制，以保证数据库的安全性和完整性。用户通过dbms访问数据库中的数据，数据库管理员也通过dbms进行数据库的维护工作。它可使多个应用程序和用户用不同的方法在同时版或不同时刻去建立，修改和询问数据库。\",\"DBMS提供数据定义语言DDL（Data Definition Language）与数据操作语言DML（DataManipulation Language），供用户定义数据库的模式结构与权限约束，实现对数据的追加权、删除等操作。\"]},{\"header\":\"11、为什么需要预编译\",\"slug\":\"_11、为什么需要预编译\",\"contents\":[\"定义：SQL 预编译指的是数据库驱动在发送 SQL 语句和参数给 DBMS 之前对 SQL 语句进行编译，这样 DBMS 执行 SQL 时，就不需要重新编译。\",\"原因：JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 SQL 的执行。预编译之后的 SQL 多数情况下可以直接执行，DBMS 不需要再次编译，越复杂的SQL，编译的复杂度将越大，预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用。把一个 SQL 预编译后产生的 PreparedStatement 对象缓存下来，下次对于同一个SQL，可以直接使用这个缓存的 PreparedState 对象。Mybatis默认情况下，将对所有的 SQL 进行预编译。还有一个原因就是复制SQL注入\"]},{\"header\":\"12、Mybatis都有哪些Executor执行器？它们之间的区别是什么？\",\"slug\":\"_12、mybatis都有哪些executor执行器-它们之间的区别是什么\",\"contents\":[\"SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。\",\"ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map<String, Statement>内，供下一次使用。简言之，就是重复使用Statement对象。\",\"BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。\"]},{\"header\":\"13、Mybatis中如何指定使用哪一种Executor执行器？\",\"slug\":\"_13、mybatis中如何指定使用哪一种executor执行器\",\"contents\":[\"在Mybatis配置文件中，在设置（settings）可以指定默认的ExecutorType执行器类型，也可以手动给DefaultSqlSessionFactory的创建SqlSession的方法传递ExecutorType类型参数，如SqlSession openSession(ExecutorType execType)。 配置默认的执行器。SIMPLE 就是普通的执行器；REUSE 执行器会重用预处理语句（preparedstatements）； BATCH 执行器将重用语句并执行批量更新。\",\"<settings> <setting name=\\\"defaultExecutorType\\\" value=\\\"SIMPLE\\\"/> </settings> \"]},{\"header\":\"14、Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？\",\"slug\":\"_14、mybatis是否支持延迟加载-如果支持-它的实现原理是什么\",\"contents\":[\"Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。\",\"它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。\"]},{\"header\":\"15、#{}和${}的区别\",\"slug\":\"_15、-和-的区别\",\"contents\":[\"#{}是占位符，预编译处理；${}是拼接符，字符串替换，没有预编译处理。\",\"Mybatis在处理#{}时，#{}传入参数是以字符串传入，会将SQL中的#{}替换为?号，调用PreparedStatement的set方法来赋值。\",\"#{} 可以有效的防止SQL注入，提高系统安全性；${} 不能防止SQL 注入\",\"#{} 的变量替换是在DBMS 中；${} 的变量替换是在 DBMS 外\"]},{\"header\":\"16、模糊查询like语句该怎么写\",\"slug\":\"_16、模糊查询like语句该怎么写\",\"contents\":[\"‘%${question}%’：可能引起SQL注入，不推荐\",\"\\\"%\\\"#{question}\\\"%\\\" 注意：因为#{…}解析成sql语句时候，会在变量外侧自动加单引号’ '，所以这里 % 需要使用双引号\\\" \\\"，不能使用单引号 ’ '，不然会查不到任何结果。\",\"CONCAT(’%’,#{question},’%’) 使用CONCAT()函数，（推荐）\",\"使用bind标签（不推荐）\",\"<select id=\\\"listUserLikeUsername\\\" resultType=\\\"com.jourwon.pojo.User\\\"> &emsp;&emsp;<bind name=\\\"pattern\\\" value=\\\"'%' + username + '%'\\\" /> &emsp;&emsp;select id,sex,age,username,password from person where username LIKE #{pattern} </select> \"]},{\"header\":\"17、在mapper中如何传递多个参数\",\"slug\":\"_17、在mapper中如何传递多个参数\",\"contents\":[]},{\"header\":\"方法1：顺序传参法\",\"slug\":\"方法1-顺序传参法\",\"contents\":[\"public User selectUser(String name, int deptId); \",\"<select id=\\\"selectUser\\\" resultMap=\\\"UserResultMap\\\"> select * from user where user_name = #{0} and dept_id = #{1} </select> \",\"#{}里面的数字代表传入参数的顺序。\",\"这种方法不建议使用，sql层表达不直观，且一旦顺序调整容易出错。\"]},{\"header\":\"方法2：@Param注解传参法\",\"slug\":\"方法2-param注解传参法\",\"contents\":[\"public User selectUser(@Param(\\\"userName\\\") String name, @Param(\\\"deptId\\\")int deptId); \",\"<select id=\\\"selectUser\\\" resultMap=\\\"UserResultMap\\\"> select * from user where user_name = #{userName} and dept_id = #{deptId} </select> \",\"#{}里面的名称对应的是注解@Param括号里面修饰的名称。\",\"这种方法在参数不多的情况还是比较直观的，（推荐使用）。\"]},{\"header\":\"方法3：Map传参法\",\"slug\":\"方法3-map传参法\",\"contents\":[\"public User selectUser(Map<String, Object> params); \",\"<select id=\\\"selectUser\\\" parameterType=\\\"java.util.Map\\\" resultMap=\\\"UserResultMap\\\"> select * from user where user_name = #{userName} and dept_id = #{deptId} </select> \",\"#{}里面的名称对应的是Map里面的key名称。\",\"这种方法适合传递多个参数，且参数易变能灵活传递的情况。（推荐使用）。\"]},{\"header\":\"方法4：Java Bean传参法\",\"slug\":\"方法4-java-bean传参法\",\"contents\":[\"public User selectUser(User user); \",\"<select id=\\\"selectUser\\\" parameterType=\\\"com.jourwon.pojo.User\\\" resultMap=\\\"UserResultMap\\\"> select * from user where user_name = #{userName} and dept_id = #{deptId} </select> \",\"#{}里面的名称对应的是User类里面的成员属性。\",\"这种方法直观，需要建一个实体类，扩展不容易，需要加属性，但代码可读性强，业务逻辑处理方便，推荐使用。（推荐使用）。\"]},{\"header\":\"18、Mybatis如何执行批量操作\",\"slug\":\"_18、mybatis如何执行批量操作\",\"contents\":[\"使用foreach标签\",\"foreach的主要用在构建in条件中，它可以在SQL语句中进行迭代一个集合。foreach标签的属性主要有item，index，collection，open，separator，close。\",\"item：表示集合中每一个元素进行迭代时的别名，随便起的变量名；\",\"index：指定一个名字，用于表示在迭代过程中，每次迭代到的位置，不常用；\",\"open：表示该语句以什么开始，常用“(”；\",\"separator：表示在每次进行迭代之间以什么符号作为分隔符，常用“,”；\",\"close：表示以什么结束，常用“)”。\",\"在使用foreach的时候最关键的也是最容易出错的就是collection属性，该属性是必须指定的，但是在不同情况下，该属性的值是不一样的，主要有一下3种情况：\",\"如果传入的是单参数且参数类型是一个List的时候，collection属性值为list\",\"如果传入的是单参数且参数类型是一个array数组的时候，collection的属性值为array\",\"如果传入的参数是多个的时候，我们就需要把它们封装成一个Map了，当然单参数也可以封装成map，实际上如果你在传入参数的时候，在MyBatis里面也是会把它封装成一个Map的，map的key就是参数名，所以这个时候collection属性值就是传入的List或array对象在自己封装的map里面的key\",\"第一种\",\"<!-- 批量保存(foreach插入多条数据两种方法) int addEmpsBatch(@Param(\\\"emps\\\") List<Employee> emps); --> <!-- MySQL下批量保存，可以foreach遍历 mysql支持values(),(),()语法 --> //推荐使用 <insert id=\\\"addEmpsBatch\\\"> INSERT INTO emp(ename,gender,email,did) VALUES <foreach collection=\\\"emps\\\" item=\\\"emp\\\" separator=\\\",\\\"> (#{emp.eName},#{emp.gender},#{emp.email},#{emp.dept.id}) </foreach> </insert> \",\"第二种\",\"<!-- 这种方式需要数据库连接属性allowMutiQueries=true的支持 如jdbc.url=jdbc:150mysql://localhost:3306/mybatis?allowMultiQueries=true --> <insert id=\\\"addEmpsBatch\\\"> <foreach collection=\\\"emps\\\" item=\\\"emp\\\" separator=\\\";\\\"> INSERT INTO emp(ename,gender,email,did) VALUES(#{emp.eName},#{emp.gender},#{emp.email},#{emp.dept.id}) </foreach> </insert> \",\"第三种：使用ExecutorType.BATCH\",\"Mybatis内置的ExecutorType有3种，默认为simple,该模式下它为每个语句的执行创建一个新的预处理语句，单条提交sql；而batch模式重复使用已经预处理的语句，并且批量执行所有更新语句，显然batch性能将更优； 但batch模式也有自己的问题，比如在Insert操作时，在事务没有提交之前，是没有办法获取到自增的id，这在某型情形下是不符合业务要求的\",\"@Test public void testBatch() throws IOException{ SqlSessionFactory sqlSessionFactory = getSqlSessionFactory(); //可以执行批量操作的sqlSession SqlSession openSession = sqlSessionFactory.openSession(ExecutorType.BATCH); //批量保存执行前时间 long start = System.currentTimeMillis(); try { EmployeeMapper mapper = openSession.getMapper(EmployeeMapper.class); for (int i = 0; i < 1000; i++) { mapper.addEmp(new Employee(UUID.randomUUID().toString().substring(0, 5), \\\"b\\\", \\\"1\\\")); } openSession.commit(); long end = System.currentTimeMillis(); //批量保存执行后的时间 System.out.println(\\\"执行时长\\\" + (end - start)); //批量 预编译sql一次==》设置参数==》10000次==》执行1次 677 //非批量 （预编译=设置参数=执行 ）==》10000次 1121 } finally { openSession.close(); } } \",\"mapper和mapper.xml\",\"public interface EmployeeMapper { //批量保存员工 Long addEmp(Employee employee); } \",\"<mapper namespace=\\\"com.jourwon.mapper.EmployeeMapper\\\"> <!--批量保存员工 --> <insert id=\\\"addEmp\\\"> insert into employee(lastName,email,gender) values(#{lastName},#{email},#{gender}) </insert> </mapper> \"]},{\"header\":\"19、如何获取生成的主键\",\"slug\":\"_19、如何获取生成的主键\",\"contents\":[\"新增标签中添加：keyProperty=\\\" ID \\\" 即可\",\"<insert id=\\\"insert\\\" useGeneratedKeys=\\\"true\\\" keyProperty=\\\"userId\\\" > insert into user( user_name, user_password, create_time) values(#{userName}, #{userPassword} , #{createTime, jdbcType= TIMESTAMP}) </insert> \"]},{\"header\":\"20、当实体类中的属性名和表中的字段名不一样 ，怎么办\",\"slug\":\"_20、当实体类中的属性名和表中的字段名不一样-怎么办\",\"contents\":[\"第一种：通过在查询的SQL语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。\",\"<select id=\\\"getOrder\\\" parameterType=\\\"int\\\" resultType=\\\"com.jourwon.pojo.Order\\\"> select order_id id, order_no orderno ,order_price price form orders where order_id=#{id}; </select> \",\"第2种： 通过<resultMap> 来映射字段名和实体类属性名的一一对应的关系。\",\"<select id=\\\"getOrder\\\" parameterType=\\\"int\\\" resultMap=\\\"orderResultMap\\\"> select * from orders where order_id=#{id} </select> <resultMap type=\\\"com.jourwon.pojo.Order\\\" id=\\\"orderResultMap\\\"> <!–用id属性来映射主键字段–> <id property=\\\"id\\\" column=\\\"order_id\\\"> <!–用result属性来映射非主键字段，property为实体类属性名，column为数据库表中的属 性–> <result property =\\\"orderno\\\" column =\\\"order_no\\\"/> <result property=\\\"price\\\" column=\\\"order_price\\\" /> </reslutMap> \"]},{\"header\":\"21、Mapper 编写有哪几种方式？\",\"slug\":\"_21、mapper-编写有哪几种方式\",\"contents\":[]},{\"header\":\"第一种：接口实现类继承 SqlSessionDaoSupport：使用此种方法需要编写mapper 接口，mapper 接口实现类、mapper.xml 文件。\",\"slug\":\"第一种-接口实现类继承-sqlsessiondaosupport-使用此种方法需要编写mapper-接口-mapper-接口实现类、mapper-xml-文件。\",\"contents\":[\"在 sqlMapConfig.xml 中配置 mapper.xml 的位置\",\"<mappers> <mapper resource=\\\"mapper.xml 文件的地址\\\" /> <mapper resource=\\\"mapper.xml 文件的地址\\\" /> </mappers> \",\"定义 mapper 接口\",\"实现类集成 SqlSessionDaoSupport，mapper 方法中可以 this.getSqlSession()进行数据增删改查。\",\"spring 配置\",\"<bean id=\\\" \\\" class=\\\"mapper 接口的实现\\\"> <property name=\\\"sqlSessionFactory\\\" ref=\\\"sqlSessionFactory\\\"></property> </bean> \"]},{\"header\":\"第二种：使用 org.mybatis.spring.mapper.MapperFactoryBean\",\"slug\":\"第二种-使用-org-mybatis-spring-mapper-mapperfactorybean\",\"contents\":[\"在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和mappre 接口的名称相同且在同一个目录，这里可以不用配置\",\"定义 mapper 接口：\",\"<mappers> <mapper resource=\\\"mapper.xml 文件的地址\\\" /> <mapper resource=\\\"mapper.xml 文件的地址\\\" /> </mappers> \",\"mapper.xml 中的 namespace 为 mapper 接口的地址\",\"mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致\",\"Spring 中定义\",\"<bean id=\\\"\\\" class=\\\"org.mybatis.spring.mapper.MapperFactoryBean\\\"> <property name=\\\"mapperInterface\\\" value=\\\"mapper 接口地址\\\" /> <property name=\\\"sqlSessionFactory\\\" ref=\\\"sqlSessionFactory\\\" /> </bean> \"]},{\"header\":\"第三种：使用 mapper 扫描器\",\"slug\":\"第三种-使用-mapper-扫描器\",\"contents\":[\"mapper.xml 文件编写：\",\"mapper.xml 中的 namespace 为 mapper 接口的地址；\",\"mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致；\",\"如果将 mapper.xml 和 mapper 接口的名称保持一致则不用在 sqlMapConfig.xml中进行配置。\",\"定义 mapper 接口\",\"注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录\",\"配置 mapper 扫描器\",\"<bean class=\\\"org.mybatis.spring.mapper.MapperScannerConfigurer\\\"> <property name=\\\"basePackage\\\" value=\\\"mapper 接口包地址\\\"></property> <property name=\\\"sqlSessionFactoryBeanName\\\"value=\\\"sqlSessionFactory\\\"/> </bean> \",\"使用扫描器后从 spring 容器中获取 mapper 的实现对象。\"]},{\"header\":\"22、什么是MyBatis的接口绑定？有哪些实现方式？\",\"slug\":\"_22、什么是mybatis的接口绑定-有哪些实现方式\",\"contents\":[\"接口绑定：就是在MyBatis中任意定义接口，然后把接口里面的方法和SQL语句绑定，我们直接调用接口方法就可以，这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。\",\"接口绑定有两种实现方式\",\"通过注解绑定，就是在接口的方法上面加上 @Select、@Update等注解，里面包含Sql语句来绑定；\",\"通过xml里面写SQL来绑定， 在这种情况下，要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候，用注解绑定， 当SQL语句比较复杂时候，用xml绑定，一般用xml绑定的比较多。\"]},{\"header\":\"23、使用MyBatis的mapper接口调用时有哪些要求？\",\"slug\":\"_23、使用mybatis的mapper接口调用时有哪些要求\",\"contents\":[\"Mapper接口方法名和mapper.xml中定义的每个sql的id相同。\",\"Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同。\",\"Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同。\",\"Mapper.xml文件中的namespace即是mapper接口的类路径。\"]},{\"header\":\"24、这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗\",\"slug\":\"_24、这个dao接口的工作原理是什么-dao接口里的方法-参数不同时-方法能重载吗\",\"contents\":[\"Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。\",\"Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。\"]},{\"header\":\"25、Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？\",\"slug\":\"_25、mybatis的xml映射文件中-不同的xml映射文件-id是否可以重复\",\"contents\":[\"不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。\",\"原因就是namespace+id是作为Map<String, MappedStatement>的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。\"]},{\"header\":\"26、简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？\",\"slug\":\"_26、简述mybatis的xml映射文件和mybatis内部数据结构之间的映射关系\",\"contents\":[\"Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中， <parameterMap> 标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。<resultMap> 标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个<select> 、<insert> 、<update> 、<delete> 标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。\"]},{\"header\":\"27、Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？\",\"slug\":\"_27、mybatis映射文件中-如果a标签通过include引用了b标签的内容-请问-b标签能否定义在a标签的后面-还是说必须定义在a标签的前面\",\"contents\":[\"虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。\",\"原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。\"]},{\"header\":\"28、Mybatis能执行一对多，一对一的联系查询吗，有哪些实现方法\",\"slug\":\"_28、mybatis能执行一对多-一对一的联系查询吗-有哪些实现方法\",\"contents\":[\"能，不止可以一对多，一对一还可以多对多，一对多\",\"实现方式：\",\"单独发送一个SQL去查询关联对象，赋给主对象，然后返回主对象\",\"使用嵌套查询，似JOIN查询，一部分是A对象的属性值，另一部分是关联对 象 B的属性值，好处是只要发送一个属性值，就可以把主对象和关联对象查出来\",\"子查询\"]},{\"header\":\"29、Mybatis是否可以映射Enum枚举类？\",\"slug\":\"_29、mybatis是否可以映射enum枚举类\",\"contents\":[\"Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。\",\"TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。\"]},{\"header\":\"30、Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理吗？\",\"slug\":\"_30、mybatis动态sql是做什么的-都有哪些动态sql-能简述一下动态sql的执行原理吗\",\"contents\":[\"Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能\",\"Mybatis提供了9种动态sql标签 trim|where|set|foreach|if|choose|when|otherwise|bind\",\"其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。\"]},{\"header\":\"31、Mybatis是如何进行分页的？分页插件的原理是什么？\",\"slug\":\"_31、mybatis是如何进行分页的-分页插件的原理是什么\",\"contents\":[\"Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。\",\"分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。\",\"举例：select * from student，拦截sql后重写为：select t.* from (select * from student) t limit 0, 10\"]},{\"header\":\"32、简述Mybatis的插件运行原理，以及如何编写一个插件。\",\"slug\":\"_32、简述mybatis的插件运行原理-以及如何编写一个插件。\",\"contents\":[\"Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。\",\"实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。\"]},{\"header\":\"33、Mybatis的一级、二级缓存\",\"slug\":\"_33、mybatis的一级、二级缓存\",\"contents\":[\"一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Sessionflush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。\",\"二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置\",\"对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。\"]}]},\"/interview/framework/\":{\"title\":\"\",\"contents\":[]},\"/interview/framework/Spring%20Boot.html\":{\"title\":\"Spring Boot 18道面试题\",\"contents\":[{\"header\":\"1、什么是Spring Boot？\",\"slug\":\"_1、什么是spring-boot\",\"contents\":[\"它使用**“习惯优于配置”**（项目中存在大量的配置，此外还内置了一个习惯性的配置，让你无需手动配置）的理念让你的项目快速运行起来。\",\"Spring Boot整合了所有框架\",\"简化Spring应用开发的一个框架\"]},{\"header\":\"2、Spring Boot有哪些优点？\",\"slug\":\"_2、spring-boot有哪些优点\",\"contents\":[\"创建独立的Spring应用程序\",\"直接嵌入Tomcat，Jetty或Undertow（无需部署WAR文件）\",\"提供“初始”的POM文件内容，以简化Maven配置\",\"尽可能自动配置Spring\",\"提供生产就绪的功能，如指标，健康检查和外部化配置\",\"绝对无代码生成，也不需要XML配置\"]},{\"header\":\"3、Spring Boot核心注解是哪个，由哪几个组成呢？\",\"slug\":\"_3、spring-boot核心注解是哪个-由哪几个组成呢\",\"contents\":[\"启动类注解@SpringBootApplication = @Configuration + @EnableAutoConfiguration + @ComponentScan\",\"@Configuration：标明该类使用Spring基于Java的配置\",\"@EnableAutoConfiguration：启动自动配置功能。\",\"@ComponentScan：启用组件扫描，这样你写的Web控制器类和其他组件才能被自动发现并注册为Spring应用程序上下文里的Bean。\"]},{\"header\":\"4、Spring Boot配置文件加载位置\",\"slug\":\"_4、spring-boot配置文件加载位置\",\"contents\":[\"springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件\",\"–file:./config/\",\"–file:./\",\"–classpath:/config/\",\"–classpath:/（默认）\",\"优先级**由高到低，高优先级的配置会覆盖**低优先级的配置；\",\"SpringBoot会从这四个位置全部加载主配置文件；互补配置；\"]},{\"header\":\"5、Spring Boot外部配置加载顺序\",\"slug\":\"_5、spring-boot外部配置加载顺序\",\"contents\":[\"SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置\",\"命令行参数 ：所有的配置都可以在命令行上进行指定 java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --server.port=8087 --server.context-path=/abc 多个配置用空格分开； --配置项=值\",\"来自java:comp/env的JNDI属性\",\"Java系统属性（System.getProperties()）\",\"操作系统环境变量\",\"RandomValuePropertySource配置的random.*属性值；由jar包外向jar包内进行寻找；优先加载带profile\",\"jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件\",\"jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件，再来加载不带profile\",\"jar包外部的application.properties或application.yml(不带spring.profile)配置文件\",\"jar包内部的application.properties或application.yml(不带spring.profile)配置文件\",\"@Configuration注解类上的@PropertySource\",\"通过SpringApplication.setDefaultProperties指定的默认属性；所有支持的配置加载来源；参考官方文档\"]},{\"header\":\"6、SpringBoot事务的使用\",\"slug\":\"_6、springboot事务的使用\",\"contents\":[\"SpringBoot的事务很简单，首先使用注解EnableTransactionManagement开启事务之后，然后在Service方法上添加注解Transactional便可。\"]},{\"header\":\"7、Async异步调用方法\",\"slug\":\"_7、async异步调用方法\",\"contents\":[\"在SpringBoot中使用异步调用是很简单的，只需要在方法上使用@Async注解即可实现方法的异步调用。 注意：需要在启动类加入@EnableAsync使异步调用@Async注解生效。 注：异步方法不能跟调用方法在同一个类中，否则会导致异步失效\"]},{\"header\":\"8、 Spring Boot 中如何解决跨域问题\",\"slug\":\"_8、-spring-boot-中如何解决跨域问题\",\"contents\":[\"@Configuration public class ResourcesConfig implements WebMvcConfigurer { /** * 跨域配置 */ @Bean public CorsFilter corsFilter() { UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(true); // 设置访问源地址 config.addAllowedOrigin(\\\"*\\\"); // 设置访问源请求头 config.addAllowedHeader(\\\"*\\\"); // 设置访问源请求方法 config.addAllowedMethod(\\\"*\\\"); // 对接口配置跨域设置 source.registerCorsConfiguration(\\\"/**\\\", config); return new CorsFilter(source); } } \"]},{\"header\":\"9、Spring Boot 中的监视器是什么？\",\"slug\":\"_9、spring-boot-中的监视器是什么\",\"contents\":[\"Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的 REST 端点来检查状态。\"]},{\"header\":\"10、如何在 Spring Boot 中禁用 Actuator 端点安全性？\",\"slug\":\"_10、如何在-spring-boot-中禁用-actuator-端点安全性\",\"contents\":[\"默认情况下，所有敏感的 HTTP 端点都是安全的，只有具有 ACTUATOR 角色的用户才能访问它们。安全性是使用标准的 HttpServletRequest.isUserInRole 方法实施的。\",\"我们可以使用management.security.enabled = false来禁用安全性。只有在执行机构端点在防火墙后访问时，才建议禁用安全性。\"]},{\"header\":\"11、如何实现 Spring Boot 应用程序的安全性？\",\"slug\":\"_11、如何实现-spring-boot-应用程序的安全性\",\"contents\":[\"为了实现 Spring Boot 的安全性，我们使用 spring-boot-starter-security 依赖项，并且必须添加安全配置。它只需要很少的代码。配置类将必须扩展 WebSecurityConfigurerAdapter 并覆盖其方法。\"]},{\"header\":\"12、什么是 Spring Profiles？\",\"slug\":\"_12、什么是-spring-profiles\",\"contents\":[\"Spring Profiles 允许用户根据配置文件（dev，test，prod 等）来注册 bean。因此，当应用程序在开发中运行时，只有某些 bean 可以加载，而在 PRODUCTION 中，某些其他 bean 可以加载。假设我们的要求是 Swagger 文档仅适用于 QA 环境，并且禁用所有其他文档。这可以使用配置文件来完成。Spring Boot 使得使用配置文件非常简单。\"]},{\"header\":\"13、如何使用 Spring Boot 实现异常处理？\",\"slug\":\"_13、如何使用-spring-boot-实现异常处理\",\"contents\":[\"Spring 提供了一种使用 ControllerAdvice 处理异常的非常有用的方法。 我们通过实现一个@ControlerAdvice 类，来处理控制器类抛出的所有异常。\"]},{\"header\":\"14、ControllerAdvice 和 RestControllerAdvice 有什么区别\",\"slug\":\"_14、controlleradvice-和-restcontrolleradvice-有什么区别\",\"contents\":[\"区别就好比：Controller和RestController一样\",\"RestControllerAdvice = RequestMapping + ControllerAdvice\"]},{\"header\":\"15、 如何在 Spring Boot 启动的时候运行一些特定的代码？\",\"slug\":\"_15、-如何在-spring-boot-启动的时候运行一些特定的代码\",\"contents\":[\"可以实现接口 ApplicationRunner 或者 CommandLineRunner，这两个接口实现方式一样，它们都只提供了一个 run 方法\"]},{\"header\":\"16、Spring Boot怎么禁用循环依赖？\",\"slug\":\"_16、spring-boot怎么禁用循环依赖\",\"contents\":[\"直接允许循环依赖，在Spring Boot配置文件中配置，默认为false\",\"spring: main: allow-circular-references: true \",\"使用方法的返回值获取实例对象，替换通过成员变量注入实例对象\",\"public IAService getStaffService(){ return SpringUtils.getBean(IBService.class); } \"]},{\"header\":\"17、Spring Boot自动配置的原理\",\"slug\":\"_17、spring-boot自动配置的原理\",\"contents\":[]},{\"header\":\"@SpringBootApplication=@Configuration+@EnableAutoConfiguration+@ComponentScan\",\"slug\":\"springbootapplication-configuration-enableautoconfiguration-componentscan\",\"contents\":[\"@Target({ElementType.TYPE}) // 注解的适用范围，其中TYPE用于描述类、接口（包括包注解类型）或enum声明 @Retention(RetentionPolicy.RUNTIME)// 注解的生命周期，保留到class文件中（三个生命周期） @Documented // 表明这个注解应该被javadoc记录 @Inherited // 子类可以继承该注解 @SpringBootConfiguration // 继承了Configuration，表示当前是注解类 @EnableAutoConfiguration // 开启springboot的注解功能，springboot的四大神器之一，其借助@import的帮助 @ComponentScan( // 扫描路径设置 excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) public @interface SpringBootApplication { ...... } \"]},{\"header\":\"@Configuration\",\"slug\":\"configuration\",\"contents\":[\"标明该类使用Spring基于Java的配置。虽然本书不会写太多配置，但我们会更倾向于使用基于Java而不是XML的配置。\"]},{\"header\":\"@EnableAutoConfiguration\",\"slug\":\"enableautoconfiguration\",\"contents\":[\"Spring Boot 的@EnableAutoConfiguration ： 这个不起眼的小注解也可以称为@Abracadabra(咒语)，就是这一\",\"行配置开启了Spring Boot自动配置。简单概括一下就是，借助@Import的支持，将所有符合自动配置条件的bean定义加载到IoC容器。\",\"@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage //自动配置包 @Import({AutoConfigurationImportSelector.class}) //导入自动配置的组件 public @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = \\\"spring.boot.enableautoconfiguration\\\"; Class<?>[] exclude() default {}; String[] excludeName() default {}; } \"]},{\"header\":\"@ComponentScan\",\"slug\":\"componentscan\",\"contents\":[\"Spring的@ComponentScan：启用组件扫描，这样你写的Web控制器类和其他组件才能被自动发现并注册为Spring应用程序上下文里的Bean。本章稍后会写一个简单的Spring MVC控制器，使用@Controller进行注解，这样组件扫描才能找到它。\"]},{\"header\":\"自动配置幕后英雄：SpringFactoriesLoader\",\"slug\":\"自动配置幕后英雄-springfactoriesloader\",\"contents\":[\"借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置功效才得以大功告成！\",\"SpringFactoriesLoader属于Spring框架私有的一种扩展方案，其主要功能就是从指定的配置文件META-INF/spring.factories加载配置。\",\"public abstract class SpringFactoriesLoader { //... public static <T> List<T> loadFactories(Class<T> factoryClass, ClassLoader classLoader) { ... } public static List<String> loadFactoryNames(Class<?> factoryClass, ClassLoader classLoader) { .... } } \"]},{\"header\":\"18、Spring Boot排除某些自动配置？\",\"slug\":\"_18、spring-boot排除某些自动配置\",\"contents\":[\"在某些情况下，一些自动配置可能是我们不需要的，需要排除，最常见的就是DataSourceAutoConfiguration.class\",\"使用 @SpringBootApplication 注解的时候，使用 exclude 属性进行排除指定的类\",\"@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class}) public class Application { // ... } \",\"单独使用 @EnableAutoConfiguration 注解的时候\",\"@... @EnableAutoConfiguration (exclude = {DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class}) public class Application { // ... } \",\"使用 Spring Cloud 和 @SpringCloudApplication 注解的时候\",\"@... @EnableAutoConfiguration (exclude = {DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class}) @SpringCloudApplication public class Application { // ... } \",\"在配置文件中排除\",\"# 写法一 spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration # 写法二 spring.autoconfigure.exclude[0]=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration spring.autoconfigure.exclude[1]=org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration \",\"spring: autoconfigure: exclude: - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration - org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration \"]}]},\"/interview/framework/Spring%20Cloud%20Alibaba.html\":{\"title\":\"Spring Cloud Alibaba 20道面试题\",\"contents\":[{\"header\":\"1、有了Spring Cloud，为什么又出现了Spring Cloud Alibaba？\",\"slug\":\"_1、有了spring-cloud-为什么又出现了spring-cloud-alibaba\",\"contents\":[\"Spring Cloud netflix进入维护模式,将模块置于维护模式，意味着 Spring Cloud 团队将不会再向模块添加新功能。我们将修复 block 级别的 bug 以及安全问题，我们也会考虑并审查社区的小型 pull request。\"]},{\"header\":\"2、什么是Spring Cloud Alibaba?\",\"slug\":\"_2、什么是spring-cloud-alibaba\",\"contents\":[\"Spring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。\"]},{\"header\":\"3、Spring Cloud Alibaba有哪些功能？\",\"slug\":\"_3、spring-cloud-alibaba有哪些功能\",\"contents\":[\"服务限流降级：默认支持 WebServlet、WebFlux, OpenFeign、RestTemplate、Spring Cloud Gateway, Zuul, Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。\",\"服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。\",\"分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。\",\"消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。\",\"分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。\",\"阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。\",\"分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。\",\"阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\"]},{\"header\":\"4、Spring Cloud Alibaba的常用组件有哪些？\",\"slug\":\"_4、spring-cloud-alibaba的常用组件有哪些\",\"contents\":[\"Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。\",\"Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。\",\"RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。\",\"Dubbo：Apache Dubbo™ 是一款高性能 Java RPC 框架。\",\"Seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。\",\"Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。\",\"Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。\",\"Alibaba Cloud SMS: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\"]},{\"header\":\"5、什么是Nacos？\",\"slug\":\"_5、什么是nacos\",\"contents\":[\"前面组件中也有简单介绍；Nacos命名的由来：前四个字母分别是Naming和Configuration的前两个字母，后面的s是Service\",\"Nacos是一个易于使用的动态服务发现，配置和服务管理平台，用于构建云本机应用程序。使用Spring Cloud Alibaba Nacos Discovery，您可以基于Spring Cloud的编程模型快速访问Nacos服务注册功能。\",\"Nacos就是注册中心+配置中心的组合\",\"Nacos = Eureka + Config + Bus\"]},{\"header\":\"6、Nacos有什么作用？\",\"slug\":\"_6、nacos有什么作用\",\"contents\":[\"替代Eureka做服务注册\",\"替代Config做配置中心\",\"服务发现与服务健康检查 Nacos使服务更容易注册，并通过DNS或HTTP接口发现其他服务，Nacos还提供服务的实时健康检查，以防 止向不健康的主机或服务实例发送请求。\",\"动态配置管理 动态配置服务允许您在所有环境中以集中和动态的方式管理所有服务的配置。Nacos消除了在更新配置时重新 部署应用程序，这使配置的更改更加高效和灵活。\",\"动态DNS服务 Nacos提供基于DNS 协议的服务发现能力，旨在支持异构语言的服务发现，支持将注册在Nacos上的服务以 域名的方式暴露端点，让三方应用方便的查阅及发现。\",\"服务和元数据管理 Nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周 期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略。\"]},{\"header\":\"7、目前主流配置中心的区别？\",\"slug\":\"_7、目前主流配置中心的区别\",\"contents\":[\"目前市面上用的比较多的配置中心有：Spring Cloud Config、Apollo、Nacos和Disconf等。 由于Disconf不再维护，下面主要对比一下Spring Cloud Config、Apollo和Nacos。\",\"对比项目\",\"Spring Cloud Config\",\"Apollo\",\"Nacos\",\"配置实时推送\",\"支持(Spring Cloud Bus)\",\"支持(HTTP长轮询1s内)\",\"支持(HTTP长轮询1s内)\",\"版本管理\",\"支持(Git)\",\"支持\",\"支持\",\"配置回滚\",\"支持(Git)\",\"支持\",\"支持\",\"灰度发布\",\"支持\",\"支持\",\"不支持\",\"权限管理\",\"支持(依赖Git)\",\"支持\",\"不支持\",\"多集群\",\"支持\",\"支持\",\"支持\",\"多环境\",\"支持\",\"支持\",\"支持\",\"监听查询\",\"支持\",\"支持\",\"支持\",\"多语言\",\"只支持Java\",\"主流语言，提供了Open API\",\"主流语言，提供了Open API\",\"配置格式校验\",\"不支持\",\"支持\",\"支持\",\"单机读(QPS)\",\"7(限流所致)\",\"9000\",\"15000\",\"单击写(QPS)\",\"5(限流所致)\",\"1100\",\"1800\",\"3节点读(QPS)\",\"21(限流所致)\",\"27000\",\"45000\",\"3节点写(QPS)\",\"5(限流所致)\",\"3300\",\"5600\",\"从配置中心角度来看，性能方面Nacos的读写性能最高，Apollo次之，Spring Cloud Config依赖Git场景不适合开放的大规模自动化运维API。\",\"功能方面Apollo最为完善，nacos具有Apollo大部分配置管理功能，而Spring CloudConfig不带运维管理界面，需要自行开发。Nacos的一大优势是整合了注册中心、配置中心功能，部署和操作相比Apollo都要直观简单，因此它简化了架构复杂度，并减轻运维及部署工作。\",\"综合来看，Nacos的特点和优势还是比较明显的。\"]},{\"header\":\"8、服务注册中心对比？\",\"slug\":\"_8、服务注册中心对比\",\"contents\":[]},{\"header\":\"9、Nacos支持Ap和Cp的切换，该如何选择呢？\",\"slug\":\"_9、nacos支持ap和cp的切换-该如何选择呢\",\"contents\":[\"一般来说，如果不需要存储服务级别的信息且服务实例是通过nacos-client注册，并能够保持心跳上报，那么就可以选择AP模式。当前主流的服务如 Spring cloud 和 Dubbo 服务，都适用于AP模式，AP模式为了服务的可能性而减弱了一致性，因此AP模式下只支持注册临时实例。\",\"如果需要在服务级别编辑或者存储配置信息，那么 CP 是必须，K8S服务和DNS服务则适用于CP模式。\",\"CP模式下则支持注册持久化实例，此时则是以 Raft 协议为集群运行模式，该模式下注册实例之前必须先注册服务，如果服务不存在，则会返回错误。\",\"模式切换代码\",\"curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP' \"]},{\"header\":\"10、Nacos读取配置文件的有哪几种方案？\",\"slug\":\"_10、nacos读取配置文件的有哪几种方案\",\"contents\":[\"Data ID方案：指定spring.profile.active和配置文件的DataID来使不同环境下读取不同的配置\",\"Group方案：通过Group实现环境区分\",\"Namespace方案：通过建立不同NameSpace来区分\"]},{\"header\":\"11、什么是Spring Cloud Sentinel？\",\"slug\":\"_11、什么是spring-cloud-sentinel\",\"contents\":[\"Sentinel 是面向分布式服务架构的流量控制组件，主要以流量为切入点，从流量控制、熔断降级、系统自适应保护等多个维度来帮助您保障微服务的稳定性。 官方地址：https://github.com/alibaba/Sentinel/wiki/介绍https://sentinelguard.io/zh-cn/docs/introduction.html\"]},{\"header\":\"12、Sentinel 基本概念有哪些？\",\"slug\":\"_12、sentinel-基本概念有哪些\",\"contents\":[\"资源 资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。在接下来的文档中，我们都会用资源来描述代码块。 只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。 规则 围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。\"]},{\"header\":\"13、Sentinel有哪些优点？\",\"slug\":\"_13、sentinel有哪些优点\",\"contents\":[\"丰富的适用场景：哨兵在阿里巴巴得到了广泛的应用，几乎覆盖了近10年双11（11.11）购物节的所有核心场景，比如需要限制突发流量的“秒杀”满足系统能力、消息削峰填谷、不依靠业务断路、流量控制等。\",\"实时监控：Sentinel 还提供实时监控能力。可以实时查看单台机器的运行时信息，以及以下 500 个节点的集群运行时信息。\",\"广泛的开源生态：Sentinel 提供与 Spring、Dubbo 和 gRPC 等常用框架和库的开箱即用集成。\",\"多语言支持：Sentinel 为 Java、Go和C++提供了本机支持。\",\"丰富的 SPI 扩展：Sentinel 提供简单易用的 SPI 扩展接口，可以让您快速自定义逻辑，例如自定义规则管理、适配数据源等。\"]},{\"header\":\"14、Sentinel有哪几种流控模式？\",\"slug\":\"_14、sentinel有哪几种流控模式\",\"contents\":[\"直接（默认）：api达到限流条件，直接限流\",\"关联：当关联的资料达到阈值时，就限流自己。当与A关联的资源B达到阈值后，就限流A\",\"链路：链路流控模式指的是，当从某个接口过来的资源达到限流条件时，开启限流；它的功能有点类似于针对 来源配置项，区别在于：针对来源是针对上级微服务，而链路流控是针对上级接口，也就是说它的粒度 更细；\"]},{\"header\":\"15、Sentinel有哪几种流控效果呢？\",\"slug\":\"_15、sentinel有哪几种流控效果呢\",\"contents\":[\"直接（默认的流控处理）:该方式是默认的流量控制方式，当QPS超过任意规则的阈值后，新的请求就会被立即拒绝，拒绝方式为抛出FlowException\",\"预热(Warm Up)：阈值除以coldFactor(默认为3)，经过预热时长后才达到阈值，案例，阀值为10+预热时长设置5秒。系统初始化的阀值为10 / 3 约等于3,即阀值刚开始为3；然后过了5秒后阀值才慢慢升高恢复到10\",\"排队等待：匀速器（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER）方式。这种方式严格控制了请求通过的间隔时间，也即是让请求以均匀的速度通过，对应的是漏桶算法\"]},{\"header\":\"16、Sentinel 有哪些降级规则（熔断策略）？\",\"slug\":\"_16、sentinel-有哪些降级规则-熔断策略\",\"contents\":[\"慢调用比例 (SLOW_REQUEST_RATIO)：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。\",\"异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。\",\"异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。\"]},{\"header\":\"17、 分布式事务存在的问题？\",\"slug\":\"_17、-分布式事务存在的问题\",\"contents\":[\"单体应用被拆分成微服务应用，原来的三个模块被拆分成三个独立的应用，分别使用三个独立的数据源， 业务操作需要调用三个服务来完成。此时每个服务内部的数据一致性由本地事务来保证，但是全局的数据一致性问题没法保证。\"]},{\"header\":\"18、什么是Spring Cloud Seata？\",\"slug\":\"_18、什么是spring-cloud-seata\",\"contents\":[\"Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。\"]},{\"header\":\"19、分布式事务的处理过程是怎样的？\",\"slug\":\"_19、分布式事务的处理过程是怎样的\",\"contents\":[\"分布式事务处理过程的唯一ID+三组件模型：\",\"Transaction ID XID（全局唯一的事务ID）\",\"三组件概念\",\"TC (Transaction Coordinator) - 事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚。\",\"TM (Transaction Manager) - 事务管理器：定义全局事务的范围：开始全局事务、提交或回滚全局事务。\",\"RM (Resource Manager) - 资源管理器：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。\",\"处理过程 \",\"TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID；\",\"XID 在微服务调用链路的上下文中传播；\",\"RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖；\",\"TM 向 TC 发起针对 XID 的全局提交或回滚决议；\",\"TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。\"]},{\"header\":\"20、Seata分布式事务框架实现原理？\",\"slug\":\"_20、seata分布式事务框架实现原理\",\"contents\":[\"Seata有三个组成部分：事务协调器TC：协调者、事务管理器TM：发起方、资源管理器RM：参与方\",\"发起方会向协调者申请一个全局事务id，并保存到ThreadLocal中（为什么要保存到ThreadLocal中？弱引用，线程之间不会发生数据冲突）\",\"Seata数据源代理发起方和参与方的数据源，将前置镜像和后置镜像写入到undo_log表中，方便后期回滚使用\",\"发起方获取全局事务id，通过改写Feign客户端请求头传入全局事务id。\",\"参与方从请求头中获取全局事务id保存到ThreadLocal中，并把该分支注册到SeataServer中。\",\"如果没有出现异常，发起方会通知协调者，协调者通知所有分支，通过全局事务id和本地事务id删除undo_log数据，如果出现异常，通过undo_log逆向生成sql语句并执行，然后删除undo_log语句。如果处理业务逻辑代码超时，也会回滚。\"]}]},\"/interview/framework/Spring%20Cloud.html\":{\"title\":\"Spring Cloud 62道面试题\",\"contents\":[{\"header\":\"1、什么是微服务？\",\"slug\":\"_1、什么是微服务\",\"contents\":[\"The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services , which may be written in different programming languages and use different data storage technologies. \",\"首先微服务并没有一个官方的定义，想要直接描述微服务比较困难，我们可以通过对比传统WEB应用，来理解什么是微服务。\"]},{\"header\":\"1、传统的web项目VS微服务\",\"slug\":\"_1、传统的web项目vs微服务\",\"contents\":[]},{\"header\":\"1、传统的Web项目\",\"slug\":\"_1、传统的web项目\",\"contents\":[\"传统的WEB应用核心分为业务逻辑、适配器以及API或通过UI访问的WEB界面。业务逻辑定义业务流程、业务规则以及领域实体。适配器包括数据库访问组件、消息组件以及访问接口等。\",\"尽管也是遵循模块化开发，但最终它们会打包并部署为单体式应用。例如Java应用程序会被打包成WAR，部署在Tomcat或者Jetty上。\",\"这种单体应用比较适合于小项目，优点是：\",\"开发简单直接，集中式管理\",\"基本不会重复开发\",\"功能都在本地，没有分布式的管理开销和调用开销\",\"它的缺点也十分明显，特别对于互联网公司来说：\",\"开发效率低：所有的开发在一个项目改代码，递交代码相互等待，代码冲突不断\",\"代码维护难：代码功能耦合在一起，新人不知道何从下手\",\"部署不灵活：构建时间长，任何小修改必须重新构建整个项目，这个过程往往很长\",\"稳定性不高：一个微不足道的小问题，可以导致整个应用挂掉\",\"扩展性不够：无法满足高并发情况下的业务需求\"]},{\"header\":\"2、微服务项目\",\"slug\":\"_2、微服务项目\",\"contents\":[\"现在主流的设计一般会采用微服务架构。其思路不是开发一个巨大的单体式应用，而是将应用分解为小的、互相连接的微服务。一个微服务完成某个特定功能，比如乘客管理和下单管理等。每个微服务都有自己的业务逻辑和适配器。一些微服务还会提供API接口给其他微服务和应用客户端使用\",\"微服务架构的优点\",\"解决了复杂性问题\",\"它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务。这些服务定义了明确的RPC或消息驱动的API边界。微服务架构强化了应用模块化的水平，而这通过单体代码库很难实现。因此，微服务开发的速度要快很多，更容易理解和维护。\",\"单独开发每个服务，与其他服务互不干扰\",\"只要符合服务API契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响。\",\"可以独立部署每个微服务\",\"开发人员无需协调对服务升级或更改的部署。这些更改可以在测试通过后立即部署。所以微服务架构也使得CI／CD成为可能。\",\"微服务的缺点\",\"多服务运维难度\",\"系统部署依赖\",\"服务间通信成本\",\"数据一致性\",\"系统集成测试\",\"重复工作\",\"性能监控\"]},{\"header\":\"2、实现微服务要解决的四个问题？\",\"slug\":\"_2、实现微服务要解决的四个问题\",\"contents\":[\"客户端如何访问这些服务？\",\"服务之间如何通信？\",\"这么多服务，怎么找?\",\"服务挂了怎么办？\"]},{\"header\":\"1、客户端如何访问这些服务\",\"slug\":\"_1、客户端如何访问这些服务\",\"contents\":[\"原来的服务都是可以进行单独调用，现在按功能拆分成独立的服务，变成了一个独立的Java进程了。客户端UI如何访问他的？后台有N个服务，前台就需要记住管理N个服务，一个服务下线/更新/升级，前台就要重新部署，这明显不服务我们拆分的理念，特别当前台是移动应用的时候，通常业务变化的节奏更快。另外，N个小服务的调用也是一个不小的网络开销。还有一般微服务在系统内部，通常是无状态的，用户登录信息和权限管理最好有一个统一的地方维护管理（OAuth）。\",\"所以，一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway，他的作用包括\",\"提供统一服务入口，让微服务对前台透明\",\"聚合后台的服务，节省流量，提升性能\",\"提供安全，过滤，流控等API管理功能\",\"我的理解其实这个API Gateway可以有很多广义的实现办法，可以是一个软硬一体的盒子，也可以是一个简单MVC框架，甚至是一个Node.js的服务端。他们最重要的作用是为前台（通常是移动应用）提供后台服务的聚合，提供一个统一的服务出口，解除他们之间的耦合，不过API Gateway也有可能成为单点故障点或者性能的瓶颈。\"]},{\"header\":\"2、 服务之间如何通信？\",\"slug\":\"_2、-服务之间如何通信\",\"contents\":[\"因为所有的微服务都是独立的Java进程跑在独立的虚拟机上，所以服务间的通行就是IPC（inter process communication），已经有很多成熟的方案。现在基本最通用的有两种方式。\",\"同步调用\",\"REST（JAX-RS）\",\"RPC（Dubbo）\",\"异步消息调用(Kafka, Notify, MetaQ)\",\"一般同步调用比较简单，一致性强，但是容易出调用问题，性能体验上也会差些，特别是调用层次多的时候。RESTful和RPC的比较也是一个很有意思的话题。一般REST基于HTTP，更容易实现，更容易被接受，服务端实现技术也更灵活些，各个语言都能支持，同时能跨客户端，对客户端没有特殊的要求，只要封装了HTTP的SDK就能调用，所以相对使用的广一些。RPC也有自己的优点，传输协议更高效，安全更可控，特别在一个公司内部，如果有统一个的开发规范和统一的服务框架时，他的开发效率优势更明显些。就看各自的技术积累实际条件，自己的选择了。\",\"而异步消息的方式在分布式系统中有特别广泛的应用，他既能减低调用服务之间的耦合，又能成为调用之间的缓冲，确保消息积压不会冲垮被调用方，同时能保证调用方的服务体验，继续干自己该干的活，不至于被后台性能拖慢。不过需要付出的代价是一致性的减弱，需要接受数据最终一致性；还有就是后台服务一般要实现幂等性，因为消息发送出于性能的考虑一般会有重复（保证消息的被收到且仅收到一次对性能是很大的考验）；最后就是必须引入一个独立的broker，如果公司内部没有技术积累，对broker分布式管理也是一个很大的挑战。\"]},{\"header\":\"3、这么多服务，怎么找?\",\"slug\":\"_3、这么多服务-怎么找\",\"contents\":[\"在微服务架构中，一般每一个服务都是有多个拷贝，来做负载均衡。一个服务随时可能下线，也可能应对临时访问压力增加新的服务节点。服务之间如何相互感知？服务如何管理？这就是服务发现的问题了。一般有两类做法，也各有优缺点。基本都是通过zookeeper等类似技术做服务注册信息的分布式管理。当服务上线时，服务提供者将自己的服务信息注册到ZK（或类似框架），并通过心跳维持长链接，实时更新链接信息。服务调用者通过ZK寻址，根据可定制算法，找到一个服务，还可以将服务信息缓存在本地以提高性能。当服务下线时，ZK会发通知给服务\",\"客户端。\",\"客户端做：优点是架构简单，扩展灵活，只对服务注册器依赖。缺点是客户端要维护所有调用服务的地址，有\",\"技术难度，一般大公司都有成熟的内部框架支持，比如Dubbo。\",\"服务端做：优点是简单，所有服务对于前台调用方透明，一般在小公司在云服务上部署的应用采用的比较多。\"]},{\"header\":\"4、这么多服务，服务挂了怎么办？\",\"slug\":\"_4、这么多服务-服务挂了怎么办\",\"contents\":[\"前面提到，Monolithic方式开发一个很大的风险是，把所有鸡蛋放在一个篮子里，一荣俱荣，一损俱损。而分布式最大的特性就是网络是不可靠的。通过微服务拆分能降低这个风险，不过如果没有特别的保障，结局肯定是噩梦。我们刚遇到一个线上故障就是一个很不起眼的SQL计数功能，在访问量上升时，导致数据库load彪高，影响了所在应用的性能，从而影响所有调用这个应用服务的前台应用。所以当我们的系统是由一系列的服务调用链组成的时候，我们必须确保任一环节出问题都不至于影响整体链路。相应的手段有很多：\",\"重试机制\",\"限流\",\"熔断机制\",\"负载均衡\",\"降级（本地缓存）\"]},{\"header\":\"3、分布式和微服务有什么区别？\",\"slug\":\"_3、分布式和微服务有什么区别\",\"contents\":[\"分布式，就是将巨大的一个系统划分为多个模块，这一点和微服务是一样的，都是要把系统进行拆分，部署到不同机器上，因为一台机器可能承受不了这么大的访问压力，或者说要支撑这么大的访问压力需要采购一台性能超级好的服务器，其财务成本非常高，有这些预算完全可以采购很多台普通的服务器了，分布式系统各个模块通过接口进行数据交互，其实分布式也是一种微服务，因为都是把模块拆分变为独立的单元，提供接口来调用，那么它们本质的区别是什么？\",\"它们的本质的区别体现在“目标”上， 何为目标，就是你采用分布式架构或者采用微服务架构，你最终是为了什么，要达到什么目的？\",\"分布式架构的目标是什么？就是访问量很大一台机器承受不了，或者是成本问题，不得不使用多台机器来完成服务的部署；\",\"而微服务的目标是什么？只是让各个模块拆分开来，不会被互相影响，比如模块的升级或者出现BUG或者是重构等等都不要影响到其他模块，微服务它是可以在一台机器上部署；\",\"但是：分布式也是微服务的一种，微服务也属于分布式；\"]},{\"header\":\"4、微服务与Spring Cloud的关系或区别？\",\"slug\":\"_4、微服务与spring-cloud的关系或区别\",\"contents\":[\"微服务只是一种项目的架构方式、架构理念，或者说是一种概念，就如同我们的MVC架构一样， 那么Spring Cloud便是对这种架构方式的技术落地实现；\"]},{\"header\":\"5、微服务一定要使用Spring Cloud吗？\",\"slug\":\"_5、微服务一定要使用spring-cloud吗\",\"contents\":[\"微服务只是一种项目的架构方式、架构理念，所以任何技术都可以实现这种架构理念，只是微服务架构里面有很多问题需要我们去解决，比如：负载均衡，服务的注册与发现，服务调用，服务路由，服务熔断等等一系列问题，如果你自己从0开始实现微服务的架构理念，那头发都掉光了，所以Spring Cloud 帮我们做了这些事情，Spring Cloud将处理这些问题的的技术全部打包好了，我们只需要开箱即用；\"]},{\"header\":\"6、什么是Spring Cloud?\",\"slug\":\"_6、什么是spring-cloud\",\"contents\":[\"官方解释：Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。\"]},{\"header\":\"7、Spring Cloud的优缺点有哪些？\",\"slug\":\"_7、spring-cloud的优缺点有哪些\",\"contents\":[\"优点：\",\"服务拆分粒度更细，有利于资源重复利用，有利于提高开发效率\",\"可以更精准的制定优化服务方案，提高系统的可维护性\",\"微服务架构采用去中心化思想，服务之间采用Restful等轻量级通讯，比ESB更轻量\",\"适于互联网时代，产品迭代周期更短\",\"缺点：\",\"微服务过多，治理成本高，不利于维护系统\",\"分布式系统开发的成本高（容错，分布式事务等）对团队挑战大\",\"总的来说优点大过于缺点，目前看来SpringCloud是一套非常完善的分布式框架，目前很多企业开始用微服务、Spring Cloud的优势是显而易见的。因此对于想研究微服务架构的同学来说，学习 Spring Cloud 是一个不错的选择。\"]},{\"header\":\"8、Spring Cloud版本号了解吗？\",\"slug\":\"_8、spring-cloud版本号了解吗\",\"contents\":[\"pring Cloud 采用了英国伦敦地铁站的名称来命名，并由地铁站名称字母A-Z依次类推的形式来发布迭代版本\",\"SpringCloud是一个由许多子项目组成的综合项目，各子项目有不同的发布节奏。为了管理SpringCloud与各子项目的版本依赖关系，发布了一个清单，其中包括了某个SpringCloud版本对应的子项目版本。为了避免SpringCloud版本号与子项目版本号混淆，SpringCloud版本采用了名称而非版本号的命名，这些版本的名字采用了伦敦地铁站的名字，根据字母表的顺序来对应版本时间顺序。例如Angel是第一个版本, Brixton是第二个版本。\",\"当SpringCloud的发布内容积累到临界点或者一个重大BUG被解决后，会发布一个\\\"service releases\\\"版本，简称SRX版本，比如Greenwich.SR2就是SpringCloud发布的Greenwich版本的第2个SRX版本。\"]},{\"header\":\"9、Spring Cloud与Spring Boot依赖关系？\",\"slug\":\"_9、spring-cloud与spring-boot依赖关系\",\"contents\":[\"官方提供版本之间的依赖关系，请参考官方：https://spring.io/projects/spring-cloud#overview\",\"更详细版本对应的查看：https://start.spring.io/actuator/info\"]},{\"header\":\"10、 SpringBoot和SpringCloud的区别？\",\"slug\":\"_10、-springboot和springcloud的区别\",\"contents\":[\"SpringBoot专注于快速方便的开发单个个体微服务；SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务\",\"SpringBoot可以离开SpringCloud独立使用开发项目， 但是SpringCloud离不开SpringBoot ，属于依赖的关系\",\"SpringBoot专注于快速、方便的开发单个微服务个体；SpringCloud关注全局的服务治理框架。\"]},{\"header\":\"11、 Spring Cloud 和Dubbo区别?\",\"slug\":\"_11、-spring-cloud-和dubbo区别\",\"contents\":[\"Spring Cloud\",\"Dubbo\",\"服务调用方式\",\"Rest APi\",\"RPC\",\"注册中心\",\"Eureka、Zookeeper、Nacos、Consul\",\"Zookeeper\",\"服务网关\",\"Zuul、GateWay\",\"第三方整合\"]},{\"header\":\"12、Spring Cloud有哪些组件?\",\"slug\":\"_12、spring-cloud有哪些组件\",\"contents\":[\"服务发现--Netflix Eureka\",\"客户端负载均衡--Netflix Ribbon\",\"断路器--Netflix Hystrix\",\"服务网关--Netflix Zuul\",\"分布式配置--Spring Cloud Config\"]},{\"header\":\"13、Eureka的服务治理是什么？\",\"slug\":\"_13、eureka的服务治理是什么\",\"contents\":[\"Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务治理。\",\"在传统的rpc远程调用框架中，管理每个服务与服务之间依赖关系比较复杂，管理比较复杂，所以需要使用服务治理，管理服务于服务之间依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册。\"]},{\"header\":\"14、Eureka的服务注册是什么？\",\"slug\":\"_14、eureka的服务注册是什么\",\"contents\":[\"Eureka采用了CS的设计架构，Eureka Server 作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用 Eureka的客户端连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。\",\"在服务注册与发现中，有一个注册中心。当服务器启动的时候，会把当前自己服务器的信息 比如 服务地址通讯地址等以别名方式注册到注册中心上。另一方（消费者|服务提供者），以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用RPC远程调用框架核心设计思想：在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。在任何rpc远程框架中，都会有一个注册中心(存放服务地址相关信息(接口地址))\"]},{\"header\":\"15、Eureka如何实现高可用？\",\"slug\":\"_15、eureka如何实现高可用\",\"contents\":[\"搭建集群环境，服务之间相互注册\"]},{\"header\":\"16、Eureka如何获取服务信息？\",\"slug\":\"_16、eureka如何获取服务信息\",\"contents\":[\"通过服务发现的方式获取，自动装配DiscoveryClient，调用里面方法即可\",\"//获取所有服务 List<String> services = discoveryClient.getServices(); //指定服务名获取对应的信息 List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); \"]},{\"header\":\"17、Eureka的自我保护模式是什么？\",\"slug\":\"_17、eureka的自我保护模式是什么\",\"contents\":[\"默认情况下，如果EurekaServer在一定时间内没有接收到某个微服务实例的心跳，EurekaServer将会注销该实例（默认90秒）。但是当网络分区故障发生(延时、卡顿、拥挤)时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题——当EurekaServer节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。\",\"在自我保护模式中，Eureka Server会保护服务注册表中的信息，不再注销任何服务实例。\",\"它的设计哲学就是宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例。一句话讲解：好死不如赖活着.\",\"综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留）也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。\"]},{\"header\":\"18、为什么会产生Eureka的自我保护呢？\",\"slug\":\"_18、为什么会产生eureka的自我保护呢\",\"contents\":[\"为了防止EurekaClient可以正常运行，但是 与 EurekaServer网络不通情况下，EurekaServer不会立刻将EurekaClient服务剔除.\"]},{\"header\":\"19、如何关闭Eureka的自我保护机制？\",\"slug\":\"_19、如何关闭eureka的自我保护机制\",\"contents\":[\"出厂默认，自我保护机制是开启的\",\"关闭可以通过配置文件进行关闭👇\",\"eureka: instance: hostname: eureka7001.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://eureka7002.com:7002/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 server: #关闭自我保护机制，保证不可用服务被及时踢除 enable-self-preservation: false eviction-interval-timer-in-ms: 1000 \"]},{\"header\":\"20、Consul是什么？\",\"slug\":\"_20、consul是什么\",\"contents\":[\"Consul 是一套开源的分布式服务发现和配置管理系统，由 HashiCorp 公司用 Go 语言开发。提供了微服务系统中的服务治理、配置中心、控制总线等功能。这些功能中的每一个都可以根据需要单独使用，也可以一起使用以构建全方位的服务网格，总之Consul提供了一种完整的服务网格解决方案。\",\"它具有很多优点。包括：基于 raft 协议，比较简洁；支持健康检查, 同时支持 HTTP 和 DNS 协议 支持跨数据中心的 WAN 集群 提供图形界面 跨平台，支持 Linux、Mac、Windows\"]},{\"header\":\"21、Consul有哪些特性？\",\"slug\":\"_21、consul有哪些特性\",\"contents\":[\"服务发现：提供HTTP和DNS两种服务发现方式\",\"健康监测：支持多种方式，Http、TCP、Docker、Shell脚本定制化监控\",\"KV存储：key、value的存储方式\",\"多数据中心：支持多数据中心\",\"可视化Web界面\"]},{\"header\":\"22、Eureka、Consul、Zookeeper三者都是注册中心，有什么区别？\",\"slug\":\"_22、eureka、consul、zookeeper三者都是注册中心-有什么区别\",\"contents\":[\"Eureka\",\"Consul\",\"Zookeeper\",\"语言\",\"Java\",\"Go\",\"Java\",\"CAP\",\"AP\",\"CP\",\"CP\",\"服务健康检查\",\"可配置支持\",\"支持\",\"支持\",\"对外暴露接口\",\"HTTP\",\"HTTP/DNS\",\"客户端\",\"Spring Cloud\",\"可集成\",\"可集成\",\"可集成\"]},{\"header\":\"CAP\",\"slug\":\"cap\",\"contents\":[\"C：consistency(强一致性)\",\"A：Availability(可用性)\",\"P：Partition tolerance(分区容错性)\"]},{\"header\":\"23、什么是Spring Cloud Ribbon？\",\"slug\":\"_23、什么是spring-cloud-ribbon\",\"contents\":[\"Spring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具。\",\"简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法和服务调用。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。\"]},{\"header\":\"24、LB负载均衡(Load Balance)是什么\",\"slug\":\"_24、lb负载均衡-load-balance-是什么\",\"contents\":[\"简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA（高可用）。\"]},{\"header\":\"25、Ribbon的本质是什么？\",\"slug\":\"_25、ribbon的本质是什么\",\"contents\":[\"Ribbon就是负载均衡+RestTemplate调用\"]},{\"header\":\"26、Ribbon负载均衡算法，你了解吗？\",\"slug\":\"_26、ribbon负载均衡算法-你了解吗\",\"contents\":[\"负载均衡算法：rest接口第几次请求数 % 服务器集群总数量 = 实际调用服务器位置下标 ，每次服务重启动后rest接口计数从1开始。\",\"List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); 如： List [0] instances = 127.0.0.1:8002 List [1] instances = 127.0.0.1:8001 \",\"8001+ 8002 组合成为集群，它们共计2台机器，集群总数为2， 按照轮询算法原理：\",\"当总请求数为1时：1 % 2 =1 对应下标位置为1 ，则获得服务地址为127.0.0.1:8001\",\"当总请求数位2时：2 % 2 =0 对应下标位置为0 ，则获得服务地址为127.0.0.1:8002\",\"当总请求数位3时：3 % 2 =1 对应下标位置为1 ，则获得服务地址为127.0.0.1:8001\",\"当总请求数位4时：4 % 2 =0 对应下标位置为0 ，则获得服务地址为127.0.0.1:8002\",\"依次类推......\"]},{\"header\":\"27、Ribbon负载均衡策略有哪些？\",\"slug\":\"_27、ribbon负载均衡策略有哪些\",\"contents\":[\"RoundRobinRule（轮询策略）：轮询，按照顺序依次选择\",\"RandomRule（随机策略）：随机，随机选择一个服务\",\"RetryRule（重试策略）：先按照RoundRobinRule的 策略获取服务，如果服务获取失败，则在指定的时间内重试，获取可用的服务\",\"RestAvailableRule（最小连接策略）：先过滤调由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务\",\"AvailabilityFulteringRule（可用性敏感策略）：先过滤调故障实例，再选择并发量最小的实例\",\"WeightedResponseTimeRule（权重策略）：对RoundRobinRule的扩展，响应速度越快的实例选择权重越大，越容易被选择。它的实现原理是，刚开始使用轮询策略并开启一个计时器，每一段时间收集一次所有服务提供者的平均响应时间，然后再给每个服务提供者附上一个权重，权重越高被选中的概率也越大。\",\"ZoneAvoidanceRule（区域敏感策略）：默认规则，复合判断server所在区域的性能和server的可用性选择服务器\"]},{\"header\":\"28、 Ribbon底层实现原理？\",\"slug\":\"_28、-ribbon底层实现原理\",\"contents\":[\"Ribbon使用discoveryClient从注册中心读取目标服务信息，对同一接口请求进行计数，使用%取余算法获取目标服务集群索引，返回获取到的目标服务信息。\"]},{\"header\":\"29、Ribbon本地负载均衡客户端 VS Nginx服务端负载均衡区别？\",\"slug\":\"_29、ribbon本地负载均衡客户端-vs-nginx服务端负载均衡区别\",\"contents\":[\"Nginx是服务器负载均衡，客户端所有请求都会交给nginx，然后由nginx实现转发请求。即负载均衡是由服务端实现的。\",\"Ribbon本地负载均衡，在调用微服务接口时候，会在注册中心上获取注册信息服务列表之后缓存到JVM本地，从而在本地实现RPC远程服务调用技术。\"]},{\"header\":\"28、Spring Cloud Feign是什么？\",\"slug\":\"_28、spring-cloud-feign是什么\",\"contents\":[\"Feign是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单。\",\"它的使用方法是定义一个服务接口然后在上面添加注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡\"]},{\"header\":\"29、Feign与OpenFeign的区别？\",\"slug\":\"_29、feign与openfeign的区别\",\"contents\":[\"Feign\",\"OpenFeign\",\"Feign是Spring Cloud组件中的一个轻量级RESTful的HTTP服务客户端；Feign内置了Ribbon，用来做客户端负载均衡，去调用服务注册中心的服务。Feign的使用方式是：使用Feign的注解定义接口，调用这个接口，就可以调用服务注册中心的服务\",\"OpenFeign是Spring Cloud 在Feign的基础上支持了SpringMVC的注解，如@RequesMapping等等。OpenFeign的@FeignClient可以解析SpringMVC的@RequestMapping注解下的接口，并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务。\",\"<dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-feign</artifactId></dependency>\",\"<dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-openfeign</artifactId></dependency> \"]},{\"header\":\"30、OpenFeign的超时控制你了解？\",\"slug\":\"_30、openfeign的超时控制你了解\",\"contents\":[\"默认Feign客户端只等待一秒钟，但是服务端处理需要超过1秒钟，导致Feign客户端不想等待了，直接返回报错。\",\"为了避免这样的情况，有时候我们需要设置Feign客户端的超时控制。\",\"#设置feign客户端超时时间(OpenFeign默认支持ribbon) ribbon: #指的是建立连接所用的时间，适用于网络状况正常的情况下,两端连接所用的时间 ReadTimeout: 5000 #指的是建立连接后从服务器读取到可用资源所用的时间 ConnectTimeout: 5000 \"]},{\"header\":\"31、什么是Hystrix断路器？\",\"slug\":\"_31、什么是hystrix断路器\",\"contents\":[\"Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。\"]},{\"header\":\"32、Hystrix实现延迟和容错的方法有哪些？\",\"slug\":\"_32、hystrix实现延迟和容错的方法有哪些\",\"contents\":[\"包裹请求：使用HystrixCommand包裹对依赖的调用逻辑，每个命令在独立线程中执行。这使用了设计模式中的“命令模式”。\",\"跳闸机制：当某服务的错误率超过一定的阈值时，Hystrix可以自动或手动跳闸，停止请求该服务一段时间。\",\"资源隔离：Hystrix为每个依赖都维护了一个小型的线程池（或者信号量）。如果该线程池已满，发往该依赖的请求就被立即拒绝，而不是排队等待，从而加速失败判定。\",\"监控：Hystrix可以近乎实时地监控运行指标和配置的变化，例如成功、失败、超时、以及被拒绝的请求等。\",\"回退机制：当请求失败、超时、被拒绝，或当断路器打开时，执行回退逻辑。回退逻辑由开发人员自行提供，例如返回一个缺省值。\",\"自我修复：断路器打开一段时间后，会自动进入“半开”状态。\"]},{\"header\":\"33、雪崩效应，你了解吗？\",\"slug\":\"_33、雪崩效应-你了解吗\",\"contents\":[\"在微服务架构中，一个请求需要调用多个服务是非常常见的。如客户端访问A服务，而A服务需要调用B服务，B服务需要调用C服务，由于网络原因或者自身的原因，如果B服务或者C服务不能及时响应，A服务将处于阻塞状态，直到B服务C服务响应。此时若有大量的请求涌入，容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，造成连锁反应，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的“雪崩”效应。\",\"造成雪崩效应的原因：\",\"单个服务的代码存在bug\",\"请求访问量激增导致服务发生崩溃(如大型商城的枪红包，秒杀功能)\",\"服务器的硬件故障也会导致部分服务不可用\"]},{\"header\":\"34、服务降级，你了解吗？\",\"slug\":\"_34、服务降级-你了解吗\",\"contents\":[\"所谓降级，就是当某个服务熔断之后，服务器将不再被调用，此时客户端可以自己准备一个本地的fallback回调，返回一个缺省值。也可以理解为兜底方法。\",\"会发生降级的情况\",\"程序运行异常\",\"超时\",\"服务熔断触发服务降级\",\"线程池、信号量打满也会导致服务降级\"]},{\"header\":\"35、服务熔断，你了解吗？\",\"slug\":\"_35、服务熔断-你了解吗\",\"contents\":[\"类比保险丝达到最大的服务访问后，直接拒绝访问，拉闸限电，然后调用服务降级的方法并返回友好提示\",\"就相当于保险丝，服务的降级-----》进而熔断-----》恢复调用链路\"]},{\"header\":\"36、服务限流，你了解吗？\",\"slug\":\"_36、服务限流-你了解吗\",\"contents\":[\"限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳固运行，一旦达到的需要限制的阈值，就需要限制流量并采取少量措施以完成限制流量的目的。比方：推迟解决，拒绝解决，或者者部分拒绝解决等等。秒杀高并发等操作，严禁一窝蜂的一样拥挤，排队有序进行，一秒钟N个，有序进行\"]},{\"header\":\"37、Hystrix工作流程？\",\"slug\":\"_37、hystrix工作流程\",\"contents\":[\"序号\",\"操作\",\"1\",\"创建 HystrixCommand（用在依赖的服务返回单个操作结果的时候） 或 HystrixObserableCommand（用在依赖的服务返回多个操作结果的时候） 对象。\",\"2\",\"命令执行。其中 HystrixComand 实现了下面前两种执行方式；而 HystrixObservableCommand 实现了后两种执行方式：execute()：同步执行，从依赖的服务返回一个单一的结果对象， 或是在发生错误的时候抛出异常。queue()：异步执行， 直接返回 一个Future对象， 其中包含了服务执行结束时要返回的单一结果对象。observe()：返回 Observable 对象，它代表了操作的多个结果，它是一个 Hot Obserable（不论 \\\"事件源\\\" 是否有 \\\"订阅者\\\"，都会在创建后对事件进行发布，所以对于 Hot Observable 的每一个 \\\"订阅者\\\" 都有可能是从 \\\"事件源\\\" 的中途开始的，并可能只是看到了整个操作的局部过程）。toObservable()：同样会返回 Observable 对象，也代表了操作的多个结果，但它返回的是一个Cold Observable（没有 \\\"订阅者\\\" 的时候并不会发布事件，而是进行等待，直到有 \\\"订阅者\\\" 之后才发布事件，所以对于 Cold Observable 的订阅者，它可以保证从一开始看到整个操作的全部过程）。\",\"3\",\"若当前命令的请求缓存功能是被启用的， 并且该命令缓存命中， 那么缓存的结果会立即以 Observable 对象的形式 返回。\",\"4\",\"检查断路器是否为打开状态。如果断路器是打开的，那么Hystrix不会执行命令，而是转接到 fallback 处理逻辑（第 8 步）；如果断路器是关闭的，检查是否有可用资源来执行命令（第 5 步）。\",\"5\",\"线程池/请求队列/信号量是否占满。如果命令依赖服务的专有线程池和请求队列，或者信号量（不使用线程池的时候）已经被占满， 那么 Hystrix 也不会执行命令， 而是转接到 fallback 处理逻辑（第8步）。\",\"6\",\"Hystrix 会根据我们编写的方法来决定采取什么样的方式去请求依赖服务。HystrixCommand.run() ：返回一个单一的结果，或者抛出异常。HystrixObservableCommand.construct()：返回一个Observable 对象来发射多个结果，或通过 onError 发送错误通知。\",\"7\",\"Hystrix会将 \\\"成功\\\"、\\\"失败\\\"、\\\"拒绝\\\"、\\\"超时\\\" 等信息报告给断路器， 而断路器会维护一组计数器来统计这些数据。断路器会使用这些统计数据来决定是否要将断路器打开，来对某个依赖服务的请求进行 \\\"熔断/短路\\\"。\",\"8\",\"当命令执行失败的时候， Hystrix 会进入 fallback 尝试回退处理， 我们通常也称该操作为 \\\"服务降级\\\"。而能够引起服务降级处理的情况有下面几种：第4步：当前命令处于\\\"熔断/短路\\\"状态，断路器是打开的时候。第5步：当前命令的线程池、 请求队列或 者信号量被占满的时候。第6步：HystrixObservableCommand.construct() 或 HystrixCommand.run() 抛出异常的时候。\",\"9\",\"当Hystrix命令执行成功之后， 它会将处理结果直接返回或是以Observable 的形式返回。\",\"tips：如果我们没有为命令实现降级逻辑或者在降级处理逻辑中抛出了异常， Hystrix 依然会返回一个 Observable 对象， 但是它不会发射任何结果数据， 而是通过 onError 方法通知命令立即中断请求，并通过onError()方法将引起命令失败的异常发送给调用者。\"]},{\"header\":\"38、 什么是Spring Cloud Zuul？\",\"slug\":\"_38、-什么是spring-cloud-zuul\",\"contents\":[\"Zuul是对SpringCloud提供的成熟对的路由方案，他会根据请求的路径不同，网关会定位到指定的微服务，并代理请求到不同的微服务接口，他对外隐蔽了微服务的真正接口地址。三个重要概念：动态路由表，路由定位，反向代理\",\"动态路由表：Zuul支持Eureka路由，手动配置路由，这俩种都支持自动更新\",\"路由定位：根据请求路径，Zuul有自己的一套定位服务规则以及路由表达式匹配\",\"反向代理：客户端请求到路由网关，网关受理之后，在对目标发送请求，拿到响应之后在 给客户端\"]},{\"header\":\"39、Zuul的应用场景有哪些？\",\"slug\":\"_39、zuul的应用场景有哪些\",\"contents\":[\"对外暴露，权限校验，服务聚合，日志审计\"]},{\"header\":\"40、网关与过滤器有什么区别？\",\"slug\":\"_40、网关与过滤器有什么区别\",\"contents\":[\"网关是对所有服务的请求进行分析过滤，过滤器是对单个服务而言\"]},{\"header\":\"41、Zuul与Nginx有什么区别？\",\"slug\":\"_41、zuul与nginx有什么区别\",\"contents\":[\"Zuul是java语言实现的，主要为java服务提供网关服务，尤其在微服务架构中可以更加灵活的对网关进行操作。\",\"Nginx是使用C语言实现，性能高于Zuul，但是实现自定义操作需要熟悉lua语言，对程序员要求较高，可以使用Nginx做Zuul集群。\"]},{\"header\":\"42、 ZuulFilter常用有那些方法？\",\"slug\":\"_42、-zuulfilter常用有那些方法\",\"contents\":[\"Run()：过滤器的具体业务逻辑\",\"shouldFilter()：判断过滤器是否有效\",\"fifilterOrder()：过滤器执行顺序\",\"fifilterType()：过滤器拦截位置\"]},{\"header\":\"43、什么是Spring Cloud GateWay？\",\"slug\":\"_43、什么是spring-cloud-gateway\",\"contents\":[\"Spring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在Spring Cloud 2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 1.x非Reactor模式的老版本。\",\"而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。\",\"Spring Cloud Gateway的目标提供统一的路由方式且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。\"]},{\"header\":\"44、为什么我们选择GateWay？\",\"slug\":\"_44、为什么我们选择gateway\",\"contents\":[\"netflix不靠谱，迟迟不发布\",\"因为Zuul1.0已经进入了维护阶段，而且Gateway是SpringCloud团队研发的，是亲儿子产品，值得信赖。而且很多功能Zuul都没有用起来也非常的简单便捷。\",\"Gateway是基于异步非阻塞模型上进行开发的，性能方面不需要担心。虽然Netflix早就发布了最新的 Zuul 2.x，但 Spring Cloud 貌似没有整合计划。而且Netflix相关组件都宣布进入维护期；不知前景如何？\",\"多方面综合考虑Gateway是很理想的网关选择。\",\"Spring Cloud GateWay有很多特性\",\"基于Spring Framework 5, Project Reactor 和 Spring Boot 2.0 进行构建；\",\"动态路由：能够匹配任何请求属性；\",\"可以对路由指定 Predicate（断言）和 Filter（过滤器）；\",\"集成Hystrix的断路器功能；\",\"集成 Spring Cloud 服务发现功能；\",\"易于编写的 Predicate（断言）和 Filter（过滤器）；\",\"请求限流功能；\",\"支持路径重写。\"]},{\"header\":\"45、Spring Cloud Gateway 与 Zuul的区别？\",\"slug\":\"_45、spring-cloud-gateway-与-zuul的区别\",\"contents\":[\"Zuul 1.x，是一个基于阻塞 I/ O 的 API Gateway\",\"Zuul 1.x 基于Servlet 2. 5使用阻塞架构它不支持任何长连接(如 WebSocket) Zuul 的设计模式和Nginx较像，每次 I/ O 操作都是从工作线程中选择一个执行，请求线程被阻塞到工作线程完成，但是差别是Nginx 用C++ 实现，Zuul 用 Java 实现，而 JVM 本身会有第一次加载较慢的情况，使得Zuul 的性能相对较差。\",\"Zuul 2.x理念更先进，想基于Netty非阻塞和支持长连接，但SpringCloud目前还没有整合。Zuul 2.x的性能较 Zuul 1.x 有较大提升。在性能方面，根据官方提供的基准测试， Spring Cloud Gateway 的 RPS（每秒请求数）是Zuul 的 1. 6 倍。\",\"Spring Cloud Gateway 建立 在 Spring Framework 5、 Project Reactor 和 Spring Boot 2 之上， 使用非阻塞 API。\",\"Spring Cloud Gateway 还 支持 WebSocket， 并且与Spring紧密集成拥有更好的开发体验\"]},{\"header\":\"46、Spring Cloud GateWay工作流程？\",\"slug\":\"_46、spring-cloud-gateway工作流程\",\"contents\":[\"客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。\",\"Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。\",\"过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。\",\"Filter在“pre”类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等，\",\"在“post”类型的过滤器中可以做响应内容、响应头的修改，日志的输出，流量监控等有着非常重要的作用。\",\"核心逻辑：路由转发+执行过滤链\"]},{\"header\":\"47、Spring Cloud GateWay网关配置的两种方式？\",\"slug\":\"_47、spring-cloud-gateway网关配置的两种方式\",\"contents\":[\"yaml配置\",\" server: port: 9527 spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/selectOne/** # 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 \",\"代码中注入RouteLocator的Bean\",\" @Configuration public class GateWayConfig { @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder routeBuilder){ return routeBuilder.routes().route(\\\"xiaobear-config\\\",r -> r.path(\\\"/spring-cloud-gateway-configuration\\\").uri(\\\"https://www.bilibili.com/video/BV18E411x7eT\\\")).build(); } } \"]},{\"header\":\"48、Spring Cloud GateWay常用的断言方式有哪些？\",\"slug\":\"_48、spring-cloud-gateway常用的断言方式有哪些\",\"contents\":[\"After Route Predicate Factory：表示在这个时区里的时间点才允许访问，时间格式为Java 8新特性的时区时间格式\",\" ZonedDateTime zbj = ZonedDateTime.now(); // 默认时区 ZonedDateTime zny = ZonedDateTime.now(ZoneId.of(\\\"America/New_York\\\")); // 用指定时区获取当前时间 \",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - After=2021-05-24T15:17:53.623+08:00[Asia/Shanghai] # 断言，路径相匹配的进行路由 \",\"Before Route Predicate Factory：在时间点前才允许访问\",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - Before=2021-05-24T15:17:53.623+08:00[Asia/Shanghai] # 断言，路径相匹配的进行路由 \",\"Between Route Predicate Factory：在两者之间\",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - Between=2021-05-24T15:17:53.623+08:00[Asia/Shanghai], 2021-05-24T15:30:53.623+08:00[Asia/Shanghai] # 断言，路径相匹配的进行路由 \",\"Cookie Route Predicate Factory：Cookie Route Predicate需要两个参数，一个是 Cookie name ,一个是正则表达式。\",\"路由规则会通过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由，如果没有匹配上则不执行\",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - Between=2021-05-24T15:17:53.623+08:00[Asia/Shanghai], 2021-05-24T15:30:53.623+08:00[Asia/Shanghai] # 断言，路径相匹配的进行路由 \",\"Header Route Predicate Factory：两个参数：一个是属性名称和一个正则表达式，这个属性值和正则表达式匹配则执行。\",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - Header=X-Request-Id, \\\\d+ # 请求头要有X-Request-Id属性并且值为整数的正则表达式 \",\"Host Route Predicate Factory：Host Route Predicate 接收一组参数，一组匹配的域名列表，这个模板是一个 ant 分隔的模板，用.号作为分隔符。它通过参数中的主机地址作为匹配规则。\",\" spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** # 断言，路径相匹配的进行路由 - Host=**.xiaobear.com # 断言，路径相匹配的进行路由 \",\"其他的请参照官网：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories\",\"总结：Predicate就是为了实现一组匹配规则，让请求找到对应的Route进行处理\"]},{\"header\":\"49、什么是Spring Cloud GateWay Filter过滤？\",\"slug\":\"_49、什么是spring-cloud-gateway-filter过滤\",\"contents\":[\"路由过滤器可用于修改进入的HTTP请求和返回的HTTP响应，路由过滤器只能指定路由进行使用。\",\"Spring Cloud Gateway 内置了多种路由过滤器，他们都由GatewayFilter的工厂类来产生。\"]},{\"header\":\"50、Spring Cloud GateWay Filter过滤器的分类有哪些？\",\"slug\":\"_50、spring-cloud-gateway-filter过滤器的分类有哪些\",\"contents\":[\"按生命周期分类\",\"Pre\",\"Post\",\"按种类分类\",\"GatewayFilter Factories\",\"Global Filters\"]},{\"header\":\"51、如何自定义Spring Cloud GateWay全局过滤器？\",\"slug\":\"_51、如何自定义spring-cloud-gateway全局过滤器\",\"contents\":[\"自定义全局过滤器Global Filters\",\"主要是实现两个接口 implements GlobalFilter,Ordered\",\"具体案例请参考：实例教程\"]},{\"header\":\"52、什么是Spring Cloud Config？\",\"slug\":\"_52、什么是spring-cloud-config\",\"contents\":[\"SpringCloud Config为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置。\",\"SpringCloud Config分为服务端和客户端两部分。\",\"服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口\",\"客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。\"]},{\"header\":\"53、Spring Cloud Config有什么作用？\",\"slug\":\"_53、spring-cloud-config有什么作用\",\"contents\":[\"集中管理配置文件\",\"不同环境不同配置，动态化的配置更新，分环境部署，比如/dev、/test、/prod、/beta、/release\",\"运行期间动态调整配置，不再需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息\",\"当配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置\",\"将配置信息以REST接口的形式暴露\",\"post、curl访问刷新即可\"]},{\"header\":\"54、Spring Cloud Config怎么读取配置文件？\",\"slug\":\"_54、spring-cloud-config怎么读取配置文件\",\"contents\":[\"/{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties \",\"其中“应用程序”作为SpringApplication中的spring.config.name注入（即常规Spring Boot应用程序中通常为“应用程序”），“配置文件”是活动配置文件（或逗号分隔列表）的属性），“label”是可选的git标签（默认为“master”）\",\"label：分支(branch)\",\"name ：服务名\",\"profiles：环境(dev/test/prod)\"]},{\"header\":\"55、Spring Cloud Config修改配置文件，如何动态刷新？\",\"slug\":\"_55、spring-cloud-config修改配置文件-如何动态刷新\",\"contents\":[\"配合Spring Cloud Bus实现配置的动态的刷新\"]},{\"header\":\"56、什么是Spring Cloud Bus消息总线？\",\"slug\":\"_56、什么是spring-cloud-bus消息总线\",\"contents\":[\"Spring Cloud Bus将分布式系统的节点与轻量级消息代理链接。这可以用于广播状态更改（例如配置更改）或其他管理指令。一个关键的想法是，Bus就像一个扩展的Spring Boot应用程序的分布式执行器，但也可以用作应用程序之间的通信渠道。当前唯一的实现是使用AMQP代理作为传输，但是相同的基本功能集（还有一些取决于传输）在其他传输的路线图上。\"]},{\"header\":\"57、Spring Cloud Bus如何动态刷新全局广播？\",\"slug\":\"_57、spring-cloud-bus如何动态刷新全局广播\",\"contents\":[\"利用消息总线触发一个客户端/bus/refresh，而刷新所有客户端配置\",\"利用消息总线触发一个服务端ConfigServer的/bus/refresh端点，而刷新所有客户端的配置\",\"**推荐：**第二个思想更合适一点，第一个不适合原因如下：\",\"打破了微服务的单一职责性，因为微服务本身是业务模块，它本不应该承担配置刷新的职责\",\"破坏了微服务各节点的对等性\",\"有一定的局限性。比如微服务在迁移时，它的网络地址时常发生变化，这时要想做到自动刷新，那就会增加更多的修改\"]},{\"header\":\"58、Spring Cloud Bus如何动态刷新定点通知？\",\"slug\":\"_58、spring-cloud-bus如何动态刷新定点通知\",\"contents\":[\"不想通知所有刷新，而只是想刷新一部分配置，公式如下👇\",\"http://localhost:配置中心的端口号/actuator/bus-refresh/\",\"bus/refresh请求不再发送到具体的服务实例上，而是发给config server并通过destination参数类指定需要更新配置的服务或实例\",\"示例：以刷新运行在3355端口上的config-client为例\",\"curl -X POST \\\"http://localhost:3344/actuator/bus-refresh/config-client:3355\\\"\"]},{\"header\":\"59、Spring Cloud Stream消息驱动是什么？\",\"slug\":\"_59、spring-cloud-stream消息驱动是什么\",\"contents\":[\"屏蔽底层消息中间件的差异,降低切换成本，统一消息的编程模型；轻量级事件驱动微服务框架，可以使用简单的声明式模型来发送及接收消息，主要实现为Apache Kafka及RabbitMQ；\"]},{\"header\":\"60、为什么Spring Cloud Stream可以统一底层差异？\",\"slug\":\"_60、为什么spring-cloud-stream可以统一底层差异\",\"contents\":[\"在没有绑定器这个概念的情况下，我们的SpringBoot应用要直接与消息中间件进行信息交互的时候，由于各消息中间件构建的初衷不同，它们的实现细节上会有较大的差异性通过定义绑定器作为中间层，完美地实现了应用程序与消息中间件细节之间的隔离。通过向应用程序暴露统一的Channel通道，使得应用程序不需要再考虑各种不同的消息中间件实现。通过定义绑定器Binder作为中间层，实现了应用程序与消息中间件细节之间的隔离。\"]},{\"header\":\"61、Spring Cloud Stream如何解决重复消费和持久化？\",\"slug\":\"_61、spring-cloud-stream如何解决重复消费和持久化\",\"contents\":[\"加入分组属性Group，微服务应用放置于同一个Group中，就能够保证消息只会被其中一个应用消费一次。不同的组是可以消费的，同一个组内会发生竞争关系，只有其中一个可以消费。\"]},{\"header\":\"62、什么是Spring Cloud Sleuth？\",\"slug\":\"_62、什么是spring-cloud-sleuth\",\"contents\":[\"Spring Cloud Sleuth提供了一套完整的服务跟踪的解决方案，在分布式系统中提供追踪解决方案并且兼容支持了zipkin。\"]}]},\"/interview/framework/Spring%20MVC.html\":{\"title\":\"Spring MVC 27道面试题\",\"contents\":[{\"header\":\"1、什么是Spring MVC?\",\"slug\":\"_1、什么是spring-mvc\",\"contents\":[\"Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。\"]},{\"header\":\"2、Spring MVC的优点？\",\"slug\":\"_2、spring-mvc的优点\",\"contents\":[\"可以支持各种视图技术，而不仅仅局限于JSP\",\"与Spring 框架继承（IOC、AOP）\",\"清晰的角色分配：前端控制器（DispatcherServlet）,请求到处理器映射（HandleMapping）,处理器适配器（HandleAdapter）,视图解析器（ViewResolver）\",\"支持各种请求资源的映射策略\",\"易于扩展的\"]},{\"header\":\"3、Spring MVC的主要组件\",\"slug\":\"_3、spring-mvc的主要组件\",\"contents\":[\"前端控制器（DispatcherServlet）\",\"作用：接收请求、响应结果，相当于转发器，有了DispatcherServlet就减少了其他组件之间的耦合度\",\"处理器映射器HandlerMapping（不需要程序员开发）\",\"作用：根据请求的URL来查找Handler\",\"处理器适配器HandlerAdapter\",\"在编写Handler的时候要按照HandlerAdapter要求的规则去编写，这样适配器HandlerAdapter才可以正确的去执行Handler。\",\"处理器Handler（需要程序员开发）\",\"视图解析器 ViewResolver（不需要程序员开发）\",\"作用：进行视图的解析，根据视图逻辑名解析成真正的视图（view）\",\"视图View（需要程序员开发jsp）\",\"View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等）\"]},{\"header\":\"4、什么是DispatcherServlet\",\"slug\":\"_4、什么是dispatcherservlet\",\"contents\":[\"Spring的MVC框架是围绕DispatcherServlet来设计的，它用来处理所有的HTTP请求和响应。\"]},{\"header\":\"5、什么是Spring MVC框架的控制器？\",\"slug\":\"_5、什么是spring-mvc框架的控制器\",\"contents\":[\"控制器提供一个访问应用程序的行为，此行为通常通过服务接口实现。控制器解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。\"]},{\"header\":\"6、Spring MVC的控制器是不是单例模式,如果是,有什么问题,怎么解决？\",\"slug\":\"_6、spring-mvc的控制器是不是单例模式-如果是-有什么问题-怎么解决\",\"contents\":[\"是单例模式,所以在多线程访问的时候有线程安全问题,不要用同步,会影响性能的,解决方案是在控制器里面不能写字段。\"]},{\"header\":\"7、Spring MVC的工作原理\",\"slug\":\"_7、spring-mvc的工作原理\",\"contents\":[]},{\"header\":\"8、MVC是什么？MVC设计模式的好处有哪些？\",\"slug\":\"_8、mvc是什么-mvc设计模式的好处有哪些\",\"contents\":[\"mvc是一种设计模式（设计模式就是日常开发中编写代码的一种好的方法和经验的总结）。模型（model）-视图（view）-控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。\",\"好处： \",\"分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。\",\"有利于系统的并行开发，提升开发效率。\"]},{\"header\":\"9、注解的原理是什么\",\"slug\":\"_9、注解的原理是什么\",\"contents\":[\"注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。通过代理对象调用自定义注解的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。\"]},{\"header\":\"10、Spring MVC常用的注解有哪些？\",\"slug\":\"_10、spring-mvc常用的注解有哪些\",\"contents\":[\"@RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上。用于类上，则表示类中的所有响应请求的方法都是以该地址作为父路径。\",\"@RequestBody：注解实现接收http请求的json数据，将json转换为java对象。\",\"@ResponseBody：注解实现将conreoller方法返回对象转化为json对象响应给客户。\",\"@Conntroller：控制器的注解，表示是表现层,不能用用别的注解代替\"]},{\"header\":\"11、SpingMvc中的控制器的注解一般用哪个,有没有别的注解可以替代？\",\"slug\":\"_11、spingmvc中的控制器的注解一般用哪个-有没有别的注解可以替代\",\"contents\":[\"一般用@Controller注解,也可以使用@RestController,@RestController注解相当于@ResponseBody ＋ @Controller,表示是表现层,除此之外，一般不用别的注解代替。\"]},{\"header\":\"12、@RequestMapping注解的作用\",\"slug\":\"_12、-requestmapping注解的作用\",\"contents\":[\"RequestMapping是一个用来处理请求地址映射的注解，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。\",\"RequestMapping注解有六个属性\",\"value， method \",\"value： 指定请求的实际地址，指定的地址可以是URI Template 模式（后面将会说明）\",\"method： 指定请求的method类型， GET、POST、PUT、DELETE等\",\"consumes，produces \",\"consumes： 指定处理请求的提交内容类型（Content-Type），例如application/json,text/html;\",\"produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回；\",\"params，headers \",\"params： 指定request中必须包含某些参数值是，才让该方法处理。\",\"headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求。\"]},{\"header\":\"13、@ResponseBody注解的作用\",\"slug\":\"_13、-responsebody注解的作用\",\"contents\":[\"作用： 该注解用于将Controller的方法返回的对象，通过适当的HttpMessageConverter转换为指定格式后，写入到Response对象的body数据区。\",\"使用时机：返回的数据不是html标签的页面，而是其他某种格式的数据时（如json、xml等）使用；\"]},{\"header\":\"14、@PathVariable和@RequestParam的区别\",\"slug\":\"_14、-pathvariable和-requestparam的区别\",\"contents\":[\"请求路径上有个id的变量值，可以通过@PathVariable来获取 @RequestMapping(value =“/page/{id}”, method = RequestMethod.GET)\",\"@RequestParam用来获得静态的URL请求入参 spring注解时action里用到。\"]},{\"header\":\"15、Spring MVC与Struts2区别\",\"slug\":\"_15、spring-mvc与struts2区别\",\"contents\":[\"相同点：都是基于mvc的表现层框架，都用于web项目的开发。\",\"不同点：\",\"前端控制器不一样。 \",\"Spring MVC的前端控制器是servlet：DispatcherServlet。\",\"struts2的前端控制器是filter：StrutsPreparedAndExcutorFilter。\",\"请求参数的接收方式不一样。 \",\"Spring MVC是使用方法的形参接收请求的参数，基于方法的开发，线程安全，可以设计为单例或者多例的开发，推荐使用单例模式的开发（执行效率更高），默认就是单例开发模式。\",\"struts2是通过类的成员变量接收请求的参数，是基于类的开发，线程不安全，只能设计为多例的开发。\",\"Struts采用值栈存储请求和响应的数据，通过OGNL存取数据；Spring MVC通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。\",\"与spring整合不一样。 \",\"Spring MVC是spring框架的一部分，不需要整合。在企业项目中，SpringMVC使用更多一些。\"]},{\"header\":\"16、Spring MVC怎么样设定重定向和转发的？\",\"slug\":\"_16、spring-mvc怎么样设定重定向和转发的\",\"contents\":[\"重定向：在返回值前面加\\\"forward:\\\"，譬如\\\"forward:user.do?name=method4\\\"\",\"转发：在返回值前面加\\\"redirect:\\\"，譬如\\\"redirect:www.baidu.com\\\"\"]},{\"header\":\"17、Spring MVC怎么和AJAX相互调用的？\",\"slug\":\"_17、spring-mvc怎么和ajax相互调用的\",\"contents\":[\"通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象。具体步骤如下：\",\"加入Jackson.jar\",\"在配置文件中配置json的映射\",\"在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解。\"]},{\"header\":\"18、如何解决POST请求中文乱码问题，GET的又如何处理呢？\",\"slug\":\"_18、如何解决post请求中文乱码问题-get的又如何处理呢\",\"contents\":[\"POST处理中文乱码：在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8；\",\"<filter> <filter-name>CharacterEncodingFilter</filter-name> <filterclass> org.springframework.web.filter.CharacterEncodingFilter</filter-class> <init-param> <param-name>encoding</param-name> <param-value>utf-8</param-value> </init-param> </filter> <filter-mapping> <filter-name>CharacterEncodingFilter</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> \",\"GET请求中文参数出现乱码解决方法:\",\"修改tomcat配置文件添加编码与工程编码一致，如下：\",\"<ConnectorURIEncoding=\\\"utf-8\\\" connectionTimeout=\\\"20000\\\" port=\\\"8080\\\"protocol=\\\"HTTP/1.1\\\" redirectPort=\\\"8443\\\"/> \",\"对参数进行重新编码\",\"String userName = new String(request.getParamter(“userName”).getBytes(“ISO8859-1”),“utf- 8”) \",\"ISO8859-1是tomcat默认编码，需要将tomcat编码后的内容按utf-8编码。\"]},{\"header\":\"19、Spring MVC的异常处理\",\"slug\":\"_19、spring-mvc的异常处理\",\"contents\":[\"可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。\"]},{\"header\":\"20、如果在拦截请求中，我想拦截get方式提交的方法,怎么配置\",\"slug\":\"_20、如果在拦截请求中-我想拦截get方式提交的方法-怎么配置\",\"contents\":[\"可以在@RequestMapping注解里面加上method=RequestMethod.GET。\"]},{\"header\":\"21、怎样在方法里面得到Request,或者Session？\",\"slug\":\"_21、怎样在方法里面得到request-或者session\",\"contents\":[\"直接在方法的形参中声明request,Spring MVC就自动把request对象传入。\"]},{\"header\":\"22、如果想在拦截的方法里面得到从前台传入的参数,怎么得到？\",\"slug\":\"_22、如果想在拦截的方法里面得到从前台传入的参数-怎么得到\",\"contents\":[\"直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。\"]},{\"header\":\"23、如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？\",\"slug\":\"_23、如果前台有很多个参数传入-并且这些参数都是一个对象的-那么怎么样快速得到这个对象\",\"contents\":[\"直接在方法中声明这个对象,Spring MVC就自动会把属性赋值到这个对象里面。\"]},{\"header\":\"24、Spring MVC中函数的返回值是什么？\",\"slug\":\"_24、spring-mvc中函数的返回值是什么\",\"contents\":[\"返回值可以有很多类型,有String, ModelAndView。ModelAndView类把视图和数据都合并的一起的，但一般用String比较好。\"]},{\"header\":\"25、Spring MVC用什么对象从后台向前台传递数据的？\",\"slug\":\"_25、spring-mvc用什么对象从后台向前台传递数据的\",\"contents\":[\"通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。\"]},{\"header\":\"26、怎么样把ModelMap里面的数据放入Session里面？\",\"slug\":\"_26、怎么样把modelmap里面的数据放入session里面\",\"contents\":[\"可以在类上面加上@SessionAttributes注解,里面包含的字符串就是要放入session里面的key。\"]},{\"header\":\"27、Spring MVC里面拦截器是怎么写的\",\"slug\":\"_27、spring-mvc里面拦截器是怎么写的\",\"contents\":[\"有两种写法,一种是实现HandlerInterceptor接口，另外一种是继承适配器类，接着在接口方法当中，实现处理逻辑；然后在Spring MVC的配置文件中配置拦截器即可：\"]}]},\"/interview/framework/Spring.html\":{\"title\":\"Spring 52道面试题\",\"contents\":[{\"header\":\"1、什么是Spring?\",\"slug\":\"_1、什么是spring\",\"contents\":[\"Spring是一个轻量级Java开发框架，最早有Rod Johnson创建，目的是为了解决企业级应用开发的业务逻辑层和其他各层的耦合问题。它是一个分层的JavaSE/JavaEE full-stack（一站式）轻量级开源框架，为开发Java应用程序提供全面的基础架构支持。Spring负责基础架构，因此Java开发者可以专注于应用程序的开发。 Spring最根本的使命是解决企业级应用开发的复杂性，即简化Java开发。 Spring可以做很多事情，它为企业级开发提供给了丰富的功能，但是这些功能的底层都依赖于它的两个核心特性，也就是依赖注入（dependency injection，DI）和面向切面编程（aspectorientedprogramming，AOP）。\",\"为了降低Java开发的复杂性，Spring采取了以下4种关键策略：\",\"基于POJO的轻量级和最小侵入性编程；\",\"通过依赖注入和面向接口实现松耦合；\",\"基于切面和惯例进行声明式编程；\",\"通过切面和模板减少样板式代码。\"]},{\"header\":\"2、Spring的两大核心概念\",\"slug\":\"_2、spring的两大核心概念\",\"contents\":[]},{\"header\":\"1、IOC（控制反转）\",\"slug\":\"_1、ioc-控制反转\",\"contents\":[\"控制反转，也叫依赖注入，他就是不会直接创建对象，只是把对象声明出来，在代码 中不直接与对象和服务进行连接，但是在配置文件中描述了哪一项组件需要哪一项服 务，容器将他们组件起来。在一般的IOC场景中容器创建了所有的对象，并设置了必 要的属性将他们联系在一起，等到需要使用的时候才把他们声明出来，使用注解就跟 方便了，容器会自动根据注解把对象组合起来\"]},{\"header\":\"2、AOP（面向切面编程）\",\"slug\":\"_2、aop-面向切面编程\",\"contents\":[\"面对切面编程，这是一种编程模式，他允许程序员通过自定义的横切点进行模块化，将那些影响多个类的行为封装到可重用的模块中。 例子：比如日志输出，不使用AOP的话就需要把日志的输出语句放在所有类中，方法中，但是有了AOP就可以把日志输出语句封装一个可重用模块，在以声明的方式将他们放在类中，每次使用类就自动完成了日志输出。\"]},{\"header\":\"3、Spring的优缺点是什么？\",\"slug\":\"_3、spring的优缺点是什么\",\"contents\":[]},{\"header\":\"1、优点\",\"slug\":\"_1、优点\",\"contents\":[\"方便解耦，简化开发\",\"Spring就是一个大工厂，可以将所有对象的创建和依赖关系的维护，交给Spring管理。\",\"AOP编程的支持\",\"Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能。\",\"声明式事务的支持\",\"只需要通过配置就可以完成对事务的管理，而无需手动编程。\",\"方便程序的测试\",\"Spring对Junit4支持，可以通过注解方便的测试Spring程序。\",\"方便集成各种优秀框架\",\"Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架的直接支持（如：Struts、Hibernate、MyBatis等）。\",\"降低JavaEE API的使用难度\",\"Spring对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。\"]},{\"header\":\"2、缺点\",\"slug\":\"_2、缺点\",\"contents\":[\"Spring明明一个很轻量级的框架，却给人感觉大而全\",\"Spring依赖反射，反射影响性能\",\"使用门槛升高，入门Spring需要较长时间\"]},{\"header\":\"4、Spring由哪些模块组成？\",\"slug\":\"_4、spring由哪些模块组成\",\"contents\":[\"Spring 总共大约有 20 个模块， 由 1300 多个不同的文件构成。 而这些组件被分别整合在核心容器（Core Container） 、 AOP（Aspect Oriented Programming）和设备支持（Instrmentation） 、数据访问与集成（Data Access/Integeration） 、 Web 、 消息（Messaging） 、 Test 等 6 个模块中。\",\"Spring core：提供了框架的基本组成部分，包括控制反转（Inversion of Control，IOC）和依赖注入（Dependency Injection，DI）功能。\",\"spring beans：提供了BeanFactory，是工厂模式的一个经典实现，Spring将管理对象称为Bean。\",\"spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法。\",\"spring jdbc：提供了一个JDBC的抽象层，消除了烦琐的JDBC编码和数据库厂商特有的错误代码解析， 用于简化JDBC。\",\"spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等。\",\"spring Web：提供了针对 Web 开发的集成特性，例如文件上传，利用 servlet listeners 进行 ioc容器初始化和针对 Web 的 ApplicationContext。\",\"spring test：主要为测试提供支持的，支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试。\"]},{\"header\":\"5、Spring中用到了哪些设计模式？\",\"slug\":\"_5、spring中用到了哪些设计模式\",\"contents\":[\"工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例\",\"单例模式：Bean默认为单例模式\",\"代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术\",\"模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate\",\"观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。\"]},{\"header\":\"6、讲解一下核心容器（spring context应用上下文) 模块\",\"slug\":\"_6、讲解一下核心容器-spring-context应用上下文-模块\",\"contents\":[\"这是基本的Spring模块，提供Spring框架的基础功能，BeanFactory 是 任何以spring为基础的应用的核心。Spring 框架建立在此模块之上，它使Spring成为一个容器。\",\"Bean工厂是工厂模式的一个体现，提供了控制反转的功能，用来把应用的配置和依赖从真正的应用代码分离出来，最常用的就是org.springframework.beans.factory.xml.XmlBeanFactory ，它根据XML文件中的定义加载beans。该容器从XML 文件读取配置元数据并用它去创建一个完全配置的系统或应用。\"]},{\"header\":\"7、Spring框架中有哪些不同类型的事件？\",\"slug\":\"_7、spring框架中有哪些不同类型的事件\",\"contents\":[\"上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。\",\"上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。\",\"上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。\",\"上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。\",\"请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。\"]},{\"header\":\"8、什么是Spring IOC容器？\",\"slug\":\"_8、什么是spring-ioc容器\",\"contents\":[\"控制反转即IOC (Inversion of Control)：它把传统上由程序代码直接操控的对象的调用权交给容器，通过容器来实现对象组件的装配和管理。所谓的“控制反转”概念就是对组件对象控制权的转移，从程序代码本身转移到了外部容器。\",\"Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。\"]},{\"header\":\"9、控制反转（IOC）有什么用？\",\"slug\":\"_9、控制反转-ioc-有什么用\",\"contents\":[\"管理对象的创建和依赖关系的维护。对象的创建并不是一件简单的事，在对象关系比较复杂时，如果依赖关系需要我们来维护的话，那是相当头疼的\",\"解耦，由Spring容器去维护具体的对象\",\"托管了类的产生过程，比如我们需要在类的产生过程中做一些处理，最直接的例子就是代理，如果有容器程序可以把这部分处理交给容器，应用程序则无需去关心类是如何完成代理的\"]},{\"header\":\"10、IOC的优点有哪些？\",\"slug\":\"_10、ioc的优点有哪些\",\"contents\":[\"IOC(依赖注入)把应用的代码量降到最低\",\"它使应用容易测试，单元测试不在需要单例和JNDI查找机制\",\"最小的代价和最小的侵入性使松散耦合得以实现\",\"IOC容器支持加载服务时的饿汉式初始化和懒加载\"]},{\"header\":\"11、Spring IOC的实现机制\",\"slug\":\"_11、spring-ioc的实现机制\",\"contents\":[\"工厂模式+反射机制\",\"interface Fruit { public abstract void eat(); } class Apple implements Fruit { public void eat(){ System.out.println(\\\"Apple\\\"); } } class Orange implements Fruit { public void eat(){ System.out.println(\\\"Orange\\\"); } } class Factory { public static Fruit getInstance(String ClassName) { Fruit f=null; try { f=(Fruit)Class.forName(ClassName).newInstance(); } catch (Exception e) { e.printStackTrace(); } return f; } } class Client { public static void main(String[] a) { Fruit f=Factory.getInstance(\\\"io.github.dunwu.spring.Apple\\\"); if(f!=null){ f.eat(); } } } \"]},{\"header\":\"12、BeanFactory 和 ApplicationContext有什么区别？\",\"slug\":\"_12、beanfactory-和-applicationcontext有什么区别\",\"contents\":[\"BeanFactory和ApplicationContext是Spring的两大核心接口，都可以当做Spring的容器。其中ApplicationContext是BeanFactory的子接口。\",\"不同\",\"BeanFactory\",\"ApplicationContext\",\"依赖关系\",\"是Spring里面最底层的接口，包含了各种Bean的定义，读取bean配置文档，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。\",\"作为BeanFactory的派生，除了提供BeanFactory所具有的功能外，还提供了更完整的框架功能：继承MessageSource，因此支持国际化。统一的资源文件访问方式。提供在监听器中注册bean的事件。同时加载多个配置文件。载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。\",\"加载方式\",\"采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。\",\"它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。\",\"创建方式\",\"以编程的方式创建\",\"以声明的形式创建；如使用ContextLoader\",\"注册方式\",\"手动注册\",\"自动注册\",\"相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。\"]},{\"header\":\"13、Spring 如何设计容器的，BeanFactory和ApplicationContext的关系详解\",\"slug\":\"_13、spring-如何设计容器的-beanfactory和applicationcontext的关系详解\",\"contents\":[\"Spring 作者 Rod Johnson 设计了两个接口用以表示容器。\",\"BeanFactory\",\"BeanFactory 简单粗暴，可以理解为就是个 HashMap，Key 是 BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 “低级容器”。\",\"ApplicationContext\",\"ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。例如资源的获取，支持多种消息（例如 JSP tag 的支持），对BeanFactory 多了工具级别的支持等待。所以你看他的名字，已经不是 BeanFactory 之类的工厂了，而是 “应用上下文”， 代表着整个大容器的所有功能。该接口定义了一个 refresh 方法，此方法是所有阅读 Spring 源码的人的最熟悉的方法，用于刷新整个容器，即重新加载/刷新所有的bean。\",\"最上面的是 BeanFactory，下面的 3 个绿色的，都是功能扩展接口\",\"看下面的隶属 ApplicationContext 粉红色的 “高级容器”，依赖着 “低级容器”，这里说的是依赖，不是继承哦。他依赖着 “低级容器” 的 getBean 功能。而高级容器有更多的功能：支持不同的信息源头，可以访问文件资源，支持应用事件（Observer 模式）\",\"通常用户看到的就是 “高级容器”。 但 BeanFactory 也非常够用啦\",\"左边灰色区域的是 “低级容器”， 只负载加载 Bean，获取 Bean。容器其他的高级功能是没有的。例如上图画的 refresh 刷新 Bean 工厂所有配置，生命周期事件回调等\",\"总结：\",\"IOC在Spring里，只需要低级容器（Spring 低级容器（BeanFactory）的 IOC）就可以实现\",\"加载配置文件，解析成 BeanDefinition 放在 Map 里\",\"调用 getBean 的时候，从 BeanDefinition 所属的 Map 里，拿出 Class 对象进行实例化，同时，如果有依赖关系，将递归调用 getBean 方法 —— 完成依赖注入\"]},{\"header\":\"14、ApplicationContext通常的实现是什么？\",\"slug\":\"_14、applicationcontext通常的实现是什么\",\"contents\":[\"FileSystemXmlApplicationContext ：此容器从一个XML文件中加载beans的定义，XML Bean配置文件的全路径名必须提供给它的构造函数\",\"ApplicationContext context = new FileSystemXmlApplicationContext(“bean.xml”); \",\"ClassPathXmlApplicationContext：从classpath的XML配置文件中读取上下文，并生成上下文定义。应用程序上下文从程序环境变量中\",\"ApplicationContext context = new ClassPathXmlApplicationContext(“bean.xml”); \",\"WebXmlApplicationContext：由Web应用的XML文件读取上下文。\"]},{\"header\":\"15、什么是Spring的依赖注入？\",\"slug\":\"_15、什么是spring的依赖注入\",\"contents\":[\"控制反转IOC是一个很大的概念，可以用不同的方式来实现。其主要实现方式有两种：依赖注入和依赖查找依赖注入；\",\"依赖注入（Dependency Injection），即组件之间的依赖关系由容器在应用系统运行期来决定，也就是由容器动态地将某种依赖关系的目标对象实例注入到应用系统中的各个关联的组件之中。组件不做定位查询，只提供普通的Java方法让容器去决定依赖关系。\"]},{\"header\":\"16、依赖注入的基本原则\",\"slug\":\"_16、依赖注入的基本原则\",\"contents\":[\"应用组件不应该负责查找资源或者其他依赖的协作对象。\",\"配置对象的工作应该由IOC容器负责，“查找资源”的逻辑应该从应用组件的代码中抽取出来，交给IOC容器负责。\",\"容器全权负责组件的装配，它会把符合依赖关系的对象通过属性（JavaBean中的setter）或者是构造器传递给需要的对象。\"]},{\"header\":\"17、依赖注入有什么优势？\",\"slug\":\"_17、依赖注入有什么优势\",\"contents\":[\"依赖注入之所以更流行是因为它是一种更可取的方式：让容器全权负责依赖查询，受管组件只需要暴露JavaBean的setter方法或者带参数的构造器或者接口，使容器可以在初始化时组装对象的依赖关系。其与依赖查找方式相比，主要优势为：\",\"查找定位操作与应用代码完全无关。\",\"不依赖于容器的API，可以很容易地在任何容器以外使用应用对象。\",\"不需要特殊的接口，绝大多数对象可以做到完全不必依赖容器。\"]},{\"header\":\"18、依赖注入有几种实现方式？\",\"slug\":\"_18、依赖注入有几种实现方式\",\"contents\":[\"依赖注入是时下最流行的IOC实现方式，依赖注入分为接口注入（Interface Injection），Setter方法注入（Setter Injection）和构造器注入（Constructor Injection）三种方式。其中接口注入由于在灵活性和易用性比较差，现在从Spring4开始已被废弃。\",\"构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。\",\"Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。\"]},{\"header\":\"19、构造器依赖注入和 Setter方法注入的区别？\",\"slug\":\"_19、构造器依赖注入和-setter方法注入的区别\",\"contents\":[\"构造器注入\",\"Setter方法注入\",\"没有部分注入\",\"有部分注入\",\"不会覆盖 setter 属性\",\"会覆盖 setter 属性\",\"任意修改都会创建一个新实例\",\"任意修改都不会创建一个新实例\",\"适用于设置很多属性\",\"适用于设置少量属性\",\"两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。\"]},{\"header\":\"20、什么是Spring Bean?\",\"slug\":\"_20、什么是spring-bean\",\"contents\":[\"Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义\"]},{\"header\":\"21、如何给Spring 容器提供配置元数据？Spring有几种配置方式\",\"slug\":\"_21、如何给spring-容器提供配置元数据-spring有几种配置方式\",\"contents\":[\"XML配置文件\",\"基于注解的配置\",\"基于Java的配置\"]},{\"header\":\"22、Spring基于xml注入bean的几种方式\",\"slug\":\"_22、spring基于xml注入bean的几种方式\",\"contents\":[\"Setter方法注入\",\"构造器注入\",\"通过index设置参数的位置\",\"通过Type设置参数的类型\",\"静态工厂注入\",\"实例工厂\"]},{\"header\":\"23、Spring支持的几种bean的作用域\",\"slug\":\"_23、spring支持的几种bean的作用域\",\"contents\":[\"作用域\",\"描述\",\"singleton\",\"（默认）将单个 bean 定义范围限定为每个 Spring IoC 容器的单个对象实例。\",\"prototype\",\"一个bean的定义可以有多个实例\",\"request\",\"每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效\",\"session\",\"在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效\",\"global-session\",\"在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效\",\"application\",\"将单个 bean 定义范围限定为ServletContext. 仅在 web-aware Spring 的上下文中有效ApplicationContext\",\"webSocket\",\"将单个 bean 定义范围限定为WebSocket. 仅在 web-aware Spring 的上下文中有效ApplicationContext\",\"注意： 缺省的Spring bean 的作用域是Singleton。使用 prototype 作用域需要慎重的思考，因为频繁创建和销毁 bean 会带来很大的性能开销。\"]},{\"header\":\"24、Spring框架中的单例bean是线程安全的吗？\",\"slug\":\"_24、spring框架中的单例bean是线程安全的吗\",\"contents\":[\"不是。spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理。\",\"实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于new Bean()了，所以就可以保证线程安全了。\",\"有状态就是有数据存储功能。\",\"无状态就是不会保存数据。\"]},{\"header\":\"25、Spring如何处理线程并发问题？\",\"slug\":\"_25、spring如何处理线程并发问题\",\"contents\":[\"在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题\",\"ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。而ThreadLocal采用了“空间换时间”的方式。\",\"ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。\"]},{\"header\":\"26、Spring Bean的生命周期\",\"slug\":\"_26、spring-bean的生命周期\",\"contents\":[\"在传统的Java应用中，bean的生命周期很简单。使用Java关键字new进行bean实例化，然后该bean就可以使用了。一旦该bean不再被使用，则由Java自动进行垃圾回收。相比之下，Spring容器中的bean的生命周期就显得相对复杂多了\",\"Spring对Bean进行实例化\",\"Spring将值和bean的引用注入到Bean对应的属性中\",\"如果bean实现了BeanNameAware接口，Spring将bean的ID传递给setBean-Name()方法；\",\"如果bean实现了BeanFactoryAware接口，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入；\",\"如果bean实现了ApplicationContextAware接口，Spring将调用setApplicationContext()方法，将bean所在的应用上下文的引用传入进来；\",\"如果bean实现了BeanPostProcessor接口，Spring将调用它们的post-ProcessBeforeInitialization()方法；\",\"如果bean实现了InitializingBean接口，Spring将调用它们的after-PropertiesSet()方法。类似地，如果bean使用initmethod声明了初始化方法，该方法也会被调用；\",\"如果bean实现了BeanPostProcessor接口，Spring将调用它们的post-ProcessAfterInitialization()方法；\",\"此时，bean已经准备就绪，可以被应用程序使用了，它们将一直驻留在应用上下文中，直到该应用上下文被销毁；\",\"如果bean实现了DisposableBean接口，Spring将调用它的destroy()接口方法。同样，如果bean使用destroy-method声明了销毁方法，该方法也会被调用。\"]},{\"header\":\"27、什么是Spring的内部bean？什么是Spring inner beans？\",\"slug\":\"_27、什么是spring的内部bean-什么是spring-inner-beans\",\"contents\":[\"在Spring框架中，当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean。内部bean可以用setter注入“属性”和构造方法注入“构造参数”的方式来实现，内部bean通常是匿名的，它们的Scope一般是prototype。\"]},{\"header\":\"28、什么是bean的自动装配？\",\"slug\":\"_28、什么是bean的自动装配\",\"contents\":[\"装配：在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。\",\"自动装配：在Spring框架中，在配置文件中设定bean的依赖关系是一个很好的机制，Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。这意味着 Spring可以通过向Bean Factory中注入的方式自动搞定bean之间的依赖关系。自动装配可以设置在每个bean上，也可以设定在特定的bean上。\"]},{\"header\":\"29、解释不同方式的自动装配，spring 自动装配 bean 有哪些方式？\",\"slug\":\"_29、解释不同方式的自动装配-spring-自动装配-bean-有哪些方式\",\"contents\":[\"在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。\",\"在Spring框架xml配置中共有5种自动装配：\",\"no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。\",\"byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。\",\"byType：通过参数的数据类型进行自动装配。\",\"constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。\",\"autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用byType的方式自动装配。\"]},{\"header\":\"30、使用@Autowired注解自动装配的过程是怎样的？\",\"slug\":\"_30、使用-autowired注解自动装配的过程是怎样的\",\"contents\":[\"使用@Autowired注解来自动装配指定的bean。在使用@Autowired注解之前需要在Spring配置文件进行配置\",\"<context:annotation-config /> \",\"在启动spring IOC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IOC容器自动查找需要的bean，并装配给该对象的属性\",\"在使用@Autowired时，首先在容器中查询对应类型的bean： \",\"如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据；\",\"如果查询的结果不止一个，那么@Autowired会根据名称来查找；\",\"如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。\"]},{\"header\":\"31、自动装配有哪些局限性？\",\"slug\":\"_31、自动装配有哪些局限性\",\"contents\":[\"重写：需要用配置来定义依赖，意味着总要重写自动装配\",\"基本数据类型：不能自动装配简单的属性，如基本数据类型，String字符串和类\",\"模糊特性：自动装配不如显示装配精确，如果有可能，建议使用显示装配\"]},{\"header\":\"32、可以在Spring中注入一个null 和一个空字符串吗？\",\"slug\":\"_32、可以在spring中注入一个null-和一个空字符串吗\",\"contents\":[\"可以\"]},{\"header\":\"33、什么是基于Java的Spring注解配置？\",\"slug\":\"_33、什么是基于java的spring注解配置\",\"contents\":[\"基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。\",\"以@Configuration 注解为例，它用来标记类可以当做一个bean的定义，被Spring IOC容器使用。\",\"另一个例子是@Bean注解，它表示此方法将要返回一个对象，作为一个bean注册进Spring应用上下文\",\"@Configuration public class StudentConfig { @Bean public StudentBean myStudent() { return new StudentBean(); } } \"]},{\"header\":\"34、@Component, @Controller, @Repository, @Service 有何区别？\",\"slug\":\"_34、-component-controller-repository-service-有何区别\",\"contents\":[\"@Component：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。\",\"@Controller：这将一个类标记为 Spring Web MVC 控制器。标有它的 Bean 会自动导入到 IOC 容器中。\",\"@Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component，因为它以更好的方式指定了意图。\",\"@Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IOC 容器，并使未经检查的异常有资格转换为 SpringDataAccessException。\"]},{\"header\":\"35、@Autowired 注解有什么作用？\",\"slug\":\"_35、-autowired-注解有什么作用\",\"contents\":[\"@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。@Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。它的用法和@Required一样，修饰setter方法、构造器、属性或者具有任意名称和/或多个参数的PN方法。\",\"public class Employee { private String name; @Autowired public void setName(String name) { this.name=name; } public string getName(){ return name; } } \"]},{\"header\":\"36、@Autowired和@Resource之间的区别\",\"slug\":\"_36、-autowired和-resource之间的区别\",\"contents\":[\"@Autowired和@Resource可用于：构造函数、成员变量、Setter方法\",\"@Autowired和@Resource之间的区：\",\"@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。\",\"@Resource默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。\"]},{\"header\":\"37、@Qualifier 注解有什么作用\",\"slug\":\"_37、-qualifier-注解有什么作用\",\"contents\":[\"当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义\"]},{\"header\":\"38、Spring支持的事务管理类型， spring 事务实现方式有哪些？\",\"slug\":\"_38、spring支持的事务管理类型-spring-事务实现方式有哪些\",\"contents\":[\"编程式事务管理：通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。\",\"声明式事务管理：可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。\"]},{\"header\":\"39、Spring事务的实现方式和实现原理\",\"slug\":\"_39、spring事务的实现方式和实现原理\",\"contents\":[\"Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。\"]},{\"header\":\"40、Spring的事务传播行为\",\"slug\":\"_40、spring的事务传播行为\",\"contents\":[\"spring事务的传播行为说的是，当多个事务同时存在的时候，spring如何处理这些事务的行为。\",\"PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。\",\"PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。\",\"PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。\",\"PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。\",\"PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。\",\"PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。\",\"PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。\"]},{\"header\":\"41、spring 的事务隔离？\",\"slug\":\"_41、spring-的事务隔离\",\"contents\":[\"spring 有五大隔离级别，默认值为 ISOLATION_DEFAULT（使用数据库的设置），其他四个隔离级别和数据库的隔离级别一致：\",\"ISOLATION_DEFAULT：用底层数据库的设置隔离级别，数据库设置的是什么我就用什么；\",\"ISOLATION_READ_UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）；\",\"ISOLATION_READ_COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读），SQL server 的默认级别；\",\"ISOLATION_REPEATABLE_READ：可重复读，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读），MySQL 的默认级别；\",\"ISOLATION_SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读。\",\"脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。\",\"不可重复读 ：是指在一个事务内，多次读同一数据。\",\"幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。\"]},{\"header\":\"42、Spring框架的事务管理有哪些优点？\",\"slug\":\"_42、spring框架的事务管理有哪些优点\",\"contents\":[\"为不同的事务API 如 JTA，JDBC，Hibernate，JPA 和JDO，提供一个不变的编程模式。\",\"为编程式事务管理提供了一套简单的API而不是一些复杂的事务API\",\"支持声明式事务管理。\",\"和Spring各种数据访问抽象层很好得集成。\"]},{\"header\":\"43、你更倾向用那种事务管理类型？\",\"slug\":\"_43、你更倾向用那种事务管理类型\",\"contents\":[\"大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。\"]},{\"header\":\"44、什么是AOP?\",\"slug\":\"_44、什么是aop\",\"contents\":[\"OOP(Object-Oriented Programming)面向对象编程，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。\",\"AOP(Aspect-Oriented Programming)，一般称为面向切面编程，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理等。\"]},{\"header\":\"45、Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？\",\"slug\":\"_45、spring-aop-and-aspectj-aop-有什么区别-aop-有哪些实现方式\",\"contents\":[\"AOP实现的关键在于代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。\",\"AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，它会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。\",\"Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。\"]},{\"header\":\"46、JDK动态代理和CGLIB动态代理的区别？\",\"slug\":\"_46、jdk动态代理和cglib动态代理的区别\",\"contents\":[\"JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。\",\"如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。\",\"静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。\",\"InvocationHandler 的 invoke(Object proxy,Method method,Object[] args)：proxy是最终生成的代理实例; method 是被代理目标实例的某个具体方法; args 是被代理目标实例某个方法的具体入参, 在方法反射调用时使用。\"]},{\"header\":\"47、Spring AOP里面的几个名词\",\"slug\":\"_47、spring-aop里面的几个名词\",\"contents\":[\"名词\",\"描述\",\"切面（Aspect）\",\"切面是通知和切点的结合。通知和切点共同定义了切面的全部内容。 在Spring AOP中，切面可以使用通用类（基于模式的风格） 或者在普通类中以 @AspectJ 注解来实现。\",\"连接点（Join point）\",\"指方法，在Spring AOP中，一个连接点 总是 代表一个方法的执行。应用可能有数以千计的时机应用通知。这些时机被称为连接点。连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。\",\"通知（Advice）\",\"在AOP术语中，切面的工作被称为通知。\",\"切入点（Pointcut）\",\"切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或是利用正则表达式定义所匹配的类和方法名称来指定这些切点。\",\"引入（Introduction）\",\"引入允许我们向现有类添加新方法或属性。\",\"目标对象（Target Object）\",\"被一个或者多个切面（aspect）所通知（advise）的对象。它通常是一个代理对象。也有人把它叫做 被通知（adviced） 对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个 被代理（proxied） 对象\",\"织入（Weaving）\",\"织入是把切面应用到目标对象并创建新的代理对象的过程。在目标对象的生命周期里有多少个点可以进行织入编译期：切面在目标类编译时被织入。AspectJ的织入编译器是以这种方式织入切面的。类加载期：切面在目标类加载到JVM时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ5的加载时织入就支持以这种方式织入切面。运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面。\"]},{\"header\":\"48、Spring在运行时通知对象\",\"slug\":\"_48、spring在运行时通知对象\",\"contents\":[\"通过在代理类中包裹切面，Spring在运行期把切面织入到Spring管理的bean中。代理封装了目标类，并拦截被通知方法的调用，再把调用转发给真正的目标bean。当代理拦截到方法调用时，在调用目标bean方法之前，会执行切面逻辑。\",\"直到应用需要被代理的bean时，Spring才创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被代理的对象。因为Spring运行时才创建代理对象，所以我们不需要特殊的编译器来织入SpringAOP的切面。\"]},{\"header\":\"49、Spring只支持方法级别的连接点？\",\"slug\":\"_49、spring只支持方法级别的连接点\",\"contents\":[\"因为Spring基于动态代理，所以Spring只支持方法连接点。Spring缺少对字段连接点的支持，而且它不支持构造器连接点。方法之外的连接点拦截功能，我们可以利用Aspect来补充。\"]},{\"header\":\"50、在Spring AOP 中，关注点和横切关注的区别是什么？在 springaop 中 concern 和 cross-cutting concern 的不同之处？\",\"slug\":\"_50、在spring-aop-中-关注点和横切关注的区别是什么-在-springaop-中-concern-和-cross-cutting-concern-的不同之处\",\"contents\":[\"**关注点（concern）**是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 **横切关注点（cross-cutting concern）**是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。\"]},{\"header\":\"51、Spring通知有哪些类型？\",\"slug\":\"_51、spring通知有哪些类型\",\"contents\":[\"前置通知（Before）：在目标方法被调用之前调用通知功能；\",\"后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么；\",\"返回通知（After-returning ）：在目标方法成功执行之后调用通知；\",\"异常通知（After-throwing）：在目标方法抛出异常后调用通知；\",\"环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。\"]},{\"header\":\"52、什么是切面 Aspect？\",\"slug\":\"_52、什么是切面-aspect\",\"contents\":[\"aspect 由 pointcount 和 advice 组成，切面是通知和切点的结合。 它既包含了横切逻辑的定义,也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑编织到切面所指定的连接点中. AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:\",\"如何通过 pointcut 和 advice 定位到特定的 joinpoint 上\",\"如何在 advice 中编写切面代码.\",\"可以简单地认为, 使用 @Aspect 注解的类就是切面.\"]}]},\"/interview/javaBasics/collection.html\":{\"title\":\"Java集合\",\"contents\":[{\"header\":\"1、什么是集合？\",\"slug\":\"_1、什么是集合\",\"contents\":[\"集合就是一个放数据的容器，准确的说是放数据对象引用的容器\",\"集合类存放的都是对象的引用，而不是对象的本身\",\"集合类型主要有3种：set(集）、list(列表）和map(映射)。\",\"集合的特点主要有如下两点：\",\"集合用于存储对象的容器，对象是用来封装数据，对象多了也需要存储集中式管理。\",\"和数组对比对象的大小不确定。因为集合是可变长度的。数组需要提前定义大小\"]},{\"header\":\"2、常用的集合类有哪些？\",\"slug\":\"_2、常用的集合类有哪些\",\"contents\":[\"Collection集合主要有List和Set两大接口：\",\"List：一个有序（元素存入集合的顺序和取出的顺序一致）容器，元素可以重复，可以插入多个null元素，元素都有索引。常用的实现类有 ArrayList、LinkedList 和 Vector。\",\"Set：一个无序（存入和取出顺序有可能不一致）容器，不可以存储重复元素，只允许存入一个null元素，必须保证元素唯一性。Set 接口常用实现类是 HashSet、LinkedHashSet 以及TreeSet。\",\"Map是一个键值对集合，存储键、值和之间的映射。 Key无序，唯一；value 不要求有序，允许重复。Map没有继承于Collection接口，从Map集合中检索元素时，只要给出键对象，就会返回对应的值对象。\",\"Map 的常用实现类：HashMap、TreeMap、HashTable、LinkedHashMap、ConcurrentHashMap\"]},{\"header\":\"3、快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？\",\"slug\":\"_3、快速失败-fail-fast-和安全失败-fail-safe-的区别是什么\",\"contents\":[\"Iterator 的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有的集合类都是快速失败的，而 java.util.concurrent 包下面的所有的类都是安全失败的。快速失败的迭代器会抛出 ConcurrentModificationException 异常，而安全失败的迭代器永远不会抛出这样的异常。\"]},{\"header\":\"4、迭代器Iterator是什么？\",\"slug\":\"_4、迭代器iterator是什么\",\"contents\":[\"Iterator 接口提供遍历任何 Collection 的接口。我们可以从一个 Collection 中使用迭代器方法来获取迭代器实例。迭代器取代了 Java 集合框架中的 Enumeration，迭代器允许调用者在迭代过程中移除元素。\",\"public interface Collection<E> extends Iterable<E> { ...... } \"]},{\"header\":\"5、Iterator怎么使用？有什么特点？\",\"slug\":\"_5、iterator怎么使用-有什么特点\",\"contents\":[\"使用：\",\"List<String> list = new ArrayList<>(); Iterator<String> it = list.iterator(); while(it.hasNext()){ String obj = it.next(); System.out.println(obj); } \",\"Iterator 的特点是只能单向遍历，但是更加安全，因为它可以确保，在当前遍历的集合元素被更改的时候，就会抛出 ConcurrentModifificationException 异常。\"]},{\"header\":\"6、如何边遍历边移除Collection中的元素？\",\"slug\":\"_6、如何边遍历边移除collection中的元素\",\"contents\":[\"边遍历边修改 Collection 的唯一正确方式是使用 Iterator.remove() 方法\",\"Iterator<Integer> it = list.iterator(); while(it.hasNext()){ *// do something* it.remove(); } \",\"常见错误代码：用for循环进行移除\",\"for(Integer i : list){ list.remove(i) } \",\"解析：运行以上错误代码会报 ConcurrentModifificationException异常。这是因为当使用foreach(for(Integer i : list)) 语句时，会自动生成一个iterator 来遍历该 list，但同时该 list 正在被Iterator.remove() 修改。Java 一般不允许一个线程在遍历 Collection 时另一个线程修改它。\"]},{\"header\":\"7、 Iterator和ListIterator有什么区别？\",\"slug\":\"_7、-iterator和listiterator有什么区别\",\"contents\":[\"Iterator 可以遍历 Set 和 List 集合，而 ListIterator 只能遍历 List。\",\"Iterator 只能单向遍历，而 ListIterator 可以双向遍历（向前/后遍历）。\",\"ListIterator 实现 Iterator 接口，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。\"]},{\"header\":\"8、 遍历一个List有哪些不同的方式？每种方法的实现原理是什么？Java中List遍历的最佳实践是什么？\",\"slug\":\"_8、-遍历一个list有哪些不同的方式-每种方法的实现原理是什么-java中list遍历的最佳实践是什么\",\"contents\":[\"遍历方式以及原理：\",\"for循环：基于计数器。在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后停止。\",\"迭代器遍历：Iterator 是面向对象的一个设计模式，目的是屏蔽不同数据集合的特点，统一遍历集合的接口。Java 在 Collections 中支持了 Iterator 模式。\",\"foreach 循环：foreach 内部也是采用了 Iterator 的方式实现，使用时不需要显式声明Iterator 或计数器。优点是代码简洁，不易出错；缺点是只能做简单的遍历，不能在遍历过程中操作数据集合，例如删除、替换。\",\"最佳实践方式：\",\"Java Collections 框架中提供了一个 RandomAccess 接口，用来标记 List 实现是否支持 Random Access。\",\"如果一个数据集合实现了该接口，就意味着它支持 Random Access，按位置读取元素的平均时间复杂度为 O(1)，如ArrayList。\",\"如果没有实现该接口，表示不支持 Random Access，如LinkedList。\",\"推荐的做法：支持 Random Access 的列表可用 for 循环遍历，否则建议用 Iterator 或foreach 遍历。\"]},{\"header\":\"9、 ArrayList的优缺点？\",\"slug\":\"_9、-arraylist的优缺点\",\"contents\":[\"优点：\",\"ArrayList 底层以数组实现，是一种随机访问模式。ArrayList 实现了 RandomAccess 接口，因此查找的时候非常快。\",\"ArrayList 在顺序添加一个元素的时候非常方便。\",\"缺点：\",\"删除元素的时候，需要做一次元素复制操作。如果要复制的元素很多，那么就会比较耗费性能。\",\"插入元素的时候，也需要做一次元素复制操作，缺点同上。\",\"ArrayList 比较适合顺序添加、随机访问的场景。\"]},{\"header\":\"10、 List 的三个子类的特点？\",\"slug\":\"_10、-list-的三个子类的特点\",\"contents\":[\"ArrayList：底层结构是数组，非线程安全，底层查询快，增删慢。\",\"LinkedList：底层结构是链表型的，非线程安全，增删快，查询慢。\",\"vector：底层结构是数组，线程安全的，增删慢，查询慢。\"]},{\"header\":\"11、如何实现数组和List之间的转换？\",\"slug\":\"_11、如何实现数组和list之间的转换\",\"contents\":[\"数组转List：使用Arrays.asList(array)进行转换\",\"List转数组：使用List自带的toArray()方法\",\"// list to array List<String> list = new ArrayList<String>(); list.add(\\\"123\\\"); list.add(\\\"456\\\"); list.toArray(); // array to list String[] array = new String[]{\\\"123\\\",\\\"456\\\"}; Arrays.asList(array); \"]},{\"header\":\"12、Java 中 ArrayList 和 Linkedlist 区别？\",\"slug\":\"_12、java-中-arraylist-和-linkedlist-区别\",\"contents\":[\"ArrayList 和 Vector 使用了数组的实现，可以认为 ArrayList 或者 Vector 封装了对内部数组的操作，比如向数组中添加，删除，插入新的元素或者数据的扩展和重定向。\",\"ArrayList 是基于索引的数据接口，它的底层是数组。它可以以 O(1)时间复杂度对元素进行随机访问。与此对应，LinkedList 是以元素列表的形式存储它的数据，每一个元素都和它的前一个和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是 O(n)。\",\"相对于 ArrayList，LinkedList 的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。LinkedList 比 ArrayList 更占内存，因为 LinkedList 为每一个节点存储了两个引用，一个指向前\",\"一个元素，一个指向下一个元素。\"]},{\"header\":\"13、List a=new ArrayList()和 ArrayList a =new ArrayList()的区别？\",\"slug\":\"_13、list-a-new-arraylist-和-arraylist-a-new-arraylist-的区别\",\"contents\":[\"List list = new ArrayList(); \",\"这句话创建了一个ArrayList的对象，然后上溯到了List，此时list已经是List对象了，有些ArrayList有的属性和方法，而List没有的属性和方法，list就不能再使用了。\",\"ArrayList list=new ArrayList(); \",\"创建一对象则保留了ArrayList 的所有属性。\",\"例如：\",\"List list = new ArrayList(); ArrayList arrayList = new ArrayList(); list.trimToSize(); //错误，没有该方法。 arrayList.trimToSize(); //ArrayList 里有该方法。 \"]},{\"header\":\"14、多线程场景下如何使用ArrayList？\",\"slug\":\"_14、多线程场景下如何使用arraylist\",\"contents\":[\"ArrayList不是线程安全的，如若遇到多线程场景，可以通过 Collections 的 synchronizedList 方法将其转换成线程安全的容器后再使用。\",\"List<String> synchronizedList = Collections.synchronizedList(list); synchronizedList.add(\\\"aaa\\\"); synchronizedList.add(\\\"bbb\\\"); for (int i = 0; i < synchronizedList.size(); i++) { System.out.println(synchronizedList.get(i)); } \"]},{\"header\":\"15、List与Set的区别\",\"slug\":\"_15、list与set的区别\",\"contents\":[\"List ， Set 都是继承自Collection 接口\",\"List 特点：一个有序（元素存入集合的顺序和取出的顺序一致）容器，元素可以重复，可以插入多个null元素，元素都有索引。常用的实现类有 ArrayList、LinkedList 和 Vector。\",\"Set 特点：一个无序（存入和取出顺序有可能不一致）容器，不可以存储重复元素，只允许存入一个null元素，必须保证元素唯一性。Set 接口常用实现类是 HashSet、LinkedHashSet 以及TreeSet。\",\"另外 List 支持for循环，也就是通过下标来遍历，也可以用迭代器，但是set只能用迭代，因为他无序，无法用下标来取得想要的值。\",\"Set和List对比\",\"Set：检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。\",\"List：和数组类似，List可以动态增长，查找元素效率高，插入删除元素效率低，因为会引起其他元素位置改变\"]},{\"header\":\"16、HashSet的实现原理？\",\"slug\":\"_16、hashset的实现原理\",\"contents\":[\"HashSet 是基于 HashMap 实现的，HashSet的值存放于HashMap的key上，HashMap的value统一为present，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层HashMap 的相关方法来完成，HashSet 不允许重复的值。\"]},{\"header\":\"17、Hash如何检查重复？如何保证数据不重复？\",\"slug\":\"_17、hash如何检查重复-如何保证数据不重复\",\"contents\":[\"检查重复：向HashSet 中add ()元素时，判断元素是否存在的依据，不仅要比较hash值，同时还要结合equles 方法比较。\",\"HashSet 中的add ()方法会使用HashMap 的put()方法。\",\"保证数据不重复：HashMap 的 key 是唯一的，由源码可以看出 HashSet 添加进去的值就是作为HashMap 的key，\",\"并且在HashMap中如果K/V相同时，会用新的V覆盖掉旧的V，然后返回旧的V。所以不会重复（HashMap 比较key是否相等是先比较hashcode 再比较equals ）。\",\"HashSet的部分源码：\",\"private static final Object PRESENT = new Object(); private transient HashMap<E,Object> map; public HashSet() { map = new HashMap<>(); } public boolean add(E e) { // 调用HashMap的put方法,PRESENT是一个至始至终都相同的虚值 return map.put(e, PRESENT)==null; } \",\"扩展：\",\"hashCode（）与equals（）的相关规定：\",\"如果两个对象相等，则hashcode一定也是相同的，hashCode是jdk根据对象的地址或者字符串或者数字算出来的int类型的数值\",\"两个对象相等，对象两个equals方法返回true\",\"两个对象有相同的hashcode值，它们也不一定是相等的\",\"综上，equals方法被覆盖过，则hashCode方法也必须被覆盖\",\"hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。\",\"==与equals的区别\",\"==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同\",\"==是指对内存地址进行比较 equals()是对字符串的内容进行比较\"]},{\"header\":\"18、HashSet与HashMap的区别\",\"slug\":\"_18、hashset与hashmap的区别\",\"contents\":[\"HashSet\",\"HashMap\",\"实现Set接口\",\"实现Map接口\",\"仅存储对象\",\"存储键值对\",\"调用add()向Set中添加元素，\",\"调用put()向Map中添加元素\",\"使用成员对象计算hashcode值，对于两个对象来说，hashcode可能相同，所以equals()方法用来判断对象的相等性\",\"HashMap使用键（Key）计算\",\"HashSet较HashMap来说比较慢\",\"HashMap相对于HashSet较快，因为它使用唯一的键获取对象\"]},{\"header\":\"19、什么Hash算法\",\"slug\":\"_19、什么hash算法\",\"contents\":[\"哈希算法是指把任意长度的二进制映射为固定长度的较小的二进制值，这个较小的二进制值叫做哈希值。\"]},{\"header\":\"20、HashMap的实现原理\",\"slug\":\"_20、hashmap的实现原理\",\"contents\":[\"HashMap概述：HashMap是基于哈希表的Map接口的非同步实现。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。\",\"HashMap的数据结构： 在Java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，HashMap也不例外。HashMap实际上是一个“**链表散列”**的数据结构，即数组和链表的结合体。\",\"HashMap 基于 Hash 算法实现的\",\"当我们往HashMap中put元素时，利用key的hashCode重新hash计算出当前对象的元素在数组中的下标\",\"存储时，如果出现hash值相同的key，此时有两种情况。 \",\"如果key相同，则覆盖原始值；\",\"如果key不同（出现冲突），则将当前的key-value放入链表中\",\"获取时，直接找到hash值对应的下标，在进一步判断key是否相同，从而找到对应值。\",\"理解了以上过程就不难明白HashMap是如何解决hash冲突的问题，核心就是使用了数组的存储方式，然后将冲突的key的对象放入链表中，一旦发现冲突就在链表中做进一步的对比。\",\"需要注意Jdk 1.8中对HashMap的实现做了优化，当链表中的节点数据超过八个之后，该链表会转为红黑树来提高查询效率，从原来的O(n)到O(logn)\"]},{\"header\":\"21、HashMap在JDK1.7与JDK1.8中有哪些不同？\",\"slug\":\"_21、hashmap在jdk1-7与jdk1-8中有哪些不同\",\"contents\":[\"不同\",\"JDK1.7\",\"JDK1.8\",\"存储结构\",\"数组+链表\",\"数组+链表+红黑树\",\"初始化方式\",\"单独函数： inflateTable()\",\"直接集成到了扩容函数resize() 中\",\"hash值计算方式\",\"扰动处理 = 9次扰动 = 4次位运算 + 5次异或运算\",\"扰动处理 = 2次扰动 = 1次位运算 + 1次异或运算\",\"存放数据的规则\",\"无冲突时，存放数组；冲突时，存放链表\",\"无冲突时，存放数组；冲突 & 链表长度 <8：存放单链表；冲突 & 链表长度 > 8：树化并存放红黑树\",\"插入数据方式\",\"头插法（先讲原位置的数据移到后1位，再插入数据到该位置）\",\"尾插法（直接插入到链表尾部/红黑树）\",\"扩容后存储位置的计算方式\",\"全部按照原来方法进行计算（即hashCode ->> 扰动函数 ->> (h&length-1)）\",\"按照扩容后的规律计算（即扩容后的位置=原位置 or 原位置 + 旧容量）\"]},{\"header\":\"22、HashMap的put方法的具体流程？\",\"slug\":\"_22、hashmap的put方法的具体流程\",\"contents\":[\"当我们put的时候，首先计算 key 的hash 值，这里调用了 hash 方法， hash 方法实际是让key.hashCode() 与key.hashCode()>>>16 进行异或操作，高16bit补0，一个数和0异或不变，\",\"所以 hash 函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或，目的是减少碰撞。按照函数注释，因为bucket数组大小是2的幂，计算下标index = (table.length - 1) &hash ，如果不做 hash 处理，相当于散列生效的只有几个低 bit 位，\",\"为了减少散列的碰撞，设计者综合考虑了速度、作用、质量之后，使用高16bit和低16bit异或来简单处理减少碰撞，而且JDK8中 用了复杂度 O（logn）的树结构来提升碰撞下的性能。\"]},{\"header\":\"23、能否使用任何类作为 Map 的 key？\",\"slug\":\"_23、能否使用任何类作为-map-的-key\",\"contents\":[\"可以使用任何类作为 Map 的 key，然而在使用之前，需要考虑以下几点：\",\"如果类重写了 equals() 方法，也应该重写 hashCode() 方法。\",\"类的所有实例需要遵循与 equals() 和 hashCode() 相关的规则。\",\"如果一个类没有使用 equals()，不应该在 hashCode() 中使用它。\",\"用户自定义 Key 类最佳实践是使之为不可变的，这样 hashCode() 值可以被缓存起来，拥有更好的性能。不可变的类也可以确保 hashCode() 和 equals() 在未来不会改变，这样就会解决与可变相关的问题了。\"]},{\"header\":\"23、为什么HashMap中String、Integer这样的包装类适合作为Key？\",\"slug\":\"_23、为什么hashmap中string、integer这样的包装类适合作为key\",\"contents\":[\"String、Integer等包装类的特性能够保证hash值的不可更改性和计算准确性，能够有效较少hash的碰撞几率。\",\"原因：\",\"都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况。\",\"内部已重写了equals() 、hashCode() 等方法，遵守了HashMap内部的规范（不清楚可以去上面看看putValue的过程），不容易出现Hash值计算错误的情况；\"]},{\"header\":\"24、如果使用Object作为HashMap的Key，应该怎么办呢？\",\"slug\":\"_24、如果使用object作为hashmap的key-应该怎么办呢\",\"contents\":[\"重写hashCode() 和equals() 方法\",\"重写hashCode() 是因为需要计算存储数据的存储位置，需要注意不要试图从散列码计算中 排除掉一个对象的关键部分来提高性能，这样虽然能更快但可能会导致更多的Hash碰撞；\",\"重写equals() 方法，需要遵守自反性、对称性、传递性、一致性以及对于任何非null的引用 值x，x.equals(null)必须返回false的这几个特性，目的是为了保证key在哈希表中的唯一性；\"]},{\"header\":\"25、HashMap 与 HashTable 有什么区别？\",\"slug\":\"_25、hashmap-与-hashtable-有什么区别\",\"contents\":[\"不同\",\"HashMap\",\"HashTable\",\"线程安全\",\"非线程安全\",\"线程安全，内部通过synchronized修饰\",\"效率\",\"HashMap 要比 HashTable 效率高一点\",\"几乎不使用，线程安全使用ConcurrentHashMap\",\"对Null key 和Null value的支持\",\"null作为key，有且只有一个，但可以对应一个或多个null的value\",\"不支持，只要有，就会抛出NullPointerException\",\"初始容量大小和每次扩充容量大小的不同**：创建不指定容量**\",\"初始为16，每次扩充变为原来的2倍\",\"初始为11，扩充每次变为原来的2n+1\",\"初始容量大小和每次扩充容量大小的不同**：创建指定容量**\",\"扩充为2的幂次方大小\",\"指定容量大小\",\"底层数据结构\",\"哈希表结构\",\"哈希表结构\"]},{\"header\":\"26、如何决定使用 HashMap 还是 TreeMap？\",\"slug\":\"_26、如何决定使用-hashmap-还是-treemap\",\"contents\":[\"对于在Map中插入、删除和定位元素这类操作，HashMap是最好的选择。然而，假如你需要对一个有序的key集合进行遍历，TreeMap是更好的选择。基于你的collection的大小，也许向HashMap中添加元素会更快，将map换为TreeMap进行有序key的遍历。\"]},{\"header\":\"27、HashMap 和 ConcurrentHashMap 的区别？\",\"slug\":\"_27、hashmap-和-concurrenthashmap-的区别\",\"contents\":[\"ConcurrentHashMap对整个桶数组进行了分割分段(Segment)，然后在每一个分段上都用lock锁进行保护，相对于HashTable的synchronized锁的粒度更精细了一些，并发性能更好，而HashMap没有锁机制，不是线程安全的。（JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。）\",\"HashMap的键值对允许有null，但是ConCurrentHashMap都不允许。\"]},{\"header\":\"28、Collection 和 Collections 有什么区别？\",\"slug\":\"_28、collection-和-collections-有什么区别\",\"contents\":[\"java.util.Collection 是一个集合接口（集合类的一个顶级接口）。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式，其直接继承接口有List与Set。\",\"Collections则是集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。\"]}]},\"/interview/javaBasics/exception.html\":{\"title\":\"Java异常\",\"contents\":[{\"header\":\"1、Java 中异常分为哪些种类？\",\"slug\":\"_1、java-中异常分为哪些种类\",\"contents\":[\"按照异常需要处理的时机分为编译时异常（也叫强制性异常）也叫 CheckedException 和运行时异常（也叫非强制性异常）也叫 RuntimeException。\",\"只有 java 语言提供了 Checked 异常，Java 认为 Checked异常都是可以被处理的异常，所以 Java 程序必须显式处理 Checked 异常。如果程序没有处理 Checked 异常，该程序在编译时就会发生错误无法编译。这体现了 Java 的设计哲学：没有完善错误处理的代码根本没有机会被执行。\",\"对 Checked 异常处理方法有两种：\",\"当前方法知道如何处理该异常，则用 try...catch 块来处理该异常。\",\"当前方法不知道如何处理，则在定义该方法是声明抛出该异常。\",\"运行时异常只有当代码在运行时才发行的异常，编译时不需要 try catch。Runtime 如除数是 0 和数组下标越界等，其产生频繁，处理麻烦，若显示申明或者捕获将会对程序的可读性和运行效率影响很大。所以由系统自动检测并将它们交给缺省的异常处理程序。当然如果你有处理要求也可以显示捕获它们。\"]},{\"header\":\"2、调用下面的方法，得到的返回值是什么？\",\"slug\":\"_2、调用下面的方法-得到的返回值是什么\",\"contents\":[\"public int getNum(){ try{ int i = 10 / 0; return 1; }catch(Exception e){ return 2; }finally{ return 3; } } \",\"分析：\",\"代码走到第3行的时候，遇到了一个 MathException，因此第4行不会执行了，代码跳到catch里面\",\"代码走到第6行的时候，异常机制有这么一个原则：如果在 catch 中遇到了 return 或者异常等能使该函数终止的话，那么有 finally 就必须先执行完 finally 代码块里面的代码，然后再返回值。因此跳到第8行。\",\"第8行是一个return语句，这个时候就结束了，第6行的值无法被返回。返回值为3.\",\"若第8行不是一个return语句，而是一个释放资源的操作，则返回值为2.\"]},{\"header\":\"3、Error 和 Exception 区别是什么？\",\"slug\":\"_3、error-和-exception-区别是什么\",\"contents\":[\"Error 类型的错误通常为虚拟机相关错误，如系统崩溃，内存不足，堆栈溢出等，编译器不会对这类错误进行检测，JAVA 应用程序也不应对这类错误进行捕获，一旦这类错误发生，通常应用程序会被终止，仅靠应用程序本身无法恢复；\",\"Exception 类的错误是可以在应用程序中进行捕获并处理的，通常遇到这种错误，应对其进行处理，使应用程序可以继续正常运行。\"]},{\"header\":\"4、运行时异常和一般异常(受检异常)区别是什么？\",\"slug\":\"_4、运行时异常和一般异常-受检异常-区别是什么\",\"contents\":[\"运行时异常包括 RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。 Java 编译器不会检查运行时异常。\",\"受检异常是Exception 中除 RuntimeException 及其子类之外的异常。 Java 编译器会检查受检异常。\",\"RuntimeException异常和受检异常之间的区别：是否强制要求调用者必须处理此异常，如果强制要求调用者必须进行处理，那么就使用受检异常，否则就选择非受检异常(RuntimeException)。\",\"一般来讲，如果没有特殊的要求，我们建议使用RuntimeException异常。\"]},{\"header\":\"5、throw 和 throws 的区别是什么？\",\"slug\":\"_5、throw-和-throws-的区别是什么\",\"contents\":[\"throw:\",\"throw 语句用在方法体内，表示抛出异常，由方法体内的语句处理。\",\"throw 是具体向外抛出异常的动作，所以它抛出的是一个异常实例，执行 throw 一定是抛出了某种异常。\",\"throws:\",\"throws 语句是用在方法声明后面，表示如果抛出异常，由该方法的调用者来进行异常的处理。\",\"throws 主要是声明这个方法会抛出某种类型的异常，让它的使用者要知道需要捕获的异常的类型。\",\"throws 表示出现异常的一种可能性，并不一定会发生这种异常。\"]},{\"header\":\"6、final、finally、finalize 的区别？\",\"slug\":\"_6、final、finally、finalize-的区别\",\"contents\":[\"final：可用于修饰属性、方法、类。修饰的属性不可变（不能重新被赋值），方法不能重写，类不能继承。\",\"finally：异常处理语句try-catch的一部分，一般将一定要执行的代码放在finally代码块中，总是被执行，一般用来存放一些关闭资源的操作。\",\"finalize：Object 类的一个方法，在垃圾回收器执行的时候会调用被回收对象的此方法，可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等。该方法更像是一个对象生命周期的临终方法，当该方法被系统调用则代表该对象即将“死亡”，但是需要注意的是，我们主动行为上去调用该方法并不会导致该对象“死亡”，这是一个被动的方法（其实就是回调方法），不需要我们调用。\"]},{\"header\":\"7、常见的 RuntimeException 有哪些？\",\"slug\":\"_7、常见的-runtimeexception-有哪些\",\"contents\":[\"ClassCastException：数据类型转换异常\",\"IndexOutOfBoundsException：数组下标越界异常，常见于操作数组对象时发生。\",\"NullPointerException：空指针异常；出现原因：调用了未经初始化的对象或者是不存在的对象。\",\"ClassNotFoundException：指定的类找不到；出现原因：类的名称和路径加载错误；通常都是程序试图通过字符串来加载某个类时可能引发异常。\",\"NumberFormatException：字符串转换为数字异常；出现原因：字符型数据中包含非数字型字符。\",\"IllegalArgumentException：方法传递参数异常\",\"NoClassDefFoundException：未找到定义类异常\",\"SQLException SQL：常见于操作数据库时的 SQL 语句错误。\",\"InstantiationException：实例化异常。\",\"NoSuchMethodException：方法不存在异常\",\"ArrayStoreException：数据存储异常，操作数组时类型不一致\",\"还有IO操作的BufferOverflowException异常\"]},{\"header\":\"8、finally内存回收的情况？\",\"slug\":\"_8、finally内存回收的情况\",\"contents\":[\"如果在try... catch 部分用Connection 对象连接了数据库，而且在后继部台不会再用到这个连接对象，那么一定要在 finally从句中关闭该连接对象， 否则该连接对象所占用的内存资源无法被回收。\",\"如果在try... catch 部分用到了一些IO对象进行了读写操作，那么也一定要在finally 中关闭这些IO对象，否则，IO对象所占用的内存资源无法被回收。\",\"如果在try .catch 部分用到了ArrayList 、Linkedlist 、Hash Map 等集合对象，而且这些对象之后不会再被用到，那么在finally中建议通过调用clear方法来清空这些集合。\",\"例如，在try .catch 语句中育一个对象obj 指向7一块比较大的内存空间（假设100MB) ，而且之后不会再被用到，那么在 finally 从句中建议写上 obj=null，这样能提升内存使用效率。\"]},{\"header\":\"9、异常的设计原则有哪些？\",\"slug\":\"_9、异常的设计原则有哪些\",\"contents\":[\"不要将异常处理用于正常的控制流\",\"对可以恢复的情况使用受检异常，对编程错误使用运行时异常\",\"避免不必要的使用受检异常\",\"优先使用标准的异常\",\"每个方法抛出的异常都要有文档\",\"保持异常的原子性\",\"不要在 catch 中忽略掉捕获到的异常\"]}]},\"/interview/javaBasics/innerClass.html\":{\"title\":\"Java内部类\",\"contents\":[{\"header\":\"1、静态内部类与非静态内部类有什么区别？\",\"slug\":\"_1、静态内部类与非静态内部类有什么区别\",\"contents\":[\"静态内部类可以有静态成员(方法，属性)，而非静态内部类则不能有静态成员(方法，属性)。\",\"静态内部类只能够访问外部类的静态成员和静态方法,而非静态内部类则可以访问外部类的所有成员(方法，属性)。\",\"实例化静态内部类与非静态内部类的方式不同\",\"调用内部静态类的方法或静态变量,可以通过类名直接调用\"]},{\"header\":\"2、静态内部类如何定义？\",\"slug\":\"_2、静态内部类如何定义\",\"contents\":[\"定义在类内部的静态类，就是静态内部类。\",\"public class Out{ public static int a; private int b; public static class Inner{ public void print(){ System.out.println(a); } } } \",\"静态内部类可以访问外部类所有的静态变量和方法，即使是 private 的也一样。\",\"静态内部类和一般类一致，可以定义静态变量、方法，构造方法等。\",\"其它类使用静态内部类需要使用“外部类.静态内部类”方式，如下所示：Out.Inner inner = new Out.Inner();inner.print();\",\"Java集合类HashMap内部就有一个静态内部类Entry。Entry是HashMap存放元素的抽象，HashMap 内部维护 Entry 数组用了存放元 素，但是 Entry 对使用者是透明的。像这种和外部类关系密切的，且不依赖外部类实例的，都可以使用静态内部类。\"]},{\"header\":\"3、什么是成员内部类？\",\"slug\":\"_3、什么是成员内部类\",\"contents\":[\"定义在类内部的非静态类，就是成员内部类。成员内部类不能定义静态方法和变量（final修饰的除外）。这是因为成员内部类是非静态的，类初始化的时候先初始化静态成员，如果允许成员内部类定义静态变量，那么成员内部类的静态变量初始化顺序是有歧义的。\",\"public class Out{ public static int a; private int b; public class Inner{ public void print(){ System.out.println(a); } } } \"]},{\"header\":\"4、Anonymous Inner Class(匿名内部类)是否可以继承其它类？是否可以实现接口？\",\"slug\":\"_4、anonymous-inner-class-匿名内部类-是否可以继承其它类-是否可以实现接口\",\"contents\":[\"可以继承其他类或实现其他接口，在 Swing 编程和 Android 开发中常用此方式来实现事件监听和回调。\"]},{\"header\":\"5、内部类可以引用它的包含类（外部类）的成员吗？有没有什么限制？\",\"slug\":\"_5、内部类可以引用它的包含类-外部类-的成员吗-有没有什么限制\",\"contents\":[\"一个内部类对象可以访问创建它的外部类对象的成员，包括私有成员。\"]},{\"header\":\"6、是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用？\",\"slug\":\"_6、是否可以从一个静态-static-方法内部发出对非静态-non-static-方法的调用\",\"contents\":[\"不可以， 静态方法只能访问静态成员，因为非静态方法的调用要先创建对象， 在调用静态方法时可能对象并没有被初始化。\"]}]},\"/interview/javaBasics/io.html\":{\"title\":\"Java IO\",\"contents\":[{\"header\":\"1、什么是比特(Bit),什么是字节(Byte),什么是字符(Char),它们长度是多少,各有什么区别？\",\"slug\":\"_1、什么是比特-bit-什么是字节-byte-什么是字符-char-它们长度是多少-各有什么区别\",\"contents\":[\"Bit最小的二进制单位 ，是计算机的操作部分取值0或者1。\",\"Byte是计算机中存储数据的单元，是一个8位的二进制数，（计算机内部，一个字节可表示一个英文字母，两个字节可表示一个汉字。） 取值（-128-127）\",\"Char是用户的可读写的最小单位，他只是抽象意义上的一个符号。如‘5’，‘中’，‘￥’等等。在java里面由16位bit组成Char 取值（0-65535）\",\"Bit 是最小单位 计算机他只能认识0或者1\",\"Byte是8个字节 是给计算机看的\",\"字符 是看到的东西 一个字符=二个字节\"]},{\"header\":\"2、什么是IO？\",\"slug\":\"_2、什么是io\",\"contents\":[\"Java IO：是以流为基础进行数据的输入输出的，所有数据被串行化(所谓串行化就是数据要按顺序进行输入输出)写入输出流。简单来说就是java通过io流方式和外部设备进行交互。 在Java类库中，IO部分的内容是很庞大的，因为它涉及的领域很广泛：标准输入输出，文件的操作，网络上的数据传输流，字符串流，对象流等等等。\",\"比如程序从服务器上下载图片，就是通过流的方式从网络上以流的方式到程序中，在到硬盘中\"]},{\"header\":\"3、在了解不同的IO之前先了解：同步与异步，阻塞与非阻塞的区别？\",\"slug\":\"_3、在了解不同的io之前先了解-同步与异步-阻塞与非阻塞的区别\",\"contents\":[\"同步：一个任务的完成之前不能做其他操作，必须等待（等于在打电话） 异步：一个任务的完成之前，可以进行其他操作（等于在聊QQ） 阻塞：是相对于CPU来说的， 挂起当前线程，不能做其他操作只能等待 非阻塞：无须挂起当前线程，可以去执行其他操作\"]},{\"header\":\"4、什么是BIO?\",\"slug\":\"_4、什么是bio\",\"contents\":[\"BIO：同步并阻塞，服务器实现一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，没处理完之前此线程不能做其他操作（如果是单线程的情况下，我传输的文件很大呢？），当然可以通过线程池机制改善。\",\"BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。\"]},{\"header\":\"5、什么是NIO？\",\"slug\":\"_5、什么是nio\",\"contents\":[\"NIO：同步非阻塞，服务器实现一个连接一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。\",\"NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4之后开始支持。\"]},{\"header\":\"6、什么是AIO？\",\"slug\":\"_6、什么是aio\",\"contents\":[\"AIO：异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由操作系统先完成了再通知服务器应用去启动线程进行处理，AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用操作系统参与并发操作，编程比较复杂，JDK1.7之后开始支持。\",\"AIO属于NIO包中的类实现，其实IO主要分为BIO和NIO，AIO只是附加品，解决IO不能异步的实现在以前很少有Linux系统支持AIO，Windows的IOCP就是该AIO模型。但是现在的服务器一般都是支持AIO操作\"]},{\"header\":\"7、什么是Netty?\",\"slug\":\"_7、什么是netty\",\"contents\":[\"Netty是由JBOSS提供的一个Java开源框架。Netty提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。 Netty 是一个基于NIO的客户、服务器端编程框架，使用Netty 可以确保你快速和简单的开发出一个网络应用，例如实现了某种协议的客户，服务端应用。Netty相当简化和流线化了网络应用的编程开发过程，例如，TCP和UDP的socket服务开发。\",\"Netty是由NIO演进而来，使用过NIO编程的用户就知道NIO编程非常繁重，Netty是能够能跟好的使用NIO\"]},{\"header\":\"8、BIO和NIO、AIO的区别？\",\"slug\":\"_8、bio和nio、aio的区别\",\"contents\":[\"BIO是阻塞的，NIO是非阻塞的.\",\"BIO是面向流的，只能单向读写，NIO是面向缓冲的, 可以双向读写\",\"使用BIO做Socket连接时，由于单向读写，当没有数据时，会挂起当前线程，阻塞等待，为防止影响其它连接,，需要为每个连接新建线程处理.，然而系统资源是有限的,，不能过多的新建线程，线程过多带来线程上下文的切换，从来带来更大的性能损耗，因此需要使用NIO进行BIO多路复用，使用一个线程来监听所有Socket连接，使用本线程或者其他线程处理连接\",\"AIO是非阻塞 以异步方式发起 I/O 操作。当 I/O 操作进行时可以去做其他操作，由操作系统内核空间提醒IO操作已完成（不懂的可以往下看）\"]},{\"header\":\"9、IO流的分类?\",\"slug\":\"_9、io流的分类\",\"contents\":[\"按流的方向：\",\"输入流（InputStream）：从文件读入到内存。只能进行读操作。\",\"输出流（OuputStream）：从内存读出到文件。只能进行写操作。\",\"注：输出流可以帮助我们创建文件，而输入流不会。\",\"按处理数据单位：\",\"字节流：以字节为单位，每次次读入或读出是8位数据。可以读任何类型数据，图片、文件、音乐视频等。 (Java代码接收数据只能为byte数组)\",\"字符流：以字符为单位，每次读入或读出是16位数据。其只能读取字符类型数据。 (Java代码接收数据为一般为char数组，也可以是别的)\",\"按角色：\",\"节点流：直接与数据源相连，读入或读出。\",\"处理流：也叫包装流，是对一个对于已存在的流的连接进行封装，通过所封装的流的功能调用实现数据读写。如添加个Buffering缓冲区。（意思就是有个缓存区，等于软件和mysql中的redis）\",\"注意：为什么要有处理流？主要作用是在读入或写出时，对数据进行缓存，以减少I/O的次数，\"]},{\"header\":\"10、5种IO模型\",\"slug\":\"_10、5种io模型\",\"contents\":[]},{\"header\":\"1、阻塞BIO(Blocking IO)\",\"slug\":\"_1、阻塞bio-blocking-io\",\"contents\":[\"例：A拿着一支鱼竿在河边钓鱼，并且一直在鱼竿前等，在等的时候不做其他的事情，十分专心。只有鱼上钩的时，才结束掉等的动作，把鱼钓上来。 在内核将数据准备好之前，系统调用会一直等待所有的套接字，默认的是阻塞方式。\"]},{\"header\":\"2、非阻塞NIO(NoBlocking IO)\",\"slug\":\"_2、非阻塞nio-noblocking-io\",\"contents\":[\"B也在河边钓鱼，但是B不想将自己的所有时间都花费在钓鱼上，在等鱼上钩这个时间段中，B也在做其他的事情（一会看看书，一会读读报纸，一会又去看其他人的钓鱼等），但B在做这些事情的时候，每隔一个固定的时间检查鱼是否上钩。一旦检查到有鱼上钩，就停下手中的事情，把鱼钓上来。 B在检查鱼竿是否有鱼，是一个轮询的过程。\"]},{\"header\":\"3、异步AIO(asynchronous IO)\",\"slug\":\"_3、异步aio-asynchronous-io\",\"contents\":[\"C也想钓鱼，但C有事情，于是他雇来了D、E、F，让他们帮他等待鱼上钩，一旦有鱼上钩，就打电话给C，C就会将鱼钓上去\",\"当应用程序请求数据时，内核一方面去取数据报内容返回，另一方面将程序控制权还给应用进程，应用进程继续处理其他事情，是一种非阻塞的状态。\"]},{\"header\":\"4、信号驱动IO(signal blocking IO)\",\"slug\":\"_4、信号驱动io-signal-blocking-io\",\"contents\":[\"G也在河边钓鱼，但与A、B、C不同的是，G比较聪明，他给鱼竿上挂一个铃铛，当有鱼上钩的时候，这个铃铛就会被碰响，G就会将鱼钓上来。\",\"信号驱动IO模型，应用进程告诉内核：当数据报准备好的时候，给我发送一个信号，对SIGIO信号进行捕捉，并且调用我的信号处理函数来获取数据报。\"]},{\"header\":\"5、IO多路转接(IO multiplexing)\",\"slug\":\"_5、io多路转接-io-multiplexing\",\"contents\":[\"H同样也在河边钓鱼，但是H生活水平比较好，H拿了很多的鱼竿，一次性有很多鱼竿在等，H不断的查看每个鱼竿是否有鱼上钩。增加了效率，减少了等待的时间。\",\"IO多路转接是多了一个select函数，select函数有一个参数是文件描述符集合，对这些文件描述符进行循环监听，当某个文件描述符就绪时，就对这个文件描述符进行处理。\",\"IO多路转接是属于阻塞IO，但可以对多个文件描述符进行阻塞监听，所以效率较阻塞IO的高。\"]},{\"header\":\"11、什么叫对象序列化？什么是反序列化？实现对象序列化需要做哪些工作？\",\"slug\":\"_11、什么叫对象序列化-什么是反序列化-实现对象序列化需要做哪些工作\",\"contents\":[\"对象序列化：将对象以二进制的形式保存在硬盘上 反序列化：将二进制的文件转化为对象读取 准备工作：实现serializable接口，不想让字段放在硬盘上就加transient\"]},{\"header\":\"12、在实现序列化接口是时候一般要生成一个serialVersionUID字段,它叫做什么,\",\"slug\":\"_12、在实现序列化接口是时候一般要生成一个serialversionuid字段-它叫做什么\",\"contents\":[\"一般有什么用\",\"如果用户没有自己声明一个serialVersionUID,接口会默认生成一个serialVersionUID，但是强烈建议用户自定义一个serialVersionUID,因为默认的serialVersinUID对于class的细节非常敏感，反序列化时可能会导致InvalidClassException这个异常。 比如说先进行序列化，然后在反序列化之前修改了类，那么就会报错。因为修改了类，对应的SerialversionUID也变化了，而序列化和反序列化就是通过对比其SerialversionUID来进行的，一旦SerialversionUID不匹配，反序列化就无法成功。\"]},{\"header\":\"13、怎么生成SerialversionUID？\",\"slug\":\"_13、怎么生成serialversionuid\",\"contents\":[\"可序列化类可以通过声明名为 \\\"serialVersionUID\\\" 的字段（该字段必须是静态 (static)、最终(final) 的 long 型字段）显式声明其自己的 serialVersionUID\",\"private static final long serialVersionUID = 1L; \"]},{\"header\":\"14、BufferedReader属于哪种流,它主要是用来做什么的,它里面有那些经典的方法\",\"slug\":\"_14、bufferedreader属于哪种流-它主要是用来做什么的-它里面有那些经典的方法\",\"contents\":[\"属于处理流中的缓冲流，可以将读取的内容存在内存里面，有readLine（）方法\"]},{\"header\":\"15、Java中流类的超类主要有那些？\",\"slug\":\"_15、java中流类的超类主要有那些\",\"contents\":[\"超类代表顶端的父类（都是抽象类）\",\"java.io.InputStream\",\"java.io.OutputStream\",\"java.io.Reader\",\"java.io.Writer\"]}]},\"/interview/javaBasics/javaSE.html\":{\"title\":\"Java SE语法\",\"contents\":[{\"header\":\"1、Java 有没有 goto 语句？\",\"slug\":\"_1、java-有没有-goto-语句\",\"contents\":[\"goto是 Java 中的保留字，在目前版本的 Java 中没有使用。根据 James Gosling Java 之父）编写的《 The Java Programming Language 》一书的附录中给出了一个 Java 关键字列表，其中有 goto 和 const ，但是这两个是目前无法使用的关键字，因此有些地方将其称之为保留字，其实保留字这个词应该有更广泛的意义，因为熟悉 C 语言的程序员都知道，在系统类库中使 用过的有特殊意义的单词或单词的组合都被视为保留字 。\"]},{\"header\":\"2、& 和 && 的区别\",\"slug\":\"_2、-和-的区别\",\"contents\":[\"&运算符有两种用法：\",\"按位与\",\"逻辑与\",\"&&运算符是短路与运算。逻辑与跟短路与的差别是非常 巨大的，虽然二者都要求运算符左右两端的布尔值都是true 整个表达式的值才是 true\",\"&&之所以称为短路 运算是因为，如果 左边的表达式的值是 false ，右边的表达式会被直接短路掉，不会进行运算。很多时候 我们可能都需要用 而不是 &&，例如在验证用户登录时判定用户名不是 null 而且不是空字符串，应当写为 username != null &&!username.equals(\\\"\\\") equals(\\\"\\\")，二者的顺序不能交换，更不能用 运算符，因为第一个条件如果不 成立，根本不能进行字符串的 equals 比较，否则会产生 NullPointerException 异常。 注意：逻辑或运算符（|）和短路或运算符（||)的差别也是如此。 \"]},{\"header\":\"3、在 Java 中，如何跳出当前的多重嵌套循环？\",\"slug\":\"_3、在-java-中-如何跳出当前的多重嵌套循环\",\"contents\":[\"使用标签跳出循环，在最外层循环前加一个标记如A ，然后用 break A; 可以跳出多重循环。\",\"loop: for (int i = 0; i < 10; i++) { for (int j = 0; j < 10; j++) { for (int k = 0; k < 10; k++) { for (int h = 0; h < 10; h++) { if (h == 6) { break loop; } System.out.print(h); } } } } \",\"其次：\",\"break是跳出当前for循环\",\"continue是跳出当前循环，开始下一循环\"]},{\"header\":\"4、两个对象值相同 (x.equals(y) == true) ，但却可有不同的 hashCode 这句\",\"slug\":\"_4、两个对象值相同-x-equals-y-true-但却可有不同的-hashcode-这句\",\"contents\":[\"话对不对？\",\"不对。如果x.equals(y) == true，则它们的哈希码应当相同\",\"Java对于 eqauls 方法和 hashCode 方法是这样规定的： (1)如果两个对象相同（ equals 方法返回 true ），那么它们的 hashCode 值一定要相同； (2)如果两个对象的 hashCode 相同，它们并不一定相同。当然，你未必要按照要求去做，但是如果你违背了上述原则就会发现在使用容器时，相同的对象可以出现在 Set 集合中，同时增加新元素的效率会大大下降（对于使用哈希存储的系统，如果哈希码频繁的冲突将会造成存取性能急剧下降）。 \",\"首先equals 方法必须满足\",\"自反性（ x.equals(x) 必须返回 true ）\",\"对称性 x.equals(y) 返回 true 时， y.equals(x)也必须返回 true ）\",\"传递性 x .equals( 和 y.equals(z) 都返回 true 时， x.equals(z) 也必须返回 true ）\",\"一致性（当x 和 y 引用的对象信息没有被修改时，多次调用 x.equals(y) 应该得到同样的返回值）\",\"而且对于任何非 null 值的引用 x x.equals(null) 必须返回 false 。\",\"实现高质量的 equals 方法的诀窍包括：\",\"使用 操作符检查 参数是否为这个对象的引用\",\"使用 instanceof 操作符检查 参数是否为正确的类型\",\"对于类中的关键属性，检查参数传入对象的属性是否与之相匹配；\",\"编写完 equals 方法后，问自己它是否满足对称性、传递性、一致性；\",\"重写 equals 时总是要重写 hashCode\",\"不要将 equals 方法参数中的 Object 对象替换为其他的类型，在重写时不要忘掉@Override 注解。\"]},{\"header\":\"5、是否可以继承 String\",\"slug\":\"_5、是否可以继承-string\",\"contents\":[\"不可以。String类是final类，不可以继承。\"]},{\"header\":\"6、当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递？\",\"slug\":\"_6、当一个对象被当作参数传递到一个方法后-此方法可改变这个对象的属性-并可返回变化后的结果-那么这里到底是值传递还是引用传递\",\"contents\":[\"是值传递。 Java 语言的方法调用只支持参数的值传递。当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的。 C++和 C# 中可以通过传引用或传输出参数来改变传入的参数的值 。\",\"说明： Java 中没有传引用实在是非常的不方便，这一点在 Java 8 中仍然没有得到改进，正是如此在 Java 编写的代码中才会出现大量的 Wrapper 类（将需要通过方法调用修改的引用置于一个 Wrapper 类中，再将 Wrapper 对象传入方法），这样的做法只会让代码变得臃肿，尤其是让从 C 和 C++ 转型为 Java 程序员的开发者无法容忍。\"]},{\"header\":\"7、重载(overload)和重写(override)的区别 重载的方法能否根据返回类型进行区分？\",\"slug\":\"_7、重载-overload-和重写-override-的区别-重载的方法能否根据返回类型进行区分\",\"contents\":[\"方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。\",\"重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；\",\"重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。\",\"重载对返回类型没有特殊的要求。\",\"方法重载的规则：\",\"方法名一致，参数列表中参数的顺序，类型，个数不同。\",\"重载与方法的返回值无关，存在于父类和子类， 同类中。\",\"可以抛出不同的异常，可以有不同修饰符。\",\"方法重写的规则：\",\"参数列表必须完全与被重写方法的一致，返回类型必须完全与被重写方法的返回类型一致。\",\"构造方法不能被重写，声明为 final 的方法不能被重写，声明为 static 的方法不能被重写，但是能够被再次声明。\",\"访问权限不能比父类中被重写的方法的访问权限更低。\",\"重写的方法能够抛出任何非强制异常（ UncheckedException ，也叫非运行时异常），无论被重写的方法是否抛出异常。但是，重写的方法不能抛出新的强制性异常，或者比被重写方法声明的更广泛的强制性异 常，反之则可以。\"]},{\"header\":\"8、为什么函数不能根据返回类型来区分重载？\",\"slug\":\"_8、为什么函数不能根据返回类型来区分重载\",\"contents\":[\"因为调用时不能指定类型信息，编译器不知道你要调用哪个函数。\",\"例如：\",\"float findMin(int a,int b); int findMin(int a,int b); \",\"当调用 max(1, 2); 时无法确定调用的是哪个，单从这一点上来说，仅返回值类型不同的重载是不应该允许的。\",\"函数的返回值只是作为函数运行之后的一个“状态”，他是保持方法的调用者与被调用者进行通信的关键。并不能作为某个方法的“标识”。\"]},{\"header\":\"9、char 型变量中能不能存储一个中文汉字，为什么？\",\"slug\":\"_9、char-型变量中能不能存储一个中文汉字-为什么\",\"contents\":[\"char 类型可以存储一个中文汉字，因为 Java 中使用的编码是 Unicode （不选择任何特定的编码，直接使用字符在字符集中的编号，这是统一的唯一方法），一个 char 类型占 2 个字节（ 16 比特），所以放一个中文是没问题的。\",\"补充：使用 Unicode 意味着字符在 JVM 内部和外部有不同的表现形式，在 JVM 内部都是 Unicode ，当这个字符被从 JVM 内部转移到外部时（例如存入文件系统中），需要进行编码转换。所以 Java 中有字节流和字符流，以及在字符流和字节流之间进行转换的转换流，如 InputStreamReader 和 OutputStreamReader ，这两个类是字节流和字符流之间的适配器类，承担了编码转换的任务；\"]},{\"header\":\"10、抽象类(abstract)和接口(interface) 有什么异同？\",\"slug\":\"_10、抽象类-abstract-和接口-interface-有什么异同\",\"contents\":[\"抽象类\",\"接口\",\"不同点\",\"1.抽象类中可以定义构造器2.可以有抽象方法和具体方法3.接口中的成员全都是 public 的4.抽象类中可以定义成员变量5.有抽象方法的类必须被声明为抽象类，而抽象类未必要有抽象方法6.抽象类中可以包含静态方法7.一个类只能继承一个抽象类\",\"1.接口中不能定义构造器2.方法全部都是抽象方法3.抽象类中的成员可以是 private 、默认、 protected 、 public4.接口中定义的成员变量实际上都是常量5.接口中不能有静态方法6.一个类可以实现多个接口\",\"相同点：\",\"不能够实例化\",\"可以将抽象类和接口类型作为引用类型\",\"一个类如果继承了某个抽象类或者实现了某个接口都需要对其中的抽象方法全部进行实现，否则该类仍然需要被声明为抽象类\"]},{\"header\":\"11、抽象的(abstract) 方法是否可同时是静态的 (static)， 是否可同时是本地方法(native)，是否可同时被 synchronized?\",\"slug\":\"_11、抽象的-abstract-方法是否可同时是静态的-static-是否可同时是本地方法-native-是否可同时被-synchronized\",\"contents\":[\"都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及细节，因此也是互相矛盾的。\"]},{\"header\":\"12、阐述静态变量和实例变量的区别？\",\"slug\":\"_12、阐述静态变量和实例变量的区别\",\"contents\":[\"静态变量：是被static修饰的变量，也称为类变量，它属于类，不属于类中的任何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个拷贝。\",\"实例变量：必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。\"]},{\"header\":\"13、 == 和 equals 的区别？\",\"slug\":\"_13、-和-equals-的区别\",\"contents\":[\"equals与 == 的最大区别：一个是方法，一个是运算符\",\"==：\",\"如果比较的对象是基本类型，则比较数值是否相等\",\"如果比较的对象是封装类型，则比较对象的地址值是否相等\",\"equals：用来比较方法两个对象的内容是否相等\",\"注：equals 方法不能用于基本数据类型的变量，如果没有对 equals 方法进行重写，则比较的是引用类型的变量所指向的对象的地址。\",\"String a = \\\"a\\\"; String b = \\\"a\\\"; //这样定义的a和b指向的是字符串常量区变量，地址是一样的，即用equals为true，用==也为true。 String s1=new String(\\\"xyz\\\"); //创建了String类型的内容为xyz的s1对象 String s2=new String(\\\"xyz\\\"); //创建了String类型的内容为xyz的s2对象 Boolean b1=s1.equals(s2); //比较s1对象和s2对象的内容相等，返回true。 Boolean b2=(s1==s2); //比较s1和s2两个对象的存储地址是否相等，明显两者分别存储在不同的地址，所以返回：false \"]},{\"header\":\"14、break 和 continue 的区别？\",\"slug\":\"_14、break-和-continue-的区别\",\"contents\":[\"break和 continue 都是用来控制循环的 语句。 break：用于完全结束一个循环，跳出循环体执行循环后面的语句。 continue：用于跳过本次循环，执行下次循环。\"]},{\"header\":\"15、String s = \\\"Hello\\\";s = s + \\\" world!\\\"; 这两行代码执行后，原始的 String 对象中的内容到底变了没有？\",\"slug\":\"_15、string-s-hello-s-s-world-这两行代码执行后-原始的-string-对象中的内容到底变了没有\",\"contents\":[\"没有。因为String类是不可变类，它的所有对象都是不可变对象。在这段代码中， s 原先指向一个 String 对象，内容是\\\"Hello\\\"，然后我们对 s 进行了“ 操作，那么 s 所指向的那个对象是否发生了改变呢？答案是没有。这时， s 不指向原来那个对象了，而指向了另一个 String 对象，内容为 \\\"Hello world!\\\"，原来那个对象还是存在内存中。只是s这个引用变量不再指向它了\"]}]},\"/interview/javaBasics/objectOriented.html\":{\"title\":\"Java面向对象\",\"contents\":[{\"header\":\"1、面向对象都有哪些特性以及你对这些特性的理解？\",\"slug\":\"_1、面向对象都有哪些特性以及你对这些特性的理解\",\"contents\":[\"继承：继承是从已有类得到继承信息创建新类的过程。提供继承信息的类被称为父类（超类、基类）；得到继承信息的 类被称为子类（派生类）。继承让变化中的软件系统有了一定的延续性，同时继承也是封装程序中可变因素的重要手段。\",\"封装：通常认为封装是把数据和操作数据的方法绑定起来，对数据的访问只能通过已定义的接口。面向对象的本质就是将现实世界描绘成一系列完全自治、封闭的对象。我们在类中编写的方法就是对实现细节的一种封装；我们编写一个类就是对数据和数据操作的封装。可以说，封装就是隐藏一切可隐藏的东西，只向外界提供最简单的编程接口。\",\"多态性：多态性是指允许不同子类型的对象对同一消息作出不同的响应。简单的说就是用同样的对象引用调用同样的方法但是做了不同的事情。多态性分为编译时的多态性和运行时的多态性。如果将对象的方法视为对象向外界提供的服务，那么运行时的多态性可以解释为：当 A 系统访问 B 系统提供的服务时， B 系统有多种提供服务的方式，但一切对 A 系统来说都是透明的。方法重载（ overload ）实现的是编译时的多态性（也称为前绑定），而方法重写override ）实现的是运行时的多态性（也称为后绑定）。运行时的多态是面向对象最精髓的东西，要实现 多态需要做两件事： 1. 方法重写（子类继承父类并重写父类中已有的或抽象的方法）； 2. 对象造型（用父类型引用引用子类型对象，这样同样的引用调用同样的方法就会根据子类对象的不同而表现出不同的行为）。\",\"抽象：抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有哪些属性和行为，并不关注这些行为的细节是什么。\"]},{\"header\":\"2、访问 权限 修饰符 public 、 private 、 protected， 以及不写（默认）时的区别\",\"slug\":\"_2、访问-权限-修饰符-public-、-private-、-protected-以及不写-默认-时的区别\",\"contents\":[\"修饰符\",\"当前类\",\"同包\",\"子类\",\"其他包\",\"public\",\"√\",\"√\",\"√\",\"√\",\"protected\",\"√\",\"√\",\"√\",\"×\",\"default\",\"√\",\"√\",\"×\",\"×\",\"private\",\"√\",\"×\",\"×\",\"×\"]},{\"header\":\"3、如何理解 clone 对象\",\"slug\":\"_3、如何理解-clone-对象\",\"contents\":[]},{\"header\":\"1、为什么要用 clone\",\"slug\":\"_1、为什么要用-clone\",\"contents\":[\"在实际编程过程中，我们常常要遇到这种情况：有一个对象A ，在某一时刻 A 中已经包含了一些有效值，此时可能会需要一个和 A 完全相同新对象 B ，并且此后对 B 任何改动都不会影响到 A 中的值，也就是说， A 与 B 是两个独立的对象，但 B 的初始值是由 A 对象确定的。在 Java 语言中，用简单的赋值语句是不能满足这种需求的。要满足这种需求虽然有很多途径，但实现 clone （）方法是其中最简单，也是最高效的手段。\"]},{\"header\":\"2、new一个对象的过程和 clone一个对象的过程区别？\",\"slug\":\"_2、new一个对象的过程和-clone一个对象的过程区别\",\"contents\":[\"new操作符的本意是分配内存。程序执行到 new 操作符时，首先去看 new 操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。\",\"clone在第一步是和 new 相似的，都是分配内存，调用 clone 方法时，分配的内存和原对象（即调用 clone 方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域，填充完成之后， clone 方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。\"]},{\"header\":\"3、clone对象的使用？\",\"slug\":\"_3、clone对象的使用\",\"contents\":[]},{\"header\":\"1、复制对象和复制引用的区别\",\"slug\":\"_1、复制对象和复制引用的区别\",\"contents\":[\"Person p = new Person(23,\\\"xiaobear\\\"); Person p1 = p; System.out.println(p); System.out.println(p1); \",\"当Person p1 = p; 执行之后， 是创建了一个新的对象吗？ 首先看打印结果：\",\"Person@2f9ee1ac Person@2f9ee1ac \",\"可以看出，打印的地址值是相同的，既然地址都是相同的，那么肯定是同一个对象。p 和 p1 只是引用而已，他们都指向了一个相同的对象 Person(23, zhang ””) 。 可以把这种现象叫做引用的复制。上面代码执行完成之后，内存中的情景如下图所示：\",\"而下面的代码是真真正正的克隆了一个对象。\",\"Person p = new Person(23, \\\"xiaobear\\\"); Person p1 = (Person) p.clone(); System.out.println(p); System.out.println(p1); \",\"从打印结果可以看出，两个对象的地址是不同的，也就是说创建了新的对象，而不是把原对象的地址赋给了一个新的引用变量：\",\"Person@2f9ee1ac Person@67f1fba0 \",\"以上代码执行完成后，内存中的情景如下图所示：\"]},{\"header\":\"2、深拷贝和浅拷贝\",\"slug\":\"_2、深拷贝和浅拷贝\",\"contents\":[\"上面的示例代码中，Person 中有两个成员变量，分别是 name 和 age name 是 String 类型， age 是 int 类型。代码非常简单，如下所示：\",\"public class Person implements Cloneable{ private int age ; private String name; public Person(int age, String name) { this.age = age; this.name = name; } public Person() {} public int getAge() { return age; } public String getName() { return name; } @Override protected Object clone() throws CloneNotSupportedException { return (Person)super.clone(); } } \",\"由于age 是基本数据类型， 那么对它的拷贝没有什么疑议，直接将一个 4 字节的整数值拷贝过来就行。但是 name是 String 类型的， 它只是一个引用， 指向一个真正的 String 对象，那么对它的拷贝 有两种方式： 直接将原对象中的 name 的引用值拷贝给新对象的 name 字段， 或者是根据原 Person 对象中的 name 指向的字符串对象创建一个新的相同的字符串对象，将这个新字符串对象的引用赋给新拷贝的 Person 对象的 name 字段。这两种拷贝方式分别 叫做浅拷贝和深拷贝。深拷贝和浅拷贝的原理如下图所示：\",\"下面通过代码进行验证。如果两个Person 对象的 name 的地址值相同， 说明两个对象的 name 都指向同一个String 对象，也就是浅拷贝， 而如果两个对象的 name 的地址值不同， 那 么就说明指向不同的 String 对象， 也就是在拷贝 Person 对象的时候， 同时拷贝了 name 引用的 String 对象， 也就是深拷贝。验证代码如下：\",\" Person p = new Person(23, \\\"xiaobear\\\"); Person p1 = (Person) p.clone(); String result = p.getName() == p1.getName() ? \\\"clone 是浅拷贝的 \\\" : \\\"clone 是深拷贝的 System.out.println(result); \",\"打印结果为：\",\"clone 是浅拷贝的 \",\"所以，clone 方法执行的是浅拷贝。\"]},{\"header\":\"3、如何进行深拷贝\",\"slug\":\"_3、如何进行深拷贝\",\"contents\":[\"如果想要深拷贝一个对象，这个对象必须要实现Cloneable 接口，实现 clone方法，并且在 clone 方法内部，把该对象引用的其他对象也要 clone 一份，这就要求这个被引用的对象必须也要实现Cloneable 接口并且实现 clone 方法。\",\"public class DeepCopy { static class Body implements Cloneable{ public Head head; public Body(){ } public Body(Head head) { this.head = head; } @Override protected Object clone() throws CloneNotSupportedException { Body clone = (Body) super.clone(); clone.head = (Head) head.clone(); return clone; } } static class Head implements Cloneable{ public Face face; public Head(Face deepCopy) { } @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } static class Face implements Cloneable{ @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } public static void main(String[] args) throws CloneNotSupportedException { Body body = new Body(new Head(new Face())); Body body1 = (Body) body.clone(); System.out.println(\\\"body == body1 : \\\" + (body == body1) ); System.out.println(\\\"body.head == body1.head : \\\" + (body.head == body1.head)); } } \",\"打印输出结果：\",\"body == body1 : false body.head == body1.head : false \"]},{\"header\":\"4、用户不能调用构造方法，只能通过new关键字自动调用？\",\"slug\":\"_4、用户不能调用构造方法-只能通过new关键字自动调用\",\"contents\":[\"错误\",\"在类内部可以用户可以使用关键字**this.构造方法名()**调用（参数决定调用的是本类对应的构造方法）\",\"在子类中用户可以通过关键字**super.父类构造方法名()**调用（参数决定调用的是父类对应的构造方法。）\",\"在反射中可以使用newInstance()的方式调用。\"]},{\"header\":\"5、讲讲类的实例化顺序，比如父类静态数据，构造函数，子类静态数据，构造函数?\",\"slug\":\"_5、讲讲类的实例化顺序-比如父类静态数据-构造函数-子类静态数据-构造函数\",\"contents\":[\"基本上代码块分为三种：Static静态代码块、构造代码块、普通代码块\",\"代码块执行顺序静态代码块——> 构造代码块 ——> 构造函数——> 普通代码块\",\"继承中代码块执行顺序：父类静态块——>子类静态块——>父类代码块——>父类构造器——>子类代码块——>子类构造器\",\"public class Parent{ { System.out.println(\\\"父类非静态代码块\\\"); } static{ System.out.println(\\\"父类静态代码块\\\"); } public Parent(){ System.out.println(\\\"父类构造器\\\"); } } public class Son extends parent{ public Son(){ System.out.println(\\\"子类构造器\\\"); } static{ System.out.println(\\\"子类静态代码块\\\"); } { System.out.println(\\\"子类非静态代码块\\\"); } } public class Test { public static void main(String[] args){ Son son = new Son(); } } \",\"运行结果：\",\"父类静态块 子类静态代码块 父类非静态代码块 父类构造器 子类非静态代码块 子类构造器 \",\"类实例化顺序为：父类静态代码块/静态域->子类静态代码块/静态域 -> 父类非静态代码块 -> 父类构造器 -> 子类非静态代码块 -> 子类构造器\"]},{\"header\":\"6、构造器（constructor）是否可被重写（override）？\",\"slug\":\"_6、构造器-constructor-是否可被重写-override\",\"contents\":[\"构造器不能被继承，因此不能被重写，但可以被重载。每一个类必须有自己的构造函数，负责构造自己这部分的构造。子类不会覆盖父类的构造函数，相反必须一开始调用父类的构造函数。\"]},{\"header\":\"7、创建对象的几种方式？\",\"slug\":\"_7、创建对象的几种方式\",\"contents\":[\"new创建新对象\",\"通过反射机制\",\"采用clone机制\",\"通过序列化机制\"]},{\"header\":\"8、Super与this表示什么？\",\"slug\":\"_8、super与this表示什么\",\"contents\":[\"Super表示当前类的父类对象 This表示当前类的对象\"]},{\"header\":\"9、Java四种引用类型\",\"slug\":\"_9、java四种引用类型\",\"contents\":[\"强引用：强引用是平常中使用最多的引用，强引用在程序内存不足（OOM）的时候也不会被回收。\",\"String str = new String(\\\"str\\\"); \",\"软引用：软引用在程序内存不足时，会被回收。\",\"// 注意：wrf这个引用也是强引用，它是指向SoftReference这个对象的， // 这里的软引用指的是指向new String(\\\"str\\\")的引用，也就是SoftReference类中T SoftReference<String> wrf = new SoftReference<String>(new String(\\\"str\\\")); \",\"可用场景： 创建缓存的时候，创建的对象放进缓存中，当内存不足时，JVM就会回收早先创建的对象。\",\"弱引用：只要JVM垃圾回收器发现了它，就会将之回收\",\"WeakReference<String>wrf=newWeakReference<String>(str); \",\"可用场景：Java源码中的java.util.WeakHashMap中的key就是使用弱引用，我的理解就是，一旦我不需要某个引用，JVM会自动帮我处理它，这样我就不需要做其它操作。\",\"虚引用：虚引用的回收机制跟弱引用差不多，但是它被回收之前，会被放入ReferenceQueue中。注意哦，其它引用是被JVM回收后才被传入ReferenceQueue中的。由于这个机制，所以虚引用大多被用于引用销毁前的处理工作。还有就是，虚引用创建的时候，必须带有ReferenceQueue\",\"PhantomReference<String>prf=newPhantomReference<String>(new String(\\\"str\\\"),newReferenceQueue<>()); \",\"可用场景： 对象销毁前的一些操作，比如说资源释放等。Object.finalize() 虽然也可以做这类动作，但是这个方式即不安全又低效上诉所说的几类引用，都是指对象本身的引用，而不是指 Reference 的四个子类的引用( SoftReference 等)。\"]}]},\"/interview/javaBasics/overview.html\":{\"title\":\"Java概述\",\"contents\":[{\"header\":\"1、什么是Java\",\"slug\":\"_1、什么是java\",\"contents\":[\"Java是一门面向对象编程语言，不仅吸收了C++语言的各种优点，还摒弃了C++里难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。Java语言作为静态面向对象编程语言的代表，极好地实现了面向对象理论，允许程序员以优雅的思维方式进行复杂的编程 。\"]},{\"header\":\"2、 JDK 和 JRE 有什么区别？\",\"slug\":\"_2、-jdk-和-jre-有什么区别\",\"contents\":[\"JDK：Java Development Kit 的简称，Java 开发工具包，提供了 Java 的开发环境和运行环境。\",\"JRE：Java Runtime Environment 的简称，Java 运行环境，为 Java 的运行提供了所需环境。\",\"具体来说 JDK 其实包含了 JRE，同时还包含了编译 Java 源码的编译器 Javac，还包含了很多 Java 程序调试和分析的工具。简单来说：如果你需要运行 Java 程序，只需安装 JRE 就可以了，如果你需要编写 Java 程序，需要安装 JDK。\"]}]},\"/interview/javaBasics/typeOfData.html\":{\"title\":\"Java数据类型\",\"contents\":[{\"header\":\"1、Java 的基本数据类型都有哪些各占几个字节？\",\"slug\":\"_1、java-的基本数据类型都有哪些各占几个字节\",\"contents\":[]},{\"header\":\"2、String 是最基本的数据类型吗？\",\"slug\":\"_2、string-是最基本的数据类型吗\",\"contents\":[\"不是，String是引用类型，底层是用char数组实现的。\",\"Java 中的基本数据类型只有8 个：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（referencetype）， Java 5 以后引入的枚举类型也算是一种比较特殊的引用类型。\"]},{\"header\":\"3、运行short s1 = 1， s1 = s1 + 1 ；会出现什么结果？运行short s1 = 1; s1 += 1 ；又会出现什么结果？\",\"slug\":\"_3、运行short-s1-1-s1-s1-1-会出现什么结果-运行short-s1-1-s1-1-又会出现什么结果\",\"contents\":[\"运行第一个会报错，因为1是int类型，而s是short类型，通过+运算后s1自动转换成int型。错误提示：Error:(21, 17) java: 不兼容的类型: 从int转换到short可能会有损失\",\"运行第二个是正确的，s1=2，+1是int类型的操作，s1自动转换int类型\"]},{\"header\":\"4、int 和Integer 有什么区别？\",\"slug\":\"_4、int-和integer-有什么区别\",\"contents\":[\"Java 是一个近乎纯洁的面向对象编程语言，但是为了编程的方便还是引入了基本数据类型，但是为了能够将这些基本数据类型当成对象操作，Java 为每一个基本数据类型都引入了对应的包装类型（wrapper class），int 的包装类就是Integer，从Java 5 开始引入了自动装箱/拆箱机制，使得二者可以相互转换。\",\"原始类型: boolean， char， byte， short， int， long， float，double\",\"包装类型：Boolean，Character，Byte，Short，Integer，Long，Float，Double\",\"public class AutomaticUnboxing { public static void main(String[] args) { Integer a1 = 100,a2 = 100,z3 = 139, z4 =139; System.out.println(a1 == a2); //true System.out.println(z3 == z4); //false } } \",\"如果整型字面量的值在-128 到127 之间，那么不会new 新的Integer对象，而是直接引用常量池中的Integer 对象\"]},{\"header\":\"5、float f=3.4;是否正确？\",\"slug\":\"_5、float-f-3-4-是否正确\",\"contents\":[\"不正确。3.4是双精度。将双精度型（double） 赋值给浮点型（float）属于下转型（ down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换float f =(float)3.4; 或者写成float f =3.4F;。\"]},{\"header\":\"6、用最高效率的方法算出2 乘以8 等于多少。\",\"slug\":\"_6、用最高效率的方法算出2-乘以8-等于多少。\",\"contents\":[\"移位运算符：int i = 2 << 3;\"]},{\"header\":\"7、String 类常用方法\",\"slug\":\"_7、string-类常用方法\",\"contents\":[\"方法\",\"描述\",\" int length()\",\"返回此字符串的长度\",\"int indexOf(int ch)\",\"返回指定字符在此字符串中第一次出现处的索引\",\" int indexOf(int ch, int fromIndex)\",\"返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索\",\" int lastIndexOf(int ch)\",\"返回指定字符在此字符串中最后一次出现处的索引\",\" String concat(String str)\",\"将指定字符串连接到此字符串的结尾。\",\" boolean endsWith(String suffix)\",\"测试此字符串是否以指定的后缀结束。\",\" String replace(char oldChar, char newChar)\",\"返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。\",\"String[] split(String regex)\",\"根据给定正则表达式的匹配拆分此字符串。\",\"String substring(int beginIndex)\",\"返回一个新的字符串，它是此字符串的一个子字符串\",\" String trim()\",\"返回字符串的副本，忽略前导空白和尾部空白\",\" boolean equals(Object anObject)\",\"将此字符串与指定的对象比较。\"]},{\"header\":\"8、String 、 StringBuffer 、 StringBuilder 的区别？\",\"slug\":\"_8、string-、-stringbuffer-、-stringbuilder-的区别\",\"contents\":[\"1、可变与不可变\",\"String：字符串常量，在修改时不改变自身；若修改，等于生成新的字符串对象\",\"StringBuffer：在修改时会改变对象自身，每次操作都是对 StringBuffer 对象本身进行修改，不是生成新的对象；使用场景：对字符串经常改变情况下，主要方法： append insert （）等。\",\"2、线程是否安全\",\"String：对象定义后不可变，线程安全。\",\"StringBuffer：是线程安全的（对调用方法加入同步锁），执行效率较慢，适用于多线程下操作字符串缓冲区大量数据。\",\"StringBuilder ：是线程不安全的，适用于单线程下操作字符串缓冲区大量数据。\",\"3、共同点\",\"StringBuilder 与 StringBuffer 有公共父类 AbstractStringBuilder(抽象类)。\",\"public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence \",\"StringBuilder、StringBuffer 的方法都会调用 AbstractStringBuilder 中的公共方法，如 super.append(...)。只是 StringBuffer 会在方法上加 synchronized 关键字，进行同步。最后，如果程序不是多线程的，那么使用StringBuilder 效率高于StringBuffer。\",\"对于三者使用的总结\",\"如果要操作少量的数据用 = String\",\"单线程操作字符串缓冲区下操作大量数据 = StringBuilder\",\"多线程操作字符串缓冲区下操作大量数据 = StringBuffffer\"]},{\"header\":\"9、 while 和do while 有什么区别？\",\"slug\":\"_9、-while-和do-while-有什么区别\",\"contents\":[\"while是先判断再执行；do...while是先执行再判断，同等条件下，后者多执行了一次。\"]},{\"header\":\"10、switch 语句能否作用在byte 、long 、String 上？\",\"slug\":\"_10、switch-语句能否作用在byte-、long-、string-上\",\"contents\":[\"可以用在byte、int、short、char以及它们的封装类上\",\"不能用在其他基本类型上long、double、float、boolean以及封装类\",\"jdk1.7及以上，可以用以字符串\",\"可以用于枚举类型\"]},{\"header\":\"11、 String s ＝new String(\\\"xyz\\\")； ，创建了几个String 对象？二者之间再什么区别。\",\"slug\":\"_11、string-s-new-string-xyz-创建了几个string-对象-二者之间再什么区别。\",\"contents\":[\"创建了2个对象，一个是内存中的“xyz”，还有一个是s，指向xyz\"]},{\"header\":\"12、自动装箱与拆箱\",\"slug\":\"_12、自动装箱与拆箱\",\"contents\":[\"自动装箱：将基本类型用他们的引用类型包装起来\",\"自动拆箱：将包装类型转换为基本类型\"]},{\"header\":\"13、Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？\",\"slug\":\"_13、math-round-11-5-等于多少-math-round-11-5-等于多少\",\"contents\":[\"Math.round(11.5)的返回值是 12，Math.round(-11.5)的返回值是-11。四舍五入的原理是在参数上加 0.5 然后进行下取整。\"]},{\"header\":\"14、下面代码运行结果是多少？\",\"slug\":\"_14、下面代码运行结果是多少\",\"contents\":[\"int count = 0; for (int k = 0; k < 100; k++) { count = count++; } System.out.println(count); \",\"解析：++是先赋值，再自增，所以count永远是0\"]},{\"header\":\"15、Java中基本类型是如何转换的？\",\"slug\":\"_15、java中基本类型是如何转换的\",\"contents\":[\"基本类型等级从低到高：\",\"byte、short、int、long、float、double\",\"char、int、long、float、double\",\"自动转换：运算过程中，低级可以自动向高级进行转换\",\"强制转换：高级需要强制转换成低级，可能会丢失精度\",\"规则：\",\"= 右边先自动转换成表达式中最高级的数据类型，再进行运算。整型经过运算会自动转化最低 int 级别，如两个 char 类型的相加，得到的是一个 int 类型的数值。\",\"= 左边数据类型级别 大于 右边数据类型级别，右边会自动升级\",\"= 左边数据类型级别 小于 右边数据类型级别，需要强制转换右边数据类型\",\"char 与 short，char 与 byte 之间需要强转，因为 char 是无符号类型\"]},{\"header\":\"16、String. intern() 你了解吗？\",\"slug\":\"_16、string-intern-你了解吗\",\"contents\":[\"String.intern()是一个Native(本地)方法，它的作用是如果字符串常量池已经包含一个等于此String对象的字符串，则返回字符串常量池中这个字符串的引用, 否则将当前String对象的引用地址（堆中）添加到字符串常量池中并返回。\",\"public class StringInternTest { public static void main(String[] args) { // 基本数据类型之间的 == 是比较值，引用数据类型 == 比较的是地址值 // 1：在Java Heap中创建对象 2：在字符串常量池中添加 小熊学Java String a = new String(\\\"小熊学Java\\\"); // 调用 intern 方法，因上一步中已经将 小熊学Java 存入常量池中，这里直接返回常量池 小熊学Java 的引用地址 String b = a.intern(); // a 的地址在Java Heap中 ， b的地址在 常量池中 ，所以结果是flase System.out.println(a == b); // 因为常量池中已经包含小熊学Java，所以直接返回 String c = \\\"小熊学Java\\\"; // b c 的地址一致，所以是true System.out.println(b == c); } } //结果 false true \"]},{\"header\":\"16、String类为什么要设置成不可变?\",\"slug\":\"_16、string类为什么要设置成不可变\",\"contents\":[\"线程安全性：不可变的String对象可以在多线程环境下安全地共享，因为它们的值不能被修改。这消除了在并发环境中进行同步的需要，提高了程序的性能和可靠性。\",\"缓存哈希值：String类经常被用作哈希表的键，因此将String设置为不可变可以确保哈希值的稳定性。如果String是可变的，那么在修改String的值后，它的哈希值也会改变，导致在哈希表中无法正确找到对应的键。\",\"安全性：不可变的String对象可以被安全地用作方法的参数，因为调用方法时无法修改它们的值。这样可以防止恶意代码通过修改参数值来破坏方法的行为。\",\"字符串池：Java中的字符串池是一种字符串缓存机制，它可以重用相同值的字符串对象，以节省内存。由于String是不可变的，可以将相同值的字符串对象存储在字符串池中，从而提高内存利用率。\"]}]},\"/interview/javaHighLevel/concurrency.html\":{\"title\":\"Java 并发编程 面试题\",\"contents\":[{\"header\":\"1、Java中垃圾回收有什么目的？什么时候进行垃圾回收？\",\"slug\":\"_1、java中垃圾回收有什么目的-什么时候进行垃圾回收\",\"contents\":[\"垃圾回收的目的：识别并且丢弃应用不再使用的对象来释放和重用资源。\",\"垃圾回收：是在内存中存在没有引用的对象或超过作用域的对象时进行的。\"]},{\"header\":\"2、线程之间如何通信及线程之间如何同步？\",\"slug\":\"_2、线程之间如何通信及线程之间如何同步\",\"contents\":[\"通信：指线程之间如何来交换信息。\",\"线程之间的通信机制：共享内存和消息传递\",\"Java采用的是共享内存模型，Java线程之间的通信总是隐式的进行，整个通信机制对程序员完全透明。\"]},{\"header\":\"3、什么是Java内存模型？\",\"slug\":\"_3、什么是java内存模型\",\"contents\":[\"共享内存模型指的就是Java内存模型(简称JMM)，JMM决定一个线程对共享变量的写入时,能对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。\"]},{\"header\":\"4、线程池有几种实现方式？\",\"slug\":\"_4、线程池有几种实现方式\",\"contents\":[\"线程池的创建方法总共有 7 种，但总体来说可分为 2 类：\",\"通过 ThreadPoolExecutor 创建的线程池；\",\"通过 Executors 创建的线程池。\",\"线程池的创建方式总共包含以下 7 种（其中 6 种是通过 Executors 创建的，1 种是通过 ThreadPoolExecutor 创建的）：\",\"Executors.newFixedThreadPool：创建一个固定大小的线程池，可控制并发的线程数，超出的线程会在队列中等待；\",\"Executors.newCachedThreadPool：创建一个可缓存的线程池，若线程数超过处理所需，缓存一段时间后会回收，若线程数不够，则新建线程；\",\"Executors.newSingleThreadExecutor：创建单个线程数的线程池，它可以保证先进先出的执行顺序；\",\"Executors.newScheduledThreadPool：创建一个可以执行延迟任务的线程池；\",\"Executors.newSingleThreadScheduledExecutor：创建一个单线程的可以执行延迟任务的线程池；\",\"Executors.newWorkStealingPool：创建一个抢占式执行的线程池（任务执行顺序不确定）【JDK 1.8 添加】。\",\"ThreadPoolExecutor：最原始的创建线程池的方式，它包含了 7 个参数可供设置，会更加可控。\"]},{\"header\":\"5、自定义线程池的各个参数含义？\",\"slug\":\"_5、自定义线程池的各个参数含义\",\"contents\":[\"参数 1：corePoolSize\",\"核心线程数，线程池中始终存活的线程数。\",\"参数 2：maximumPoolSize\",\"最大线程数，线程池中允许的最大线程数，当线程池的任务队列满了之后可以创建的最大线程数。\",\"参数 3：keepAliveTime\",\"最大线程数可以存活的时间，当线程中没有任务执行时，最大线程就会销毁一部分，最终保持核心线程数量的线程。\",\"参数 4：unit\",\"单位是和参数 3 存活时间配合使用的，合在一起用于设定线程的存活时间 ，参数 keepAliveTime 的时间单位有以下 7 种可选：\",\"TimeUnit.DAYS：天\",\"TimeUnit.HOURS：小时\",\"TimeUnit.MINUTES：分\",\"TimeUnit.SECONDS：秒\",\"TimeUnit.MILLISECONDS：毫秒\",\"TimeUnit.MICROSECONDS：微妙\",\"TimeUnit.NANOSECONDS：纳秒\",\"参数 5：workQueue\",\"一个阻塞队列，用来存储线程池等待执行的任务，均为线程安全，它包含以下 7 种类型：\",\"ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列；\",\"LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列；\",\"SynchronousQueue：一个不存储元素的阻塞队列，即直接提交给线程不保持它们；\",\"PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列；\",\"DelayQueue：一个使用优先级队列实现的无界阻塞队列，只有在延迟期满时才能从中提取元素；\",\"LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。与SynchronousQueue类似，还含有非阻塞方法；\",\"LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。\",\"较常用的是 LinkedBlockingQueue 和 Synchronous，线程池的排队策略与 BlockingQueue 有关。\",\"参数 6：threadFactory\",\"线程工厂，主要用来创建线程，默认为正常优先级、非守护线程。\",\"参数 7：handler\",\"拒绝策略，拒绝处理任务时的策略，系统提供了 4 种可选：\",\"AbortPolicy：拒绝并抛出异常。\",\"CallerRunsPolicy：使用当前调用的线程来执行此任务。\",\"DiscardOldestPolicy：抛弃队列头部（最旧）的一个任务，并执行当前任务。\",\"DiscardPolicy：忽略并抛弃当前任务。\",\"默认策略为 AbortPolicy。\",\"ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(4, 9, 0L, TimeUnit.SECONDS, new LinkedBlockingDeque<>(2)); \",\"创建了一个ThreadPoolExecutor对象，并设置了以下参数：\",\"corePoolSize：线程池的核心线程数为4，即线程池中始终保持的活动线程数。\",\"maximumPoolSize：线程池的最大线程数为9，即线程池中允许的最大线程数，包括核心线程和非核心线程。\",\"keepAliveTime：空闲线程的存活时间为0秒，在空闲时间超过该值时，多余的线程将被终止。\",\"unit：存活时间的时间单位，这里是秒。\",\"workQueue：使用了一个容量为2的LinkedBlockingDeque作为等待队列，用于存储等待执行的任务。\",\"这个线程池的特点是：\",\"当有新任务提交时，如果核心线程数小于corePoolSize，则会创建新的核心线程来执行任务。\",\"如果核心线程数已达到corePoolSize，但是等待队列未满，则任务会被添加到等待队列中。\",\"如果等待队列已满，但是当前线程数小于maximumPoolSize，则会创建新的非核心线程来执行任务。\",\"如果当前线程数已达到maximumPoolSize，并且等待队列也已满，则根据线程池的拒绝策略来处理新任务。\",\"可以使用threadPoolExecutor对象来提交任务并执行。例如，您可以使用execute()方法提交Runnable任务或使用submit()方法提交Callable任务。请记得在不需要使用线程池时，调用shutdown()方法来关闭线程池，以释放资源。\"]},{\"header\":\"6、wait vs sleep的区别\",\"slug\":\"_6、wait-vs-sleep的区别\",\"contents\":[\"共同点：wait() ，wait(long) 和 sleep(long) 的效果都是让当前线程暂时放弃 CPU 的使用权，进入阻塞状态\",\"不同点：\",\"不同点\",\"wait\",\"sleep\",\"方法归属\",\"wait()，wait(long) 都是 Object 的成员方法，每个对象都有\",\"sleep(long) 是 Thread 的静态方法\",\"醒来时机\",\"wait(long) 和 wait() 还可以被 notify 唤醒，wait() 如果不唤醒就一直等下去,它们都可以被打断唤醒\",\"执行 sleep(long) 和 wait(long) 的线程都会在等待相应毫秒后醒来,它们都可以被打断唤醒\",\"锁特性\",\"wait 方法的调用必须先获取 wait 对象的锁wait 方法执行后会释放对象锁，允许其它线程获得该对象锁（我放弃 cpu，但你们还可以用）\",\"而 sleep 则无此限制 sleep 如果在 synchronized 代码块中执行，并不会释放对象锁（我放弃 cpu，你们也用不了）\"]},{\"header\":\"7、 lock vs synchronized的区别\",\"slug\":\"_7、-lock-vs-synchronized的区别\",\"contents\":[\"lock和synchronized都是用于实现线程同步的机制，但它们有一些区别：\",\"语法和使用方式：synchronized是Java语言内置的关键字，可以直接在方法或代码块中使用。而lock是一个接口，需要通过调用其方法来获取锁和释放锁。\",\"灵活性：lock提供了更大的灵活性。它可以实现更复杂的同步需求，例如可重入性、公平性、条件变量等。而synchronized是基于Java语言内置的监视器锁实现的，功能较为简单，只能实现基本的同步。\",\"性能：在低竞争的情况下，synchronized的性能可能比lock更好，因为synchronized是由JVM底层实现的，经过了优化。但在高竞争的情况下，lock的性能可能更好，因为它提供了更细粒度的控制和更高的并发性。\",\"异常处理：lock可以在获取锁时设置超时时间，并在超时后返回，避免线程一直等待。而synchronized在获取锁时，如果无法获取到锁，线程会一直阻塞等待。\",\"可中断性：lock提供了lockInterruptibly()方法，可以在等待锁的过程中响应中断请求。而synchronized无法响应中断请求，一旦获取锁，线程会一直执行，直到释放锁。\"]},{\"header\":\"8、悲观锁 vs 乐观锁的区别\",\"slug\":\"_8、悲观锁-vs-乐观锁的区别\",\"contents\":[\"悲观锁的代表是 synchronized 和 Lock 锁\",\"其核心思想是【线程只有占有了锁，才能去操作共享变量，每次只有一个线程占锁成功，获取锁失败的线程，都得停下来等待】\",\"线程从运行到阻塞、再从阻塞到唤醒，涉及线程上下文切换，如果频繁发生，影响性能\",\"实际上，线程在获取 synchronized 和 Lock 锁时，如果锁已被占用，都会做几次重试操作，减少阻塞的机会\",\"乐观锁的代表是 AtomicInteger，使用 cas 来保证原子性\",\"其核心思想是【无需加锁，每次只有一个线程能成功修改共享变量，其它失败的线程不需要停止，不断重试直至成功】\",\"由于线程一直运行，不需要阻塞，因此不涉及线程上下文切换\",\"它需要多核 cpu 支持，且线程数不应超过 cpu 核数\"]},{\"header\":\"9、你了解ThreadLocal吗？\",\"slug\":\"_9、你了解threadlocal吗\",\"contents\":[\"作用\",\"ThreadLocal 可以实现【资源对象】的线程隔离，让每个线程各用各的【资源对象】，避免争用引发的线程安全问题\",\"ThreadLocal 同时实现了线程内的资源共享\",\"原理\",\"每个线程内有一个 ThreadLocalMap 类型的成员变量，用来存储资源对象\",\"调用 set 方法，就是以 ThreadLocal 自己作为 key，资源对象作为 value，放入当前线程的 ThreadLocalMap 集合中\",\"调用 get 方法，就是以 ThreadLocal 自己作为 key，到当前线程中查找关联的资源值\",\"调用 remove 方法，就是以 ThreadLocal 自己作为 key，移除当前线程关联的资源值\",\"ThreadLocalMap 的一些特点\",\"key 的 hash 值统一分配\",\"初始容量 16，扩容因子 2/3，扩容容量翻倍\",\"key 索引冲突后用开放寻址法解决冲突\",\"弱引用 key\",\"ThreadLocalMap 中的 key 被设计为弱引用，原因如下\",\"Thread 可能需要长时间运行（如线程池中的线程），如果 key 不再使用，需要在内存不足（GC）时释放其占用的内存\",\"内存释放时机\",\"被动 GC 释放 key \",\"仅是让 key 的内存释放，关联 value 的内存并不会释放\",\"懒惰被动释放 value \",\"get key 时，发现是 null key，则释放其 value 内存\",\"set key 时，会使用启发式扫描，清除临近的 null key 的 value 内存，启发次数与元素个数，是否发现 null key 有关\",\"主动 remove 释放 key，value \",\"会同时释放 key，value 的内存，也会清除临近的 null key 的 value 内存\",\"推荐使用它，因为一般使用 ThreadLocal 时都把它作为静态变量（即强引用），因此无法被动依靠 GC 回收\"]},{\"header\":\"10、start VS run的区别\",\"slug\":\"_10、start-vs-run的区别\",\"contents\":[\"start（）方法来启动线程，真正实现了多线程运行。这时无需等待 run 方法体代码执行完毕， 可以直接继续执行下面的代码。\",\"通过调用 Thread 类的 start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。\",\"方法 run()称为线程体，它包含了要执行的这个线程的内容，线程就进入了运行状态，开始运行 run 函数当中的代码。 Run 方法运行结束， 此线程终止。然后CPU 再调度其它线程。\"]},{\"header\":\"11、什么是volatile关键字？它的作用是什么？\",\"slug\":\"_11、什么是volatile关键字-它的作用是什么\",\"contents\":[\"volatile是Java中的关键字，用于修饰变量。它的主要作用是确保多个线程之间对变量的可见性和有序性。当一个变量被声明为\",\"volatile时，它将具备以下特性：\",\"可见性：对一个volatile变量的写操作会立即被其他线程可见，读操作也会读取最新的值。\",\"有序性：volatile变量的读写操作具备一定的顺序性，不会被重排序。\"]},{\"header\":\"12、 volatile 关键字和 synchronized 关键字有什么区别？\",\"slug\":\"_12、volatile关键字和synchronized关键字有什么区别\",\"contents\":[\"volatile关键字用于修饰变量，而synchronized关键字用于修饰代码块或方法。\",\"volatile关键字保证了变量的可见性和有序性，但不提供原子性。而synchronized关键字不仅保证了可见性和有序性，还提供了原子性。\",\"多个线程访问volatile变量时，不会阻塞线程，而synchronized关键字会对代码块或方法进行加锁，可能会阻塞其他线程的访问。\"]},{\"header\":\"13、 volatile 关键字如何确保可见性和有序性？\",\"slug\":\"_13、volatile关键字如何确保可见性和有序性\",\"contents\":[\"volatile关键字通过使用内存屏障和禁止重排序来确保可见性和有序性。\",\"写操作：对一个volatile变量的写操作会立即刷新到主内存中，并清空本地缓存，使其他线程可见。\",\"读操作：对一个volatile变量的读操作会从主内存中读取最新的值，并刷新本地缓存。\"]},{\"header\":\"14、什么情况下应该使用 volatile 关键字？\",\"slug\":\"_14、什么情况下应该使用volatile关键字\",\"contents\":[\"当多个线程访问同一个变量时，且其中有一个线程进行写操作，其他线程进行读操作，可以考虑使用volatile关键字。\",\"当变量的值不依赖于当前值，或者只有单一的写操作，可以考虑使用volatile关键字。\"]},{\"header\":\"15、 volatile 关键字能否替代锁（ synchronized ）？\",\"slug\":\"_15、volatile关键字能否替代锁-synchronized\",\"contents\":[\"volatile关键字不能替代锁（synchronized），因为它无法提供原子性的操作。volatile只能保证可见性和有序性，但无法解决多线程并发修改同一变量的问题。\"]},{\"header\":\"16、 volatile 关键字是否能够解决线程安全问题？\",\"slug\":\"_16、volatile关键字是否能够解决线程安全问题\",\"contents\":[\"volatile关键字不能解决所有的线程安全问题。它只能保证对变量的单个读/写操作具有可见性和有序性。对于复合操作（例如自增、自减等），volatile无法保证原子性。\"]},{\"header\":\"17、 volatile 关键字和原子性有什么关系？\",\"slug\":\"_17、volatile关键字和原子性有什么关系\",\"contents\":[\"volatile关键字不能保证操作的原子性。原子性指的是一个操作是不可中断的，要么全部执行成功，要么全部失败。\",\"如果需要保证操作的原子性，可以使用synchronized关键字或java.util.concurrent.atomic包下的原子类。\"]},{\"header\":\"18、 volatile 关键字对于单例模式中的双重检查锁定有什么作用？\",\"slug\":\"_18、volatile关键字对于单例模式中的双重检查锁定有什么作用\",\"contents\":[\"在双重检查锁定的单例模式中，使用volatile关键字修饰单例对象的引用，可以确保多线程环境下的正确性。\",\"volatile关键字可以防止指令重排序，从而避免在多线程环境下获取到未完全初始化的实例对象。\"]}]},\"/interview/javaHighLevel/jvm.html\":{\"title\":\"JVM高频面试题\",\"contents\":[{\"header\":\"1、什么是JVM？\",\"slug\":\"_1、什么是jvm\",\"contents\":[\"JVM（Java Virtual Machine）是用于运行Java字节码的虚拟机，包括一套字节码指令集、一组程序寄存器、一个虚拟机栈、一个虚拟机堆、一个方法区和一个垃圾回收器。JVM运行在操作系统之上，不与硬件设备直接交互。\",\"Java源文件在通过编译器之后被编译成相应的.Class文件（字节码文件），.Class文件又被JVM中的解释器编译成机器码在不同的操作系统（Windows、Linux、Mac）上运行。每种操作系统的解释器都是不同的，但基于解释器实现的虚拟机是相同的，这也是Java能够跨平台的原因。在一个Java进程开始运行后，虚拟机就开始实例化了，有多个进程启动就会实例化多个虚拟机实例。进程退出或者关闭，则虚拟机实例消亡，在多个虚拟机实例之间不能共享数据。\"]},{\"header\":\"2、JVM虚拟机包含了哪些区域？\",\"slug\":\"_2、jvm虚拟机包含了哪些区域\",\"contents\":[\"Java虚拟机包括一个类加载器子系统（Class Loader SubSystem）、运行时数据区（Runtime Data Area）、执行引擎和本地接口库（Native Interface Library）。本地接口库通过调用本地方法库（Native Method Library）与操作系统交互\",\"其中：\",\"类加载器子系统用于将编译好的.Class文件加载到JVM中；\",\"运行时数据区用于存储在JVM运行过程中产生的数据，包括程序计数器、方法区、本地方法区、虚拟机栈和虚拟机堆；\",\"执行引擎包括即时编译器和垃圾回收器，即时编译器用于将Java字节码编译成具体的机器码，垃圾回收器用于回收在运行过程中不再使用的对象；\",\"本地接口库用于调用操作系统的本地方法库完成具体的指令操作。\"]},{\"header\":\"3、在JVM后台运行的线程有哪些？\",\"slug\":\"_3、在jvm后台运行的线程有哪些\",\"contents\":[\"虚拟机线程（JVMThread）：虚拟机线程在JVM到达安全点（SafePoint）时出现。\",\"周期性任务线程：通过定时器调度线程来实现周期性操作的执行。\",\"GC线程：GC线程支持JVM中不同的垃圾回收活动。\",\"编译器线程：编译器线程在运行时将字节码动态编译成本地平台机器码，是JVM跨平台的具体实现。\",\"信号分发线程：接收发送到JVM的信号并调用JVM方法\"]},{\"header\":\"4、JVM内存区域怎么区分？\",\"slug\":\"_4、jvm内存区域怎么区分\",\"contents\":[\"JVM内存区域分为：线程私有区域和线程共享区域以及直接内存\",\"线程私有区域：程序计数器、虚拟机、本地方法区\",\"线程共享区域：虚拟机堆、方法区\"]},{\"header\":\"5、JVM内存区域的生命周期\",\"slug\":\"_5、jvm内存区域的生命周期\",\"contents\":[\"线程私有区域：与线程的生命周期相同，随线程的启动而创建，随线程的结束而销毁\",\"线程共享区域：与虚拟机的生命周期相同，随虚拟机的启动而创建，随虚拟机的结束而销毁\"]},{\"header\":\"6、JVM直接内存，你了解吗？\",\"slug\":\"_6、jvm直接内存-你了解吗\",\"contents\":[\"直接内存也叫作堆外内存，它并不是JVM运行时数据区的一部分，但在并发编程中被频繁使用。JDK的NIO模块提供的基于Channel与Buffer的I/O操作方式就是基于堆外内存实现的，NIO模块通过调用Native函数库直接在操作系统上分配堆外内存，然后使用DirectByteBuffer对象作为这块内存的引用对内存进行操作，Java进程可以通过堆外内存技术避免在Java堆和Native堆中来回复制数据带来的资源占用和性能消耗，因此堆外内存在高并发应用场景下被广泛使用（Netty、Flink、HBase、Hadoop都有用到堆外内存）\"]},{\"header\":\"7、什么是程序计数器？\",\"slug\":\"_7、什么是程序计数器\",\"contents\":[\"程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器；在Java虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。\",\"由于Java虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\",\"如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）。\",\"此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何OutOfMemoryError情况的区域。\"]},{\"header\":\"8、什么是虚拟机栈？\",\"slug\":\"_8、什么是虚拟机栈\",\"contents\":[\"虚拟机栈描述的是Java方法执行的线程内存模型：每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息。\",\"每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。\"]},{\"header\":\"9、什么是本地方法栈（本地方法区）？\",\"slug\":\"_9、什么是本地方法栈-本地方法区\",\"contents\":[\"本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native）方法服务。\"]},{\"header\":\"10、什么是Java堆？\",\"slug\":\"_10、什么是java堆\",\"contents\":[\"在JVM运行过程中创建的对象和产生的数据都被存储在堆中，堆是被线程共享的内存区域，也是垃圾收集器进行垃圾回收的最主要的内存区域。\"]},{\"header\":\"11、什么是方法区？\",\"slug\":\"_11、什么是方法区\",\"contents\":[\"方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。\"]},{\"header\":\"12、什么是运行时常量池？\",\"slug\":\"_12、什么是运行时常量池\",\"contents\":[\"用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中.\"]},{\"header\":\"13、对象的创建过程？\",\"slug\":\"_13、对象的创建过程\",\"contents\":[\"Java虚拟机遇到一个new字节码指令\",\"检查指令参数是否在常量池定位的引用，引用代表的类是否执行了类加载机制\",\"若未找到，则进行类加载机制的7个阶段\",\"类加载机制通过后，为对象分配内存\",\"将分配到的内存空间（但不包括对象头）都初始化为零值\",\"对创建的对象进行设置（这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码（实际上对象的哈希码会延后到真正调用Object::hashCode()方法时才计算）、对象的GC分代年龄等信息）\",\"此时，对于虚拟机来说，对象的创建已经完成，但对于Java程序来说，才刚开始，构造函数还没初始化\",\"调用init()方法后，对象才算真正创建成功\"]},{\"header\":\"14、对象在内存中的分布布局?\",\"slug\":\"_14、对象在内存中的分布布局\",\"contents\":[\"在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：\",\"对象头（Header）：一部分用于存储对象自身的运行时数据；另一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针 来确定该对象是哪个类的实例\",\"实例数据（Instance Data）：对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来\",\"对齐填充（Padding)：它仅仅起着占位符的作用\"]},{\"header\":\"15、对象的访问定位？\",\"slug\":\"_15、对象的访问定位\",\"contents\":[\"创建对象自然是为了后续使用该对象，我们的Java程序会通过栈上的reference数据来操作堆上的具体对象。由于reference类型在《Java虚拟机规范》里面只规定了它是一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式有：\",\"使用句柄：Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息\",\"直接指针：Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销\"]},{\"header\":\"16、哪些区域会存在OutOfMemoryError异常？\",\"slug\":\"_16、哪些区域会存在outofmemoryerror异常\",\"contents\":[\"前面其实已经提到了\",\"Java堆溢出\",\"虚拟机栈和本地方法栈溢出\",\"方法区和运行时常量池溢出\",\"本机内存溢出\"]},{\"header\":\"17、如何确定垃圾可以回收？\",\"slug\":\"_17、如何确定垃圾可以回收\",\"contents\":[\"方法：引用计数器和可达性分析\",\"引用计数器：在Java 中如果要操作对象，就必须先获取该对象的引用，因此可以通过引用计数法来判断一个对象是否可以被回收。在为对象添加一个引用时，引用计数加l ；在为对象删除一个引用时， 引进计数减l ；如果一个对象的引用计数为0 ，则表示此刻该对象没有被引用，可以被回收。\",\"存在的问题：如果两个互相引用，则不会回收\",\"可达性分析：为了解决引用计数器方法的循环引用问题，首先定义一些GC Root s 对象，然后以这些GC Roots 对象作为起 点向下搜索，如果在GC roots 和一个对象之间没有可达路径， 则称该对象是不可达的。不可达对象要经过至少两次标记才能判定其是否可以被囚lj史，如果在两次标记后该对象仍然是不可达的，则将被垃圾收集器回收。\"]},{\"header\":\"18、在Java体系中，可固定作为GC Roots的对象有哪些？\",\"slug\":\"_18、在java体系中-可固定作为gc-roots的对象有哪些\",\"contents\":[\"在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等\",\"在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量\",\"在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用\",\"在本地方法栈中JNI（即通常所说的Native方法）引用的对象\",\"Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器\",\"所有被同步锁（synchronized关键字）持有的对象\",\"反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存\"]},{\"header\":\"19、Java中的4中引用类型？\",\"slug\":\"_19、java中的4中引用类型\",\"contents\":[\"强引用（Strongly Re-ference）：强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“Object obj=new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象\",\"软引用（Soft Reference）：用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内 存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用\",\"弱引用（Weak Reference）：也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只 能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用\",\"虚引用（Phantom Reference）:也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的 存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2 版之后提供了PhantomReference类来实现虚引用\"]},{\"header\":\"20、垃圾回收算法有哪些？\",\"slug\":\"_20、垃圾回收算法有哪些\",\"contents\":[\"Java 中常用的垃圾回收算法有：\",\"标记清除（ Mark-Sweep ）\",\"标记复制（ Copying ）\",\"标记整理( Mark-Compact ）\",\"分代收集（ Generational Collecting ）\"]},{\"header\":\"21、什么是分代收集算法？\",\"slug\":\"_21、什么是分代收集算法\",\"contents\":[\"分代收集算法根据对象的不同类型将内存划分为不同的区域， JVM 将堆划分为新生代和老年代。新生代主要存放新生成的对象，其特点是对象数量多但是生命周期短，在每次进行垃圾回收时都有大量的对象被回收；老年代主要存放大对象和生命周期长的对 象，因此可回收的对象相对较少。因此， JVM 根据不同的区域对象的特点选择了不同的算法。\"]},{\"header\":\"22、什么是GC?\",\"slug\":\"_22、什么是gc\",\"contents\":[\"GC：是garbage collection的缩写，垃圾回收\",\"Minor GC/Young GC：新生代垃圾回收\",\"Major GC/Old GC：目标老年代垃圾回收，不同资料可能有不同的所指\",\"Mixed GC：混合收集，收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为\",\"Full GC：整堆收集，收集整个堆和方法区的垃圾\"]},{\"header\":\"23、什么是标记清除算法？\",\"slug\":\"_23、什么是标记清除算法\",\"contents\":[\"标记清除，顾名思义，就是把标记的清除掉，分两个阶段，第一阶段，标记，第二阶段，清除；\",\"首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回 收所有未被标记的对象。标记过程就是对象是否属于垃圾的判定过程，它是最早出现也是最基础的算法\",\"缺点：\",\"执行效率不高：如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低。\",\"内存空间的碎片化问题：标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作\"]},{\"header\":\"24、什么是标记复制算法？\",\"slug\":\"_24、什么是标记复制算法\",\"contents\":[\"复制算法是为了解决标记清除算法内存碎片化的问题而设计的。复制算法首先将内存划分为两块大小相等的内存区域， 即区域l 和区\",\"域2 ， 新生成的对象都被存放在区域l中，在区域l 内的对象存储满后会对区域l 进行一次标记，并将标记后仍然存活的对象全部复制\",\"到区域2 中，这时区域l 将不存在任何存活的对象，直接清理整个区域l 的内存即可.\"]},{\"header\":\"25、什么是标记整理算法？\",\"slug\":\"_25、什么是标记整理算法\",\"contents\":[\"标记整理算法结合了标记清除算法和复制算法的优点，其标记阶段和标记清除算法的标记阶段相同，在标记完成后将存活的对象移到内存的另一端，然后清除该端的对象并释放内存.\"]}]},\"/interview/javaHighLevel/multithreading.html\":{\"title\":\"Java多线程 45道面试题\",\"contents\":[{\"header\":\"1、进程与线程的区别？\",\"slug\":\"_1、进程与线程的区别\",\"contents\":[\"进程：是实现某个独立功能的程序，它是操作系统（如windows 系统）进行资源分配和调度的一个独立单位，也是可以独立运行的一段程序。\",\"线程：是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位，可以使⽤用多线程对进⾏行行运算提速。⽐如，如果⼀个线程完成⼀个任务要100毫秒，那么⽤⼗个线程完成改任务只需10毫秒\",\"线程与进程之间的区别：\",\"进程间相互独立，但同一进程内的各个线程会共享该进程拥有的资源，而进程则是用独占的方式来占有资源，进程间不能共享资源。\",\"线程上下文切换（从一个线程切换到另一个线程）的速度要比进程上下文切换速度快的多。\",\"每一个线程都有一个运行的入口、顺序执行序列和出口，但线程不能独立运行，必须依靠进程的调度和控制线程的执行。\",\"一般操作系统级别的会注重“进程”的角度和管理，而应用项目会偏重于“线程”。在编程中偏重于多线程，而不是多进程。\"]},{\"header\":\"2、什么是线程安全与线程不安全？\",\"slug\":\"_2、什么是线程安全与线程不安全\",\"contents\":[\"线程安全：多线程访问时，采用了加锁机制，当一个线程访问该类的某一个数据时，会对该数据进行保护，其他线程无法访问，直到该线程读取完，其他线程才可使用，不会出现数据不一致或数据污染。\",\"线程不安全：不提供数据访问保护，有可能出现先后更改数据造成所得到的数据是脏数据\",\"线程安全都是由全局变量和静态变量引起的。\",\"若每个线程中对全局变量、静态变量只有读操作，⽽无写操作，⼀般来说，这个全局变量是线程安全的；\",\"若有多个线程同时执行写操作，一般都需要考虑线程同步，否则的话就可能影响线程安全。\"]},{\"header\":\"3、什么是多线程？多线程的优点与缺点？\",\"slug\":\"_3、什么是多线程-多线程的优点与缺点\",\"contents\":[\"多线程：多线程是指程序中包含多个执行流，即在一个程序中可以同时运行多个不同的线程来执行不同的任务。\",\"优点：可以提高 CPU 的利用率。在多线程程序中，一个线程必须等待的时候，CPU 可以运行其它的线程而不是等待，这样就大大提高了程序的效率。也就是说允许单个程序创建多个并行执行的线程来完成各自的任务。\",\"缺点：\",\"线程也是程序，所以线程需要占用内存，线程越多占用内存也越多；\",\"多线程需要协调和管理，所以需要 CPU 时间跟踪线程；\",\"线程之间对共享资源的访问会相互影响，必须解决竞用共享资源的问题。\"]},{\"header\":\"4、什么是上下文切换？\",\"slug\":\"_4、什么是上下文切换\",\"contents\":[\"多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。\",\"概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\",\"上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU时间，事实上，可能是操作系统中时间消耗最大的操作。\",\"Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\"]},{\"header\":\"5、守护线程与用户线程有什么区别？\",\"slug\":\"_5、守护线程与用户线程有什么区别\",\"contents\":[\"守护线程：运行在后台，为其他前台线程服务。也可以说守护线程是 JVM 中非守护线程的 “佣人”。一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作。\",\"用户线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程。\"]},{\"header\":\"6、什么是线程死锁？\",\"slug\":\"_6、什么是线程死锁\",\"contents\":[\"死锁：指两个或两个以上的线程（进程），在执行过程中，由于竞争资源或由于彼此通信而造成的一种阻塞现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。\",\"多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。\",\"例如：线程A与线程B都拥有各自的资源，但试图去获取对方的资源，这两个线程就会互相等待而进入死锁状态。\"]},{\"header\":\"7、形成死锁的四个必要条件是什么？\",\"slug\":\"_7、形成死锁的四个必要条件是什么\",\"contents\":[\"互斥条件：在一段时间内，某个资源只由一个进程占用。如果此时其他进程请求资源，就只能等待，直到占有资源的进程释放。\",\"占有且等待条件：进程至少保持一个资源，但又提出了新的资源的请求，而请求的资源被其他进程占有，此时请求进程阻塞，对自己已经获取的资源保持不放。\",\"不可抢占条件：别人已经占有了某项资源，你不能因为自己也需要该资源，就去把别人的资源抢过来。\",\"循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。（比如一个进程集合，A在等B，B在等C，C在等A）\"]},{\"header\":\"8、如何避免死锁？\",\"slug\":\"_8、如何避免死锁\",\"contents\":[\"避免一个线程同时获得多个锁\",\"避免一个线程在锁内同时占有多个资源，尽量保证每个锁只占有一个资源\",\"尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制\"]},{\"header\":\"9、创建线程的四种方式？\",\"slug\":\"_9、创建线程的四种方式\",\"contents\":[\"继承Thread类\",\"实现 Runnable 接口\",\"使用 Callable 和 Future 创建线程\",\"使用线程池创建线程\"]},{\"header\":\"1、继承Thread类\",\"slug\":\"_1、继承thread类\",\"contents\":[\"步骤：\",\"定义Thread类的子类，并重写该类的run()方法，该方法的方法体就是线程需要完成的任务，run()方法也称为线程执行体；\",\"创建Thread子类的实例，也就是创建了线程对象；\",\"启动线程，调用线程的start()方法。\",\"public class CreateThread extends Thread{ @Override public void run() { //获取线程名 System.out.println(Thread.currentThread().getName()); } public static void main(String[] args) { CreateThread createThread = new CreateThread(); //线程启动 createThread.start(); } } \"]},{\"header\":\"2、实现Runnable接口\",\"slug\":\"_2、实现runnable接口\",\"contents\":[\"步骤：\",\"定义Runnable接口的实现类，一样要重写run()方法，这个run()方法和Thread中的run()方法一样是线程的执行体；\",\"创建Runnable实现类的实例，并用这个实例作为Thread的target来创建Thread对象，这个Thread对象才是真正的线程对象；\",\"调用线程对象的start()方法来启动线程。\",\"public class RunnableCreateThread implements Runnable{ @Override public void run() { System.out.println(\\\"实现Runnable接口创建线程\\\"); } public static void main(String[] args) { new Thread(new RunnableCreateThread()).start(); } } \"]},{\"header\":\"3、使用Callable和Future创建线程\",\"slug\":\"_3、使用callable和future创建线程\",\"contents\":[\"与 Runnable 接口不一样，Callable 接口提供了一个 call() 方法作为线程执行体，call() 方法比 run() 方法功能要强大，比如：call() 方法可以有返回值、call() 方法可以声明抛出异常。\",\"Java5 提供了 Future 接口来代表 Callable 接口里 call() 方法的返回值，并且为 Future 接口提供了一个实现类 FutureTask，这个实现类既实现了 Future 接口，还实现了 Runnable 接口，因此可以作为 Thread 类的 target。在 Future 接口里定义了几个公共方法来控制它关联的 Callable 任务。\",\"步骤：\",\"创建实现 Callable 接口的类 MyCallable；\",\"以 myCallable 为参数创建 FutureTask 对象；\",\"将 FutureTask 作为参数创建 Thread 对象；\",\"调用线程对象的 start() 方法\",\"public class MyCallable implements Callable { @Override public Object call() throws Exception { System.out.println(Thread.currentThread().getName()); return \\\"huahua\\\"; } public static void main(String[] args) { //创建 FutureTask 对象 FutureTask futureTask = new FutureTask<>(new MyCallable()); //创建线程并启动 Thread thread = new Thread(futureTask); thread.start(); try { Thread.sleep(1000); //获取返回值 System.out.println(\\\"返回的结果是：\\\" + futureTask.get()); } catch (Exception e) { e.printStackTrace(); } } } \"]},{\"header\":\"4、基于线程池创建线程\",\"slug\":\"_4、基于线程池创建线程\",\"contents\":[\"Executors 提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService 接口。\",\"主要有四种：\",\"newFixedThreadPool：创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。\",\"newCachedThreadPool：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。\",\"newSingleThreadExecutor：创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。\",\"newScheduledThreadPool：创建一个定长线程池，支持定时及周期性任务执行。\",\"public class CreateThreadByExecutors implements Runnable { public static void main(String[] args) { ExecutorService service = Executors.newSingleThreadExecutor(); CreateThreadByExecutors thread = new CreateThreadByExecutors(); for (int i = 0; i < 10; i++) { service.execute(thread); System.out.println(\\\"=======任务开始=========\\\"); service.shutdown(); } } @Override public void run() { System.out.println(\\\"hello Thread\\\"); } } \"]},{\"header\":\"10、实现Runnable接口与实现Callable接口有什么区别？\",\"slug\":\"_10、实现runnable接口与实现callable接口有什么区别\",\"contents\":[\"相同点：\",\"都是接口\",\"都是编写多线程程序\",\"都采用Thread.start()启动程序\",\"主要区别：\",\"Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果\",\"Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息\",\"注：Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。\"]},{\"header\":\"11、线程的 run()和 start()有什么区别？\",\"slug\":\"_11、线程的-run-和-start-有什么区别\",\"contents\":[\"每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，run()方法称为线程体。通过调用Thread类的start()方法来启动一个线程。\",\"start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start()只能调用一次。\",\"start()方法来启动一个线程，真正实现了多线程运行。调用start()方法无需等待run方法体代码执行完毕，可以直接继续执行其他的代码； 此时线程是处于就绪状态，并没有运行。 然后通过此Thread类调用方法run()来完成其运行状态， run()方法运行结束， 此线程终止。然后CPU再调度其它线程。\",\"run()方法是在本线程里的，只是线程里的一个函数，而不是多线程的。 如果直接调用run()，其实就相当于是调用了一个普通函数而已，直接待用run()方法必须等待run()方法执行完毕才能执行下面的代码，所以执行路径还是只有一条，根本就没有线程的特征，所以在多线程执行时要使用start()方法而不是run()方法。\"]},{\"header\":\"12、为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用run() 方法？\",\"slug\":\"_12、为什么我们调用-start-方法时会执行-run-方法-为什么我们不能直接调用run-方法\",\"contents\":[\"new 一个线程Thread，线程进入新建状态；调用start()，会启动一个线程并使线程进入就绪状态，当分配到时间片就可以开始工作了，start()会执行线程的相应准备工作，然后自动执行run()方法的内容，这是真正的多线程工作。\",\"而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。\",\"总结：调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。\"]},{\"header\":\"13、什么是 Callable 和 Future?\",\"slug\":\"_13、什么是-callable-和-future\",\"contents\":[\"Callable 接口类似于 ，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。\",\"Future 接口表示异步任务，是一个可能还没有完成的异步任务的结果。所以说 Callable用于产生结果，Future 用于获取结果。\"]},{\"header\":\"14、什么是 FutureTask？\",\"slug\":\"_14、什么是-futuretask\",\"contents\":[\"FutureTask 表示一个异步运算的任务。FutureTask 里面可以传入一个 Callable 的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是Runnable 接口的实现类，所以 FutureTask 也可以放入线程池中。\"]},{\"header\":\"15、线程有哪5个状态？\",\"slug\":\"_15、线程有哪5个状态\",\"contents\":[\"线程5个状态：创建、就绪、运行、阻塞和死亡。\",\"创建状态(new)：在⽣生成线程对象，并没有调⽤用该对象的start⽅方法，这是线程处于创建状态。\",\"就绪状态(Runnable)：当调用了线程对象的start方法之后，该线程就进入了就绪状态，但是此时线程调度程序还没有把该线程设置为当前线程，此时处于就绪状态，等待线程被调度选中，获取CPU的使用权。在线程运行之后，从等待或者睡眠中回来之后，也会处于就绪状态。\",\"运行状态(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中；\",\"阻塞状态(block)：处于运行状态中的线程由于某种原因，暂时放弃对 CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才 有机会再次被 CPU 调用以进入到运行状态。\",\"阻塞的情况：\",\"等待阻塞：运行状态中的线程**执行 wait()**方法，JVM会把该线程放入等待队列(waittingqueue)中，使本线程进入到等待阻塞状态；\",\"同步阻塞：线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，，则JVM会把该线程放入锁池(lock pool)中，线程会进入同步阻塞状态；\",\"其他阻塞：通过调用线程的 sleep()或 join()或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。\",\"死亡状态(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，或调用stop()方法，则该线程结束生命周期。死亡的线程不可再次复生。\"]},{\"header\":\"16、Java 中用到的线程调度算法是什么？\",\"slug\":\"_16、java-中用到的线程调度算法是什么\",\"contents\":[\"计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。（Java是由JVM中的线程计数器来实现线程调度）\",\"有两种调度模型：分时调度模型和抢占式调度模型。\",\"分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU的时间片这个也比较好理解。\",\"Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。\"]},{\"header\":\"17、线程的调度策略？\",\"slug\":\"_17、线程的调度策略\",\"contents\":[\"线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：\",\"线程体中调用了 yield 方法让出了对 cpu 的占用权利\",\"线程体中调用了 sleep 方法使线程进入睡眠状态\",\"线程由于 IO 操作受到阻塞\",\"另外一个更高优先级线程出现\",\"在支持时间片的系统中，该线程的时间片用完\"]},{\"header\":\"18、什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？\",\"slug\":\"_18、什么是线程调度器-thread-scheduler-和时间分片-time-slicing\",\"contents\":[\"线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。 时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU 时间可以基于线程优先级或者线程等待的时间。 线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。\"]},{\"header\":\"19、线程同步以及线程调度相关的方法有哪些？\",\"slug\":\"_19、线程同步以及线程调度相关的方法有哪些\",\"contents\":[\"wait()：使一个线程处于等待或阻塞状态，并且释放所持有对象的锁\",\"sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常\",\"notify()：唤醒一个处于等待的线程，在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪一个线程，而且跟线程的优先级有关。\",\"notifyAll()：唤醒所有处于等待的线程，该方法并不是将对象的锁给所有线程，而是由他们竞争，只有获得锁的线程才会进入就绪状态。\"]},{\"header\":\"20、sleep() 和 wait() 有什么区别？\",\"slug\":\"_20、sleep-和-wait-有什么区别\",\"contents\":[\"两者都可以暂停线程的执行。\",\"不同\",\"sleep()\",\"wait()\",\"类的不同\",\"是 Thread线程类的静态方法\",\"是 Object类的方法\",\"是否释放锁\",\"不释放\",\"释放\",\"用途不同\",\"通常被用于暂停执行\",\"通常被用于线程间交互/通信\",\"用法不同\",\"sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒\",\"被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify()或者 notifyAll() 方法\"]},{\"header\":\"21、你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？\",\"slug\":\"_21、你是如何调用-wait-方法的-使用-if-块还是循环-为什么\",\"contents\":[\"处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。\",\"wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。\",\"sychronized(monitor){ //判断条件是否满足 while(!locked){ //等待唤醒 monitor.wait(); } //处理其他业务逻辑 } \"]},{\"header\":\"22、为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？\",\"slug\":\"_22、为什么线程通信的方法-wait-notify-和-notifyall-被定义在-object-类里\",\"contents\":[\"因为Java所有类的都继承了Object，Java想让任何对象都可以作为锁，并且 wait()，notify()等方法用于等待对象的锁或者唤醒线程，在 Java 的线程中并没有可供任何对象使用的锁，所以任意对象调用方法一定定义在Object类中。\",\"既然是线程放弃对象锁，那也可以把wait()定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait()方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。\"]},{\"header\":\"23、为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？\",\"slug\":\"_23、为什么-wait-notify-和-notifyall-必须在同步方法或者同步块中被调用\",\"contents\":[\"当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。\",\"同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。\",\"由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。\"]},{\"header\":\"24、Thread 类中的 yield 方法有什么作用？\",\"slug\":\"_24、thread-类中的-yield-方法有什么作用\",\"contents\":[\"使当前线程从执行状态**（运行状态）变为可执行态（就绪状态）**。 当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。\"]},{\"header\":\"25、线程的 sleep()方法和 yield()方法有什么区别？\",\"slug\":\"_25、线程的-sleep-方法和-yield-方法有什么区别\",\"contents\":[\"相同点：sleep()与yield()方法都是静态的\",\"Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。\",\"不同点：\",\"不同\",\"sleep()\",\"yield()\",\"线程机会\",\"会给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会\",\"只会给相同优先级或更高优先级的线程以运行的机会\",\"调用后状态\",\"转入阻塞（blocked）状态\",\"转入就绪（ready）状态\",\"声明异常\",\"声明抛出 InterruptedException\",\"没有声明任何异常\",\"可移植性\",\"比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性\",\"不建议使用来控制并发线程的执行\"]},{\"header\":\"26、如何停止一个正在运行的线程？\",\"slug\":\"_26、如何停止一个正在运行的线程\",\"contents\":[\"使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。\",\"使用stop()方法强行停止，但是不推荐，因为stop和suspend及resume一样都是过期作废的方法。\",\"使用interrupt方法中断线程\"]},{\"header\":\"27、Java 中 interrupted 和 isInterrupted 方法的区别？\",\"slug\":\"_27、java-中-interrupted-和-isinterrupted-方法的区别\",\"contents\":[\"interrupt：用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。\",\"注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。\",\"interrupted：是静态方法，查看当前中断信号是true还是false并且清除中断信号。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。\",\"isInterrupted：是可以返回当前中断信号是true还是false，与interrupt最大的差别\"]},{\"header\":\"28、什么是阻塞式方法？\",\"slug\":\"_28、什么是阻塞式方法\",\"contents\":[\"阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。\"]},{\"header\":\"29、Java 中你怎样唤醒一个阻塞的线程？\",\"slug\":\"_29、java-中你怎样唤醒一个阻塞的线程\",\"contents\":[\"如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException 来唤醒它。\",\"如果线程遇到了IO 阻塞，无能为力，因为IO是操作系统实现的，Java 代码并没有办法直接接触到操作系统。\"]},{\"header\":\"30、notify() 和 notifyAll() 有什么区别？\",\"slug\":\"_30、notify-和-notifyall-有什么区别\",\"contents\":[\"如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。\",\"notifyAll() 会唤醒所有的线程，notify() 只会唤醒一个线程。\",\"notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。\"]},{\"header\":\"31、如何在两个线程间共享数据？\",\"slug\":\"_31、如何在两个线程间共享数据\",\"contents\":[\"在两个线程间共享变量即可实现共享。\",\"一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。\"]},{\"header\":\"32、Java 如何实现多线程之间的通讯和协作？\",\"slug\":\"_32、java-如何实现多线程之间的通讯和协作\",\"contents\":[\"可以通过中断和共享变量的方式实现线程间的通讯和协作\",\"比如说最经典的生产者-消费者模型：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此，一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作。 \",\"Java中线程通信协作的最常见方式：\",\"syncrhoized加锁的线程的Object类的wait()/notify()/notifyAll()\",\"ReentrantLock类加锁的线程的Condition类的await()/signal()/signalAll()\",\"线程间直接的数据交换：通过管道进行线程间通信：字节流、字符流\"]},{\"header\":\"33、同步方法和同步块，哪个是更好的选择？\",\"slug\":\"_33、同步方法和同步块-哪个是更好的选择\",\"contents\":[\"原则：同步的范围越小越好\",\"同步代码块是更好的选择，因为它不会锁住整个对象（当然，你也可以让它锁住整个对象）；同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并且需要等待获得这个对象上的锁。\",\"同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。\",\"synchronized(this)以及非static的synchronized方法（至于static synchronized方法请往下看），只能防止多个线程同时执行同一个对象的同步代码段。 如果要锁住多个对象⽅方法，可以锁住一个固定的对象，或者锁住这个类的Class对象。 synchronized锁住的是括号里的对象，而不是代码。\",\"public class MySynchronized{ public synchronized void testA(){ System.out.println(\\\"testA..\\\"); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } public void testB(){ synchronized (this) { System.out.println(\\\"testB..\\\"); } } } \"]},{\"header\":\"34、什么是线程同步和线程互斥，有哪几种实现方式？\",\"slug\":\"_34、什么是线程同步和线程互斥-有哪几种实现方式\",\"contents\":[\"线程同步：当一个线程对共享的数据进行操作时，应使之成为一个”原子操作“，即在没有完成相关操作之前，不允许其他线程打断它，否则，就会破坏数据的完整性，必然会得到错误的处理结果。\",\"线程互斥：是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。\",\"线程间同步的方法分为两类：用户模式和内核模式\",\"内核模式：就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态。\",\"方法：\",\"事件\",\"信号量\",\"互斥量\",\"用户模式：不需要切换到内核态，只在用户态完成操作。\",\"方法：\",\"原子操作\",\"临界区\",\"线程同步的方法：\",\"同步代码方法：sychronized关键字修饰的方法\",\"同步代码块：sychronized关键字修饰的代码块\",\"使用特殊变量域Volatile实现线程同步：volatile关键字为域变量的访问提供了一种免锁机制\",\"使用重入锁实现线程同步：reentrantlock类是可冲入、互斥、实现了lock接口的锁他与sychronized方法具有相同的基本行为和语义\"]},{\"header\":\"35、在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？\",\"slug\":\"_35、在监视器-monitor-内部-是如何做线程同步的-程序应该做哪种级别的同步\",\"contents\":[\"在 java 虚拟机中，监视器和锁在Java虚拟机中是一块使用的。监视器监视一块同步代码块，确保一次只有一个线程执行同步代码块。每一个监视器都和一个对象引用相关联。线程在获取锁之前不允许执行同步代码。\",\"一旦方法或者代码块被 synchronized 修饰，那么这个部分就放入了监视器的监视区域，确保一次只能有一个线程执行该部分的代码，线程在获取锁之前不允许执行该部分的代码\",\"另外 java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案\"]},{\"header\":\"36、如果你提交任务时，线程池队列已满，这时会发生什么？\",\"slug\":\"_36、如果你提交任务时-线程池队列已满-这时会发生什么\",\"contents\":[\"存在两种可能：\",\"如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务\",\"如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是 AbortPolicy\"]},{\"header\":\"37、什么叫线程安全？servlet 是线程安全吗?\",\"slug\":\"_37、什么叫线程安全-servlet-是线程安全吗\",\"contents\":[\"线程安全的概念前面有说到\",\"Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。\",\"Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。\",\"SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。\",\"Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。\"]},{\"header\":\"38、在 Java 程序中怎么保证多线程的运行安全？\",\"slug\":\"_38、在-java-程序中怎么保证多线程的运行安全\",\"contents\":[\"使用安全类，比如 java.util.concurrent 下的类，使用原子类AtomicInteger\",\"使用自动锁sychronized\",\"使用手动锁Lock\",\"手动锁代码：\",\"Lock lock = new ReentrantLock(); lock. lock(); try { System. out. println(\\\"获得锁\\\"); } catch (Exception e) { // TODO: handle exception } finally { System. out. println(\\\"释放锁\\\"); lock. unlock(); } \"]},{\"header\":\"39、你对线程优先级的理解是什么？\",\"slug\":\"_39、你对线程优先级的理解是什么\",\"contents\":[\"每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。\",\"Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。\",\"当然，如果你真的想设置优先级可以通过setPriority()方法设置，但是设置了不一定会该变，这个是不准确的\"]},{\"header\":\"40、线程类的构造方法、静态块是被哪个线程调用的？\",\"slug\":\"_40、线程类的构造方法、静态块是被哪个线程调用的\",\"contents\":[\"线程类的构造方法、静态代码块是被new这个线程类所在的线程所调用的，而run()方法里面的代码才是被线程自身所调用的。\",\"例：假设 Thread2 中 new 了Thread1，main 函数中 new 了 Thread2\",\"Thread2 的构造方法、静态块是 main 线程调用的，Thread2 的 run()方法是Thread2 自己调用的\",\"Thread1 的构造方法、静态块是 Thread2 调用的，Thread1 的 run()方法是Thread1 自己调用的\"]},{\"header\":\"41、Java 中怎么获取一份线程 dump 文件？你如何在 Java 中获取线程堆栈？\",\"slug\":\"_41、java-中怎么获取一份线程-dump-文件-你如何在-java-中获取线程堆栈\",\"contents\":[\"Dump文件是进程的内存镜像。可以把程序的执行状态通过调试器保存到dump文件中。\",\"在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java应用的 dump 文件。\",\"在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。\"]},{\"header\":\"42、一个线程运行时发生异常会怎样？\",\"slug\":\"_42、一个线程运行时发生异常会怎样\",\"contents\":[\"如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候，JVM会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException()方法进行处理。\"]},{\"header\":\"43、Java 线程数过多会造成什么异常？\",\"slug\":\"_43、java-线程数过多会造成什么异常\",\"contents\":[\"线程的生命周期开销非常高\",\"消耗过多的CPU\",\"资源如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争 CPU资源时还将产生其他性能的开销。 \",\"降低稳定性JVM\",\"在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括 JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。 \"]},{\"header\":\"44、多线程的常用方法？\",\"slug\":\"_44、多线程的常用方法\",\"contents\":[\"方法名\",\"描述\",\"sleep()\",\"强迫一个线程睡眠Ｎ毫秒\",\"isAlive()\",\"判断一个线程是否存活\",\"join()\",\"等待线程终止\",\"activeCount()\",\"程序中活跃的线程数\",\"enumerate()\",\"枚举程序中的线程\",\"currentThread()\",\"得到当前线程\",\"isDaemon()\",\"是否为守护线程\",\"setDaemon()\",\"设置为守护线程\",\"setName()\",\"为线程设置一个名称\",\"wait()\",\"强迫一个线程等待\",\"notify()\",\"通知一个线程继续运行\",\"setPriority()\",\"设置一个线程的优先级\"]},{\"header\":\"45、新建T1、T2、T3 三个线程，如何保证它们按顺序执行？\",\"slug\":\"_45、新建t1、t2、t3-三个线程-如何保证它们按顺序执行\",\"contents\":[\"1、join\",\"Thread类的join方法它会让主线程等待子线程运行结束后，才能继续运行\",\"T1 t1 = new T1(); t1.join(); t1.start(); T2 t2 = new T2(); t2.join(); t2.start(); T3 t3 = new T3(); t3.start(); \",\"2、newSingleThreadExecutor\",\"可以使用JDK自带的Excutors类的newSingleThreadExecutor方法，创建一个单线程的线程池\",\" public static void main(String[] args) { ExecutorService executorService = Executors.newSingleThreadExecutor(); T1 t1 = new T1(); T2 t2 = new T2(); T3 t3 = new T3(); executorService.submit(t1); executorService.submit(t2); executorService.submit(t3); executorService.shutdown(); } \",\"3、CountDownLatch\",\"CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。\"]}]},\"/interview/javaHighLevel/reflection.html\":{\"title\":\"Java反射面试题\",\"contents\":[{\"header\":\"1、什么是Java反射机制？\",\"slug\":\"_1、什么是java反射机制\",\"contents\":[\"Java的反射（reflection）机制是指在程序的运行状态中，可以构造任意一个类的对象，可以了解任意一个对象所属的类，可以了解任意一个类的成员变量和方法，可以调用任意一个对象的属性和方法。 这种动态获取程序信息以及动态调用对象的功能称为Java语言的反射机制。\"]},{\"header\":\"1、除了使用new创建对象之外，还可以用什么方法创建对象？\",\"slug\":\"_1、除了使用new创建对象之外-还可以用什么方法创建对象\",\"contents\":[\"使用Java反射可以创建对象!\"]},{\"header\":\"2、Java反射创建对象效率高还是通过new创建对象的效率高？\",\"slug\":\"_2、java反射创建对象效率高还是通过new创建对象的效率高\",\"contents\":[\"通过new创建对象的效率比较高。通过反射时，先找查找类资源，使用类加载器创建，过程比较繁琐，所以效率较低\"]},{\"header\":\"3、java反射的作用？\",\"slug\":\"_3、java反射的作用\",\"contents\":[\"反射机制是在运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意个对象，都能够调用它的任意一个方法。在java中，只要给定类的名字，就可以通过反射机制来获得类的所有信息。\",\"这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制\",\"在运行时判定任意一个对象所属的类\",\"在运行时构造任意一个类的对象\",\"在运行时判定任意一个类所具有的成员变量和方法\",\"在运行时调用任意一个对象的成员变量和 方法\",\"在运行调用任意一个对象的方法\",\"生成动态代码\"]},{\"header\":\"4、哪些地方会用到反射？\",\"slug\":\"_4、哪些地方会用到反射\",\"contents\":[\"JDBC中，利用反射动态加载数据库驱动程序\",\"Web服务器中利用反射调用Servlet的服务方法\",\"框架用到反射机制，注入属性，调用方法，如Spring\"]},{\"header\":\"5、反射的实现方法？\",\"slug\":\"_5、反射的实现方法\",\"contents\":[\"Class.forName(“类的路径”)\",\"类名.class\",\"对象名.getClass()\",\"基本类型的包装类，可以调用包装类的Type属性来获得该包装类的Class对象\"]},{\"header\":\"6、实现Java反射的类\",\"slug\":\"_6、实现java反射的类\",\"contents\":[\"Class：表示正在运行的Java应用程序中的类和接口 注意： 所有获取对象的信息都需要Class类来实现。\",\"Field：提供有关类和接口的属性信息，以及对它的动态访问权限。\",\"Constructor：提供关于类的单个构造方法的信息以及它的访问权限\",\"Method：提供类或接口中某个方法的信息\"]},{\"header\":\"7、反射机制有哪些优缺点？\",\"slug\":\"_7、反射机制有哪些优缺点\",\"contents\":[\"优点：\",\"能够运行时动态获取类的实例，提高灵活性；\",\"与动态编译结合\",\"缺点：\",\"使用反射性能较低，需要解析字节码，将内存中的对象进行解析。\",\"解决方案：\",\"通过setAccessible(true)关闭JDK的安全检查来提升反射速度；\",\"多次创建一个类的实例时，有缓存会快很多\",\"ReflflectASM工具类，通过字节码生成的方式加快反射速度\",\"相对不安全，破坏了封装性（因为通过反射可以获得私有方法和属性）\"]},{\"header\":\"8、Java反射API\",\"slug\":\"_8、java反射api\",\"contents\":[\"反射 API 用来生成 JVM 中的类、接口或则对象的信息\",\"Class 类：反射的核心类，可以获取类的属性，方法等信息。\",\"Field 类：Java.lang.reflec 包中的类，表示类的成员变量，可以用来获取和设置类之中的属性 值。\",\"Method 类： Java.lang.reflec 包中的类，表示类的方法，它可以用来获取类中的方法信息或 者执行方法。\",\"Constructor 类： Java.lang.reflec 包中的类，表示类的构造方法。\"]},{\"header\":\"9、反射使用步骤（获取 Class 对象、调用对象方法）\",\"slug\":\"_9、反射使用步骤-获取-class-对象、调用对象方法\",\"contents\":[\"获取想要操作的类的 Class 对象，他是反射的核心，通过 Class 对象我们可以任意调用类的方法。\",\"调用 Class 类中的方法，既就是反射的使用阶段。\",\"使用反射 API 来操作这些信息。\"]},{\"header\":\"10、获取 Class 对象有几种方法\",\"slug\":\"_10、获取-class-对象有几种方法\",\"contents\":[\"调用某个对象的 getClass()方法\",\"Person p=new Person(); Class clazz=p.getClass(); \",\"调用某个类的 class 属性来获取该类对应的 Class 对象\",\"Class clazz=Person.class; \",\"使用 Class 类中的 forName()静态方法(最安全/性能最好)\",\"Class clazz=Class.forName(\\\"类的全路径\\\"); //最常用 \",\"当我们获得了想要操作的类的 Class 对象后，可以通过 Class 类中的方法获取并查看该类中的方法和属性。\",\"//获取 Person 类的 Class 对象 Class clazz=Class.forName(\\\"reflection.Person\\\"); //获取 Person 类的所有方法信息 Method[] method=clazz.getDeclaredMethods(); for(Method m:method){ System.out.println(m.toString()); } //获取 Person 类的所有成员属性信息 Field[] field=clazz.getDeclaredFields(); for(Field f:field){ System.out.println(f.toString()); } //获取 Person 类的所有构造方法信息 Constructor[] constructor=clazz.getDeclaredConstructors(); for(Constructor c:constructor){ System.out.println(c.toString()); } \"]},{\"header\":\"11、利用反射动态创建对象实例\",\"slug\":\"_11、利用反射动态创建对象实例\",\"contents\":[\"Class 对象的 newInstance()\",\"使用 Class 对象的 newInstance()方法来创建该 Class 对象对应类的实例，但是这种方法要求该 Class 对象对应的类有默认的空构造器。\",\"调用 Constructor 对象的 newInstance()\",\"先使用 Class 对象获取指定的 Constructor 对象，再调用 Constructor 对象的 newInstance()方法来创建 Class 对象对应类的实例,通过这种方法可以选定构造方法创建实例。\",\"//获取 Person 类的 Class 对象 Class clazz=Class.forName(\\\"reflection.Person\\\"); //使用.newInstane 方法创建对象 Person p=(Person) clazz.newInstance(); //获取构造方法并创建对象 Constructor c=clazz.getDeclaredConstructor(String.class,String.class,int.class); //创建对象并设置属性13/04/2018 Person p1=(Person) c.newInstance(\\\"李四\\\",\\\"男\\\",20); \"]}]},\"/interview/sourceCode/Spring-MVC-execution-flow.html\":{\"title\":\"\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"把整个流程分成三个阶段：准备阶段——> 匹配阶段——>执行阶段\"]},{\"header\":\"准备阶段\",\"slug\":\"准备阶段\",\"contents\":[\"在 Web 容器第一次用到 DispatcherServlet 的时候，会创建其对象并执行 init 方法\",\"init 方法内会创建 Spring Web 容器，并调用容器 refresh 方法\",\"refresh 过程中会创建并初始化 SpringMVC 中的重要组件， 例如 MultipartResolver，HandlerMapping，HandlerAdapter，HandlerExceptionResolver、ViewResolver 等\",\"DispatcherServlet源码分析\",\"protected void onRefresh(ApplicationContext context) { this.initStrategies(context); } protected void initStrategies(ApplicationContext context) { //初始化多部 请求解析器，没有默认的实现 this.initMultipartResolver(context); //初始化地域解析器,默认的实现为AcceptHeaderLocaleResolver this.initLocaleResolver(context); //初始化主题解析器，默认的实现为FixedThemeResolver this.initThemeResolver(context); //初始化处理器映射器，这是一个集合，默认的实现为 BeanNameUrlHandlerMapping this.initHandlerMappings(context); //初始化处理器适配器，这也是一个集合，默认的实现有HttpRequestHandlerAdapter、SimpleControllerHandlerAdapter this.initHandlerAdapters(context); //初始化异常解析器，也是一个集合 this.initHandlerExceptionResolvers(context); //初始化请求到视图名解析器，默认的实现为DefaultRequestToViewNameTranslator this.initRequestToViewNameTranslator(context); //初始化视图解析器，也是一个集合，InternalResourceViewResolver this.initViewResolvers(context); this.initFlashMapManager(context); } \",\"容器初始化后，会将上一步初始化好的重要组件，赋值给 DispatcherServlet 的成员变量，留待后用\"]},{\"header\":\"匹配阶段\",\"slug\":\"匹配阶段\",\"contents\":[\"用户发送的请求统一到达前端控制器 DispatcherServlet\",\"DispatcherServlet 遍历所有 HandlerMapping ，找到与路径匹配的处理器\",\"① HandlerMapping 有多个，每个 HandlerMapping 会返回不同的处理器对象，谁先匹配，返回谁的处理器。其中能识别 @RequestMapping 的优先级最高\",\"② 对应 @RequestMapping 的处理器是 HandlerMethod，它包含了控制器对象和控制器方法信息\",\"③ 其中路径与处理器的映射关系在 HandlerMapping 初始化时就会建立好\",\"将 HandlerMethod 连同匹配到的拦截器，生成调用链对象 HandlerExecutionChain 返回\",\"遍历HandlerAdapter 处理器适配器，找到能处理 HandlerMethod 的适配器对象，开始调用\"]},{\"header\":\"调用阶段\",\"slug\":\"调用阶段\",\"contents\":[\"执行拦截器 preHandle\",\"由 HandlerAdapter 调用 HandlerMethod\",\"① 调用前处理不同类型的参数\",\"② 调用后处理不同类型的返回值\",\"第 2 步没有异常\",\"① 返回 ModelAndView\",\"② 执行拦截器 postHandle 方法\",\"③ 解析视图，得到 View 对象，进行视图渲染\",\"第 2 步有异常，进入 HandlerExceptionResolver 异常处理流程\",\"最后都会执行拦截器的 afterCompletion 方法\",\"如果控制器方法标注了 @ResponseBody 注解，则在第 2 步，就会生成 json 结果，并标记 ModelAndView 已处理，这样就不会执行第 3 步的视图渲染\"]}]},\"/study-tutorial/advanced/designPatterns.html\":{\"title\":\"\",\"contents\":[{\"header\":\"1、设计模式的目的\",\"slug\":\"_1、设计模式的目的\",\"contents\":[\"编写软件过程中，程序员面临着来自耦合性，内聚性以及可维护性，可扩展性，重用性，灵活性等多方面的，挑战设计模式是为 了让程序软件，具有更好：\",\"代码重用性（相同功能的代码，不用多次编写）\",\"可读性（编程规范性，便于其他程序员的阅读和理解）\",\"可扩展性（当需要增加新的功能时，非常的方便，称为可维护性）\",\"可靠性（当我们增加新的功能时，对原来的功能没有影响）\",\"使程序呈现高内聚、低耦合的特性\",\"设计模式包含了面向对象的精髓 懂了设计模式，你就懂了面向对象分析和设计（OOA/D ）的精要\"]},{\"header\":\"2、设计模式的七大原则\",\"slug\":\"_2、设计模式的七大原则\",\"contents\":[\"设计模式原则，其实就是程序员在编程时，应当遵守的原则， 也是各种设计模式的基础 即： 设计模式为什么这样设计的依据\",\"单一职责原则\",\"接口隔离原则\",\"依赖倒转（倒置）原则\",\"里氏替换原则\",\"开闭原则\",\"迪米特法则\",\"合成复用原则\"]},{\"header\":\"1、单一职责原则\",\"slug\":\"_1、单一职责原则\",\"contents\":[\"对于一个类来说，一个类应该只负责一个职责。如类 A 负 责两个不同职责：职责1 ，职责2 。当职责1需求变更而改变A时 ，可能造成职 责 2 执行错误， 所以需要将类A的粒度分解 为A1、A2\"]},{\"header\":\"1、案例说明\",\"slug\":\"_1、案例说明\",\"contents\":[\"以交通工具为例进行说明\"]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1\",\"contents\":[\"public class SingleResponsibility { public static void main(String[] args) { Vehicle vehicle = new Vehicle(); vehicle.run(\\\"摩托\\\"); vehicle.run(\\\"汽车\\\"); //违反了单一职责原则 vehicle.run(\\\"飞机\\\"); } static class Vehicle{ public void run(String vehicle) { System.out.println(vehicle + \\\"在公路上跑！！！\\\"); } } } \",\"分析：\",\"vehicle.run(\\\"飞机\\\"); \",\"与其他两个交通工具的方式不同，违反了单一职责\",\"解决：根据交通方式的运行方法不同，分解成不同的类即可\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2\",\"contents\":[\"public class SingleResponsibility1 { public static void main(String[] args) { RoadVehicle roadVehicle = new RoadVehicle(); roadVehicle.run(\\\"汽车\\\"); AirVehicle vehicle = new AirVehicle(); vehicle.run(\\\"飞机\\\"); WaterVehicle waterVehicle = new WaterVehicle(); waterVehicle.run(\\\"船\\\"); } static class RoadVehicle{ public void run(String vehicle) { System.out.println(vehicle + \\\"在公路上运行！！！\\\"); } } static class AirVehicle{ public void run(String vehicle) { System.out.println(vehicle + \\\"在天空上运行！！！\\\"); } } static class WaterVehicle{ public void run(String vehicle) { System.out.println(vehicle + \\\"在水里上运行！！！\\\"); } } } \",\"分析：\",\"案例2遵守了单一职责原则，但这样的改动很大，将类分解，同时也要需要客户端\",\"改进：直接修改Vehicle，改动的代码会比较少==》案例3\"]},{\"header\":\"3、案例3\",\"slug\":\"_3、案例3\",\"contents\":[\"public class SingleResponsibility2 { public static void main(String[] args) { Vehicle2 vehicle2 = new Vehicle2(); vehicle2.run(\\\"汽车\\\"); vehicle2.runRir(\\\"飞机\\\"); vehicle2.runWater(\\\"船\\\"); } static class Vehicle2{ public void run(String vehicle) { System.out.println(vehicle + \\\"在公路上运行！！！\\\"); } public void runRir(String vehicle) { System.out.println(vehicle + \\\"在天空上运行！！！\\\"); } public void runWater(String vehicle) { System.out.println(vehicle + \\\"在水里上运行！！！\\\"); } } } \",\"分析：\",\"这种修改方法没有改动很大，只是增加了方法。\",\"但类级别上没有遵循单一职责原则，方法级别上遵守单一职责原则\"]},{\"header\":\"2、注意事项和细节\",\"slug\":\"_2、注意事项和细节\",\"contents\":[\"降低类的复杂度，一个类只负责一项职责\",\"提高类的可读性，可维护性\",\"降低变更引起的风险\",\"通常情况下，我们应当遵守单一职责原则，只有逻辑足够简单，才可以在代码级别违反单一职责原则；只有类中方法数量足够少，可以在方法级别保持单一职责原则\"]},{\"header\":\"2、接口隔离原则\",\"slug\":\"_2、接口隔离原则\",\"contents\":[\"客户端不应该依赖它不需要的接口，即一个类对另一个类的依赖应该建立在最小的接口上\"]},{\"header\":\"1、案例说明\",\"slug\":\"_1、案例说明-1\",\"contents\":[\"类 A 通过接口 Interface1 依赖类 B ，类 C 通过接口 Interface1 依赖类 D ，如果接口Interface1 对于类 A 和类 C 来说不是最小接口，那么类 B 和类 D 必须去实现他们不需要的方法。\"]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1-1\",\"contents\":[\"public class Segregation1 { public static void main(String[] args) { } } interface interface1{ void operation1(); void operation2(); void operation3(); void operation4(); void operation5(); } class B implements interface1{ public void operation1() { System.out.println(\\\"B 实现了operation1\\\"); } public void operation2() { System.out.println(\\\"B 实现了operation2\\\"); } public void operation3() { System.out.println(\\\"B 实现了operation3\\\"); } public void operation4() { System.out.println(\\\"B 实现了operation4\\\"); } public void operation5() { System.out.println(\\\"B 实现了operation5\\\"); } } class D implements interface1{ public void operation1() { System.out.println(\\\"D 实现了operation1\\\"); } public void operation2() { System.out.println(\\\"D 实现了operation2\\\"); } public void operation3() { System.out.println(\\\"D 实现了operation3\\\"); } public void operation4() { System.out.println(\\\"D 实现了operation4\\\"); } public void operation5() { System.out.println(\\\"D 实现了operation5\\\"); } } class A { /**A 类通过接口Interface1 依赖(使用) B 类，但是只会用到1,2,3 方法*/ public void depend1(interface1 i) { i.operation1(); } public void depend2(interface1 i) { i.operation2(); } public void depend3(interface1 i) { i.operation3(); } } class C { /** C 类通过接口Interface1 依赖(使用) D 类，但是只会用到1,4,5 方法 */ public void depend1(interface1 i) { i.operation1(); } public void depend4(interface1 i) { i.operation4(); } public void depend5(interface1 i) { i.operation5(); } } \",\"分析：\",\"类 A 通过接口 Interface1 依赖类 B ，类 C 通过接口 Interface1 依赖类 D ，如果接口Interface1 对于类 A 和类 C 来说不是最小接口，那么类 B 和类 D 必须去实现他们不需要的方法。\",\"将 接口 Interface1 拆分为独立的几个接口，类 A 和类 C 分别与他们需要的接口建立依赖关系。也就是采用接口隔离原则\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2-1\",\"contents\":[\"public class Segregation2 { public static void main(String[] args) { A a = new A(); a.depend1(new B()); a.depend2(new B()); a.depend3(new B()); //C类通过接口去依赖D类 C c = new C(); c.depend1(new D()); c.depend4(new D()); c.depend5(new D()); } } interface Interface1{ void operation1(); } interface Interface2{ void operation2(); void operation3(); } interface Interface3{ void operation4(); void operation5(); } class B implements Interface1 ,Interface2,Interface3{ public void operation1() { System.out.println(\\\"B 实现了operation1\\\"); } public void operation2() { System.out.println(\\\"B 实现了operation2\\\"); } public void operation3() { System.out.println(\\\"B 实现了operation3\\\"); } public void operation4() { System.out.println(\\\"B 实现了operation4\\\"); } public void operation5() { System.out.println(\\\"B 实现了operation5\\\"); } } class D implements Interface1 ,Interface2,Interface3{ public void operation1() { System.out.println(\\\"D 实现了operation1\\\"); } public void operation2() { System.out.println(\\\"D 实现了operation2\\\"); } public void operation3() { System.out.println(\\\"D 实现了operation3\\\"); } public void operation4() { System.out.println(\\\"D 实现了operation4\\\"); } public void operation5() { System.out.println(\\\"D 实现了operation5\\\"); } } class A { /**A 类通过接口Interface1 依赖(使用) B 类，但是只会用到1,2,3 方法*/ public void depend1(Interface1 i) { i.operation1(); } public void depend2(Interface2 i) { i.operation2(); } public void depend3(Interface2 i) { i.operation3(); } } class C { /** C 类通过接口Interface1 依赖(使用) D 类，但是只会用到1,4,5 方法 */ public void depend1(Interface1 i) { i.operation1(); } public void depend4(Interface3 i) { i.operation4(); } public void depend5(Interface3 i) { i.operation5(); } } \"]},{\"header\":\"3、依赖倒转原则\",\"slug\":\"_3、依赖倒转原则\",\"contents\":[\"依赖倒转原则Dependence Inversion Principle 是指：\",\"高层模块不应该依赖低层模块，二者都应该依赖其抽象\",\"抽象不应该依赖细节，细节应该依赖抽象\",\"依赖倒转（倒置）的中心思想是面向接口编程\",\"设计理念：相对于细节的多变性，抽象的东西要稳定的多。以抽象为基础搭建的架构要比以细节为基础的架构要稳定的多。在Java中，抽象指的是接口或抽象类，细节就是具体的实现类。\",\"使用接口或抽象类的目的是：制定好规范，而不涉及任何具体的操作，把展现的细节的任务交给他们的实现类去完成\"]},{\"header\":\"1、案例说明\",\"slug\":\"_1、案例说明-2\",\"contents\":[\"Person完成接收消息\"]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1-2\",\"contents\":[\"public class DependenceInversion { public static void main(String[] args) { Person person = new Person(); person.receive(new Email()); } static class Email{ public String getInfo(){ return \\\"发送成功！\\\"; } } static class Person{ public void receive(Email email){ System.out.println(email.getInfo()); } } } \",\"分析：\",\"案例1是比较简单，很容易想到的\",\"如果我们获取的对象是微信或者QQ等消息，则需要新增类，同时Person也要增加相应的方法。\",\"解决思路：引入一个抽象的接口IReceive，表示接收者，这样Person类与接口IReceiver发生依赖，因为Email, WeiXin 等等属于接收的范围，他们各自实现IReceiver 接口就ok, 这样我们就符号依赖倒转原则\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2-2\",\"contents\":[\"public class DependenceInversion { public static void main(String[] args) { Person person = new Person(); person.receive(new Email()); person.receive(new WeChat()); } static class Email implements IReceive{ public String getInfo(){ return \\\"发送成功！\\\"; } } static class WeChat implements IReceive{ public String getInfo(){ return \\\"WeChat发送成功！\\\"; } } interface IReceive{ public String getInfo(); } static class Person { public void receive(IReceive iReceive){ System.out.println(iReceive.getInfo()); } } } \"]},{\"header\":\"2、依赖关系传递的三种方式\",\"slug\":\"_2、依赖关系传递的三种方式\",\"contents\":[]},{\"header\":\"1、接口传递\",\"slug\":\"_1、接口传递\",\"contents\":[\"public class DependencyPassByInterface { public static void main(String[] args) { HuaWei huaWei = new HuaWei(); TvOpen iTvOpen = new TvOpen(); iTvOpen.open(huaWei); } interface ITv{ public void play(); } interface ITvOpen{ public void open(ITv iTv); } static class HuaWei implements ITv{ public void play() { System.out.println(\\\"鸿蒙电视牛逼！\\\"); } } static class TvOpen implements ITvOpen{ public void open(ITv iTv) { iTv.play(); } } } \"]},{\"header\":\"2、构造方法传递\",\"slug\":\"_2、构造方法传递\",\"contents\":[\"public class DependencyPassByConstructor { public static void main(String[] args) { HuaWei huaWei = new HuaWei(); TvOpen tvOpen = new TvOpen(huaWei); tvOpen.open(); } interface ITv{ public void play(); } interface ITvOpen{ public void open(); } static class HuaWei implements ITv{ public void play() { System.out.println(\\\"鸿蒙电视牛逼！\\\"); } } static class TvOpen implements ITvOpen{ public ITv iTv; public TvOpen(ITv iTv){ this.iTv = iTv; } public void open() { this.iTv.play(); } } } \"]},{\"header\":\"3、setter方法传递\",\"slug\":\"_3、setter方法传递\",\"contents\":[\"public class DependencyPassBySetter { public static void main(String[] args) { HuaWei huaWei = new HuaWei(); TvOpen tvOpen = new TvOpen(); tvOpen.setTv(huaWei); tvOpen.open(); } interface ITv{ public void play(); } interface ITvOpen{ public void open(); public void setTv(ITv iTv); } static class HuaWei implements ITv{ public void play() { System.out.println(\\\"鸿蒙电视牛逼！\\\"); } } static class TvOpen implements ITvOpen{ public ITv iTv; public void setTv(ITv iTv) { this.iTv = iTv; } public void open() { this.iTv.play(); } } } \"]},{\"header\":\"3、注意事项和细节\",\"slug\":\"_3、注意事项和细节\",\"contents\":[\"底层模块尽量都要有抽象类或接口，或者两者都有，程序稳定性更好\",\"变量的声明类型尽量是抽象类或接口，这样我们的变量引用和实际对象间，就存在一个缓冲层，利于程序扩展和优化\",\"继承时遵循里氏替换原则\"]},{\"header\":\"4、里氏替换原则\",\"slug\":\"_4、里氏替换原则\",\"contents\":[]},{\"header\":\"1、OO中的继承性的思考和说明\",\"slug\":\"_1、oo中的继承性的思考和说明\",\"contents\":[\"继承包含这样一层含义：父类中凡是已经实现好的方法， 实际上是在设定规范和契约，虽然它不强制要求所有的子类必须遵循这些契约，但是如果子类对这些已经实现的方法任意修改，就会对整个继承体系造成破坏 。\",\"继承在 给程序设计带 来 便利 的 同时，也带来了弊端。比如使用继承会给程序带来侵入性，程序的可移植性降低，增加对象间的耦合性，如果一个类被其他的类所继承，则当这个类需要修改时，必须考虑到所有的子类，并且父类修改后，所有涉及到子类的功能都有可能产生故障。\",\"问题提出：在编程中，如何正确的使用继承 ? => 里氏替换原则\"]},{\"header\":\"2、基本介绍\",\"slug\":\"_2、基本介绍\",\"contents\":[\"里氏替换原则 Liskov Substitution Principle 在 1988 年，由麻省理工学院的以为姓里的女士提出的 。\",\"如果对每个类型为T1的对象o1 ，都有类型为 T2 的对象 o2 ，使得以 T1 定义的所有程序P 在所有的对象 o1 都代换成 o2 时，程序 P 的行为没有发生变化，那么类型 T2 是类型 T1的子类型 。换句话说，所有引用基类的地方必须能透明地使用其子类的对象 。\",\"在 使用继承时，遵循里氏替换原则，在子类中尽量不要重写父类的方法\",\"里氏替换原则告诉我们，继承实际上让两个类耦合性增强了，在适当的情况下，可以通过聚合，组合，依赖来解决问题。\"]},{\"header\":\"3、案例说明\",\"slug\":\"_3、案例说明\",\"contents\":[]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1-3\",\"contents\":[\"public class Liskov { public static void main(String[] args) { A a = new A(); System.out.println(\\\"10 - 4 = \\\" + a.fun1(10, 4)); B b = new B(); System.out.println(\\\"10 - 4 = \\\" + b.fun1(10, 4)); System.out.println(\\\"11 + 3 = \\\" + b.fun2(11, 3)); } } class A{ public int fun1(int a, int b){ return a - b; } } class B extends A{ //类B继承类A 重写了fun1方法 @Override public int fun1(int a, int b){ return a + b; } public int fun2(int a, int b){ return a - b; } } \",\"分析：\",\"我们发现原来运行正常的相减功能发生了错误。原因就是类B 无意中重写了父类的方法，造成原有功能出现错误。在实际编程中，我们常常会通过重写父类的方法完成新的功能，这样写起来虽然简单，但整个继承体系的复用性会比较差。特别是运行多态比较频繁的时候\",\"通用的做法是：原来的父类和子类都继承一个更通俗的基类，原有的继承关系去掉，采用依赖，聚合，组合等关系代替.\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2-3\",\"contents\":[\"public class Liskov { public static void main(String[] args) { A a = new A(); System.out.println(\\\"10 - 4 = \\\" + a.fun1(10, 4)); B b = new B(); System.out.println(\\\"10 - 4 = \\\" + b.fun1(10, 4)); System.out.println(\\\"10 - 4 = \\\" + b.fun3(10, 4)); } } class Base{ //把更加基础的方法和成员写到Base 类 } class A extends Base{ public int fun1(int a, int b){ return a - b; } } class B extends Base{ private A a = new A(); public int fun1(int a, int b){ return a + b; } public int fun2(int a, int b){ return a - b; } /** * 调用A类的方法 * @param a * @param b * @return */ public int fun3(int a, int b){ return this.a.fun1(a, b); } } \"]},{\"header\":\"5、开闭原则\",\"slug\":\"_5、开闭原则\",\"contents\":[\"开闭原则（ Open Closed Principle 是编程中最基础、最重要的设计原则\",\"一个软件实体如类，模块和函数应该对扩展开放(对提供方)，对修改关闭(对使用方)。用抽象构建框架，用实现扩展细节。\",\"当软件需要变化时，尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。\",\"编程中遵循其它原则，以及使用设计模式的目的就是遵循开闭原则。\"]},{\"header\":\"1、案例说明\",\"slug\":\"_1、案例说明-3\",\"contents\":[]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1-4\",\"contents\":[\"public class Ocp { public static void main(String[] args) { GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawRectangle(new Rectangle()); graphicEditor.drawRectangle(new CieCle()); } } class GraphicEditor{ public void drawShape(Shape s){ if (s.type == 1){ drawCieCle(s); }else if(s.type == 2){ drawRectangle(s); } } public void drawRectangle(Shape r){ System.out.println(\\\"绘制矩形\\\"); } public void drawCieCle(Shape r){ System.out.println(\\\"绘制圆形\\\"); } } class Shape{ int type; } class Rectangle extends Shape{ Rectangle(){ super.type = 1; } } class CieCle extends Shape{ CieCle(){ super.type = 2; } } \",\"分析：\",\"优点是比较好理解，简单易操作\",\"缺点是违反了设计模式的ocp 原则，即对扩展开放(提供方)，对修改关闭(使用方)。即当我们给类增加新功能的时候，尽量不修改代码，或者尽可能少修改代码.\",\"比如我们这时要新增加一个图形种类三角形，我们需要做如下修改，修改的地方较多\",\"改进的思路：\",\"把创建Shape类做成抽象类，并提供一个抽象的draw方法，让子类去实现即可，这样有新的图形种类时，只需要让新的图形继承Shape，并实现draw方法即可，使用方的代码就不需要修改，满足开闭原则\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2-4\",\"contents\":[\"public class Ocp { public static void main(String[] args) { GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawShape(new CieCle()); graphicEditor.drawShape(new Rectangle()); graphicEditor.drawShape(new Triangle()); } } class GraphicEditor{ public void drawShape(Shape s){ s.draw(); } } abstract class Shape{ int type; public abstract void draw(); } class Rectangle extends Shape{ Rectangle(){ super.type = 1; } @Override public void draw() { System.out.println(\\\"绘制矩形\\\"); } } class CieCle extends Shape{ CieCle(){ super.type = 2; } @Override public void draw() { System.out.println(\\\"绘制圆形\\\"); } } class Triangle extends Shape{ Triangle(){ super.type = 3; } @Override public void draw() { System.out.println(\\\"绘制三角形\\\"); } } \"]},{\"header\":\"6、迪米特原则\",\"slug\":\"_6、迪米特原则\",\"contents\":[]},{\"header\":\"1、基本介绍\",\"slug\":\"_1、基本介绍\",\"contents\":[\"一个对象应该对其他对象保持最少的了解\",\"类与类关系越密切，耦合度越大\",\"迪米特法则(Demeter Principle)又叫最少知道原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供的public 方法，不对外泄露任何信息\",\"迪米特法则还有个更简单的定义：只与直接的朋友通信\",\"直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多，依赖，关联，组合，聚合等。其中，我们称出现成员变量，方法参数，方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部。\"]},{\"header\":\"2、案例说明\",\"slug\":\"_2、案例说明\",\"contents\":[]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1-5\",\"contents\":[\"public class Demeter1 { public static void main(String[] args) { //创建了一个SchoolManager 对象 SchoolManager schoolManager = new SchoolManager(); //输出学院的员工id 和学校总部的员工信息 schoolManager.printAllEmployee(new CollegeManager()); } } //学校总部员工类 class Employee { private String id; public void setId(String id) { this.id = id; } public String getId() { return id; } } //学院的员工类 class CollegeEmployee { private String id; public void setId(String id) { this.id = id; } public String getId() { return id; } } /** * 管理学院员工的管理类 */ class CollegeManager { /** * 返回学院的所有员工 * @return */ public List<CollegeEmployee> getAllEmployee() { List<CollegeEmployee> list = new ArrayList<CollegeEmployee>(); //这里我们增加了10 个员工到list for (int i = 0; i < 10; i++) { CollegeEmployee emp = new CollegeEmployee(); emp.setId(\\\"学院员工id= \\\" + i); list.add(emp); } return list; } } //分析SchoolManager 类的直接朋友类有哪些Employee、CollegeManager //CollegeEmployee 不是直接朋友而是一个陌生类，这样违背了迪米特法则 class SchoolManager { //返回学校总部的员工 public List<Employee> getAllEmployee() { List<Employee> list = new ArrayList<Employee>(); for (int i = 0; i < 5; i++) { //这里我们增加了5 个员工到list Employee emp = new Employee(); emp.setId(\\\"学校总部员工id= \\\" + i); list.add(emp); } return list; } //该方法完成输出学校总部和学院员工信息(id) void printAllEmployee(CollegeManager sub) { //分析问题 //1. 这里的CollegeEmployee 不是SchoolManager 的直接朋友 //2. CollegeEmployee 是以局部变量方式出现在SchoolManager //3. 违反了迪米特法则 //获取到学院员工 List<CollegeEmployee> list1 = sub.getAllEmployee(); System.out.println(\\\"------------学院员工------------\\\"); for (CollegeEmployee e : list1) { System.out.println(e.getId()); } //获取到学校总部员工 List<Employee> list2 = this.getAllEmployee(); System.out.println(\\\"------------学校总部员工------------\\\"); for (Employee e : list2) { System.out.println(e.getId()); } } } \",\"分析：\",\"前面设计的问题在于SchoolManager 中，CollegeEmployee 类并不是SchoolManager 类的直接朋友(分析)\",\"按照迪米特法则，应该避免类中出现这样非直接朋友关系的耦合\",\"对代码按照迪米特法则进行改进\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2-5\",\"contents\":[\"public class Demeter { public static void main(String[] args) { System.out.println(\\\"~~~使用迪米特法则的改进~~~\\\"); //创建了一个SchoolManager 对象 SchoolManager schoolManager = new SchoolManager(); //输出学院的员工id 和学校总部的员工信息 schoolManager.printAllEmployee(new CollegeManager()); } } //学校总部员工类 class Employee { private String id; public void setId(String id) { this.id = id; } public String getId() { return id; } } //学院的员工类 class CollegeEmployee { private String id; public void setId(String id) { this.id = id; } public String getId() { return id; } } //管理学院员工的管理类 class CollegeManager { //返回学院的所有员工 public List<CollegeEmployee> getAllEmployee() { List<CollegeEmployee> list = new ArrayList<CollegeEmployee>(); for (int i = 0; i < 10; i++) { //这里我们增加了10 个员工到list CollegeEmployee emp = new CollegeEmployee(); emp.setId(\\\"学院员工id= \\\" + i); list.add(emp); } return list; } //输出学院员工的信息 public void printEmployee() { //获取到学院员工 List<CollegeEmployee> list1 = getAllEmployee(); System.out.println(\\\"------------学院员工------------\\\"); for (CollegeEmployee e : list1) { System.out.println(e.getId()); } } } //学校管理类 //分析SchoolManager 类的直接朋友类有哪些Employee、CollegeManager //CollegeEmployee 不是直接朋友而是一个陌生类，这样违背了迪米特法则 class SchoolManager { //返回学校总部的员工 public List<Employee> getAllEmployee() { List<Employee> list = new ArrayList<Employee>(); for (int i = 0; i < 5; i++) { //这里我们增加了5 个员工到list Employee emp = new Employee(); emp.setId(\\\"学校总部员工id= \\\" + i); list.add(emp); } return list; } //该方法完成输出学校总部和学院员工信息(id) void printAllEmployee(CollegeManager sub) { //分析问题 //1. 将输出学院的员工方法，封装到CollegeManager sub.printEmployee(); //获取到学校总部员工 List<Employee> list2 = this.getAllEmployee(); System.out.println(\\\"------------学校总部员工------------\\\"); for (Employee e : list2) { System.out.println(e.getId()); } } } \"]},{\"header\":\"3、注意事项和细节\",\"slug\":\"_3、注意事项和细节-1\",\"contents\":[\"迪米特法则的核心是降低类之间的耦合\",\"但是注意：由于每个类都减少了不必要的依赖，因此迪米特法则只是要求降低类间(对象间)耦合关系， 并不是要求完全没有依赖关系\"]},{\"header\":\"7、合成复用原则\",\"slug\":\"_7、合成复用原则\",\"contents\":[\"原则是尽量使用合成/聚合的方式，而不是使用继承\"]},{\"header\":\"3、设计模式的核心思想\",\"slug\":\"_3、设计模式的核心思想\",\"contents\":[\"找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。\",\"针对接口编程，而不是针对实现编程。\",\"为了交互对象之间的松耦合设计而努力\"]},{\"header\":\"4、设计模式的分类\",\"slug\":\"_4、设计模式的分类\",\"contents\":[\"设计模式是程序员在面对同类软件工程设计问题所总结出来的有用的经验，模式不是代码，而是某类问题的通用解决方案，设计模式（Design pattern）代表了最佳的实践。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。\",\"设计模式的本质提高软件的维护性，通用性和扩展性，并降低软件的复杂度。\",\"<<设计模式>> 是经典的书，作者是Erich Gamma、Richard Helm、Ralph Johnson 和John Vlissides Design（俗称“四人组GOF”）\",\"模式 & 描述\",\"包括\",\"创建型模式 这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。\",\"工厂模式（Factory Pattern）抽象工厂模式（Abstract Factory Pattern）单例模式（Singleton Pattern）建造者模式（Builder Pattern）原型模式（Prototype Pattern）\",\"结构型模式 这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。\",\"适配器模式（Adapter Pattern）桥接模式（Bridge Pattern）过滤器模式（Filter、Criteria Pattern）组合模式（Composite Pattern）装饰器模式（Decorator Pattern）外观模式（Facade Pattern）享元模式（Flyweight Pattern）代理模式（Proxy Pattern）\",\"行为型模式 这些设计模式特别关注对象之间的通信。\",\"责任链模式（Chain of Responsibility Pattern）命令模式（Command Pattern）解释器模式（Interpreter Pattern）迭代器模式（Iterator Pattern）中介者模式（Mediator Pattern）备忘录模式（Memento Pattern）观察者模式（Observer Pattern）状态模式（State Pattern）空对象模式（Null Object Pattern）策略模式（Strategy Pattern）模板模式（Template Pattern）访问者模式（Visitor Pattern）\",\"J2EE 模式 这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。\",\"MVC 模式（MVC Pattern）业务代表模式（Business Delegate Pattern）组合实体模式（Composite Entity Pattern）数据访问对象模式（Data Access Object Pattern）前端控制器模式（Front Controller Pattern）拦截过滤器模式（Intercepting Filter Pattern）服务定位器模式（Service Locator Pattern）传输对象模式（Transfer Object Pattern）\"]},{\"header\":\"5、单例模式\",\"slug\":\"_5、单例模式\",\"contents\":[]},{\"header\":\"1、单例模式介绍\",\"slug\":\"_1、单例模式介绍\",\"contents\":[\"所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法(静态方法)。\",\"比如Hibernate 的SessionFactory，它充当数据存储源的代理，并负责创建Session 对象。SessionFactory 并不是轻量级的，一般情况下，一个项目通常只需要一个SessionFactory 就够，这是就会使用到单例模式。\"]},{\"header\":\"2、单例模式的种类\",\"slug\":\"_2、单例模式的种类\",\"contents\":[\"饿汉式（静态常量）\",\"饿汉式（静态代码块）\",\"懒汉式（线程不安全）\",\"懒汉式（线程安全，同步方法）\",\"懒汉式（线程安全，同步代码块）\",\"双重检查\",\"静态内部类\",\"枚举\"]},{\"header\":\"1、饿汉式（静态常量）\",\"slug\":\"_1、饿汉式-静态常量\",\"contents\":[\"步骤：\",\"构造方法私有化\",\"内的内部创建对象\",\"向外暴露一个静态的公共方法\",\"public class Singleton01 { private static final Singleton01 INSTANCE = new Singleton01(); /** * 构造方法私有化，防止new */ private Singleton01(){ } /** * 提供一个公有的静态方法，返回实例对象 * @return INSTANCE */ public static Singleton01 getINSTANCE() { return INSTANCE; } public static void main(String[] args) { Singleton01 instance1 = Singleton01.getINSTANCE(); Singleton01 instance2 = Singleton01.getINSTANCE(); System.out.println(instance1 == instance2); System.out.println(\\\"instance1.hashCode = \\\" + instance1.hashCode()); System.out.println(\\\"instance2.hashCode = \\\" + instance2.hashCode()); } } \",\"优缺点：\",\"优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。\",\"缺点：在类装载的时候就完成实例化，没有达到Lazy Loading 的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费\",\"这种方式基于classloder 机制避免了多线程的同步问题，不过，instance 在类装载时就实例化，在单例模式中大多数都是调用getInstance 方法，但是导致类装载的原因有很多种，因此不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化instance 就没有达到lazy loading 的效果\"]},{\"header\":\"破坏单例的情形\",\"slug\":\"破坏单例的情形\",\"contents\":[\"反射破坏单例：利用反射创建实例对象\",\"反序列化破坏单例：前提是实现了implements Serializable接口\",\"Unsafe 破坏单例：这种情形是不能避免的\"]},{\"header\":\"1、反射破坏单例\",\"slug\":\"_1、反射破坏单例\",\"contents\":[\"利用反射获取类的构造方法\",\"private static void reflection(Class<?> clazz) throws NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException { Constructor<?> constructor = clazz.getDeclaredConstructor(); constructor.setAccessible(true); System.out.println(\\\"反射创建实例:\\\" + constructor.newInstance()); } \",\"如何避免呢？👇\",\"调用的时候加一层判断，如果创建了，则返回原实例或者抛出异常\",\"private Singleton1() { if (INSTANCE != null) { throw new RuntimeException(\\\"单例对象不能重复创建\\\"); } System.out.println(\\\"private Singleton1()\\\"); } \"]},{\"header\":\"2、反序列化破坏单例\",\"slug\":\"_2、反序列化破坏单例\",\"contents\":[\"通过对象输出流将对象转成为字节流，再通过ByteArrayInputStream方法将字节流转为对象\",\"private static void serializable(Object instance) throws IOException, ClassNotFoundException { ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(instance); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bos.toByteArray())); System.out.println(\\\"反序列化创建实例:\\\" + ois.readObject()); } \",\"如何避免呢？👇\",\"//方法名是固定的 public Object readResolve() { return INSTANCE; } \"]},{\"header\":\"3、Unsafe 破坏单例\",\"slug\":\"_3、unsafe-破坏单例\",\"contents\":[\"通过反射调用JDK内置的Unsafe方法破坏单例\",\"private static void unsafe(Class<?> clazz) throws InstantiationException { Object o = UnsafeUtils.getUnsafe().allocateInstance(clazz); System.out.println(\\\"Unsafe 创建实例:\\\" + o); } \",\"如何避免呢？👇\",\"暂时还没有找到解决的方法，大佬们知道如何解决吗？\",\"总结：这种单例模式可用，可能造成内存浪费，启动就创建对象实例\"]},{\"header\":\"2、饿汉式（静态代码块）\",\"slug\":\"_2、饿汉式-静态代码块\",\"contents\":[\"public class Singleton02 { private static final Singleton02 INSTANCE ; /** * 在静态代码块中，创建单例对象 */ static { INSTANCE = new Singleton02(); } /** * 构造方法私有化，防止new */ private Singleton02(){ } /** * 提供一个公有的静态方法，返回实例对象 * @return INSTANCE */ public static Singleton02 getINSTANCE() { return INSTANCE; } public static void main(String[] args) { Singleton02 instance1 = Singleton02.getINSTANCE(); Singleton02 instance2 = Singleton02.getINSTANCE(); System.out.println(instance1 == instance2); System.out.println(\\\"instance1.hashCode = \\\" + instance1.hashCode()); System.out.println(\\\"instance2.hashCode = \\\" + instance2.hashCode()); } } \",\"优缺点：\",\"这种方式和上面的方式其实类似，只不过将类实例化的过程放在了静态代码块中，也是在类装载的时候，就执行静态代码块中的代码，初始化类的实例。优缺点和上面是一样的。\",\"总结：这种单例模式可用，但是可能造成内存浪费\"]},{\"header\":\"3、懒汉式（线程不安全）\",\"slug\":\"_3、懒汉式-线程不安全\",\"contents\":[\"public class Singleton03 { private static Singleton03 INSTANCE ; /** * 构造方法私有化，防止new */ private Singleton03(){ } /** * 提供一个静态的公有方法，当使用到该方法时，才去创建instance * @return INSTANCE */ public static Singleton03 getINSTANCE() { if (INSTANCE == null) { INSTANCE = new Singleton03(); } return INSTANCE; } public static void main(String[] args) { //模拟1000个线程访问 for (int i = 0; i < 1000; i++) { new Thread(new Runnable() { @Override public void run() { System.out.println(\\\"instance.hashCode= \\\" + Singleton03.getINSTANCE().hashCode()); } }).start(); } } } \",\"优缺点：\",\"达到了Lazy Loading 的效果，但是只能在单线程下使用。\",\"如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式。\",\"结论：在实际开发中，不要使用这种方式.\"]},{\"header\":\"4、懒汉式（线程安全，同步方法）\",\"slug\":\"_4、懒汉式-线程安全-同步方法\",\"contents\":[\"public class Singleton04 { private static Singleton04 INSTANCE ; /** * 构造方法私有化，防止new */ private Singleton04(){ } /** * 提供一个静态的公有方法，加入同步处理的代码，解决线程安全问题 * @return INSTANCE */ public static synchronized Singleton04 getINSTANCE() { if (INSTANCE == null) { INSTANCE = new Singleton04(); } return INSTANCE; } public static void main(String[] args) { for (int i = 0; i < 1000; i++) { new Thread(() ->{ System.out.println(\\\"instance.hashCode= \\\" + Singleton03.getINSTANCE().hashCode()); }).start(); } } } \",\"优缺点：\",\"解决了线程安全问题，给公有方法加上了锁\",\"效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return 就行了。方法进行同步效率太低\",\"结论：在实际开发中，不推荐使用这种方式\"]},{\"header\":\"5、懒汉式（线程安全，同步代码块）\",\"slug\":\"_5、懒汉式-线程安全-同步代码块\",\"contents\":[\"public class Singleton05 { private static Singleton05 INSTANCE ; /** * 构造方法私有化，防止new */ private Singleton05(){ } /** * 提供一个静态的公有方法，加入同步处理的代码，解决线程安全问题 * @return INSTANCE */ public static Singleton05 getINSTANCE() { if (INSTANCE == null) { //同步代码块 synchronized (Singleton03.class){ INSTANCE = new Singleton05(); } } return INSTANCE; } public static void main(String[] args) { for (int i = 0; i < 1000; i++) { new Thread(() ->{ System.out.println(\\\"instance.hashCode= \\\" + Singleton03.getINSTANCE().hashCode()); }).start(); } } } \",\"优缺点：\",\"通过试图同步代码块来提高效率，解决线程安全\",\"但事实上不可行，如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式。\"]},{\"header\":\"6、双重检查\",\"slug\":\"_6、双重检查\",\"contents\":[\"public class Singleton06 { private static volatile Singleton06 INSTANCE ; /** * 构造方法私有化，防止new */ private Singleton06(){ } /** * 提供一个静态的公有方法，加入双重检查代码，解决线程安全问题, 同时解决懒加载问题 * @return INSTANCE */ public static Singleton06 getINSTANCE() { if (INSTANCE == null) { //同步代码块 synchronized (Singleton03.class){ if (INSTANCE == null) { INSTANCE = new Singleton06(); } } } return INSTANCE; } public static void main(String[] args) { for (int i = 0; i < 1000; i++) { new Thread(() ->{ System.out.println(\\\"instance.hashCode= \\\" + Singleton06.getINSTANCE().hashCode()); }).start(); } } } \",\"为何必须加 volatile：\",\"INSTANCE = new Singleton4() 不是原子的，分成 3 步：创建对象、调用构造、给静态变量赋值，其中后两步可能被指令重排序优化，变成先赋值、再调用构造\",\"如果线程1 先执行了赋值，线程2 执行到第一个 INSTANCE == null 时发现 INSTANCE 已经不为 null，此时就会返回一个未完全构造的对象\",\"优缺点：\",\"Double-Check 概念是多线程开发中常使用到的，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。\",\"这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return 实例化对象，也避免的反复进行方法同步.\",\"线程安全；延迟加载；效率较高\",\"总结：在实际开发中，推荐使用这种单例设计模式\"]},{\"header\":\"7、静态内部类\",\"slug\":\"_7、静态内部类\",\"contents\":[\"public class Singleton07 { /** * 构造方法私有化，防止new */ private Singleton07(){ } /** * 写一个静态内部类,该类中有一个静态属性Singleton */ public static class SingletonClass{ private static final Singleton07 INSTANCE = new Singleton07(); } /** * 提供一个静态的公有方法，加入双重检查代码，解决线程安全问题, 同时解决懒加载问题 * @return INSTANCE */ public static Singleton07 getINSTANCE() { return SingletonClass.INSTANCE; } public static void main(String[] args) { for (int i = 0; i < 1000; i++) { new Thread(() ->{ System.out.println(\\\"instance.hashCode= \\\" + Singleton07.getINSTANCE().hashCode()); }).start(); } } } \",\"优缺点：\",\"这种方式采用了类装载的机制来保证初始化实例时只有一个线程。\",\"静态内部类方式在Singleton 类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance 方法，才会装载SingletonClass 类，从而完成Singleton 的实例化。\",\"类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM 帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。\",\"优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高\",\"结论：推荐使用\"]},{\"header\":\"8、枚举\",\"slug\":\"_8、枚举\",\"contents\":[\"public enum Singleton08 { INSTANCE; public void sayHello(){ System.out.println(\\\"你好，枚举单例模式！\\\"); } public static void main(String[] args) { for (int i = 0; i < 100; i++) { new Thread(() ->{ INSTANCE.sayHello(); System.out.println(\\\"instance.hashCode = \\\"+INSTANCE.hashCode()); }).start(); } } } \",\"优缺点：\",\"借助JDK1.5 中添加的枚举来实现单例模式。不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象，绝对防止多次实例化。\"]},{\"header\":\"3、单例模式在JDK 应用的源码分析\",\"slug\":\"_3、单例模式在jdk-应用的源码分析\",\"contents\":[]},{\"header\":\"1、饿汉式\",\"slug\":\"_1、饿汉式\",\"contents\":[\"JDK 中，java.lang.Runtime 就是经典的单例模式(饿汉式)\",\"public class Runtime { private static Runtime currentRuntime = new Runtime(); /** * Returns the runtime object associated with the current Java application. * Most of the methods of class <code>Runtime</code> are instance * methods and must be invoked with respect to the current runtime object. * * @return the <code>Runtime</code> object associated with the current * Java application. */ public static Runtime getRuntime() { return currentRuntime; } /** Don't let anyone else instantiate this class */ private Runtime() {} ······ } \"]},{\"header\":\"2、双检锁懒汉式单例\",\"slug\":\"_2、双检锁懒汉式单例\",\"contents\":[\"System类中的Console对象的创建就是用的双重检锁\",\"private static volatile Console cons = null; /** * Returns the unique {@link java.io.Console Console} object associated * with the current Java virtual machine, if any. * * @return The system console, if any, otherwise <tt>null</tt>. * * @since 1.6 */ public static Console console() { if (cons == null) { synchronized (System.class) { if (cons == null) { cons = sun.misc.SharedSecrets.getJavaIOAccess().console(); } } } return cons; } \"]},{\"header\":\"3、内部类懒汉式单例\",\"slug\":\"_3、内部类懒汉式单例\",\"contents\":[\"Collections类中的ReverseComparator.REVERSE_ORDER 就是内部类懒汉式单例\",\"private static class ReverseComparator implements Comparator<Comparable<Object>>, Serializable { private static final long serialVersionUID = 7207038068494060240L; static final ReverseComparator REVERSE_ORDER = new ReverseComparator(); public int compare(Comparable<Object> c1, Comparable<Object> c2) { return c2.compareTo(c1); } private Object readResolve() { return Collections.reverseOrder(); } @Override public Comparator<Comparable<Object>> reversed() { return Comparator.naturalOrder(); } } \"]},{\"header\":\"4、内部类懒汉式单例\",\"slug\":\"_4、内部类懒汉式单例\",\"contents\":[\"Collections 中的 EmptyNavigableSet 就是内部类懒汉式单例\",\"private static class EmptyNavigableSet<E> extends UnmodifiableNavigableSet<E> implements Serializable { private static final long serialVersionUID = -6291252904449939134L; public EmptyNavigableSet() { super(new TreeSet<E>()); } private Object readResolve() { return EMPTY_NAVIGABLE_SET; } } \"]},{\"header\":\"5、枚举饿汉式单例\",\"slug\":\"_5、枚举饿汉式单例\",\"contents\":[\"Comparators.NaturalOrderComparator.INSTANCE 枚举饿汉式单例\",\"class Comparators { private Comparators() { throw new AssertionError(\\\"no instances\\\"); } /** * Compares {@link Comparable} objects in natural order. * * @see Comparable */ enum NaturalOrderComparator implements Comparator<Comparable<Object>> { INSTANCE; @Override public int compare(Comparable<Object> c1, Comparable<Object> c2) { return c1.compareTo(c2); } @Override public Comparator<Comparable<Object>> reversed() { return Comparator.reverseOrder(); } } ...... } \"]},{\"header\":\"4、注意事项和细节说明\",\"slug\":\"_4、注意事项和细节说明\",\"contents\":[\"单例模式保证了系统内存中该类只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能。\",\"当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不是使用new\",\"单例模式使用的场景：\",\"需要频繁的进行创建和销毁的对象\",\"创建对象时耗时过多或耗费资源过多(即：重量级对象)\",\"但又经常用到的对象、工具类对象、频繁访问数据库或文件的对象(比如数据源、session 工厂等)\"]},{\"header\":\"5、总结\",\"slug\":\"_5、总结\",\"contents\":[\"**经验之谈：**一般情况下，不建议使用第 1 种和第 2 种懒汉方式，建议使用第 3 种饿汉方式。只有在要明确实现 lazy loading 效果时，才会使用第 5 种登记方式。如果涉及到反序列化创建对象时，可以尝试使用第 6 种枚举方式。如果有其他特殊的需求，可以考虑使用第 4 种双检锁方式。\"]},{\"header\":\"6、工厂模式\",\"slug\":\"_6、工厂模式\",\"contents\":[\"需求：便于手机种类的扩展\",\"手机的种类很多(比如HuaWeiPhone、XiaoMiPhone等)\",\"手机的制作有prepare，production, assemble, box\",\"完成手机店订购功能。\"]},{\"header\":\"1、传统模式\",\"slug\":\"_1、传统模式\",\"contents\":[\"手机抽象类\",\"public abstract class Phone { private String name; /** * 准备的抽象类 */ public abstract void prepare(); /** * 手机生产 */ public void production(){ System.out.println(name + \\\"手机production\\\"); } /** * 手机组装 */ public void assemble(){ System.out.println(name + \\\"手机assemble\\\"); } /** * 手机打包 */ public void box(){ System.out.println(name + \\\"手机box\\\"); } public String getName() { return name; } public void setName(String name) { this.name = name; } } \",\"实体类\",\"public class HuaWeiPhone extends Phone{ @Override public void prepare() { System.out.println(\\\"准备华为手机原材料！\\\"); } } \",\"public class XiaoMiPhone extends Phone{ @Override public void prepare() { System.out.println(\\\"准备小米手机原材料！\\\"); } } \",\"基于方法实现\",\"public class OrderPhone { public OrderPhone() { Phone phone; String orderType; while (true){ orderType = inputType(); if (orderType.equals(\\\"huawei\\\")){ phone = new HuaWeiPhone(); phone.setName(\\\"华为\\\"); }else if(orderType.equals(\\\"xiaomi\\\")){ phone = new XiaoMiPhone(); phone.setName(\\\"小米\\\"); }else { break; } phone.prepare(); phone.production(); phone.assemble(); phone.box(); } } /** * 键盘输入类型 * @return */ public String inputType(){ Scanner scanner = new Scanner(System.in); System.out.println(\\\"请输入手机类型：\\\"); String next = scanner.next(); return next; } } \",\"调用\",\"public class PhoneStore { public static void main(String[] args) { new OrderPhone(); } } \",\"传统方法的优缺点：\",\"优点：比较好理解，简单易操作。\",\"缺点：违反了设计模式的ocp 原则，即对扩展开放，对修改关闭。即当我们给类增加新功能的时候，尽量不修改代码，或者尽可能少修改代码.\",\"比如我们这时要新增加一个Phone的种类(oppo Phone)，就需要增加OPPOPhone类，同时也要修改OrderPhone类\",\"改进的思路：\",\"修改代码可以接受，但是如果我们在其它的地方也有创建Phone的代码，就意味着，也需要修改，而创建Phone的代码，往往有多处。\",\"思路：把创建Phone对象封装到一个类中，这样我们有新的Phone种类时，只需要修改该类就可，其它有创建到Phone对象的代码就不需要修改了=> 简单工厂模式\"]},{\"header\":\"2、简单工厂模式\",\"slug\":\"_2、简单工厂模式\",\"contents\":[]},{\"header\":\"1、介绍\",\"slug\":\"_1、介绍\",\"contents\":[\"简单工厂模式是属于创建型模式，是工厂模式的一种。简单工厂模式是由一个工厂对象决定创建出哪一种产品类的实例。简单工厂模式是工厂模式家族中最简单实用的模式。\",\"简单工厂模式：定义了一个创建对象的类，由这个类来封装实例化对象的行为(代码)\",\"在软件开发中，当我们会用到大量的创建某种、某类或者某批对象时，就会使用到工厂模式.\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"新建SimpleFactory类\",\"public class SimpleFactory { public Phone createPhone(String orderType) { Phone phone = null; if (orderType.equals(\\\"huawei\\\")){ phone = new HuaWeiPhone(); phone.setName(\\\"华为\\\"); }else if(orderType.equals(\\\"xiaomi\\\")){ phone = new XiaoMiPhone(); phone.setName(\\\"小米\\\"); } return phone; } } \",\"修改订购手机类OrderPhone\",\"public class OrderPhone { /** * 键盘输入类型 * @return */ public String inputType(){ Scanner scanner = new Scanner(System.in); System.out.println(\\\"请输入手机类型：\\\"); String next = scanner.next(); return next; } SimpleFactory simpleFactory; Phone phone = null; public OrderPhone(SimpleFactory simpleFactory){ setSimpleFactory(simpleFactory); } public void setSimpleFactory(SimpleFactory simpleFactory){ String orderType; //设置简单工厂对象 this.simpleFactory = simpleFactory; do { orderType = inputType(); phone = this.simpleFactory.createPhone(orderType); if (phone != null) { phone.prepare(); phone.production(); phone.assemble(); phone.box(); }else { System.out.println(\\\"订购失败\\\"); break; } }while (true); } } \",\"修改调用类\",\"public class PhoneStore { public static void main(String[] args) { // new OrderPhone(); new OrderPhone(new SimpleFactory()); } } \"]},{\"header\":\"3、工厂方法模式\",\"slug\":\"_3、工厂方法模式\",\"contents\":[\"新的需求：\",\"订购不同种类的、不同厂家的手机\",\"思路\",\"使用简单工厂模式，创建不同的远程工厂，这样也是可以的，之前简单工厂模式就实现了，但考虑到项目的规模，以及软件的维护性，可扩展性并不是特别好\",\"使用工厂方法模式\"]},{\"header\":\"1、工厂方法模式介绍\",\"slug\":\"_1、工厂方法模式介绍\",\"contents\":[\"定义了一个创建对象的抽象方法，由子类决定要实例化的类。工厂方法模式将对象的实例化推迟到子类。\",\"思路：\",\"将创建的过程写成抽象类和抽象方法\",\"步骤\",\"将订购手机的类变成抽象类，抽象方法为创建手机\",\"public abstract class OrderPhone { /** * 键盘输入类型 * @return */ public String inputType(){ Scanner scanner = new Scanner(System.in); System.out.println(\\\"请输入手机类型：\\\"); String next = scanner.next(); return next; } abstract Phone createPhone(String orderType); public OrderPhone(){ Phone phone; String orderType; do { orderType = inputType(); //抽象方法，由工厂子类完成 phone = createPhone(orderType); phone.prepare(); phone.production(); phone.assemble(); phone.box(); }while (true); } public static void main(String[] args) { new DomesticPhone(); } } \",\"分类手机，继承抽象类\",\"public class DomesticPhone extends OrderPhone{ @Override Phone createPhone(String orderType) { Phone phone = null; if (orderType.equals(\\\"huawei\\\")){ phone = new HuaWeiPhone(); phone.setName(\\\"华为\\\"); }else if (orderType.equals(\\\"xiaomi\\\")){ phone = new XiaoMiPhone(); phone.setName(\\\"小米\\\"); } return phone; } } \",\"public class IPhone extends OrderPhone{ @Override Phone createPhone(String orderType) { Phone phone = null; if (orderType.equals(\\\"pingguo\\\")){ phone = new PingGuoPhone(); phone.setName(\\\"苹果\\\"); }else if (orderType.equals(\\\"sanxing\\\")){ phone = new SanXingPhone(); phone.setName(\\\"三星\\\"); } return phone; } } \"]},{\"header\":\"4、抽象工厂模式\",\"slug\":\"_4、抽象工厂模式\",\"contents\":[\"定义了一个interface 用于创建相关或有依赖关系的对象簇，而无需指明具体的类\",\"抽象工厂模式可以将简单工厂模式和工厂方法模式进行整合。\",\"从设计层面看，抽象工厂模式就是对简单工厂模式的改进(或者称为进一步的抽象)。\",\"将工厂抽象成两层，AbsFactory(抽象工厂) 和具体实现的工厂子类。程序员可以根据创建对象类型使用对应的工厂子类。这样将单个的简单工厂类变成了工厂簇，更利于代码的维护和扩展。\",\"定义接口，让子类工厂进行实现\",\"public interface AbstractFactory { /** * 建Phone，让工厂子类实现 * @param orderType 类型 * @return phone */ public Phone createPhone(String orderType); } \",\"子类工厂实现接口\",\"public class DomesticPhoneFactory implements AbstractFactory { @Override public Phone createPhone(String orderType) { Phone phone = null; System.out.println(\\\"~使用的是抽象工厂模式~\\\"); if (orderType.equals(\\\"huawei\\\")){ phone = new HuaWeiPhone(); phone.setName(\\\"华为\\\"); }else if (orderType.equals(\\\"xiaomi\\\")){ phone = new XiaoMiPhone(); phone.setName(\\\"小米\\\"); } return phone; } } \",\"public class ForeignPhoneFactory implements AbstractFactory { @Override public Phone createPhone(String orderType) { Phone phone = null; if (orderType.equals(\\\"pingguo\\\")){ phone = new PingGuoPhone(); phone.setName(\\\"苹果\\\"); }else if (orderType.equals(\\\"sanxing\\\")){ phone = new SanXingPhone(); phone.setName(\\\"三星\\\"); } return phone; } } \",\"具体调用实现\",\"public class OrderPhone { AbstractFactory factory; /** * 键盘输入类型 * @return */ public String inputType(){ Scanner scanner = new Scanner(System.in); System.out.println(\\\"请输入手机类型：\\\"); String next = scanner.next(); return next; } public OrderPhone(AbstractFactory factory){ setFactory(factory); } private void setFactory(AbstractFactory factory){ Phone phone; String orderType; this.factory = factory; do { orderType = inputType(); //抽象工厂方法 phone = factory.createPhone(orderType); if (phone != null) { phone.prepare(); phone.production(); phone.assemble(); phone.box(); }else { System.out.println(\\\"订购失败！\\\"); break; } }while (true); } public static void main(String[] args) { new OrderPhone(new DomesticPhoneFactory()); } } \"]},{\"header\":\"5、工厂模式在JDK-Calendar 应用的源码分析\",\"slug\":\"_5、工厂模式在jdk-calendar-应用的源码分析\",\"contents\":[\"public class FactoryTest { public static void main(String[] args) { Calendar cal = Calendar.getInstance(); // 注意月份下标从0 开始，所以取月份要+1 System.out.println(\\\"年:\\\" + cal.get(Calendar.YEAR)); System.out.println(\\\"月:\\\" + (cal.get(Calendar.MONTH) + 1)); System.out.println(\\\"日:\\\" + cal.get(Calendar.DAY_OF_MONTH)); System.out.println(\\\"时:\\\" + cal.get(Calendar.HOUR_OF_DAY)); System.out.println(\\\"分:\\\" + cal.get(Calendar.MINUTE)); System.out.println(\\\"秒:\\\" + cal.get(Calendar.SECOND)); } } \",\"public static Calendar getInstance() { return createCalendar(TimeZone.getDefault(), Locale.getDefault(Locale.Category.FORMAT)); } private static Calendar createCalendar(TimeZone zone, Locale aLocale) { CalendarProvider provider = LocaleProviderAdapter.getAdapter(CalendarProvider.class, aLocale) .getCalendarProvider(); if (provider != null) { try { return provider.getInstance(zone, aLocale); } catch (IllegalArgumentException iae) { // fall back to the default instantiation } } Calendar cal = null; if (aLocale.hasExtensions()) { String caltype = aLocale.getUnicodeLocaleType(\\\"ca\\\"); if (caltype != null) { switch (caltype) { case \\\"buddhist\\\": cal = new BuddhistCalendar(zone, aLocale); break; case \\\"japanese\\\": cal = new JapaneseImperialCalendar(zone, aLocale); break; case \\\"gregory\\\": cal = new GregorianCalendar(zone, aLocale); break; } } } if (cal == null) { // If no known calendar type is explicitly specified, // perform the traditional way to create a Calendar: // create a BuddhistCalendar for th_TH locale, // a JapaneseImperialCalendar for ja_JP_JP locale, or // a GregorianCalendar for any other locales. // NOTE: The language, country and variant strings are interned. if (aLocale.getLanguage() == \\\"th\\\" && aLocale.getCountry() == \\\"TH\\\") { cal = new BuddhistCalendar(zone, aLocale); } else if (aLocale.getVariant() == \\\"JP\\\" && aLocale.getLanguage() == \\\"ja\\\" && aLocale.getCountry() == \\\"JP\\\") { cal = new JapaneseImperialCalendar(zone, aLocale); } else { cal = new GregorianCalendar(zone, aLocale); } } return cal; } \"]},{\"header\":\"6、工厂模式小结\",\"slug\":\"_6、工厂模式小结\",\"contents\":[\"意义：将实例化对象的代码提取出来，放到一个类中统一管理和维护，达到和主项目的依赖关系的解耦。从而提高项目的扩展和维护性。\",\"三种工厂模式：\",\"简单工厂模式\",\"工厂方法模式\",\"抽象工厂模式\",\"涉及到设计模式的依赖抽象原则\",\"创建对象实例时，不要直接new 类, 而是把这个new 类的动作放在一个工厂的方法中，并返回。有的书上说，变量不要直接持有具体类的引用。\",\"不要让类继承具体类，而是继承抽象类或者是实现interface(接口)，不要覆盖基类中已经实现的方法。\"]}]},\"/study-tutorial/components/CAT.html\":{\"title\":\"CAT链路追踪\",\"contents\":[{\"header\":\"1、CAT介绍\",\"slug\":\"_1、cat介绍\",\"contents\":[]},{\"header\":\"1、为什么要学习？\",\"slug\":\"_1、为什么要学习\",\"contents\":[\"从单体架构到微服务架构的演变， 一个业务请求往往会流转多个服务， 大型互联网产品服务架构尤为复杂，腾讯的抢红包服务， 阿里的交易支付服务， 可能就流转成百上千个服务节点， 面对众多服务， 如何监控管理？ 服务请求一旦出现问题， 如何快速定位问题？ 如何保障服务的高可用， 做到全面的监控与预警？ 如何分析统计服务的运行状况？ 看下链路监控产品如何解决这些问题。\"]},{\"header\":\"2、什么是CAT?\",\"slug\":\"_2、什么是cat\",\"contents\":[\"CAT 是基于 Java 开发的实时应用监控平台，为美团点评提供了全面的实时监控告警服务。\",\"CAT 作为服务端项目基础组件，提供了 Java, C/C++, Node.js, Python, Go 等多语言客户端，已经在美团点评的基础架构中间件框架（MVC框架，RPC框架，数据库框架，缓存框架等，消息队列，配置系统等）深度集成，为美团点评各业务线提供系统丰富的性能指标、健康状况、实时告警等。\",\"CAT 很大的优势是它是一个实时系统，CAT 大部分系统是分钟级统计，但是从数据生成到服务端处理结束是秒级别，秒级定义是48分钟40秒，基本上看到48分钟38秒数据，整体报表的统计粒度是分钟级；第二个优势，监控数据是全量统计，客户端预计算；链路数据是采样计算。\",\"优势：\",\"实时处理：信息的价值会随时间锐减，尤其是事故处理过程中。\",\"全量数据：全量采集指标数据，便于深度分析故障案例。\",\"高可用：故障的还原与问题定位，需要高可用监控来支撑。\",\"故障容忍：故障不影响业务正常运转、对业务透明。\",\"高吞吐：海量监控数据的收集，需要高吞吐能力做保证。\",\"可扩展：支持分布式、跨 IDC 部署，横向扩展的监控系统。\",\"报表丰富： 帮助管理人员从各个角度了解系统的整体状况。\"]},{\"header\":\"2、CAT报表\",\"slug\":\"_2、cat报表\",\"contents\":[\"报错统计报表\",\"业务统计报表\",\"LOGVIEW统计\",\"可视化LOGVIEW\",\"应用类型报表\",\"报表名称\",\"报表用途\",\"Transaction实时报表\",\"一段代码的运行时间/次数/分布、比如URL/Cache/SQL执行次数和响应时间\",\"Event实时报表\",\"事件产生的次数/分布，比如出现一个异常\",\"Problem实时报表\",\"根据Transaction/Event数据分析出来的系统出现的异常，包括访问较慢的程序等\",\"Heartbeat实时报表\",\"JVM内部一些状态信息，Load/Memory/GC/Thread等\",\"Metric实时报表\",\"业务指标采集监控报表\",\"Matrix实时报表\",\"一个请求调用分布统计(一次请求中调用多少次SQL/RPC/Cache等)，可评估应用设计的合理性\",\"...\",\"...\"]},{\"header\":\"3、Google Dapper论文\",\"slug\":\"_3、google-dapper论文\",\"contents\":[]},{\"header\":\"4、Dapper Deployment\",\"slug\":\"_4、dapper-deployment\",\"contents\":[]},{\"header\":\"5、链路监控工作机制\",\"slug\":\"_5、链路监控工作机制\",\"contents\":[]},{\"header\":\"7、CAT架构设计\",\"slug\":\"_7、cat架构设计\",\"contents\":[\"整体设计 简单即是最好原则设计， 主要分为三个模块cat-client，cat-consumer，cat-home。\",\"cat-client 提供给业务以及中间层埋点的底层sdk。\",\"cat-consumer 用于实时分析从客户端的提供的数据。\",\"cat-home 作为提供给用户的展示的控制端。\",\"客户端设计\",\"客户端设计是CAT系统设计中最为核心的一个环节，客户端要求是做到API简单、高可靠性能、无论在任何场景下客户端都不能影响各业务服务的性能（监控只是公司核心业务流程一个旁路环节）。\",\"服务端设计 服务端单机cat-consumer的整体架构： \",\"当某个报表处理器处理来不及时候，比如Transaction报表处理比较慢，可以通过配置支持开启多个Transaction处理线程，并发消费消息。 \"]},{\"header\":\"8、安装与使用（单机版）\",\"slug\":\"_8、安装与使用-单机版\",\"contents\":[]},{\"header\":\"1、环境准备\",\"slug\":\"_1、环境准备\",\"contents\":[\"JDK1.8+\",\"Tomcat8+\",\"MySQL5.7+\"]},{\"header\":\"2、CAT启动配置\",\"slug\":\"_2、cat启动配置\",\"contents\":[\"PS：不建议在Windows下部署， 设计上对window支持不好， 容易出各种问题。\",\"下载CAT源码, 如GIT方式过慢， 可用Download Zip 方式打包下载\",\"构建CAT服务war包 可以导入IDEA工程进行编译， 或者直接用MAVEN进行编译，这里编译的目录是：cat-home 将MAVEN加入到系统PATH， 执行mvn命令：\",\"mvn clean install -Dmaven.test.skip=true \",\"创建数据库\",\"先创建CAT数据库， 采用utf8mb4字符集， 再导入{CAT_SRC}/script/目录下的CatApplication.sql脚本。\",\"创建目录，因为cat需要/data的全部权限，运行盘下的/data/appdatas/cat和/data/applogs/cat有读写权限\",\"mkdir /data/appdatas/cat/ chmod -R 777 /data/appdatas/cat/ \",\"将打包好的war包传入tomcat的webapp下\",\"创建客户端的配置/data/appdatas/cat/client.xml (客户端使用)\",\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?> <config mode=\\\"client\\\"> <servers> <server ip=\\\"127.0.0.1\\\" port=\\\"2280\\\" http-port=\\\"8080\\\"/> </servers> </config> \",\"创建服务端的配置/data/appdatas/cat/datasources.xml (服务端使用)\",\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?> <data-sources> <data-source id=\\\"cat\\\"> <maximum-pool-size>3</maximum-pool-size> <connection-timeout>1s</connection-timeout> <idle-timeout>10m</idle-timeout> <statement-cache-size>1000</statement-cache-size> <properties> <driver>com.mysql.jdbc.Driver</driver> <url><![CDATA[jdbc:mysql://127.0.0.1:3306/cat]]></url> <!-- 请替换为真实数据库URL及Port --> <user>root</user> <!-- 请替换为真实数据库用户名 --> <password>123456</password> <!-- 请替换为真实数据库密码 --> <connectionProperties><![CDATA[useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&socketTimeout=120000]]></connectionProperties> </properties> </data-source> </data-sources> \",\"Tomcat配置，修改tomcat conf 目录下 server.xml， 检查好端口没有被其他程序占用。\",\"<Connector port=\\\"8080\\\" protocol=\\\"HTTP/1.1\\\" URIEncoding=\\\"utf-8\\\" connectionTimeout=\\\"20000\\\" redirectPort=\\\"8443\\\" /> <!-- 增加 URIEncoding=\\\"utf-8\\\" --> \",\"如需内存不足，需作调整\",\"CATALINA_OPTS=\\\"-Xms1024m -Xmx2048m -Xss1024K -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=1024m\\\" \",\"启动\",\"进入tomcat bin目录下\",\"bash startup.sh \",\"访问cat客户端\",\"打开控制台地址： http://ip:8080/cat/s/config?op=routerConfigUpdate\",\"默认用户名：admin 默认密码：admin\",\"正常可以看到后台：\"]},{\"header\":\"9、CAT集群版\",\"slug\":\"_9、cat集群版\",\"contents\":[\"具体可参考官方文档： CAT集群部署\"]},{\"header\":\"10、微服务整合CAT案例\",\"slug\":\"_10、微服务整合cat案例\",\"contents\":[]},{\"header\":\"1、服务设计\",\"slug\":\"_1、服务设计\",\"contents\":[\"设计四个服务：网关服务、订单服务、账户服务和库存服务， 三层调用关系监控\"]},{\"header\":\"2、工程结构\",\"slug\":\"_2、工程结构\",\"contents\":[\"cat-demo: 父级工程\",\"cat-demo-account: 账户服务工程\",\"cat-demo-gateway: 网关服务工程\",\"cat-demo-order: 订单服务工程\",\"cat-demo-stock: 库存服务工程\"]},{\"header\":\"3、创建父工程\",\"slug\":\"_3、创建父工程\",\"contents\":[\"命名为cat-demo\",\"pom依赖\",\"<dependencies> <!-- spring boot 依赖 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <!-- Nacos服务注册发现依赖 --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <!-- Spring Boot 监控组件依赖 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!-- CAT 组件依赖--> <dependency> <groupId>com.dianping.cat</groupId> <artifactId>cat-client</artifactId> <version>3.0.1</version> </dependency> </dependencies> \"]},{\"header\":\"4、创建cat-gateway: 网关服务工程\",\"slug\":\"_4、创建cat-gateway-网关服务工程\",\"contents\":[\"父级为cat-demo，命名为cat-gateway\",\"启动类\",\"@SpringBootApplication @EnableDiscoveryClient @ComponentScan(basePackages = {\\\"com.xiaobear\\\"}) @RestController public class CatGateWayApplication { public static void main(String[] args) { SpringApplication.run(CatGateWayApplication.class, args); } @Resource private RestTemplate restTemplate; /** * 指向订单服务的接口 */ @Value(\\\"${service2.address:localhost:8082}\\\") private String serviceAddress; /** * 模拟正常的请求 * @return * @throws Exception */ @RequestMapping(\\\"/gateway\\\") public String gateway() throws Exception { Thread.sleep(100); String response = restTemplate.getForObject(\\\"http://\\\" + serviceAddress + \\\"/order\\\", String.class); return \\\"gateway response ==> \\\" + response; } /** * 模拟一个请求异常 * @return * @throws Exception */ @RequestMapping(\\\"/timeout\\\") public String timeout() throws Exception { Transaction t = Cat.newTransaction(CatConstants.TYPE_URL, \\\"timeout\\\"); try { Thread.sleep(100); String response = restTemplate.getForObject(\\\"http://\\\" + serviceAddress + \\\"/timeout\\\", String.class); return response; }catch(Exception e) { Cat.getProducer().logError(e); t.setStatus(e); throw e; }finally { t.complete(); } } @Bean RestTemplate restTemplate() { RestTemplate restTemplate = new RestTemplate(); // 保存和传递调用链上下文 restTemplate.setInterceptors(Collections.singletonList(new CatRestInterceptor())); return restTemplate; } } \",\"yml配置文件\",\"server: port: 8081 spring: application: name: cat-gateway cloud: nacos: discovery: ip: 127.0.0.1:8848 #暴露端口 management: endpoint: web: exposure: include: ‘*’ \",\"工具类\",\"CatContext: 存放调用链上下文信息，这里需要注意的是cat的依赖包为3.0的，新版本4的没有一些接口Cat.Context \",\"public class CatContext implements Cat.Context { private Map<String, String> properties = new HashMap<>(); @Override public void addProperty(String s, String s1) { properties.put(s, s1); } @Override public String getProperty(String s) { return properties.get(s); } } \",\"CatServletFilter:过滤器，过滤访问的一些路径\",\"public class CatServletFilter implements Filter { private String[] urlPatterns = new String[0]; /** *过滤初始化 * @param filterConfig * @throws ServletException */ @Override public void init(FilterConfig filterConfig) throws ServletException { String parameter = filterConfig.getInitParameter(\\\"CatHttpModuleUrlPatterns\\\"); if (null != parameter){ parameter = parameter.trim(); urlPatterns = parameter.split(\\\",\\\"); for (int i = 0; i < urlPatterns.length; i++) { urlPatterns[i] = urlPatterns[i].trim(); } } } /** * 请求过滤处理 * @param servletRequest * @param servletResponse * @param filterChain * @throws IOException * @throws ServletException */ @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; String url = request.getRequestURL().toString(); for (String urlPattern : urlPatterns) { if (url.startsWith(urlPattern)) { url = urlPattern; } } //存放消息的上下文对象 CatContext catContext = new CatContext(); catContext.addProperty(Cat.Context.CHILD, request.getHeader(CatHttpConstants.CAT_HTTP_HEADER_CHILD_MESSAGE_ID)); catContext.addProperty(Cat.Context.PARENT, request.getHeader(CatHttpConstants.CAT_HTTP_HEADER_PARENT_MESSAGE_ID)); catContext.addProperty(Cat.Context.ROOT, request.getHeader(CatHttpConstants.CAT_HTTP_HEADER_ROOT_MESSAGE_ID)); Cat.logRemoteCallServer(catContext); Transaction t = Cat.newTransaction(com.dianping.cat.CatConstants.TYPE_URL, url); try { //日志记录 Cat.logEvent(\\\"service.method\\\",request.getMethod(), Message.SUCCESS, request.getRequestURL().toString()); Cat.logEvent(\\\"Service.client\\\", request.getRemoteHost()); filterChain.doFilter(servletRequest, servletResponse); //设置事务状态为 SUCCESS t.setStatus(Transaction.SUCCESS); }catch (Exception e){ //设置事务状态，打印日志 t.setStatus(e); Cat.logError(e); throw e; }finally { //事务完成 t.complete(); } } } \",\"CatFilterConfigure：过滤器配置类\",\"@Configuration public class CatFilterConfigure { @Bean public FilterRegistrationBean catFilter(){ FilterRegistrationBean registration = new FilterRegistrationBean(); CatServletFilter filter = new CatServletFilter(); registration.setFilter(filter); registration.addUrlPatterns(\\\"/*\\\"); registration.setName(\\\"cat-filter\\\"); registration.setOrder(1); return registration; } } \",\"CatRestInterceptor：Cat拦截器 记录 TID、PID、SID\",\"@Component public class CatRestInterceptor implements ClientHttpRequestInterceptor { @Override public ClientHttpResponse intercept(HttpRequest httpRequest, byte[] bytes, ClientHttpRequestExecution clientHttpRequestExecution) throws IOException { Transaction t = Cat.newTransaction(CatConstants.TYPE_CALL, httpRequest.getURI().toString()); try { HttpHeaders headers = httpRequest.getHeaders(); // 保存和传递CAT调用链上下文 Cat.Context ctx = new CatContext(); Cat.logRemoteCallClient(ctx); headers.add(CatHttpConstants.CAT_HTTP_HEADER_ROOT_MESSAGE_ID, ctx.getProperty(Cat.Context.ROOT)); headers.add(CatHttpConstants.CAT_HTTP_HEADER_PARENT_MESSAGE_ID, ctx.getProperty(Cat.Context.PARENT)); headers.add(CatHttpConstants.CAT_HTTP_HEADER_CHILD_MESSAGE_ID, ctx.getProperty(Cat.Context.CHILD)); // 继续执行请求 ClientHttpResponse response = clientHttpRequestExecution.execute(httpRequest, bytes); t.setStatus(Transaction.SUCCESS); return response; } catch (Exception e) { Cat.getProducer().logError(e); t.setStatus(e); throw e; } finally { t.complete(); } } } \",\"CatHttpConstants：cat常量\",\"public class CatHttpConstants { public static final String CAT_HTTP_HEADER_CHILD_MESSAGE_ID = \\\"X-CAT-CHILD-ID\\\"; public static final String CAT_HTTP_HEADER_PARENT_MESSAGE_ID= \\\"X-CAT-PARENT-ID\\\"; public static final String CAT_HTTP_HEADER_ROOT_MESSAGE_ID = \\\"X-CAT-ROOT-ID\\\"; } \",\"resources资源目录， 路径META-INF下， 必须创建app.properties， 内部只要配置app.name属性。\",\"app.name: cat-gateway \"]},{\"header\":\"5、创建cat-account: 网关服务工程\",\"slug\":\"_5、创建cat-account-网关服务工程\",\"contents\":[\"父级为cat-demo，命名为cat-account\",\"启动类\",\"@SpringBootApplication @EnableDiscoveryClient @RestController @ComponentScan(\\\"com.xiaobear\\\") public class CatAccountApplication { public static void main(String[] args) { SpringApplication.run(CatAccountApplication.class, args); } /** * 提供账户服务接口 * @return * @throws Exception */ @RequestMapping(\\\"/account\\\") public String account() throws Exception { Thread.sleep(150); return \\\"account success\\\"; } } \",\"其他均跟网关服务类似，不同点：\",\"端口号为：8082\",\"少了CatRestInterceptor：Cat拦截器 记录 TID、PID、SID\"]},{\"header\":\"6、创建cat-order: 网关服务工程\",\"slug\":\"_6、创建cat-order-网关服务工程\",\"contents\":[\"父级为cat-demo，命名为cat-order\",\"启动类\",\"@SpringBootApplication @EnableDiscoveryClient @RestController @ComponentScan(\\\"com.xiaobear\\\") @Slf4j public class CatOrderApplication { public static void main(String[] args) { SpringApplication.run(CatOrderApplication.class, args); } @Autowired RestTemplate restTemplate; /** * Account账户服务 */ @Value(\\\"${service3.address:localhost:8083}\\\") String serviceAddress3; /** * stock库存服务 */ @Value(\\\"${service4.address:localhost:8084}\\\") String serviceAddress4; /** * 异常测试端口 */ private static final int MOCK_PORT = 8765; /** * 提供下单接口 * @return * @throws InterruptedException */ @RequestMapping(\\\"/order\\\") public String service2MethodInController() throws InterruptedException { Thread.sleep(200); // 调用账户服务以及库存服务 String service3 = restTemplate.getForObject(\\\"http://\\\" + serviceAddress3 + \\\"/account\\\", String.class); String service4 = restTemplate.getForObject(\\\"http://\\\" + serviceAddress4 + \\\"/stock\\\", String.class); //打印返回结果 return String.format(\\\"Calling order service[order success] ==> Calling Account Service [%s] ==> Calling Customer Service [%s]\\\", service3, service4); } /** * 模拟异常超时接口 * @return * @throws InterruptedException */ @RequestMapping(\\\"/readtimeout\\\") public String connectionTimeout() throws InterruptedException { Transaction t = Cat.newTransaction(CatConstants.TYPE_CALL, \\\"connectionTimeout\\\"); //睡眠500ms Thread.sleep(500); try { log.info(\\\"Calling a missing service\\\"); //远程调用异常端口 restTemplate.getForObject(\\\"http://localhost:\\\" + MOCK_PORT + \\\"/readtimeout\\\", String.class); return \\\"Should blow up\\\"; } catch(Exception e) { t.setStatus(e); Cat.getProducer().logError(e); throw e; } finally { t.complete(); } } @Bean RestTemplate restTemplate() { SimpleClientHttpRequestFactory clientHttpRequestFactory = new SimpleClientHttpRequestFactory(); clientHttpRequestFactory.setConnectTimeout(2000); clientHttpRequestFactory.setReadTimeout(3000); RestTemplate restTemplate = new RestTemplate(clientHttpRequestFactory); // 保存和传递调用链上下文 restTemplate.setInterceptors(Collections.singletonList(new CatRestInterceptor())); restTemplate.setErrorHandler(new DefaultResponseErrorHandler() { @Override public boolean hasError(ClientHttpResponse response) throws IOException { try { return super.hasError(response); } catch (Exception e) { return true; } } @Override public void handleError(ClientHttpResponse response) throws IOException { try { super.handleError(response); } catch (Exception e) { log.error(\\\"Exception [\\\" + e.getMessage() + \\\"] occurred while trying to send the request\\\", e); throw e; } } }); return restTemplate; } } \",\"其他均跟网关服务类似，不同点：\",\"端口号为：8083\"]},{\"header\":\"7、创建cat-stock: 网关服务工程\",\"slug\":\"_7、创建cat-stock-网关服务工程\",\"contents\":[\"父级为cat-demo，命名为cat-stock\",\"启动类\",\"@SpringBootApplication @EnableDiscoveryClient @ComponentScan(\\\"com.xiaobear\\\") @RestController public class CatStockApplication { public static void main(String[] args) { SpringApplication.run(CatStockApplication.class, args); } /** * 提供库存接口 * @return * @throws Exception */ @RequestMapping(\\\"/stock\\\") public String stock() throws Exception { Thread.sleep(200); return \\\"stock success\\\"; } } \",\"其他均跟网关服务类似，不同点：\",\"端口号为：8084\",\"少了CatRestInterceptor：Cat拦截器 记录 TID、PID、SID\"]},{\"header\":\"8、启动验证\",\"slug\":\"_8、启动验证\",\"contents\":[\"Nacos控制台\",\"四个服务正常启动注册。注意： 这里打包时候Nacos指向地址为宿主机的HOST地址 \",\"访问Gateway服务， 本示例是部署在Linux机器上， 地址： http://10.10.20.10:8081/gateway\",\" 所有服务均正常返回结果：\",\"gateway service ==> Calling order service[order success] ==> Calling Account Service [account success] ==> Calling Customer Service [stock success] \",\"CAT控制台\",\"CAT的LOGVIEW按层级完整的记录了四个服务的请求信息， 1至4分别对应Gateway、Order、Account和Stock服务。\",\"LOGVIEW主要包含请求时间， 服务地址， 请求客户端等主要信息， 也支持图形方式呈现：\",\"CAT 还有很多指标统计与报表展示， 能有效帮助我们监控管理整体微服务调用链路状态。\"]}]},\"/study-tutorial/components/HATEOAS.html\":{\"title\":\"HATEOAS REST架构\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"HATEOAS（Hypermedia as the engine of application state）是 REST 架构风格中最复杂的约束，也是构建成熟 REST 服务的核心。\"]},{\"header\":\"2、HATEOAS示例\",\"slug\":\"_2、hateoas示例\",\"contents\":[]},{\"header\":\"3、HATEOAS常用链接类型\",\"slug\":\"_3、hateoas常用链接类型\",\"contents\":[\"REL\",\"说明\",\"SELF\",\"指向当前资源本身的链接\",\"edit\",\"指向⼀一个可以编辑当前资源的链接\",\"collection\",\"如果当前资源包含在某个集合中，指向该集合的链接\",\"search\",\"指向⼀一个可以搜索当前资源与其相关资源的链接\",\"related\",\"指向⼀一个与当前资源相关的链接\",\"first\",\"集合遍历相关的类型，指向第⼀一个资源的链接\",\"last\",\"集合遍历相关的类型，指向最后⼀一个资源的链接\",\"previous\",\"集合遍历相关的类型，指向上⼀一个资源的链接\",\"<list> <device> ...... <link rel=\\\"self\\\" href=\\\"http://host:port/res/device/11\\\"/> </device> ... <link rel=\\\"next\\\" href=\\\"http://host:port/res/device?start=10&size=10\\\"/> </list> \"]},{\"header\":\"4、HATEOAS CRUD示例\",\"slug\":\"_4、hateoas-crud示例\",\"contents\":[\"显示接口\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate/search\",\"分页查询\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate{?page,size,sort}\",\"支持排序：\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate?page=0&size=3&sort=lastUpdateUser,desc\",\"新增数据\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate\",\"传递的操作类型要改为POST， 采用JSON格式提交数据。\",\"更新数据\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate/15\",\"链接附带数据唯一ID， 提交采用PUT方式。\",\"删除数据\",\"http://39.98.152.160:10680/admin/accountWarnNotifyTemplate/15\",\"提交方式采用PUT。\"]},{\"header\":\"5、案例设计\",\"slug\":\"_5、案例设计\",\"contents\":[]},{\"header\":\"1、服务设计\",\"slug\":\"_1、服务设计\",\"contents\":[\"采用Spring Data Rest 实现 Hypermedia规范。\",\" 设计两个服务， 订单服务和股票服务， 两个服务遵循Hateoas风格。\",\"Step 1: 通过Restful的Hypermedia模型调用股票服务， 查询并打印股票信息。\",\"Step 2: 通过HTTP PUT动作更新股票价格。\",\"Step 3: 重新调用股票信息接口，打印股票名称与价格。\",\"Step 4: 以上步骤操作成功后， 订单服务调用自身接口， 生成订单信息。\"]},{\"header\":\"2、工程说明\",\"slug\":\"_2、工程说明\",\"contents\":[\"数据层采用spring data jpa，spring提供的一套简化JPA开发的框架，按照约定好的【方法命名规则】写dao层接口，就可以在不写接口实现的情况下，实现对数据库的访问和操作。同时提供了很多除了CRUD之外的功能，如分页、排序、复杂查询等等。\",\"本工程侧重Hateoas的理解， 数据库采用简化的H2内存数据库， 重新启动服务数据消失。\"]},{\"header\":\"3、构建工程\",\"slug\":\"_3、构建工程\",\"contents\":[]},{\"header\":\"1、hateoas-stocks股票服务\",\"slug\":\"_1、hateoas-stocks股票服务\",\"contents\":[\"启动类\",\"要加上EntityScan与EnableJpaRepositories注解，指定路径， 否则不生效。\",\"@SpringBootApplication @ComponentScan(\\\"com.xiaobear\\\") @EntityScan(\\\"com.xiaobear\\\") @EnableJpaRepositories(\\\"com.xiaobear\\\") public class StackApplication { public static void main(String[] args) { SpringApplication.run(StackApplication.class, args); } /** * 数据源 * @return */ @Bean public Hibernate5Module hibernate5Module(){ return new Hibernate5Module(); } /** * json解析 * @return */ @Bean public Jackson2ObjectMapperBuilderCustomizer jackson2ObjectMapperBuilderCustomizer(){ return builder -> builder.indentOutput(true); } } \",\"实体类\",\"BaseEntity：通用实体类\",\"@Data @MappedSuperclass public class BaseEntity implements Serializable { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(updatable = false) @CreationTimestamp private Date createTime; @UpdateTimestamp private Date updateTime; } \",\"股票实体类：StocksEntity\",\"@Entity @Data @AllArgsConstructor @NoArgsConstructor @Table(name = \\\"T_STOCKS\\\") public class StocksEntity extends BaseEntity{ private String name; private Double price; } \",\"·StocksRepository接口\",\"定义两个方法，根据名称集合查找多个股票信息； 根据指定名称查找股票信息。\",\"按照JPA规范，按照方法名称自动映射解析， 无须写SQL。\",\"@RepositoryRestResource(path = \\\"stocks\\\") public interface StocksRepository extends JpaRepository<StocksEntity, Long> { /** * 根据股票名称查找所对应的股票数据 * @param list * @return */ List<StocksEntity> findByNameInOrderById(@Param(\\\"list\\\")List<String> list); /** * 根据名称查询股票信息 * @param name * @return */ public StocksEntity findByName(@Param(\\\"name\\\")String name); } \",\"配置文件yml\",\"server: port: 8080 spring: application: name: hateoas-stocks jpa: hibernate: ddl-auto: none properties: hibernate: show_sql: true format_sql: true \",\"数据库文件\",\"schema.sql\",\"drop table t_stocks if exists ; create table t_stocks( id bigint auto_increment, name varchar(64), price double , create_time timestamp , update_time timestamp , primary key(id) ) \",\"data.sql\",\"insert into t_stocks(name, price, create_time, update_time) values ('中国平安', 88.8, now(), now()); insert into t_stocks(name, price, create_time, update_time) values ('工商银行', 89.8, now(), now()); insert into t_stocks(name, price, create_time, update_time) values ('长沙银行', 66.8, now(), now()); \",\"整体工程\"]},{\"header\":\"2、hateoas-order订单服务\",\"slug\":\"_2、hateoas-order订单服务\",\"contents\":[\"启动类\",\"@SpringBootApplication @ComponentScan(\\\"com.xiaobear\\\") @EntityScan(\\\"com.xiaobear\\\") @EnableJpaRepositories(\\\"com.xiaobear\\\") public class OrderApplication { public static void main(String[] args) { SpringApplication.run(OrderApplication.class, args); } /** * * @return */ @Bean public Hibernate5Module hibernate5Module(){ return new Hibernate5Module(); } @Bean public Jackson2ObjectMapperBuilderCustomizer jackson2ObjectMapperBuilderCustomizer(){ return builder -> { builder.indentOutput(true); }; } /** * 设置HTTP连接池参数 * @return */ @Bean public HttpComponentsClientHttpRequestFactory requestFactory() { PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager(30, TimeUnit.SECONDS); connectionManager.setMaxTotal(200); connectionManager.setDefaultMaxPerRoute(20); CloseableHttpClient httpClient = HttpClients.custom() .setConnectionManager(connectionManager) .evictIdleConnections(30, TimeUnit.SECONDS) .disableAutomaticRetries() // Keep-Alive 策略 .setKeepAliveStrategy(new RemoteConnectionKeepAliveStrategy()) .build(); HttpComponentsClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(httpClient); return requestFactory; } /** * 设置RestTemplate参数 * @param builder * @return */ @Bean public RestTemplate restTemplate(RestTemplateBuilder builder) { return builder .setConnectTimeout(Duration.ofMillis(2000)) .setReadTimeout(Duration.ofMillis(1800)) .requestFactory(this::requestFactory) .build(); } } \",\"实体类\",\"OrderEntity:订单实体类\",\"@Entity @Data @Table(name = \\\"t_order\\\") @NoArgsConstructor @AllArgsConstructor @Builder public class OrderEntity extends BaseEntity { /** * 用户名 */ private String user; /** * 股票名称 */ private String stockName; /** * 成交数量 */ private String volume; /** * 成交价格 */ private Double price; } \",\"股票实体类：StocksEntity\",\"BaseEntity：通用实体类\",\"OrderRepository接口\",\"@RepositoryRestResource(path = \\\"order\\\") public interface OrderRepository extends JpaRepository<OrderEntity, Long> { /** * 通过用户查询信息 * @param user * @return */ public List<OrderEntity> findByUser(@Param(\\\"user\\\") String user); } \",\"RemoteConnectionKeepAliveStrategy：远程连接\",\"public class RemoteConnectionKeepAliveStrategy implements ConnectionKeepAliveStrategy { private final long DEFAULT_SECONDS = 30; /** * 远程连接， keepalive的设置策略 * @param httpResponse * @param httpContext * @return */ @Override public long getKeepAliveDuration(HttpResponse httpResponse, HttpContext httpContext) { return Arrays.asList(httpResponse.getHeaders(HTTP.CONN_KEEP_ALIVE)) .stream() .filter(h -> StringUtils.endsWithIgnoreCase(h.getName(), \\\"timeout\\\") && StringUtils.isNumeric(h.getValue())) .findFirst() .map(h -> NumberUtils.toLong(h.getValue(), DEFAULT_SECONDS)) .orElse(DEFAULT_SECONDS) * 1000; } } \",\"runner类\",\"@Component @Slf4j public class RemoteRunner implements ApplicationRunner { private static final URI ROOT_URI = URI.create(\\\"http://localhost:8080/\\\"); @Autowired private RestTemplate restTemplate; @Autowired private OrderRepository orderRepository; @Override public void run(ApplicationArguments args) throws Exception { Link stocksLink = getLink(ROOT_URI,\\\"stocksEntities\\\"); // Step 1: 查询股票信息 queryStocks(stocksLink); // Step 2: 更新股票价格 Link updateLink= getLink(ROOT_URI.resolve(\\\"stocks/1\\\"),\\\"stocksEntity\\\"); Resource<StocksEntity> americano = updateStocks(updateLink); // Step 3: 重新查询打印股票信息 queryStocks(stocksLink); // Step 4: 生成订单信息 OrderEntity order = OrderEntity.builder() .user(\\\"mirson\\\") .stockName(\\\"建设银行\\\") .volume(String.valueOf(1000)) .price(99.9) .build(); orderRepository.save(order); } /** * 获取请求链接 * @param uri * @param rel * @return */ private Link getLink(URI uri, String rel) { ResponseEntity<Resources<Link>> rootResp = restTemplate.exchange(uri, HttpMethod.GET, null, new ParameterizedTypeReference<Resources<Link>>() {}); Link link = rootResp.getBody().getLink(rel); log.info(\\\"Link: {}\\\", link); return link; } /** * 查询股票信息 * @param stocksLink */ private void queryStocks(Link stocksLink) { ResponseEntity<PagedResources<Resource<StocksEntity>>> stocksResp = restTemplate.exchange(stocksLink.getTemplate().expand(), HttpMethod.GET, null, new ParameterizedTypeReference<PagedResources<Resource<StocksEntity>>>() {}); if(null != stocksResp.getBody() && null != stocksResp.getBody().getContent() ) { StringBuffer strs = new StringBuffer(); stocksResp.getBody().getContent().forEach((s)->{ strs.append(s.getContent().getName()).append(\\\":\\\").append(s.getContent().getPrice()).append( \\\",\\\"); }); String resp = strs.toString().replaceAll(\\\",$\\\", \\\"\\\"); log.info(\\\"query stocks ==> \\\" + resp); }else { log.info(\\\"query stocks ==> empty! \\\"); } } /** * 更新股票信息 * @param link * @return */ private Resource<StocksEntity> updateStocks(Link link) { StocksEntity americano = StocksEntity.builder() .name(\\\"中国平安\\\") .price(68.9) .build(); RequestEntity<StocksEntity> req = RequestEntity.put(link.getTemplate().expand()).body(americano); ResponseEntity<Resource<StocksEntity>> resp = restTemplate.exchange(req, new ParameterizedTypeReference<Resource<StocksEntity>>() {}); log.info(\\\"add Stocks ==> {}\\\", resp); return resp.getBody(); } } \",\"配置文件yml\",\"server: port: 8081 spring: application: name: hateoas-order jpa: hibernate: ddl-auto: none properties: hibernate: show_sql: true format_sql: true \",\"数据库文件\",\"drop table t_order if exists; create table t_order ( id bigint auto_increment, user varchar(64), stock_name varchar(64), volume int, price double , create_time timestamp , update_time timestamp, primary key (id) ) \"]},{\"header\":\"3、启动股票服务验证\",\"slug\":\"_3、启动股票服务验证\",\"contents\":[\"启动股票服务，通过HTTP访问， 来查看Rest接口信息\",\"地址： http://127.0.0.1:8080/\",\"{ \\\"_links\\\" : { \\\"stocksEntities\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks{?page,size,sort}\\\", \\\"templated\\\" : true }, \\\"profile\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/profile\\\" } } } \",\"可以看到我们定义的/stocks接口\",\"地址： http://127.0.0.1:8080/stocks\",\"{ \\\"_embedded\\\" : { \\\"stocksEntities\\\" : [ { \\\"createTime\\\" : \\\"2019-07-09T14:20:44.644+0000\\\", \\\"updateTime\\\" : \\\"2019-07-09T14:20:44.644+0000\\\", \\\"name\\\" : \\\"中国平安\\\", \\\"price\\\" : 68.6, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/1\\\" }, \\\"stocksEntity\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/1\\\" } } }, { \\\"createTime\\\" : \\\"2019-07-09T14:20:44.647+0000\\\", \\\"updateTime\\\" : \\\"2019-07-09T14:20:44.647+0000\\\", \\\"name\\\" : \\\"工商银行\\\", \\\"price\\\" : 58.8, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/2\\\" }, \\\"stocksEntity\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/2\\\" } } }, { \\\"createTime\\\" : \\\"2019-07-09T14:20:44.648+0000\\\", \\\"updateTime\\\" : \\\"2019-07-09T14:20:44.648+0000\\\", \\\"name\\\" : \\\"招商银行\\\", \\\"price\\\" : 98.9, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/3\\\" }, \\\"stocksEntity\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/3\\\" } } } ] }, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks{?page,size,sort}\\\", \\\"templated\\\" : true }, \\\"profile\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/profile/stocks\\\" }, \\\"search\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/search\\\" } }, \\\"page\\\" : { \\\"size\\\" : 20, \\\"totalElements\\\" : 3, \\\"totalPages\\\" : 1, \\\"number\\\" : 0 } } \",\"打印了所有股票信息，最下面还暴露了其他接口信息。\",\"地址： http://127.0.0.1:8080/stocks/search\",\"{ \\\"_links\\\" : { \\\"findByName\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/search/findByName{?name}\\\", \\\"templated\\\" : true }, \\\"findByNameInOrderById\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/search/findByNameInOrderById{?list}\\\", \\\"templated\\\" : true }, \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/search\\\" } } } \",\"打印了我们通过JPA定义的两个接口，findByNameInOrderById与findByName。\",\"地址： http://127.0.0.1:8080/stocks/search/findByNameInOrderById?list=中国平安,test\",\"查询中国平安与test两只股票，结果：\",\"{ \\\"_embedded\\\" : { \\\"stocksEntities\\\" : [ { \\\"createTime\\\" : \\\"2019-07-09T14:20:44.644+0000\\\", \\\"updateTime\\\" : \\\"2019-07-09T14:20:44.644+0000\\\", \\\"name\\\" : \\\"中国平安\\\", \\\"price\\\" : 68.6, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/1\\\" }, \\\"stocksEntity\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/1\\\" } } } ] }, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8080/stocks/search/findByNameInOrderById?list=%E4%B8%AD%E5%9B%BD%E5%B9%B3%E5%AE%89,latte\\\" } } } \"]},{\"header\":\"4、启动订单服务验证\",\"slug\":\"_4、启动订单服务验证\",\"contents\":[\"启动订单服务， 完整验证四个步骤处理流程。\",\"条件： 订单服务， 预期是修改中国平安的股票， 从价格68.6改成68.9；\",\"新增订单信息： 股票名称建设银行，用户名是mirson, 交易数量为1000， 价格为99.9。\",\"控制台日志\",\"看到两行日志，中国平安的价格发生了变化，从68.6修改为68.9。\",\"查看order服务的订单信息\",\"地址： http://127.0.0.1:8082/order\",\"{ \\\"_embedded\\\" : { \\\"orderEntities\\\" : [ { \\\"createTime\\\" : \\\"2019-07-09T14:35:42.520+0000\\\", \\\"updateTime\\\" : \\\"2019-07-09T14:35:42.520+0000\\\", \\\"user\\\" : \\\"mirson\\\", \\\"stockName\\\" : \\\"建设银行\\\", \\\"volume\\\" : 1000, \\\"price\\\" : 99.9, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8082/order/1\\\" }, \\\"orderEntity\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8082/order/1\\\" } } } ] }, \\\"_links\\\" : { \\\"self\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8082/order{?page,size,sort}\\\", \\\"templated\\\" : true }, \\\"profile\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8082/profile/order\\\" }, \\\"search\\\" : { \\\"href\\\" : \\\"http://127.0.0.1:8082/order/search\\\" } }, \\\"page\\\" : { \\\"size\\\" : 20, \\\"totalElements\\\" : 1, \\\"totalPages\\\" : 1, \\\"number\\\" : 0 } } \",\"生成了我们新增的订单信息。此外还可以通过Postman来模拟增删改查操作， Spring Data Rest都帮我们做好封装。\",\"通过Spring Data Rest整个实现流程非常简单， 没有Controller层， 这正是Restful的设计风格， 以资源为对象， 无需过多的流程，转换处理。\"]},{\"header\":\"4、遇到的问题\",\"slug\":\"_4、遇到的问题\",\"contents\":[]},{\"header\":\"1、SpringBoot中使用整合SpringData JPA，出现“No identifier specified for entity”\",\"slug\":\"_1、springboot中使用整合springdata-jpa-出现-no-identifier-specified-for-entity\",\"contents\":[\"先看代码\",\"import lombok.Data; import org.hibernate.annotations.CreationTimestamp; import org.hibernate.annotations.UpdateTimestamp; import org.springframework.data.annotation.Id; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import java.io.Serializable; import java.util.Date; /** * @author xiaobear * @version 1.0 * @description: 基础类 * @date 2022/10/23 13:57 */ @Data @MappedSuperclass public class BaseEntity implements Serializable { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(updatable = false) @CreationTimestamp private Date createTime; @UpdateTimestamp private Date updateTime; } \",\"后来经过百度查找一番，发现标注@Id引入的包错了\",\"错误引入：import org.springframework.data.annotation.Id;\",\"正确引入：import javax.persistence.Id;\"]},{\"header\":\"2、OrderRepository must only contain a single path segment!\",\"slug\":\"_2、orderrepository-must-only-contain-a-single-path-segment\",\"contents\":[\"先看代码\",\"@RepositoryRestResource(path = \\\"/order\\\") public interface OrderRepository extends JpaRepository<OrderEntity, Long> { /** * 通过用户查询信息 * @param user * @return */ public List<OrderEntity> findByUser(@Param(\\\"user\\\") String user); } \",\"通过查阅资料：资料链接\",\"尝试去除了路径上的'/'，启动发现正常了\"]}]},\"/study-tutorial/devTools/GitNote.html\":{\"title\":\"版本控制Git\",\"contents\":[{\"header\":\"1、版本控制\",\"slug\":\"_1、版本控制\",\"contents\":[\"学习git之前，我们需要先明白一个概念: 版本控制\",\"版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术。\",\"实现跨区域多人协同开发\",\"追踪和记载一个或者多个文件的历史记录\",\"组织和保护你的源代码和文档\",\"统计工作量\",\"并行开发、提高开发效率\",\"跟踪记录整个软件的开发过程\",\"减轻开发人员的负担，节省时间，同时降低人为错误\",\"简单说就是用于管理多人协同开发项目的技术。\",\"没有进行版本控制或者版本控制本身缺乏正确的流程管理，在软件开发过程中将会引入很多问题，如软件代码的一致性、软件内容的冗余、软件过程的事物性、软件开发过程中的并发性、软件源代码的安全性，以及软件的整合等问题。\"]},{\"header\":\"2、常见的版本控制工具\",\"slug\":\"_2、常见的版本控制工具\",\"contents\":[\"主流的版本控制器有如下这些：\",\"Git\",\"SVN（Subversion）\",\"CVS（Concurrent Versions System）\",\"VSS（Micorosoft Visual SourceSafe）\",\"TFS（Team Foundation Server）\",\"Visual Studio Online\",\"版本控制产品非常的多（Perforce、Rational ClearCase、RCS（GNU Revision Control System）、Serena Dimention、SVK、BitKeeper、Monotone、Bazaar、Mercurial、SourceGear Vault），现在影响力最大且使用最广泛的是Git与SVN\"]},{\"header\":\"3、版本控制分类\",\"slug\":\"_3、版本控制分类\",\"contents\":[]},{\"header\":\"1、本地版本控制\",\"slug\":\"_1、本地版本控制\",\"contents\":[]},{\"header\":\"2、集中版本控制 SVN\",\"slug\":\"_2、集中版本控制-svn\",\"contents\":[\"所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改\",\"所有的版本数据都存在服务器上，用户的本地只有自己以前所同步的版本，如果不连网的话，用户就看不到历史版本，也无法切换版本验证问题，或在不同分支工作。而且，所有数据都保存在单一的服务器上，有很大的风险这个服务器会损坏，这样就会丢失所有的数据，当然可以定期备份。代表产品：SVN、CVS、VSS\"]},{\"header\":\"3、分布式版本控制 Git\",\"slug\":\"_3、分布式版本控制-git\",\"contents\":[\"每个人都拥有全部的代码！安全隐患！\",\"所有版本信息仓库全部同步到本地的每个用户，这样就可以在本地查看所有版本历史，可以离线在本地提交，只需在连网时push到相应的服务器或其他用户那里。由于每个用户那里保存的都是所有的版本数据，只要有一个用户的设备没有问题就可以恢复所有的数据，但这增加了本地存储空间的占用。\",\"不会因为服务器损坏或者网络问题，造成不能工作的情况！\"]},{\"header\":\"4、Git与SVN的主要区别\",\"slug\":\"_4、git与svn的主要区别\",\"contents\":[\"SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而工作的时候，用的都是自己的电脑，所以首先要从中央服务器得到最新的版本，然后工作，完成工作后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，对网络带宽要求较高。\",\"Git是分布式版本控制系统，没有中央服务器，每个人的电脑就是一个完整的版本库，工作的时候不需要联网了，因为版本都在自己电脑上。协同的方法是这样的：比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。Git可以直接看到更新了哪些代码和文件！\",\"Git是目前世界上最先进的分布式版本控制系统。\"]},{\"header\":\"4、Git的历史\",\"slug\":\"_4、git的历史\",\"contents\":[\"同生活中的许多伟大事物一样，Git 诞生于一个极富纷争大举创新的年代。\",\"Linux 内核开源项目有着为数众广的参与者。绝大多数的 Linux 内核维护工作都花在了提交补丁和保存归档的繁琐事务上(1991－2002年间)。到 2002 年，整个项目组开始启用一个专有的分布式版本控制系统 BitKeeper 来管理和维护代码。\",\"Linux社区中存在很多的大佬！破解研究 BitKeeper ！\",\"到了 2005 年，开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。这就迫使 Linux 开源社区(特别是 Linux 的缔造者 Linus Torvalds)基于使用 BitKeeper 时的经验教训，开发出自己的版本系统。（2周左右！） 也就是后来的 Git！\",\"Git是目前世界上最先进的分布式版本控制系统。\",\"Git是免费、开源的，最初Git是为辅助 Linux 内核开发的，来替代 BitKeeper！\"]},{\"header\":\"5、Git下载\",\"slug\":\"_5、git下载\",\"contents\":[\"打开 [git官网] https://git-scm.com/，下载git对应操作系统的版本。\",\"所有东西下载慢的话就可以去找镜像！\",\"官网下载太慢，我们可以使用淘宝镜像下载：http://npm.taobao.org/mirrors/git-for-windows/\",\"下载对应的版本即可安装！\",\"安装：无脑下一步即可！安装完毕就可以使用了！\"]},{\"header\":\"6、Git使用\",\"slug\":\"_6、git使用\",\"contents\":[]},{\"header\":\"1、配置用户名和邮箱以及ssh\",\"slug\":\"_1、配置用户名和邮箱以及ssh\",\"contents\":[\"https://www.runoob.com/git/git-remote-repo.html\"]},{\"header\":\"2、Git创建仓库\",\"slug\":\"_2、git创建仓库\",\"contents\":[]},{\"header\":\"git init\",\"slug\":\"git-init\",\"contents\":[\"Git 使用 git init 命令来初始化一个 Git 仓库，Git 的很多命令都需要在 Git 的仓库中运行，所以 git init 是使用 Git 的第一个命令。在执行完成 git init 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变（不像 SVN 会在每个子目录生成 .svn 目录，Git 只在仓库的根目录生成 .git 目录）。\"]},{\"header\":\"git clone\",\"slug\":\"git-clone\",\"contents\":[\"一般情况下，我们是从GitHub或者gitee上创建好仓库，然后我们克隆下来就好了\",\" git clone https://github.com/yhx1001/JavaWeb-note.git //git clone 仓库地址 \"]},{\"header\":\"3、基本操作\",\"slug\":\"_3、基本操作\",\"contents\":[\"这里以克隆下来为例，因为我们常用的就是这个\"]},{\"header\":\"1、把自己的代码或者文件夹、文件放入克隆下来的目录\",\"slug\":\"_1、把自己的代码或者文件夹、文件放入克隆下来的目录\",\"contents\":[]},{\"header\":\"从git bash进入克隆的文件目录\",\"slug\":\"从git-bash进入克隆的文件目录\",\"contents\":[\"cd 仓库名称 \"]},{\"header\":\"2、git add\",\"slug\":\"_2、git-add\",\"contents\":[\"git add 命令可将该文件添加到缓存\",\"git add . 将所有文件添加到缓存 \"]},{\"header\":\"3、git status\",\"slug\":\"_3、git-status\",\"contents\":[\"命令用于查看项目的当前状态。\"]},{\"header\":\"4、git commit\",\"slug\":\"_4、git-commit\",\"contents\":[\"git commit 将缓存区内容添加到仓库中\",\"Git 为你的每一个提交都记录你的名字与电子邮箱地址，所以第一步需要配置用户名和邮箱地址。\"]},{\"header\":\"5、推送到远程仓库\",\"slug\":\"_5、推送到远程仓库\",\"contents\":[\"推送你的新分支与数据到某个远端仓库命令:\",\"git push [alias] [branch] \"]},{\"header\":\"7、总结\",\"slug\":\"_7、总结\",\"contents\":[]},{\"header\":\"1、git基础设置\",\"slug\":\"_1、git基础设置\",\"contents\":[]},{\"header\":\"1、设置用户名\",\"slug\":\"_1、设置用户名\",\"contents\":[\"git config --global user.name \\\"xiaobear\\\" \"]},{\"header\":\"2、设置邮箱\",\"slug\":\"_2、设置邮箱\",\"contents\":[\"git config --global user.email \\\"xxxx.@gmail.com\\\" \"]},{\"header\":\"3、查看配置\",\"slug\":\"_3、查看配置\",\"contents\":[\"git config --list \"]},{\"header\":\"2、创建版本库\",\"slug\":\"_2、创建版本库\",\"contents\":[]},{\"header\":\"1、克隆远程版本库\",\"slug\":\"_1、克隆远程版本库\",\"contents\":[\"git clone 地址 \"]},{\"header\":\"2、初始化本地版本库\",\"slug\":\"_2、初始化本地版本库\",\"contents\":[\"git init \"]},{\"header\":\"3、修改和提交\",\"slug\":\"_3、修改和提交\",\"contents\":[]},{\"header\":\"1、查看状态\",\"slug\":\"_1、查看状态\",\"contents\":[\"git status \"]},{\"header\":\"2、查看变更内容\",\"slug\":\"_2、查看变更内容\",\"contents\":[\"git diff \"]},{\"header\":\"3、跟踪所有改动过的文件\",\"slug\":\"_3、跟踪所有改动过的文件\",\"contents\":[\"git add -A \"]},{\"header\":\"4、跟踪指定文件\",\"slug\":\"_4、跟踪指定文件\",\"contents\":[\"git add <file> \"]},{\"header\":\"5、文件改名\",\"slug\":\"_5、文件改名\",\"contents\":[\"git mv <oldfile> <newfile> \"]},{\"header\":\"6、删除文件\",\"slug\":\"_6、删除文件\",\"contents\":[\"git rm <file> \"]},{\"header\":\"7、停止跟踪文件但不删除\",\"slug\":\"_7、停止跟踪文件但不删除\",\"contents\":[\"git rm --cache <file> \"]},{\"header\":\"8、提交所有更改过的文件\",\"slug\":\"_8、提交所有更改过的文件\",\"contents\":[\"git commit -m \\\"commit message\\\" \"]},{\"header\":\"9、修改最后一次更改\",\"slug\":\"_9、修改最后一次更改\",\"contents\":[\"git commit --amend \"]},{\"header\":\"4、查看提交历史\",\"slug\":\"_4、查看提交历史\",\"contents\":[]},{\"header\":\"1、查看提交历史\",\"slug\":\"_1、查看提交历史\",\"contents\":[\"git log \"]},{\"header\":\"2、查看指定文件提交历史\",\"slug\":\"_2、查看指定文件提交历史\",\"contents\":[\"git log -p <file> \"]},{\"header\":\"3、以列表方式查看指定文件提交历史\",\"slug\":\"_3、以列表方式查看指定文件提交历史\",\"contents\":[\"git blame <file> \"]},{\"header\":\"5、撤销\",\"slug\":\"_5、撤销\",\"contents\":[]},{\"header\":\"1、撤销工作目录中所有未提交文件的修改内容\",\"slug\":\"_1、撤销工作目录中所有未提交文件的修改内容\",\"contents\":[\"git reset -hard HEAD \"]},{\"header\":\"2、撤销指定未提交文件的修改内容\",\"slug\":\"_2、撤销指定未提交文件的修改内容\",\"contents\":[\"git checkout HEAD <file> \"]},{\"header\":\"3、撤销指定的提交\",\"slug\":\"_3、撤销指定的提交\",\"contents\":[\"git revert <commit> \"]},{\"header\":\"6、分支与标签\",\"slug\":\"_6、分支与标签\",\"contents\":[]},{\"header\":\"1、显示本地所有分支\",\"slug\":\"_1、显示本地所有分支\",\"contents\":[\"git branch \"]},{\"header\":\"2、切换到指定分支或标签\",\"slug\":\"_2、切换到指定分支或标签\",\"contents\":[\"git checkout <branch/tag> \"]},{\"header\":\"3、创建新分支\",\"slug\":\"_3、创建新分支\",\"contents\":[\"git branch <new branch> \"]},{\"header\":\"4、删除本地分支\",\"slug\":\"_4、删除本地分支\",\"contents\":[\"git branch -d <branch> \"]},{\"header\":\"5、列出本地所有标签\",\"slug\":\"_5、列出本地所有标签\",\"contents\":[\"git tag \"]},{\"header\":\"6、基于最新提交创建标签\",\"slug\":\"_6、基于最新提交创建标签\",\"contents\":[\"git tag <tagname> \"]},{\"header\":\"7、删除标签\",\"slug\":\"_7、删除标签\",\"contents\":[\"git tag -d <tagname> \",\"我们省去那些概念以及下载，就我们常用的来说就是8步：\",\"gitHub或gitee创建仓库\",\"git bash克隆仓库到本地\",\"git clone 地址 \",\"把你的文件夹或者文件放到克隆的目录\",\"git bash进入克隆文件夹\",\"cd 仓库名称 \",\"git add 命令可将该文件添加到缓存\",\"git add . \",\"git status 命令用于查看项目的当前状态（可要可不要）\",\"git commit 将缓存区内容添加到仓库中\",\"git commit -m \\\"注释\\\" \",\"推送到远程仓库gitHub或者gitee\",\"git push \"]}]},\"/study-tutorial/devTools/Linux.html\":{\"title\":\"Linux常用命令\",\"contents\":[{\"header\":\"1、文本文件编辑操作\",\"slug\":\"_1、文本文件编辑操作\",\"contents\":[\"cat\",\"cat 命令用于查看纯文本文件（内容较少的），格式为“cat [选项] [文件]”。\",\"如果在查看文本内容时还想顺便显示行号的话，不妨在 cat 命令后面追加一个-n 参数：\",\"[root@xiaobear ~]## cat -n xiaobear.sh 1 :wq! 2 xiaobear is very wonderful 3 [root@xiaobear ~]## \",\"tac\",\"tac命令用于倒序查看纯文本文件\",\"[root@xiaobear ~]## tac xiaobear.sh xiaobear is very wonderful :wq! \",\"more\",\"more 命令用于查看纯文本文件（内容较多的），格式为“more [选项]文件”\",\"more 命令会在最下面使用百分比的形式来提示您已经阅读了多少内容\",\"[root@xiaobear ~]## more /etc/profile ## /etc/profile ## System wide environment and startup programs, for login setup ## Functions and aliases go in /etc/bashrc ## It's NOT a good idea to change this file unless you know what you ## are doing. It's much better to create a custom.sh shell script in ## /etc/profile.d/ to make custom changes to your environment, as this ## will prevent the need for merging in future updates. pathmunge () { case \\\":${PATH}:\\\" in *:\\\"$1\\\":*) ;; *) if [ \\\"$2\\\" = \\\"after\\\" ] ; then PATH=$PATH:$1 else PATH=$1:$PATH --More--(27%) \",\"head\",\"head 命令用于查看纯文本文档的前N 行，格式为“head [选项] [文件]”\",\"[root@xiaobear ~]## head -n 20 /etc/profile ## /etc/profile ## System wide environment and startup programs, for login setup ## Functions and aliases go in /etc/bashrc ## It's NOT a good idea to change this file unless you know what you ## are doing. It's much better to create a custom.sh shell script in ## /etc/profile.d/ to make custom changes to your environment, as this ## will prevent the need for merging in future updates. pathmunge () { case \\\":${PATH}:\\\" in *:\\\"$1\\\":*) ;; *) if [ \\\"$2\\\" = \\\"after\\\" ] ; then PATH=$PATH:$1 else PATH=$1:$PATH fi [root@xiaobear ~]## \",\"tail\",\"tail 命令用于查看纯文本文档的后N 行或持续刷新内容，格式为“tail [选项] [文件]”。\",\"[root@xiaobear ~]## tail -n 20 /etc/profile ## You could check uidgid reservation validity in ## /usr/share/doc/setup-*/uidgid file if [ $UID -gt 199 ] && [ \\\"`/usr/bin/id -gn`\\\" = \\\"`/usr/bin/id -un`\\\" ]; then umask 002 else umask 022 fi for i in /etc/profile.d/*.sh /etc/profile.d/sh.local ; do if [ -r \\\"$i\\\" ]; then if [ \\\"${-#*i}\\\" != \\\"$-\\\" ]; then . \\\"$i\\\" else . \\\"$i\\\" >/dev/null fi fi done unset i unset -f pathmunge [root@xiaobear ~]## \",\"tr\",\"tr 命令用于替换文本文件中的字符，格式为“tr [原始字符] [目标字符]”。\",\"[root@xiaobear ~]## cat xiaobear.sh | tr [a-z] [A-Z] :WQ! XIAOBEAR IS VERY WONDERFUL \",\"wc\",\"wc 命令用于统计指定文本的行数、字数、字节数，格式为“wc [参数] 文本”。\",\"[root@xiaobear ~]## wc -lwc xiaobear.sh 3 5 33 xiaobear.sh \",\"stat\",\"stat 命令用于查看文件的具体存储信息和时间等信息，格式为“stat 文件名称”。\",\"[root@xiaobear ~]## stat xiaobear.sh File: ‘xiaobear.sh’ Size: 33 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 67212130 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2020-08-06 01:25:57.952302174 +0800 Modify: 2020-08-04 18:57:41.003772798 +0800 Change: 2020-08-04 18:57:41.004772798 +0800 Birth: - \",\"cut\",\"cut 命令用于按“列”提取文本字符，格式为“cut [参数] 文本”。\",\"如果按列搜索，不仅要使用-f 参数来设置需要看的列数，还需要使用-d 参数来设置间隔符号。\",\"[root@xiaobear ~]## cut -d: -f1 xiaobear.sh xiaobear is very wonderful [root@xiaobear ~]## cut -d: -f2 xiaobear.sh wq! xiaobear is very wonderful \",\"diff\",\"在使用 diff 命令时，不仅可以使用--brief 参数来确认两个文件是否不同，还可以使用-c参数来详细比较出多个文件的差异之处，这绝对是判断文件是否被篡改的有力神器\",\"[root@xiaobear ~]## cat xiaobear.sh :wq! xiaobear is very wonderful [root@xiaobear ~]## cat yhx.sh :wq! xiaobear is very very wonderful [root@xiaobear ~]## diff --brief xiaobear.sh yhx.sh Files xiaobear.sh and yhx.sh differ [root@xiaobear ~]## diff -c xiaobear.sh yhx.sh *** xiaobear.sh 2020-08-04 18:57:41.003772798 +0800 --- yhx.sh 2020-08-06 17:47:44.053530999 +0800 *************** *** 1,3 **** :wq! ! xiaobear is very wonderful --- 1,3 ---- :wq! ! xiaobear is very very wonderful \"]},{\"header\":\"2、文件目录管理\",\"slug\":\"_2、文件目录管理\",\"contents\":[\"touch\",\"touch 命令用于创建空白文件或设置文件的时间，格式为“touch [选项] [文件]”\",\"## 我们先使用ls 命令查看一个文件的修改时间，然后修改这个文件，最后再通过touch命令把修改后的文件时间设置成修改之间的时间 [root@xiaobear ~]## ls -l yhx.sh -rw-r--r-- 1 root root 38 Aug 6 17:47 yhx.sh [root@xiaobear ~]## echo \\\"come on\\\" >> yhx.sh [root@xiaobear ~]## ls -l yhx.sh -rw-r--r-- 1 root root 46 Aug 6 17:55 yhx.sh [root@xiaobear ~]## touch -d \\\"Aug 6 16:00\\\" yhx.sh [root@xiaobear ~]## ls -l yhx.sh -rw-r--r-- 1 root root 46 Aug 6 16:00 yhx.sh \",\"mkdir\",\"mkdir 命令用于创建空白的目录，格式为“mkdir [选项] 目录”。\",\"mkdir 命令还可以结合-p 参数来递归创建出具有嵌套叠层关系的文件目录\",\"[root@xiaobear ~]## mkdir yanghang [root@xiaobear ~]## ls anaconda-ks.cfg f2 f3 xiaobear xiaobear.sh yanghang yhx yhx.sh [root@xiaobear ~]## mkdir -p a/b/c [root@xiaobear ~]## cd a [root@xiaobear a]## ls b \",\"cp\",\"cp 命令用于复制文件或目录，格式为“cp [选项] 源文件目标文件”。\",\"如果目标文件是目录，则会把源文件复制到该目录中；\",\"如果目标文件也是普通文件，则会询问是否要覆盖它；\",\"如果目标文件不存在，则执行正常的复制操作。\",\"[root@xiaobear ~]## cp -r xiaobear yhx [root@xiaobear ~]## cd yhx [root@xiaobear yhx]## ls xiaobear \",\"mv\",\"mv 命令用于剪切文件或将文件重命名，格式为“mv [选项] 源文件 [目标路径|目标文件名]”。\",\"剪切操作不同于复制操作，因为它会默认把源文件删除掉，只保留剪切后的文件。如果 在同一个目录中对一个文件进行剪切操作，其实也就是对其进行重命名：\",\"[root@xiaobear ~]## mv xiaobear.sh yhx.sh mv: overwrite ‘yhx.sh’? y [root@xiaobear ~]## ls a anaconda-ks.cfg f2 f3 xiaobear yanghang yhx yhx.sh \",\"rm\",\"rm 命令用于删除文件或目录，格式为“rm [选项] 文件”。\",\"在 Linux 系统中删除文件时，系统会默认向您询问是否要执行删除操作，如果不想总是看到这种反复的确认信息，可在rm 命令后跟上-f 参数来强制删除。另外，想要删除一个目录，需要在rm 命令后面一个-r 参数才可以，否则删除不掉。\",\"[root@xiaobear ~]## rm -r yhx rm: descend into directory ‘yhx’? y rm: descend into directory ‘yhx/xiaobear’? y rm: remove directory ‘yhx/xiaobear/yhx’? y rm: remove directory ‘yhx/xiaobear’? y rm: remove directory ‘yhx’? y [root@xiaobear ~]## ls a anaconda-ks.cfg f2 f3 xiaobear yanghang yhx.sh [root@xiaobear ~]## rm -f yanghang rm: cannot remove ‘yanghang’: Is a directory [root@xiaobear ~]## rm -rf yanghang \",\"file\",\"file 命令用于查看文件的类型，格式为“file 文件名”。\",\"[root@xiaobear ~]## file yhx.sh yhx.sh: ASCII text \"]},{\"header\":\"3、打包压缩和搜索\",\"slug\":\"_3、打包压缩和搜索\",\"contents\":[\"tar\",\"tar 命令用于对文件进行打包压缩或解压，格式为“tar [选项] [文件]”\",\"一般使用“tar -czvf 压缩包名称.tar.gz 要打包的目录”命令把指定的文件进行打包压缩\",\"相应的解压命令为“tar -xzvf 压缩包名称.tar.gz”\",\"[root@xiaobear ~]## tar -czvf xiaobear.tar.gz xiaobear xiaobear/ xiaobear/yhx/ [root@xiaobear ~]## ls a anaconda-ks.cfg f2 f3 xiaobear xiaobear.tar.gz yhx.sh [root@xiaobear ~]## tar -xzvf xiaobear.tar.gz xiaobear/ xiaobear/yhx/ \",\"grep\",\"grep 命令用于在文本中执行关键词搜索，并显示匹配的结果，格式为“grep [选项] [文件]”。\",\"-n 参数用来显示搜索到信息的行号；-v 参数用于反选信息（即没有包含第2 章新手必须掌握的Linux 命令 56关键词的所有信息行）这两个参数几乎能完成日后80%的工作需要\",\"#在 Linux 系统中，/etc/passwd 文件是保存着所有的用户信息，而一旦用户的登录终端被设置成/sbin/nologin，则不再允许登录系统，因此可以使用grep 命令来查找出当前系统中不允许登录系统的所有用户信息： [root@xiaobear ~]## grep /sbin/nologin /etc/passwd bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin mail:x:8:12:mail:/var/spool/mail:/sbin/nologin operator:x:11:0:operator:/root:/sbin/nologin games:x:12:100:games:/usr/games:/sbin/nologin ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin nobody:x:99:99:Nobody:/:/sbin/nologin systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin dbus:x:81:81:System message bus:/:/sbin/nologin polkitd:x:999:998:User for polkitd:/:/sbin/nologin sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin postfix:x:89:89::/var/spool/postfix:/sbin/nologin dockerroot:x:998:995:Docker User:/var/lib/docker:/sbin/nologin apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin \",\"find\",\"find 命令用于按照指定条件来查找文件，格式为“find [查找路径] 寻找条件操作”\",\"#获取到/etc中所有以host 开头的文件列表 [root@xiaobear ~]## find /etc/ -name host* -print /etc/host.conf /etc/hosts /etc/hosts.allow /etc/hosts.deny /etc/selinux/targeted/active/modules/100/hostname /etc/hostname #查找所有带xiao的文件 find ./ -name \\\"*\\\" |xargs grep xiao \"]},{\"header\":\"4、常用系统工作命令\",\"slug\":\"_4、常用系统工作命令\",\"contents\":[\"echo\",\"echo 命令用于在终端输出字符串或变量提取后的值，格式为“echo [字符串 | $变量]”。\",\"[root@xiaobear ~]## echo $SHELL /bin/bash \",\"date\",\"date 命令用于显示及设置系统的时间或日期，格式为“date [选项] [+指定的格式]”。\",\"[root@xiaobear ~]## date Thu Aug 6 18:35:23 HKT 2020 [root@xiaobear ~]## date -s \\\"20200806 10:37:00\\\" Thu Aug 6 10:37:00 HKT 2020 [root@xiaobear ~]## date Thu Aug 6 10:37:03 HKT 2020 \",\"ps\",\"ps 命令用于查看系统中的进程状态，格式为“ps [参数]”。\",\"Linux 系统中时刻运行着许多进程，如果能够合理地管理它们，则可以优化系统的性能。在Linux 系统中，有 5 种常见的进程状态，分别为运行、中断、不可中断、僵死与停止，其各自含义如下所示。\",\"R（运行）：进程正在运行或在运行队列中等待。\",\"S（中断）：进程处于休眠中，当某个条件形成后或者接收到信号时，则脱离该 状态。\",\"D（不可中断）：进程不响应系统异步信号，即便用 kill 命令也不能将其中断。\",\"Z（僵死）：进程已经终止，但进程描述符依然存在, 直到父进程调用 wait4()系统函数后将进程释放。\",\"T（停止）：进程收到停止信号后停止运行。\",\"[root@xiaobear ~]## ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 191044 5376 ? Ss 09:15 0:01 /usr/lib/syst root 2 0.0 0.0 0 0 ? S 09:15 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? I< 09:15 0:00 [rcu_gp] root 4 0.0 0.0 0 0 ? I< 09:15 0:00 [rcu_par_gp] root 6 0.0 0.0 0 0 ? I< 09:15 0:00 [kworker/0:0H root 7 0.0 0.0 0 0 ? I 09:15 0:00 [kworker/u256 root 8 0.0 0.0 0 0 ? I< 09:15 0:00 [mm_percpu_wq root 9 0.0 0.0 0 0 ? S 09:15 0:00 [ksoftirqd/0] root 10 0.0 0.0 0 0 ? I 09:15 0:02 [rcu_sched] root 11 0.0 0.0 0 0 ? S 09:15 0:00 [migration/0] root 13 0.0 0.0 0 0 ? S 09:15 0:00 [cpuhp/0] root 14 0.0 0.0 0 0 ? S 09:15 0:00 [cpuhp/1] root 15 0.0 0.0 0 0 ? S 09:15 0:00 [migration/1] root 16 0.0 0.0 0 0 ? S 09:15 0:00 [ksoftirqd/1] root 18 0.0 0.0 0 0 ? I< 09:15 0:00 [kworker/1:0H root 19 0.0 0.0 0 0 ? S 09:15 0:00 [cpuhp/2] root 20 0.0 0.0 0 0 ? S 09:15 0:00 [migration/2] root 21 0.0 0.0 0 0 ? S 09:15 0:00 [ksoftirqd/2] root 23 0.0 0.0 0 0 ? I< 09:15 0:00 [kworker/2:0H root 24 0.0 0.0 0 0 ? S 09:15 0:00 [cpuhp/3] root 25 0.0 0.0 0 0 ? S 09:15 0:00 [migration/3] root 26 0.0 0.0 0 0 ? S 09:15 0:00 [ksoftirqd/3] root 28 0.0 0.0 0 0 ? I< 09:15 0:00 [kworker/3:0H \",\"PS： ps 命令可允许参数不加减号（-），因此可直接写成 ps aux 的样子\",\"top\",\"top 命令用于动态地监视进程活动与系统负载等信息，其格式为top。\",\"top 命令相当强大，能够动态地查看系统运维状态，完全将它看作Linux 中的“强化版的Windows 任务管理器”\",\"pidof\",\"pidof 命令用于查询某个指定服务进程的PID 值，格式为“pidof [参数] [服务名称]”。\",\"[root@xiaobear ~]## pidof sshd 1432 1354 1043 \",\"kill\",\"kill 命令用于终止某个指定PID 的服务进程，格式为“kill [参数] [进程PID]”\",\"[root@xiaobear ~]## kill 1432 \",\"killall\",\"killall 命令用于终止某个指定名称的服务所对应的全部进程，格式为：“killall [参数] [进程名称]”。\",\"[root@xiaobear ~]## pidof httpd 13581 13580 13579 13578 13577 13576 [root@xiaobear ~]## killall httpd [root@xiaobear ~]## pidof httpd \"]},{\"header\":\"5、系统状态监测命令\",\"slug\":\"_5、系统状态监测命令\",\"contents\":[\"ip addr\",\"用于获取网卡配置与网络状态等信息\",\"[root@xiaobear ~]## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:be:96:6b brd ff:ff:ff:ff:ff:ff inet 192.168.222.129/24 brd 192.168.222.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::f050:cc96:ba62:825c/64 scope link noprefixroute valid_lft forever preferred_lft forever [root@xiaobear ~]## \",\"ifconfig\",\"ifconfig 命令用于获取网卡配置与网络状态等信息，格式为“ifconfig [网络设备] [参数]”。使用 ifconfig 命令来查看本机当前的网卡配置与网络状态等信息时，其实主要查看的就是网卡名称、inet 参数后面的IP 地址、ether 参数后面的网卡物理地址（又称为MAC 地址），以及RX、TX 的接收数据包与发送数据包的个数及累计流量（即下面加粗的信息内容）：\",\"**注：**ifconfig附属在net-tools下，如果要使用ifconfig，则需要yum install net-tools -y\",\"[root@xiaobear ~]## ifconfig ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.222.129 netmask 255.255.255.0 broadcast 192.168.222.255 inet6 fe80::f050:cc96:ba62:825c prefixlen 64 scopeid 0x20<link> ether 00:0c:29:be:96:6b txqueuelen 1000 (Ethernet) RX packets 12649 bytes 4424866 (4.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 3829 bytes 680034 (664.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 (Local Loopback) RX packets 2 bytes 1152 (1.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 1152 (1.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@xiaobear ~]## \",\"uname\",\"uname 命令用于查看系统内核与系统版本等信息，格式为“uname [-a]”。\",\"在使用 uname 命令时，一般会固定搭配上-a 参数来完整地查看当前系统的内核名称、主机名、内核发行版本、节点名、系统时间、硬件名称、硬件平台、处理器类型以及操作系统名称等信息。\",\"[root@xiaobear ~]## uname -a Linux xiaobear 5.8.0-1.el7.elrepo.x86_64 #1 SMP Sun Aug 2 18:18:16 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux [root@xiaobear ~]## uname -r 5.8.0-1.el7.elrepo.x86_64 [root@xiaobear ~]## cat /etc/redhat-release CentOS Linux release 7.8.2003 (Core) \",\"uptime\",\"uptime 用于查看系统的负载信息，格式为uptime。 uptime 命令真的很棒，它可以显示当前系统时间、系统已运行时间、启用终端数量以及平均负载值等信息。平均负载值指的是系统在最近1 分钟、5 分钟、15 分钟内的压力情况（下面加粗的信息部分）；负载值越低越好，尽量不要长期超过1，在生产环境中不要超过5。\",\"[root@xiaobear ~]## uptime 11:05:21 up 1:49, 2 users, load average: 0.05, 0.01, 0.00 \",\"free\",\"free 用于显示当前系统中内存的使用量信息，格式为“free [-h]”。\",\"为了保证Linux 系统不会因资源耗尽而突然宕机，运维人员需要时刻关注内存的使用量。 在使用free 命令时，可以结合使用-h 参数以更人性化的方式输出当前内存的实时使用量信息。\",\"[root@xiaobear ~]## free -h total used free shared buff/cache available Mem: 954M 354M 259M 6.7M 340M 450M Swap: 2.0G 0B 2.0G \",\"history\",\"history 命令用于显示历史执行过的命令，格式为“history [-c]”。\",\"[root@xiaobear ~]## history 1 ip addr 2 vi /etc/sysconfig/network-scripts/ifcfg-ens33 3 ip addr 4 vi /etc/sysconfig/network-scripts/ifcfg-ens33 5 service network restart 6 ip addr 7 ping www.baidu.com 8 service network restart 9 ping www.baidu.com 10 reboot `````省略 \",\"last\",\"last 命令用于查看所有系统的登录记录，格式为“last [参数]”。\",\"[root@xiaobear ~]## last root pts/1 192.168.222.1 Thu Aug 6 18:03 - 10:53 (-7:-10) root pts/0 192.168.222.1 Thu Aug 6 17:16 still logged in root tty1 Thu Aug 6 17:16 still logged in reboot system boot 5.8.0-1.el7.elre Thu Aug 6 17:16 - 11:08 (-6:-7) root pts/0 192.168.222.1 Thu Aug 6 08:32 - 08:36 (00:04) ​`````省略 \"]},{\"header\":\"6、工作目录切换命令\",\"slug\":\"_6、工作目录切换命令\",\"contents\":[\"pwd\",\"pwd 命令用于显示用户当前所处的工作目录，格式为“pwd [选项]”。\",\"[root@xiaobear ~]## pwd /root \",\"cd\",\"cd 命令用于切换工作路径，格式为“cd [目录名称]”。\",\"“cd -”命令返回到上一次所处的目录\",\"使用“cd..”命令进入上级目录\",\"使用“cd ~”命令切换到当前用户的家目录，亦或使用“cd ~username”切换到其他用户的家目录\",\"[root@xiaobear ~]## cd /usr/local/ [root@xiaobear local]## cd .. [root@xiaobear usr]## cd ~ [root@xiaobear ~]## \",\"ls\",\"ls 命令用于显示目录中的文件信息，格式为“ls [选项] [文件] ”。\",\"使用ls 命令的“-a”参数看到全部文件（包括隐藏文件）\",\"使用“-l”参数可以查看文件的属性、大小等详细信息\",\"[root@xiaobear ~]## ls -al total 64 dr-xr-x---. 7 root root 4096 Aug 6 2020 . dr-xr-xr-x. 17 root root 244 Aug 4 22:32 .. drwxr-xr-x 3 root root 15 Aug 6 2020 a -rw-------. 1 root root 1429 Jul 25 06:51 anaconda-ks.cfg -rw-------. 1 root root 8358 Aug 6 10:53 .bash_history -rw-r--r--. 1 root root 18 Dec 29 2013 .bash_logout -rw-r--r--. 1 root root 176 Dec 29 2013 .bash_profile -rw-r--r--. 1 root root 176 Dec 29 2013 .bashrc -rw-r--r--. 1 root root 100 Dec 29 2013 .cshrc \"]},{\"header\":\"7、安装jdk\",\"slug\":\"_7、安装jdk\",\"contents\":[\"1、查找java列表\",\"yum list java* \",\"2、安装\",\"yum install java-1.8.0-openj7*k-devel.x86_64 -y \",\"3、配置环境变量\",\"JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 JRE_HOME=$JAVA_HOME/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin ## export JAVA_HOME JRE_HOME CLASS_PATH PATH \",\"4、查看版本\",\"java -version \",\"5、编译打包\",\"#1、单个文件\",\"#1.编译 javac xxx.java #编译java文件到指定位置 javac xxx.java -d 目标目录 #2.打包jar jar cvf xxx.jar xxx.class #3.运行 java -jar xxx.jar \",\"#2、多个文件\",\"#1.编译多个文件，输出到目标目录 javac helloworld.java xiaobear.java -d target/ #2.打包多个文件并指定MANIFEST vim mainfest #内容：前两行可要可不要，指定主类 Manifest-Version: 1.0 Created-By: 1.8.0_252 (Oracle Corporation) Main-Class: com.javademo.Second #打包jar jar cvfm helloworld.jar manifest helloworld.class #运行 java -jar xxx.jar #不间断运行 nohup java -jar xxx.jar \",\"6、遇到的问题\",\"#1、no main manifest attribute, in helloworld.jar\",\"##解决办法：\",\"编译指定类，步骤如下：\",\"先编译java类\",\"javac xxx.java \",\"新建一个文件，文件里面指定Main-Class\",\"#vim mainfest Main-Class:包名+java类名 \",\"打成jar包\",\"jar cvfm xxx.jar manifest xxx.class \",\"运行\",\"java -jar xxx.jar \"]},{\"header\":\"8、apache相关命令\",\"slug\":\"_8、apache相关命令\",\"contents\":[\"1、centos7+安装\",\"yum install httpd -y \",\"#1、运行\",\"systemctl start httpd \",\"#2、停止\",\"systemctl stop httpd \",\"#3、重启\",\"systemctl restart httpd \",\"#4、查看状态\",\"systemctl status httpd \"]},{\"header\":\"9、线程操作\",\"slug\":\"_9、线程操作\",\"contents\":[\"ps\",\"ps 命令用于查看系统中的进程状态，格式为“ps [参数]”。\",\"-A 列出所有的行程\",\"-w 显示加宽可以显示较多的资讯\",\"-au 显示较详细的资讯\",\"-aux 显示所有包含其他使用者的行程\",\"au(x) 输出格式 :\"]},{\"header\":\"10、yum命令合集\",\"slug\":\"_10、yum命令合集\",\"contents\":[\"1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install <package_name> 4.仅更新指定的软件命令：yum update <package_name> 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove <package_name> 7.查找软件包 命令：yum search <keyword> 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers \"]},{\"header\":\"11、防火墙\",\"slug\":\"_11、防火墙\",\"contents\":[\"查看防火墙状态\",\"systemctl status firewalld \",\"开启防火墙\",\"systemctl start firewalld \",\"开机启动防火墙\",\"systemctl enable firewalld \",\"开放端口\",\"#开放22号端口 firewall-cmd --zone=public --add-port=22/tcp --permanent \",\"重载防火墙设置\",\"firewall-cmd --reload \",\"查看系统所有放开的端口\",\"firewall-cmd --zone=public --list-ports \",\"移除我们开放的端口\",\"firewall-cmd --zone=public --remove-port=22/tcp --permanent \",\"允许192.168.1.10所有访问所有端口\",\"firewall-cmd --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.1.10\\\" accept' --permanent \",\"移除192.168.1.10所有访问所有端口\",\"firewall-cmd --zone=public --remove-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.1.10\\\" accept' --permanent \",\"允许192.168.2.0/24(0-255)所有访问所有端口\",\"firewall-cmd --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.2.0/24\\\" accept' --permanent \",\"允许192.168.1.10所有访问TCP协议的22端口\",\"firewall-cmd --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.1.10\\\" port port=22 protocol=tcp reject' --permanent \",\"允许192.168.1.10所有访问TCP协议的22端口\",\"firewall-cmd --zone=public --remove-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.1.10\\\" port port=22 protocol=tcp reject' --permanent \",\"防火墙重新载入(必须重新载入后才能生效)\",\"firewall-cmd --reload \",\"查看rich-rules（富规则)\",\"firewall-cmd --list-rich-rules \",\"查看防火墙服务规则\",\"firewall-cmd --list-services \",\"查看 防火墙所有规则\",\"firewall-cmd --list-all \",\"查看防火墙所有区域的配置规则\",\"firewall-cmd --list-all-zones \",\"查看默认区域\",\"firewall-cmd --get-default-zone \",\"查看网络接口使用区域\",\"firewall-cmd --get-active-zones \",\"查看默认的可用服务\",\"firewall-cmd --get-services \",\"要启用或禁用HTTP服务\",\"firewall-cmd --zone=public --add-service=http --permanent firewall-cmd --zone=public --remove-service=http --permanent \",\"注意：\",\"设置未生效，可尝试直接编辑规则文件，删掉原来的设置规则，重新载入一下防火墙即可\",\"vi /etc/firewalld/zones/public.xml\"]}]},\"/study-tutorial/route/mine.html\":{\"title\":\"学习路线\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"说明\",\"以下纯属个人的观点，适合自己的最重要，大家可以做个参考\",\"关于这篇，应该很早就要更新了，奈何一直拖拖拖（因为自己在跟着做项目），导致到现在才更新，今天就聊一聊\",\"关于我科班出身，自己接触Java比较早，但真正实际能到开发阶段，还是在疫情期间，因为要准备实习了，看网上这么卷，所以才开始框架的学习，在此之前，学习的态度的：三天打鱼，两天晒网；\",\"现在回想之前的大学时光，感觉自己很荒废，自己也走了不少弯路，强烈建议：如果能找到一个大佬带你学习或者给你提供学习路线等，你将会少走很多弯路！\",\"不然大部分应该和我一样，学完一个全新的框架之后，总会纠结到底该学哪个，在此件事情上浪费很多时间！\",\"现在Java的现状就是：\"]},{\"header\":\"关于路线图\",\"slug\":\"关于路线图\",\"contents\":[\"大家在网上可以看到很多学习路线图，有各个大佬总结的，很多培训机构也都有提供，这个没有哪个好的说法，适合自己最重要\",\"下面是我在网上找到的一些\",\"p5学习路线图\",\"图灵Java架构师体系图\",\"这两个路线个人感觉是比较全，很细，也有在线地址，大家取用也很方便；但是全部按照这个来学习，对于学生来说，肯定时间不够的\",\"所以我自己粗略整理了一下，如下：\"]},{\"header\":\"第一阶段：Java基础\",\"slug\":\"第一阶段-java基础\",\"contents\":[\"Java三大特性：封装、继承、多态，这三点必须要掌握，笔面试中对于应届生基本必问\",\"其次的话，就是流程控制、异常体系、数组、集合与泛型、线程、IO、反射\"]},{\"header\":\"第二阶段：数据库\",\"slug\":\"第二阶段-数据库\",\"contents\":[\"推荐MySQL，大部分公司用的都是MySQL\",\"初次学习可以不深入，能学会增删改查、多表查询、联合查询、子查询等就可以了，后面索引等再系统学习\",\"我的学习笔记：数据库笔记\"]},{\"header\":\"第三阶段：框架篇\",\"slug\":\"第三阶段-框架篇\",\"contents\":[\"如果你现在处于要实习的阶段，建议直接学习Spring Boot，快速上手，后面再慢慢补SSM\",\"如果时间不急，可以深入学习，那就可以安装Spring、SpringMVC、MyBatis的路线进行学习\",\"Spring的IOC和AOP理解上可能会有点抽象，不是很懂，我之前也是一样，到项目中用到了就知道有啥用处了\",\"其次就是Spring Boot，它就是一个快速上手的框架，总之就是有点牛逼\",\"Spring Cloud：目前用的最多的是Spring Cloud Alibaba这套，但其他也要学习，不同公司使用的框架不同\",\"框架篇笔记\"]},{\"header\":\"第四阶段：项目实战\",\"slug\":\"第四阶段-项目实战\",\"contents\":[\"学到这里，出去找工作基本就没问题了，但是还差一个项目实战，网上项目很多，这个可以自行选择\",\"时间充足，可以cloud + vue\",\"不充足的话boot + vue\",\"可不可以不实战，这个看你个人，项目可以帮你理解学习的知识以及如何运用\"]},{\"header\":\"进阶篇\",\"slug\":\"进阶篇\",\"contents\":[\"应届生主攻数据结构与算法，要想进大厂，多上力扣多刷题，计算机基础和网络也很重要，其次更深入一点的就是底层源码理解多背八股文\"]},{\"header\":\"高级篇\",\"slug\":\"高级篇\",\"contents\":[\"这里我就不说了，毕竟我自己都没达到高级，正在慢慢摸索ing，可以参考上面完整的路线图\"]},{\"header\":\"学习方法\",\"slug\":\"学习方法\",\"contents\":[\"个人建议还是边学习边做笔记整理，整理的过程其实也是加深印象，整理思维思路的过程。\",\"我的话，每学习一个新的框架都会有一个笔记文档，记录自己的操作步骤，注意事项以及采坑记录\",\"上面是我学习过程中整理的笔记，也就是今天你们看到的在线网站：https://javaxiaobear.gitee.io/，建议收藏\"]},{\"header\":\"为什么要做笔记？\",\"slug\":\"为什么要做笔记\",\"contents\":[\"很多人可能觉得，边敲代码边做笔记很耗时间，这点是肯定的！那既然耗时间，为什么还要做呢？\",\"因为在学习新知识的过程中，旧知识你也会慢慢遗忘（如果你是记忆大佬，当我没说），然后当你回顾的时候，虽然你有印象，但是关键步骤可能又得去找度娘了，以此反复！\"]},{\"header\":\"如何学习\",\"slug\":\"如何学习\",\"contents\":[\"我该怎么学，到底该不该我纠结那些细节，我的建议就是不必过多去纠结，可以先把问题记录下来，学习本来就是思维慢慢开阔的过程，遇到纠结的点很正常，有时候一个bug能卡一天，但这并不代表不适合编程，你能力不行，过两天回头看这个问题，可能就迎刃而解了\"]}]},\"/interview/database/150mysql/development.html\":{\"title\":\"MySQL开发\",\"contents\":[{\"header\":\"102、可以使用MySQL直接存储文件吗？\",\"slug\":\"_102、可以使用mysql直接存储文件吗\",\"contents\":[\"可以使用 BLOB (binary large object)，用来存储二进制大对象的字段类型。\",\"TinyBlob 255 值的长度加上用于记录长度的1个字节(8位)\",\"Blob 65K值的长度加上用于记录长度的2个字节(16位)\",\"MediumBlob 16M值的长度加上用于记录长度的3个字节(24位)\",\"LongBlob 4G 值的长度加上用于记录长度的4个字节(32位)。\"]},{\"header\":\"103、什么时候存，什么时候不存？\",\"slug\":\"_103、什么时候存-什么时候不存\",\"contents\":[\"存：需要高效查询并且文件很小的时候\",\"不存：文件比较大，数据量多或变更频繁的时候\"]},{\"header\":\"104 存储的时候有遇到过什么问题吗？\",\"slug\":\"_104存储的时候有遇到过什么问题吗\",\"contents\":[\"上传数据过大sql执行失败 调整max_allowed_packet\",\"主从同步数据时比较慢\",\"应用线程阻塞\",\"占用网络带宽\",\"高频访问的图片无法使用浏览器缓存\"]},{\"header\":\"105 Emoji乱码怎么办？\",\"slug\":\"_105emoji乱码怎么办\",\"contents\":[\"使用utf8mb4\",\"MySQL在5.5.3之后增加了这个utf8mb4的编码，mb4就是most bytes 4的意思，专门用来兼容四字节的unicode。好在utf8mb4是utf8的超集，除了将编码改为utf8mb4外不需要做其他转换。当然，一般情况下使用utf8也就够了。\"]},{\"header\":\"106 如何存储ip地址？\",\"slug\":\"_106-如何存储ip地址\",\"contents\":[\"使用字符串\",\"使用无符号整型\",\"4个字节即解决问题\",\"可以支持范围查询\",\"INET_ATON() 和 INET_NTOA() ipv6 使用 INET6_ATON() 和 INET6_NTOA()\"]},{\"header\":\"107 长文本如何存储？\",\"slug\":\"_107-长文本如何存储\",\"contents\":[\"可以使用Text存储\",\"TINYTEXT(255长度)\",\"TEXT(65535)\",\"MEDIUMTEXT（int最大值16M）\",\"LONGTEXT(long最大值4G)\"]},{\"header\":\"108、大段文本如何设计表结构？\",\"slug\":\"_108、大段文本如何设计表结构\",\"contents\":[\"或将大段文本同时存储到搜索引擎\",\"分表存储\",\"分表后多段存储\"]},{\"header\":\"109、大段文本查找时如何建立索引？\",\"slug\":\"_109、大段文本查找时如何建立索引\",\"contents\":[\"全文检索，模糊匹配最好存储到搜索引擎中\",\"指定索引长度\",\"分段存储后创建索引\"]},{\"header\":\"110、有没有在开发中使用过TEXT,BLOB 数据类型\",\"slug\":\"_110、有没有在开发中使用过text-blob-数据类型\",\"contents\":[\"BLOB 之前做ERP的时候使用过，互联网项目一般不用BLOB\",\"TEXT 文献，文章，小说类，新闻，会议内容 等\"]},{\"header\":\"111、日期，时间如何存取？\",\"slug\":\"_111、日期-时间如何存取\",\"contents\":[\"使用 TIMESTAMP，DATETIME\",\"使用字符串\"]},{\"header\":\"112、TIMESTAMP，DATETIME 的区别是什么？\",\"slug\":\"_112、timestamp-datetime-的区别是什么\",\"contents\":[\"跨时区的业务使用 TIMESTAMP，TIMESTAMP会有时区转换\",\"1、两者的存储方式不一样: 对于TIMESTAMP，它把客户端插入的时间从当前时区转化为UTC（世界标准时间）进行存储。查询时，将其又转化为客户端当前时区进行返回。 而对于DATETIME，不做任何改变，基本上是原样输入和输出。\",\"2、存储字节大小不同\",\"数据类型\",\"MySQL 5.6.4之前需要存储\",\"MySQL 5.6.4之后需要存储\",\"DATETIME\",\"8 bytes\",\"5 bytes + 小数秒存储\",\"TIMESTAMP\",\"4 bytes\",\"4 bytes + 小数秒存储\",\"分秒数精度\",\"存储字节大小\",\"0\",\"0 bytes\",\"1,2\",\"1 bytes\",\"3,4\",\"2 bytes\",\"5,6\",\"3 bytes\",\"3、两者所能存储的时间范围不一样:\",\"timestamp所能存储的时间范围为：'1970-01-01 00:00:01.000000' 到 '2038-01-19 03:14:07.999999'。\",\"datetime所能存储的时间范围为：'1000-01-01 00:00:00.000000' 到 '9999-12-31 23:59:59.999999'。\"]},{\"header\":\"113、为什么不使用字符串存储日期？\",\"slug\":\"_113、为什么不使用字符串存储日期\",\"contents\":[\"字符串无法完成数据库内部的范围筛选\",\"在大数据量存储优化索引时，查询必须加上时间范围\"]},{\"header\":\"114、如果需要使用时间戳 timestamp和int该如何选择？\",\"slug\":\"_114、如果需要使用时间戳-timestamp和int该如何选择\",\"contents\":[\"int 存储空间小，运算查询效率高，不受时区影响，精度低\",\"timestamp 存储空间小，可以使用数据库内部时间函数比如更新，精度高，需要注意时区转换，timestamp更易读\",\"一般选择timestamp，两者性能差异不明显，本质上存储都是使用的int\"]},{\"header\":\"115、char与varchar的区别？如何选择？\",\"slug\":\"_115、char与varchar的区别-如何选择\",\"contents\":[\"char的优点是存储空间固定（最大255），没有碎片，尤其更新比较频繁的时候，方便数据文件指针的操作，所以存储读取速度快。缺点是空间冗余，对于数据量大的表，非固定长度属性使用char字段，空间浪费。\",\"varchar字段，存储的空间根据存储的内容变化，空间长度为L+size，存储内容长度加描述存储内容长度信息，优点就是空间节约，缺点就是读取和存储时候，需要读取信息计算下标，才能获取完整内容。\"]},{\"header\":\"116 财务计算有没有出现过错乱？\",\"slug\":\"_116财务计算有没有出现过错乱\",\"contents\":[\"第一类：锁包括多线程，数据库，UI展示后超时提交等\",\"第二类：应用与数据库浮点运算精度丢失\",\"应用开发问题：多线程共享数据读写，\",\"之前有过丢失精度的问题，使用decimal解决\",\"使用乘法替换除法\",\"使用事务保证acid特性\",\"更新时使用悲观锁 SELECT … FOR UPDATE\",\"数据只有标记删除\",\"记录详细日志方便溯源\"]},{\"header\":\"117、decimal与float,double的区别是什么？\",\"slug\":\"_117、decimal与float-double的区别是什么\",\"contents\":[\"float：浮点型，4字节，32bit。\",\"double：双精度实型，8字节，64位\",\"decimal：数字型，128bit，不存在精度损失\",\"对于声明语法DECIMAL(M,D)，自变量的值范围如下：\",\"M是最大位数（精度），范围是1到65。可不指定，默认值是10。\",\"D是小数点右边的位数（小数位）。范围是0到30，并且不能大于M，可不指定，默认值是0。\",\"例如字段 salary DECIMAL(5,2)，能够存储具有五位数字和两位小数的任何值，因此可以存储在salary列中的值的范围是从-999.99到999.99。\"]},{\"header\":\"118、浮点类型如何选型？为什么？\",\"slug\":\"_118、浮点类型如何选型-为什么\",\"contents\":[\"需要不丢失精度的计算使用DECIMAL\",\"仅用于展示没有计算的小数存储可以使用字符串存储\",\"低价值数据允许计算后丢失精度可以使用float double\",\"整型记录不会出现小数的不要使用浮点类型\"]},{\"header\":\"119、预编译sql是什么？\",\"slug\":\"_119、预编译sql是什么\",\"contents\":[\"所谓预编译语句就是将此类 SQL 语句中的值用占位符替代，可以视为将 SQL 语句模板化或者说参数化，一般称这类语句叫Prepared Statements。 预编译语句的优势在于归纳为：一次编译、多次运行，省去了解析优化等过程；此外预编译语句能防止 SQL 注入\",\"完整解释：https://dev.MySQL.com/doc/refman/8.0/en/prepare.html\"]},{\"header\":\"120、预编译sql有什么好处？\",\"slug\":\"_120、预编译sql有什么好处\",\"contents\":[\"预编译sql会被MySQL缓存下来\",\"作用域是每个session，对其他session无效，重新连接也会失效\",\"提高安全性防止sql注入 \",\"select * from user where id =?\",\"\\\"1;delete from user where id = 1\\\";\",\"编译语句有可能被重复调用，也就是说sql相同参数不同在同一session中重复查询执行效率明显比较高\",\"MySQL 5,8 支持服务器端的预编译\"]},{\"header\":\"121 子查询与join哪个效率高？\",\"slug\":\"_121子查询与join哪个效率高\",\"contents\":[\"子查询虽然很灵活，但是执行效率并不高。\"]},{\"header\":\"122、为什么子查询效率低？\",\"slug\":\"_122、为什么子查询效率低\",\"contents\":[\"在执行子查询的时候，MySQL创建了临时表，查询完毕后再删除这些临时表\",\"子查询的速度慢的原因是多了一个创建和销毁临时表的过程。 而join 则不需要创建临时表 所以会比子查询快一点\"]},{\"header\":\"123、join查询可以无限叠加吗？MySQL对join查询有什么限制吗？\",\"slug\":\"_123、join查询可以无限叠加吗-mysql对join查询有什么限制吗\",\"contents\":[\"建议join不超过3张表关联，MySQL对内存敏感，关联过多会占用更多内存空间，使性能下降\",\"Too many tables; MySQL can only use 61 tables in a join；\",\"系统限制最多关联61个表\"]},{\"header\":\"124、join 查询算法了解吗？\",\"slug\":\"_124、join-查询算法了解吗\",\"contents\":[\"Simple Nested-Loop Join：SNLJ，简单嵌套循环连接\",\"Index Nested-Loop Join：INLJ，索引嵌套循环连接\",\"Block Nested-Loop Join：BNLJ，缓存块嵌套循环连接\"]},{\"header\":\"125、如何优化过多join查询关联？\",\"slug\":\"_125、如何优化过多join查询关联\",\"contents\":[\"适当使用冗余字段减少多表关联查询\",\"驱动表和被驱动表（小表join大表）\",\"业务允许的话 尽量使用inner join 让系统帮忙自动选择驱动表\",\"关联字段一定创建索引\",\"调整JOIN BUFFER大小\"]},{\"header\":\"126 是否有过MySQL调优经验？\",\"slug\":\"_126是否有过mysql调优经验\",\"contents\":[\"sql调优\",\"表（结构）设计调优\",\"索引调优\",\"慢查询调优\",\"操作系统调优\",\"数据库参数调优\"]},{\"header\":\"127、开发中使用过哪些调优工具？\",\"slug\":\"_127、开发中使用过哪些调优工具\",\"contents\":[\"官方自带：\",\"EXPLAIN\",\"MySQLdumpslow\",\"show profiles 时间\",\"optimizer_trace\",\"第三方：性能诊断工具，参数扫描提供建议，参数辅助优化\"]},{\"header\":\"128、如何监控线上环境中执行比较慢的sql？ 129 如何分析一条慢sql？\",\"slug\":\"_128、如何监控线上环境中执行比较慢的sql-129如何分析一条慢sql\",\"contents\":[\"开启慢查询日志，收集sql\",\"Ø 默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。\",\"Ø 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。\",\"查看及开启\",\"默认关闭\",\"SHOW VARIABLES LIKE '%slow_query_log%'; \",\"默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的，\",\"​\",\"开启：set global slow_query_log=1; 只对窗口生效，重启服务失效\",\"慢查询日志记录long_query_time时间\",\"SHOW VARIABLES LIKE '%long_query_time%'; SHOW GLOBAL VARIABLES LIKE 'long_query_time'; \",\"l 全局变量设置，对所有客户端有效。但，必须是设置后进行登录的客户端。\",\"SET GLOBAL long_query_time=0.1;\",\"l 对当前会话连接立即生效，对其他客户端无效。\",\"SET SESSION long_query_time=0.1; #session可省略\",\"假如运行时间正好等于long_query_time的情况，并不会被记录下来。也就是说，\",\"在MySQL源码里是判断大于long_query_time，而非大于等于。\",\"永久生效\",\"修改配置文件my.cnf（其它系统变量也是如此）\",\"[MySQLd]下增加或修改参数\",\"slow_query_log 和slow_query_log_file后，然后重启MySQL服务器。也即将如下两行配置进my.cnf文件\",\"slow_query_log =1\",\"slow_query_log_file=/var/lib/MySQL/localhost-slow.log\",\"long_query_time=3\",\"log_output=FILE\",\"关于慢查询的参数slow_query_log_file，它指定慢查询日志文件的存放路径，如果不设置，系统默认文件：[host_name]-slow.log\",\"case\",\"Ø 记录慢SQL并后续分析\",\"SELECT * FROM emp;\",\"SELECT * FROM emp WHERE deptid > 1;\",\"Ø 查询当前系统中有多少条慢查询记录或者直接看慢查询日志\",\"/var/lib/MySQL/localhost-slow.log\",\"SHOW GLOBAL STATUS LIKE '%Slow_queries%';\",\"日志分析工具MySQLdumpslow\",\"在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具MySQLdumpslow。\",\"查看MySQLdumpslow的帮助信息\",\"a) MySQLdumpslow --help · -a: 将数字抽象成N，字符串抽象成S · -s: 是表示按照何种方式排序； c: 访问次数 l: 锁定时间 r: 返回记录 **t:** **查询时间** al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 · -t: 即为返回前面多少条的数据； · -g: 后边搭配一个正则匹配模式，大小写不敏感的； 得到返回记录集最多的10个SQL MySQLdumpslow -s r -t 10 /var/lib/MySQL/localhost-slow.log 得到访问次数最多的10个SQL MySQLdumpslow -s c -t 10 /var/lib/MySQL/localhost-slow.log 得到按照时间排序的前10条里面含有左连接的查询语句 MySQLdumpslow -s t -t 10 -g \\\"left join\\\" /var/lib/MySQL/localhost-slow.log 另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况 MySQLdumpslow -s r -t 10 /var/lib/MySQL/localhost-slow.log | more \"]},{\"header\":\"130、如何查看当前sql使用了哪个索引？\",\"slug\":\"_130、如何查看当前sql使用了哪个索引\",\"contents\":[\"可以使用EXPLAIN，选择索引过程可以使用 optimizer_trace\"]},{\"header\":\"131、索引如何进行分析和调优？\",\"slug\":\"_131、索引如何进行分析和调优\",\"contents\":[]},{\"header\":\"132、EXPLAIN关键字中的重要指标有哪些？\",\"slug\":\"_132、explain关键字中的重要指标有哪些\",\"contents\":[\"使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。\",\"EXPLAIN的用法\",\"用法：\",\"EXPLAIN + SQL语句 \",\"数据准备：\",\"USE atguigudb; CREATE TABLE t1(id INT(10) AUTO_INCREMENT, content VARCHAR(100) NULL, PRIMARY KEY (id)); CREATE TABLE t2(id INT(10) AUTO_INCREMENT, content VARCHAR(100) NULL, PRIMARY KEY (id)); CREATE TABLE t3(id INT(10) AUTO_INCREMENT, content VARCHAR(100) NULL, PRIMARY KEY (id)); CREATE TABLE t4(id INT(10) AUTO_INCREMENT, content1 VARCHAR(100) NULL, content2 VARCHAR(100) NULL, PRIMARY KEY (id)); CREATE INDEX idx_content1 ON t4(content1); -- 普通索引 # 以下新增sql多执行几次，以便演示 INSERT INTO t1(content) VALUES(CONCAT('t1_',FLOOR(1+RAND()*1000))); INSERT INTO t2(content) VALUES(CONCAT('t2_',FLOOR(1+RAND()*1000))); INSERT INTO t3(content) VALUES(CONCAT('t3_',FLOOR(1+RAND()*1000))); INSERT INTO t4(content1, content2) VALUES(CONCAT('t4_',FLOOR(1+RAND()*1000)), CONCAT('t4_',FLOOR(1+RAND()*1000))); \"]},{\"header\":\"各字段解释\",\"slug\":\"各字段解释\",\"contents\":[]},{\"header\":\"table\",\"slug\":\"table\",\"contents\":[\"**单表：**显示这一行的数据是关于哪张表的\",\"EXPLAIN SELECT * FROM t1; \",\"**多表关联：**t1为驱动表，t2为被驱动表。\",\"注意：内连接时，MySQL性能优化器会自动判断哪个表是驱动表，哪个表示被驱动表，和书写的顺序无关\",\"EXPLAIN SELECT * FROM t1 INNER JOIN t2; \"]},{\"header\":\"id\",\"slug\":\"id\",\"contents\":[\"表示查询中执行select子句或操作表的顺序\",\"**id相同：**执行顺序由上至下\",\"EXPLAIN SELECT * FROM t1, t2, t3; \",\"**id不同：**如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行\",\"EXPLAIN SELECT t1.id FROM t1 WHERE t1.id =( SELECT t2.id FROM t2 WHERE t2.id =( SELECT t3.id FROM t3 WHERE t3.content = 't3_434' ) ); \",\"注意：查询优化器可能对涉及子查询的语句进行优化，转为连接查询\",\"EXPLAIN SELECT * FROM t1 WHERE content IN (SELECT content FROM t2 WHERE content = 'a'); \",\"**id为NULL：**最后执行\",\"EXPLAIN SELECT * FROM t1 UNION SELECT * FROM t2; \",\"小结：\",\"id如果相同，可以认为是一组，从上往下顺序执行\",\"在所有组中，id值越大，优先级越高，越先执行\",\"关注点：id号每个号码，表示一趟独立的查询, 一个sql的查询趟数越少越好\"]},{\"header\":\"select_type\",\"slug\":\"select-type\",\"contents\":[\"查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询。\",\"**SIMPLE：**简单查询。查询中不包含子查询或者UNION。\",\"EXPLAIN SELECT * FROM t1; \",\"**PRIMARY：**主查询。查询中若包含子查询，则最外层查询被标记为PRIMARY。\",\"**SUBQUERY：**子查询。在SELECT或WHERE列表中包含了子查询。\",\"EXPLAIN SELECT * FROM t3 WHERE id = ( SELECT id FROM t2 WHERE content= 'a'); \",\"**DEPENDENT SUBQUREY：**如果包含了子查询，并且查询语句不能被优化器转换为连接查询，并且子查询是相关子查询（子查询基于外部数据列），则子查询就是DEPENDENT SUBQUREY。\",\"EXPLAIN SELECT * FROM t3 WHERE id = ( SELECT id FROM t2 WHERE content = t3.content); \",\"**UNCACHEABLE SUBQUREY：**表示这个subquery的查询要受到外部系统变量的影响\",\"EXPLAIN SELECT * FROM t3 WHERE id = ( SELECT id FROM t2 WHERE content = @@character_set_server); \",\"**UNION：**对于包含UNION或者UNION ALL的查询语句，除了最左边的查询是PRIMARY，其余的查询都是UNION。\",\"**UNION RESULT：**UNION会对查询结果进行查询去重，MySQL会使用临时表来完成UNION查询的去重工作，针对这个临时表的查询就是\\\"UNION RESULT\\\"。\",\"EXPLAIN SELECT * FROM t3 WHERE id = 1 UNION SELECT * FROM t2 WHERE id = 1; \",\"**DEPENDENT UNION：**子查询中的UNION或者UNION ALL，除了最左边的查询是DEPENDENT SUBQUREY，其余的查询都是DEPENDENT UNION。\",\" EXPLAIN SELECT * FROM t1 WHERE content IN ( SELECT content FROM t2 UNION SELECT content FROM t3 ); \",\"**DERIVED：**在包含派生表（子查询在from子句中）的查询中，MySQL会递归执行这些子查询，把结果放在临时表里。\",\"EXPLAIN SELECT * FROM ( SELECT content, COUNT(*) AS c FROM t1 GROUP BY content ) AS derived_t1 WHERE c > 1; \",\"这里的<derived2>就是在id为2的查询中产生的派生表。\",\"**补充：**MySQL在处理带有派生表的语句时，优先尝试把派生表和外层查询进行合并，如果不行，再把派生表物化掉（执行子查询，并把结果放入临时表），然后执行查询。下面的例子就是就是将派生表和外层查询进行合并的例子：\",\"EXPLAIN SELECT * FROM (SELECT * FROM t1 WHERE content = 't1_832') AS derived_t1; \",\"**MATERIALIZED：**优化器对于包含子查询的语句，如果选择将子查询物化后再与外层查询连接查询，该子查询的类型就是MATERIALIZED。如下的例子中，查询优化器先将子查询转换成物化表，然后将t1和物化表进行连接查询。\",\" EXPLAIN SELECT * FROM t1 WHERE content IN (SELECT content FROM t2); \"]},{\"header\":\"partitions\",\"slug\":\"partitions\",\"contents\":[\"代表分区表中的命中情况，非分区表，该项为NULL\"]},{\"header\":\"type ☆\",\"slug\":\"type-☆\",\"contents\":[\"说明：\",\"结果值从最好到最坏依次是：\",\"system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL\",\"比较重要的包含：system、const 、eq_ref 、ref、range > index > ALL\",\"SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，最好是 consts级别。（阿里巴巴 开发手册要求）\",\"**ALL：**全表扫描。Full Table Scan，将遍历全表以找到匹配的行\",\"EXPLAIN SELECT * FROM t1; \",\"**index：**当使用覆盖索引，但需要扫描全部的索引记录时\",\"覆盖索引：如果能通过读取索引就可以得到想要的数据，那就不需要读取用户记录，或者不用再做回表操作了。一个索引包含了满足查询结果的数据就叫做覆盖索引。\",\"-- 只需要读取聚簇索引部分的非叶子节点，就可以得到id的值，不需要查询叶子节点 EXPLAIN SELECT id FROM t1; \",\"-- 只需要读取二级索引，就可以在二级索引中获取到想要的数据，不需要再根据叶子节点中的id做回表操作 EXPLAIN SELECT id, deptId FROM t_emp; \",\"**range：**只检索给定范围的行，使用一个索引来选择行。key 列显示使用了哪个索引，一般就是在你的where语句中出现了between、<、>、in等的查询。这种范围扫描索引扫描比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。\",\"EXPLAIN SELECT * FROM t1 WHERE id IN (1, 2, 3); \",\"**ref：**通过普通二级索引列与常量进行等值匹配时\",\"EXPLAIN SELECT * FROM t_emp WHERE deptId = 1; \",\"**eq_ref：**连接查询时通过主键或不允许NULL值的唯一二级索引列进行等值匹配时\",\"EXPLAIN SELECT * FROM t1, t2 WHERE t1.id = t2.id; \",\"**const：**根据主键或者唯一二级索引列与常数进行匹配时\",\"EXPLAIN SELECT * FROM t1 WHERE id = 1; \",\"**system：**MyISAM引擎中，当表中只有一条记录时。（这是所有type的值中性能最高的场景）\",\"CREATE TABLE t(i int) Engine=MyISAM; INSERT INTO t VALUES(1); EXPLAIN SELECT * FROM t; \",\"其他不太常见的类型（了解）：\",\"index_subquery：利用普通索引来关联子查询，针对包含有IN子查询的查询语句。content1是普通索引字段\",\"EXPLAIN SELECT * FROM t1 WHERE content IN (SELECT content1 FROM t4 WHERE t1.content = t4.content2) OR content = 'a'; \",\"unique_subquery：类似于index_subquery，利用唯一索引来关联子查询。t2的id是主键，也可以理解为唯一的索引字段\",\"EXPLAIN SELECT * FROM t1 WHERE id IN (SELECT id FROM t2 WHERE t1.content = t2.content) OR content = 'a'; \",\"index_merge：在查询过程中需要多个索引组合使用，通常出现在有 or 的关键字的sql中。\",\"EXPLAIN SELECT * FROM t_emp WHERE deptId = 1 OR id = 1; \",\"ref_or_null：当对普通二级索引进行等值匹配，且该索引列的值也可以是NULL值时。\",\"EXPLAIN SELECT * FROM t_emp WHERE deptId = 1 OR deptId IS NULL; \",\"**fulltext：**全文索引。一般通过搜索引擎实现，这里我们不展开。\"]},{\"header\":\"possible_keys 和 keys ☆\",\"slug\":\"possible-keys-和-keys-☆\",\"contents\":[\"possible_keys表示执行查询时可能用到的索引，一个或多个。 查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。\",\"keys表示实际使用的索引。如果为NULL，则没有使用索引。\",\"EXPLAIN SELECT id FROM t1 WHERE id = 1; \"]},{\"header\":\"key_len ☆\",\"slug\":\"key-len-☆\",\"contents\":[\"表示索引使用的字节数，根据这个值可以判断索引的使用情况，检查是否充分利用了索引，针对联合索引值越大越好。\",\"如何计算：\",\"先看索引上字段的类型+长度。比如：int=4 ; varchar(20) =20 ; char(20) =20\",\"如果是varchar或者char这种字符串字段，视字符集要乘不同的值，比如utf8要乘 3，如果是utf8mb4要乘4，GBK要乘2\",\"varchar这种动态字符串要加2个字节\",\"允许为空的字段要加1个字节\",\"-- 创建索引 CREATE INDEX idx_age_name ON t_emp(age, `name`); -- 测试1 EXPLAIN SELECT * FROM t_emp WHERE age = 30 AND `name` = 'ab%'; -- 测试2 EXPLAIN SELECT * FROM t_emp WHERE age = 30; \"]},{\"header\":\"ref\",\"slug\":\"ref\",\"contents\":[\"显示与key中的索引进行比较的列或常量。\",\"-- ref=atguigudb.t1.id 关联查询时出现，t2表和t1表的哪一列进行关联 EXPLAIN SELECT * FROM t1, t2 WHERE t1.id = t2.id; -- ref=const 与索引列进行等值比较的东西是啥，const表示一个常数 EXPLAIN SELECT * FROM t_emp WHERE age = 30; \"]},{\"header\":\"rows ☆\",\"slug\":\"rows-☆\",\"contents\":[\"MySQL认为它执行查询时必须检查的行数。值越小越好。\",\"-- 如果是全表扫描，rows的值就是表中数据的估计行数 EXPLAIN SELECT * FROM t_emp WHERE empno = '10001'; -- 如果是使用索引查询，rows的值就是预计扫描索引记录行数 EXPLAIN SELECT * FROM t_emp WHERE deptId = 1; \",\"filtered\",\"最后查询出来的数据占所有服务器端检查行数（rows）的百分比。值越大越好。\",\"-- 先根据二级索引deptId找到数据的主键，有3条记录满足条件， -- 再根据主键进行回表，最终找到3条记录，有100%的记录满足条件 EXPLAIN SELECT * FROM t_emp WHERE deptId = 1; -- 这个例子如果name列是索引列则 filtered = 100 否则filtered = 10(全表扫描) EXPLAIN SELECT * FROM t_emp WHERE `name` = '风清扬'; \"]},{\"header\":\"Extra ☆\",\"slug\":\"extra-☆\",\"contents\":[\"包含不适合在其他列中显示但十分重要的额外信息。通过这些额外信息来理解MySQL到底将如何执行当前的查询语句。MySQL提供的额外信息有好几十个，这里只挑介绍比较重要的介绍。\",\"Impossible WHERE：where子句的值总是false\",\"EXPLAIN SELECT * FROM t_emp WHERE 1 != 1; \",\"**Using where：**使用了where，但在where上有字段没有创建索引\",\"EXPLAIN SELECT * FROM t_emp WHERE `name` = '风清扬'; \",\"**Using temporary：**使了用临时表保存中间结果\",\"EXPLAIN SELECT DISTINCT content FROM t1; \",\"Using filesort：\",\"在对查询结果中的记录进行排序时，是可以使用索引的，如下所示：\",\"EXPLAIN SELECT * FROM t1 ORDER BY id; \",\"如果排序操作无法使用到索引，只能在内存中（记录较少时）或者磁盘中（记录较多时）进行排序（filesort），如下所示：\",\"EXPLAIN SELECT * FROM t1 ORDER BY content; \",\"**Using index：**使用了覆盖索引，表示直接访问索引就足够获取到所需要的数据，不需要通过索引回表\",\" EXPLAIN SELECT id, content1 FROM t4; \",\"EXPLAIN SELECT id FROM t1; \",\"**Using index condition：**叫作 Index Condition Pushdown Optimization （索引下推优化）\",\"如果没有索引下推（ICP），那么MySQL在存储引擎层找到满足content1 > 'z'条件的第一条二级索引记录。主键值进行回表，返回完整的记录给server层，server层再判断其他的搜索条件是否成立。如果成立则保留该记录，否则跳过该记录，然后向存储引擎层要下一条记录。\",\"如果使用了索引下推（ICP），那么MySQL在存储引擎层找到满足content1 > 'z'条件的第一条二级索引记录。不着急执行回表，而是在这条记录上先判断一下所有关于idx_content1索引中包含的条件是否成立，也就是content1 > 'z' AND content1 LIKE '%a'是否成立。如果这些条件不成立，则直接跳过该二级索引记录，去找下一条二级索引记录；如果这些条件成立，则执行回表操作，返回完整的记录给server层。\",\"-- content1列上有索引idx_content1 EXPLAIN SELECT * FROM t4 WHERE content1 > 'z' AND content1 LIKE '%a'; \",\"**注意：**如果这里的查询条件只有content1 > 'z'，那么找到满足条件的索引后也会进行一次索引下推的操作，判断content1 > 'z'是否成立（这是源码中为了编程方便做的冗余判断）\",\"**Using join buffer：**在连接查询时，当被驱动表不能有效的利用索引时，MySQL会为其分配一块名为连接缓冲区（join buffer）的内存来加快查询速度\",\"EXPLAIN SELECT * FROM t1, t2 WHERE t1.content = t2.content; \",\"下面这个例子就是被驱动表使用了索引：\",\"EXPLAIN SELECT * FROM t_emp, t_dept WHERE t_dept.id = t_emp.deptId; \"]},{\"header\":\"133、MySQL数据库cpu飙升的话你会如何分析\",\"slug\":\"_133、mysql数据库cpu飙升的话你会如何分析\",\"contents\":[\"重点是定位问题.\",\"1 使用top观察MySQLd的cpu利用率\",\"切换到常用的数据库\",\"使用show full processlist;查看会话\",\"观察是哪些sql消耗了资源，其中重点观察state指标\",\"定位到具体sql\",\"2 pidstat\",\"定位到线程\",\"在PERFORMANCE_SCHEMA.THREADS中记录了thread_os_id 找到线程执行的sql\",\"根据操作系统id可以到processlist表找到对应的会话\",\"在会话中即可定位到问题sql\",\"3 使用show profile观察sql各个阶段耗时\",\"4 服务器上是否运行了其他程序\",\"5 检查一下是否有慢查询\",\"6 pref top\",\"使用pref 工具分析哪些函数引发的cpu过高来追踪定位\"]},{\"header\":\"134、有没有进行过分库分表？\",\"slug\":\"_134、有没有进行过分库分表\",\"contents\":[\"这个如实回答即可，针对你的业务回答更加分\"]},{\"header\":\"135、什么是分库分表？\",\"slug\":\"_135、什么是分库分表\",\"contents\":[\"垂直分库\",\"一个数据库由很多表的构成，每个表对应着不同的业务，垂直切分是指按照业务将表进行分类，分布到不同 的数据库上面，这样也就将数据或者说压力分担到不同的库上面，如下图：\",\"​ \",\"系统被切分成了，用户，订单交易，支付几个模块。\",\"水平分表\",\"把一张表里的内容按照不同的规则 写到不同的库里\",\"相对于垂直拆分，水平拆分不是将表做分类，而是按照某个字段的某种规则来分散到多个库之中，每个表中包含一部分数据。简单来说，我们可以将数据的水平切分理解为是按照数据行的切分，就是将表中的某些行切分 到一个数据库，而另外的某些行又切分到其他的数据库中，如图：\"]},{\"header\":\"136、什么时候进行分库分表？有没有配合es使用经验？\",\"slug\":\"_136、什么时候进行分库分表-有没有配合es使用经验\",\"contents\":[\"能不分就不分\",\"单机性能下降明显的时候\",\"增加缓存（通常查询量比较大），细分业务\",\"首先尝试主被集群，读写分离\",\"尝试分库\",\"尝试分表 -> 冷热数据分离\",\"大数据量下可以配合es完成高效查询\"]},{\"header\":\"137、说一下实现分库分表工具的实现思路\",\"slug\":\"_137、说一下实现分库分表工具的实现思路\",\"contents\":[\"伪装成MySQL服务器，代理用户请求转发到真实服务器\",\"基于本地aop实现，拦截sql，改写，路由和结果归集处理。\"]},{\"header\":\"138、用过哪些分库分表工具？\",\"slug\":\"_138、用过哪些分库分表工具\",\"contents\":[]},{\"header\":\"139、分库分表后可能会有哪些问题？\",\"slug\":\"_139、分库分表后可能会有哪些问题\",\"contents\":[\"经典的问题：\",\"执行效率明显降低\",\"表结构很难再次调整\",\"引发分布式id问题\",\"产生跨库join\",\"代理类中间件网络io成为瓶颈\"]},{\"header\":\"140、说一下读写分离常见方案？\",\"slug\":\"_140、说一下读写分离常见方案\",\"contents\":[]},{\"header\":\"141、为什么要使用视图？ 什么是视图？\",\"slug\":\"_141、为什么要使用视图-什么是视图\",\"contents\":[\"视图定义： 1、视图是一个虚表，是从一个或几个基本表（或视图）导出的表。 2、只存放视图的定义，不存放视图对应的数据。 3、基表中的数据发生变化，从视图中查询出的数据也随之改变。\",\"视图的作用： 1、视图能够简化用户的操作 2、视图使用户能以多种角度看待同一数据 3、视图对重构数据库提供了一定程度的逻辑独立性 4、视图能够对机密数据提供安全保护 5、适当的利用视图可以更清晰的表达查询\"]},{\"header\":\"142、什么是存储过程？有没有使用过？\",\"slug\":\"_142、什么是存储过程-有没有使用过\",\"contents\":[\"项目中禁止使用存储过程，存储过程难以调试和扩展，更没有移植性\"]},{\"header\":\"143、有没有使用过外键？有什么需要注意的地方？\",\"slug\":\"_143、有没有使用过外键-有什么需要注意的地方\",\"contents\":[\"不得使用外键与级联，一切外键概念必须在应用层解决。\",\"说明：以学生和成绩的关系为例，学生表中的 student_id是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为 级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻 塞，存在数据库更新风暴的风险；外键影响数据库的插入速度。\"]},{\"header\":\"144、用过processlist吗？\",\"slug\":\"_144、用过processlist吗\",\"contents\":[\"关键的就是state列，MySQL列出的状态主要有以下几种：\",\"Checking table 正在检查数据表（这是自动的）。\",\"Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。这是一个很快的操作，如果不是这样的话，就应该确认磁盘空间是否已经满了或者磁盘是否正处于重负中。\",\"Connect Out 复制从服务器正在连接主服务器。\",\"Copying to tmp table on disk 由于临时结果集大于tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。\",\"Creating tmp table 正在创建临时表以存放部分查询结果。\",\"deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。\",\"deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。\",\"Flushing tables 正在执行FLUSH TABLES，等待其他线程关闭数据表。\",\"Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一小段才能死掉。如果该线程程被其他线程锁住了，那么kill请求会在锁释放时马上生效。\",\"Locked 被其他查询锁住了。\",\"Sending data 正在处理Select查询的记录，同时正在把结果发送给客户端。Sending data”状态的含义，原来这个状态的名称很具有误导性，所谓的“Sending data”并不是单纯的发送数据，而是包括“收集 + 发送 数据”。\",\"Sorting for group 正在为GROUP BY做排序。\",\"Sorting for order 正在为ORDER BY做排序。\",\"Opening tables 这个过程应该会很快，除非受到其他因素的干扰。例如，在执Alter TABLE或LOCK TABLE语句行完以前，数据表无法被其他线程打开。正尝试打开一个表。\",\"Removing duplicates 正在执行一个Select DISTINCT方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。\",\"Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。\",\"Repair by sorting 修复指令正在排序以创建索引。\",\"Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。它会比Repair by sorting慢些。\",\"Searching rows for update 正在讲符合条件的记录找出来以备更新。它必须在Update要修改相关的记录之前就完成了。\",\"Sleeping 正在等待客户端发送新请求.\",\"System lock 正在等待取得一个外部的系统锁。如果当前没有运行多个MySQLd服务器同时请求同一个表，那么可以通过增加--skip-external-locking参数来禁止外部系统锁。\",\"Upgrading lock Insert DELAYED正在尝试取得一个锁表以插入新记录。=\",\"Updating 正在搜索匹配的记录，并且修改它们。\",\"User Lock 正在等待GET_LOCK()。\",\"Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。以下几种情况下会产生这个通知：FLUSH TABLES tbl_name, Alter TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE,或OPTIMIZE TABLE。\",\"waiting for handler insert Insert DELAYED已经处理完了所有待处理的插入操作，正在等待新的请求。\"]},{\"header\":\"145、某个表有数千万数据，查询比较慢，如何优化？说一下思路\",\"slug\":\"_145、某个表有数千万数据-查询比较慢-如何优化-说一下思路\",\"contents\":[\"前端优化 减少查询 \",\"合并请求:多个请求需要的数据尽量一条sql拿出来\",\"会话保存：和用户会话相关的数据尽量一次取出重复使用\",\"避免无效刷新\",\"多级缓存 不要触及到数据库 \",\"应用层热点数据高速查询缓存（低一致性缓存）\",\"高频查询大数据量镜像缓存（双写高一致性缓存）\",\"入口层缓存（几乎不变的系统常量）\",\"使用合适的字段类型，比如varchar换成char\",\"一定要高效使用索引。 \",\"使用explain 深入观察索引使用情况\",\"检查select 字段最好满足索引覆盖\",\"复合索引注意观察key_len索引使用情况\",\"有分组，排序，注意file sort，合理配置相应的buffer大小\",\"检查查询是否可以分段查询，避免一次拿出过多无效数据\",\"多表关联查询是否可以设置冗余字段，是否可以简化多表查询或分批查询\",\"分而治之：把服务拆分成更小力度的微服务\",\"冷热数据分库存储\",\"读写分离，主被集群 然后再考虑分库分表\"]},{\"header\":\"146、count(列名)和 count(*)有什么区别？\",\"slug\":\"_146、count-列名-和-count-有什么区别\",\"contents\":[\"count()是 SQL92 定义的 标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。 说明：count()会统计值为 NULL 的行，而 count(列名)不会统计此列为 NULL 值的行。\"]},{\"header\":\"147、如果有超大分页改怎么处理？\",\"slug\":\"_147、如果有超大分页改怎么处理\",\"contents\":[\"select name from user limit 10000,10;在 使用的时候并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回 N 行\",\"通过索引优化的方案：\",\"如果主键自增可以 select name from user where id > 10000 limit 10;\",\"延迟关联\",\"需要order by时 \",\"一定注意增加筛选条件，避免全表排序 \",\"where -》 order by -》 limit\",\"减少select字段\",\"优化相关参数避免filesort\",\"一般大分页情况比较少（很少有人跳转到几百万页去查看数据），实际互联网业务中多数还是按顺序翻页，可以使用缓存提升前几页的查询效率，实际上大多数知名互联网项目也都是这么做的\",\"在阿里巴巴《Java开发手册》中的建议：\",\"【推荐】利用延迟关联或者子查询优化超多分页场景。 说明：MySQL 并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回 N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过 特定阈值的页数进行 SQL 改写。 正例：先快速定位需要获取的 id 段，然后再关联： SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id\"]},{\"header\":\"148、MySQL服务器毫无规律的异常重启如何排查问题？\",\"slug\":\"_148、mysql服务器毫无规律的异常重启如何排查问题\",\"contents\":[\"首先是查看MySQL和系统日志来定位错误\",\"最常见的是关闭swap分区后OOM问题：\",\"MySQL 分为应用进程和守护进程\",\"当应用进程内存占用过高的时候操作系统可能会kill掉进程，此时守护进程又帮我们重启了应用进程，运行一段时间后又出现OOM如此反复\",\"可以排查以下几个关键点\",\"运行时内存占用率\",\"MySQL buffer相关参数\",\"MySQL 网络连接相关参数\",\"异常关机或kill -9 MySQL 后导致表文件损坏\",\"直接使用备份\",\"配置 innodb_force_recovery 跳过启动恢复过程\"]},{\"header\":\"149、MySQL 线上修改表结构有哪些风险?\",\"slug\":\"_149、mysql-线上修改表结构有哪些风险\",\"contents\":[\"针对ddl命令，有以下几种方式\",\"copy table 锁原表，创建临时表并拷贝数据\",\"inplace 针对索引修改删除的优化，不需要拷贝所有数据\",\"Online DDL 细分DDL命令来决定是否锁表\",\"可能会锁表，导致无法读写\",\"ORM中的映射失效\",\"索引失效\",\"建议：建个新表，导入数据后重命名\"]},{\"header\":\"150 什么是MySQL多实例部署？\",\"slug\":\"_150-什么是mysql多实例部署\",\"contents\":[\"指的是在一台主机上部署多个实例\",\"主要目的是压榨服务器性能\",\"缺点是互相影响\"]}]},\"/interview/database/150mysql/\":{\"title\":\"MySQL索引\",\"contents\":[{\"header\":\"1、MySQL如何实现的索引机制？\",\"slug\":\"_1、mysql如何实现的索引机制\",\"contents\":[\"MySQL中索引分三类：B+树索引、Hash索引、全文索引\"]},{\"header\":\"2、InnoDB索引与MyISAM索引实现的区别是什么？\",\"slug\":\"_2、innodb索引与myisam索引实现的区别是什么\",\"contents\":[\"MyISAM的索引方式都是非聚簇的，与InnoDB包含1个聚簇索引是不同的。\",\"在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引相当于全部都是二级索引 。\",\"InnoDB的数据文件本身就是索引文件，而MyISAM索引文件和数据文件是分离的 ，索引文件仅保存数据记录的地址。\",\"MyISAM的表在磁盘上存储在以下文件中： *.sdi（描述表结构）、*.MYD（数据），*.MYI（索引）\",\"InnoDB的表在磁盘上存储在以下文件中： .ibd（表结构、索引和数据都存在一起）\",\"InnoDB的非聚簇索引data域存储相应记录主键的值 ，而MyISAM索引记录的是地址 。换句话说，InnoDB的所有非聚簇索引都引用主键作为data域。\",\"MyISAM的回表操作是十分快速的，因为是拿着地址偏移量直接到文件中取数据的，反观InnoDB是通过获取主键之后再去聚簇索引里找记录，虽然说也不慢，但还是比不上直接用地址去访问。\",\"InnoDB要求表必须有主键 （ MyISAM可以没有 ）。如果没有显式指定，则MySQL系统会自动选择一个可以非空且唯一标识数据记录的列作为主键。如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整型。\"]},{\"header\":\"3、一个表中如果没有创建索引，那么还会创建B+树吗？\",\"slug\":\"_3、一个表中如果没有创建索引-那么还会创建b-树吗\",\"contents\":[\"会\",\"如果有主键会创建聚簇索引\",\"如果没有主键会生成rowid作为隐式主键\"]},{\"header\":\"4、说一下B+树索引实现原理（数据结构）\",\"slug\":\"_4、说一下b-树索引实现原理-数据结构\",\"contents\":[\"假设有一个表index_demo，表中有2个INT类型的列，1个CHAR(1)类型的列，c1列为主键：\",\"CREATE TABLE index_demo(c1 INT,c2 INT,c3 CHAR(1),PRIMARY KEY(c1)) ; \",\"index_demo表的简化的行格式示意图如下：\",\"我们只在示意图里展示记录的这几个部分：\",\"record_type：表示记录的类型， 0是普通记录、 2是最小记录、 3 是最大记录、1是B+树非叶子节点记录。\",\"next_record：表示下一条记录的相对位置，我们用箭头来表明下一条记录。\",\"各个列的值：这里只记录在 index_demo 表中的三个列，分别是 c1 、 c2 和 c3 。\",\"其他信息：除了上述3种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。\",\"将其他信息项暂时去掉并把它竖起来的效果就是这样：\",\"把一些记录放到页里的示意图就是（这里一页就是一个磁盘块，代表一次IO）：\",\"name age sex\",\"MySQL InnoDB的默认的页大小是16KB，因此数据存储在磁盘中，可能会占用多个数据页。如果各个页中的记录没有规律，我们就不得不依次遍历所有的数据页。如果我们想快速的定位到需要查找的记录在哪些数据页中，我们可以这样做 ：\",\"下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值\",\"给所有的页建立目录项\",\"以页28为例，它对应目录项2 ，这个目录项中包含着该页的页号28以及该页中用户记录的最小主键值 5。我们只需要把几个目录项在物理存储器上连续存储（比如：数组），就可以实现根据主键值快速查找某条记录的功能了。比如：查找主键值为 20 的记录，具体查找过程分两步：\",\"先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 ≤ 20 < 209 ），对应页9。\",\"再到页9中根据二分法快速定位到主键值为 20 的用户记录。\",\"至此，针对数据页做的简易目录就搞定了。这个目录有一个别名，称为索引 。\"]},{\"header\":\"InnoDB中的索引方案\",\"slug\":\"innodb中的索引方案\",\"contents\":[\"我们新分配一个编号为30的页来专门存储目录项记录，页10、28、9、20专门存储用户记录：\",\"目录项记录和普通的用户记录的不同点：\",\"目录项记录 的 record_type 值是1，而 普通用户记录 的 record_type 值是0。\",\"目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，包含很多列，另外还有InnoDB自己添加的隐藏列。\",\"现在查找主键值为 20 的记录，具体查找过程分两步：\",\"先到页30中通过二分法快速定位到对应目录项，因为 12 ≤ 20 < 209 ，就是页9。\",\"再到页9中根据二分法快速定位到主键值为 20 的用户记录。\",\"更复杂的情况如下：\",\"我们生成了一个存储更高级目录项的 页33 ，这个页中的两条记录分别代表页30和页32，如果用户记录的主键值在 [1, 320) 之间，则到页30中查找更详细的目录项记录，如果主键值 不小于320 的话，就到页32中查找更详细的目录项记录。这个数据结构，它的名称是 B+树 。\"]},{\"header\":\"5、聚簇索引与非聚簇索引b+树实现有什么区别？\",\"slug\":\"_5、聚簇索引与非聚簇索引b-树实现有什么区别\",\"contents\":[]},{\"header\":\"聚簇索引\",\"slug\":\"聚簇索引\",\"contents\":[\"特点：\",\"索引和数据保存在同一个B+树中\",\"页内的记录是按照主键的大小顺序排成一个单向链表 。\",\"页和页之间也是根据页中记录的主键的大小顺序排成一个双向链表 。\",\"非叶子节点存储的是记录的主键+页号。\",\"叶子节点存储的是完整的用户记录。\",\"优点：\",\"数据访问更快 ，因为索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快。\",\"聚簇索引对于主键的排序查找和范围查找速度非常快。\",\"按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库可以从更少的数据块中提取数据，节省了大量的IO操作 。\",\"缺点：\",\"插入速度严重依赖于插入顺序 ，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键。\",\"更新主键的代价很高 ，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新。\",\"限制：\",\"只有InnoDB引擎支持聚簇索引，MyISAM不支持聚簇索引。\",\"由于数据的物理存储排序方式只能有一种，所以每个MySQL的表只能有一个聚簇索引。\",\"如果没有为表定义主键，InnoDB会选择非空的唯一索引列代替。如果没有这样的列，InnoDB会隐式的定义一个主键作为聚簇索引。\",\"为了充分利用聚簇索引的聚簇特性，InnoDB中表的主键应选择有序的id，不建议使用无序的id，比如UUID、MD5、HASH、字符串作为主键，无法保证数据的顺序增长。\"]},{\"header\":\"非聚簇索引\",\"slug\":\"非聚簇索引\",\"contents\":[\"（二级索引、辅助索引）\",\"聚簇索引，只能在搜索条件是主键值时才发挥作用，因为B+树中的数据都是按照主键进行排序的，如果我们想以别的列作为搜索条件，那么需要创建非聚簇索引。\",\"例如，以c2列作为搜索条件，那么需要使用c2列创建一棵B+树，如下所示：\",\"这个B+树与聚簇索引有几处不同：\",\"页内的记录是按照从c2列的大小顺序排成一个单向链表 。\",\"页和页之间也是根据页中记录的c2列的大小顺序排成一个双向链表 。\",\"非叶子节点存储的是记录的c2列+页号。\",\"叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。\",\"一张表可以有多个非聚簇索引：\"]},{\"header\":\"6、说一下B+树中聚簇索引的查找（匹配）逻辑\",\"slug\":\"_6、说一下b-树中聚簇索引的查找-匹配-逻辑\",\"contents\":[]},{\"header\":\"7、说一下B+树中非聚簇索引的查找（匹配）逻辑\",\"slug\":\"_7、说一下b-树中非聚簇索引的查找-匹配-逻辑\",\"contents\":[\"**例如：**根据c2列的值查找c2=4的记录，查找过程如下：\",\"根据根页面44定位到页42（因为2 ≤ 4 < 9）\",\"由于c2列没有唯一性约束，所以c2=4的记录可能分布在多个数据页中，又因为 2 ≤ 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。\",\"在页34和35中定位到具体的记录。\",\"但是这个B+树的叶子节点只存储了c2和c1（主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。\",\"like 张%\"]},{\"header\":\"8、平衡二叉树，红黑树，B树和B+树的区别是什么？都有哪些应用场景？\",\"slug\":\"_8、平衡二叉树-红黑树-b树和b-树的区别是什么-都有哪些应用场景\",\"contents\":[\"平衡二叉树\",\"基础数据结构\",\"左右平衡\",\"高度差大于1会自旋\",\"每个节点记录一个数据\",\"平衡二叉树（AVL）\",\"AVL树全称G.M. Adelson-Velsky和E.M. Landis，这是两个人的人名。\",\"平衡二叉树也叫平衡二叉搜索树（Self-balancing binary search tree）又被称为AVL树， 可以保证查询效率较高。\",\"具有以下特点：\",\"它是一棵空树或它的左右两个子树的高度差的绝对值不超过1\",\"并且左右两个子树都是一棵平衡二叉树。\",\"AVL的生成演示：https://www.cs.usfca.edu/~galles/visualization/AVLtree.html\",\"AVL的问题\",\"众所周知，IO操作的效率很低，在大量数据存储中，查询时我们不能一下子将所有数据加载到内存中，只能逐节点加载（一个节点一次IO）。如果我们利用二叉树作为索引结构，那么磁盘的IO次数和索引树的高度是相关的。平衡二叉树由于树深度过大而造成磁盘IO读写过于频繁，进而导致效率低下。\",\"为了提高查询效率，就需要 减少磁盘IO数 。为了减少磁盘IO的次数，就需要尽量降低树的高度 ，需要把原来“瘦高”的树结构变的“矮胖”，树的每层的分叉越多越好。针对同样的数据，如果我们把二叉树改成 三叉树：\",\"上面的例子中，我们将二叉树变成了三叉树，降低了树的高度。如果能够在一个节点中存放更多的数据，我们还可以进一步减少节点的数量，从而进一步降低树的高度。这就是多叉树。\",\"普通树的问题\",\"左子树全部为空，从形式上看，更像一个单链表，不能发挥BST的优势。\",\"解决方案：平衡二叉树(AVL)\",\"红黑树\",\"hashmap存储\",\"两次旋转达到平衡\",\"分为红黑节点\",\"在这个棵严格的平台树上又进化为“红黑树”{是一个非严格的平衡树 左子树与右子树的高度差不能超过1}，红黑树的长子树只要不超过短子树的两倍即可！\",\"当再次插入7的时候，这棵树就会发生旋转\",\"B+树和B树的差异：\",\"B+树中非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大值（或最小）。\",\"B+树中非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而B树中， 非叶子节点既保存索引，也保存数据记录 。\",\"B+树中所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大小从小到大顺序链接。\"]},{\"header\":\"9、一个b+树中大概能存放多少条索引记录？\",\"slug\":\"_9、一个b-树中大概能存放多少条索引记录\",\"contents\":[\"真实环境中一个页存放的记录数量是非常大的（默认16KB），假设指针与键值忽略不计（或看做10个字节），数据占 1 kb 的空间：\",\"如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放 16 条记录。\",\"如果B+树有2层，最多能存放 1600×16=25600 条记录。\",\"如果B+树有3层，最多能存放 1600×1600×16=40960000 条记录。\",\"如果存储千万级别的数据，只需要三层就够了\",\"B+树的非叶子节点不存储用户记录，只存储目录记录，相对B树每个节点可以存储更多的记录，树的高度会更矮胖，IO次数也会更少。\"]},{\"header\":\"10、使用B+树存储的索引crud执行效率如何？\",\"slug\":\"_10、使用b-树存储的索引crud执行效率如何\",\"contents\":[\"c 新增\",\"O(lognN)\",\"N = 高度\"]},{\"header\":\"11、什么是自适应哈希索引？\",\"slug\":\"_11、什么是自适应哈希索引\",\"contents\":[\"自适应哈希索引是Innodb引擎的一个特殊功能，当它注意到某些索引值被使用的非常频繁时，会在内存中基于B-Tree所有之上再创建一个哈希索引，这就让B-Tree索引也具有哈希索引的一些优点，比如快速哈希查找。这是一个完全自动的内部行为，用户无法控制或配置\",\"使用命令\",\"SHOW ENGINE INNODB STATUS \\\\G ; \",\"查看INSERT BUFFER AND ADAPTIVE HASH INDEX\"]},{\"header\":\"12、什么是2-3树 2-3-4树？\",\"slug\":\"_12、什么是2-3树-2-3-4树\",\"contents\":[\"多叉树（multiway tree）允许每个节点可以有更多的数据项和更多的子节点。2-3树，2-3-4树就是多叉树，多叉树通过重新组织节点，减少节点数量，增加分叉，减少树的高度，能对二叉树进行优化。\",\"2-3树\",\"下面2-3树就是一颗多叉树\",\"2-3树具有如下特点：\",\"2-3树的所有叶子节点都在同一层。\",\"有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点。\",\"有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点。\",\"2-3树是由二节点和三节点构成的树。\",\"对于三节点的子树的值大小仍然遵守 BST 二叉排序树的规则。\",\"2-3-4树\"]},{\"header\":\"13、为什么官方建议使用自增长主键作为索引？（说一下自增主键和字符串类型主键的区别和影响）\",\"slug\":\"_13、为什么官方建议使用自增长主键作为索引-说一下自增主键和字符串类型主键的区别和影响\",\"contents\":[\"自增主键能够维持底层数据顺序写入\",\"读取可以由b+树的二分查找定位\",\"支持范围查找，范围数据自带顺序\",\"字符串无法完成以上操作\"]},{\"header\":\"14、使用int自增主键后 最大id是10，删除id 10和9，再添加一条记录，最后添加的id是几？删除后重启MySQL然后添加一条记录最后id是几？\",\"slug\":\"_14、使用int自增主键后-最大id是10-删除id-10和9-再添加一条记录-最后添加的id是几-删除后重启mysql然后添加一条记录最后id是几\",\"contents\":[\"删除之后\",\"如果重启，会从最大的id开始递增\",\"如果没重启，会延续删除之前最大的id开始递增\"]},{\"header\":\"15、索引的优缺点是什么？\",\"slug\":\"_15、索引的优缺点是什么\",\"contents\":[\"优点\",\"聚簇（主键）索引：\",\"顺序读写\",\"范围快速查找\",\"范围查找自带顺序\",\"非聚簇索引：\",\"条件查询避免全表扫描scan\",\"范围，排序，分组查询返回行id，排序分组后，再回表查询完整数据，有可能利用顺序读写\",\"覆盖索引不需要回表操作\",\"索引的代价\",\"索引是个好东西，可不能乱建，它在空间和时间上都会有消耗：\",\"空间上的代价\",\"每建立一个索引都要为它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会占用 16KB 的存储空间，一棵很大的B+树由许多数据页组成，那就是很大的一片存储空间。\",\"时间上的代价\",\"每次对表中的数据进行 增、删、改 操作时，都需要去修改各个B+树索引。而增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位、页面分裂、页面回收等操作来维护好节点和记录的排序。如果我们建了许多索引，每个索引对应的B+树都要进行相关的维护操作，会给性能拖后腿。\",\"B 树和 B+ 树都可以作为索引的数据结构，在 MySQL 中采用的是 B+ 树。\",\"但B树和B+树各有自己的应用场景，不能说B+树完全比B树好，反之亦然。\"]},{\"header\":\"16、使用索引一定能提升效率吗？\",\"slug\":\"_16、使用索引一定能提升效率吗\",\"contents\":[\"不一定\",\"少量数据全表扫描也很快，可以直接获取到全量数据\",\"唯一索引会影响插入速度，但建议使用\",\"索引过多会影响更新，插入，删除数据速度\"]},{\"header\":\"17、如果是大段文本内容，如何创建（优化）索引？\",\"slug\":\"_17、如果是大段文本内容-如何创建-优化-索引\",\"contents\":[\"B 树和 B+ 树都可以作为 索引的数据结构，**在 MySQL 中采用的是 B+ 树。** \",\"第一种方式是分表存储，然后创建索引\",\"第二是使用es为大文本创建索引\"]},{\"header\":\"18、什么是聚簇索引？\",\"slug\":\"_18、什么是聚簇索引\",\"contents\":[\"聚簇索引数据和索引存放在一起组成一个b+树\",\"参考第5题\"]},{\"header\":\"19、一个表中可以有多个（非）聚簇索引吗？\",\"slug\":\"_19、一个表中可以有多个-非-聚簇索引吗\",\"contents\":[\"聚簇索引只能有一个\",\"非聚簇索引可以有多个\"]},{\"header\":\"20、聚簇索引与非聚集索引的特点是什么？\",\"slug\":\"_20、聚簇索引与非聚集索引的特点是什么\",\"contents\":[\"参考005题\"]},{\"header\":\"21、CRUD时聚簇索引与非聚簇索引的区别是什么？\",\"slug\":\"_21、crud时聚簇索引与非聚簇索引的区别是什么\",\"contents\":[\"聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多，因为插入要保证主键不能重复\",\"聚簇索引范围，排序查找效率高，因为是有序的\",\"非聚簇索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据\"]},{\"header\":\"22、非聚簇索引为什么不存数据地址值而存储主键？\",\"slug\":\"_22、非聚簇索引为什么不存数据地址值而存储主键\",\"contents\":[\"因为聚簇索引中有时会引发分页操作、重排操作数据有可能会移动\"]},{\"header\":\"23、什么是回表操作？\",\"slug\":\"_23、什么是回表操作\",\"contents\":[\"通俗的讲就是，如果索引的列在 select 所需获得的列中（因为在 MySQL 中索引是根据索引列的值进行排序的，所以索引节点中存在该列中的部分值）或者根据一次索引查询就能获得记录就不需要回表，如果 select 所需获得列中有大量的非索引列，索引就需要到表中找到相应的列的信息，这就叫回表。\",\"表中字段：id age name sex age -> index（建立索引） \",\"如果我们要查询id的数据，执行select * from user where id = ？，这条语句不需要回表，原因是根据主键的查询方式，则只需要搜索 ID 这棵 B+ 树。主键是唯一的，根据这个唯一的索引，MySQL 就能确定搜索的记录。\",\"如果我们根据索引来进行查询，select * from user where age >20 ,这时候需要回表，原因是通过非主键索引列进行查询时，先搜索age索引树，第一次通过索引age取回id，第二次（回表）根据id拿到完整数据，这个过程虽然用了索引，但实际上底层进行了两次索引查询，这个过程就称为回表。\"]},{\"header\":\"024 什么是覆盖索引？\",\"slug\":\"_024什么是覆盖索引\",\"contents\":[\"只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。\",\"explain的输出结果Extra字段为Using index时，能够触发索引覆盖\",\"表中字段：id age name sex 20 -> index（建立索引） \",\"实现覆盖索引：常见的方法是将被查询的字段，建立到联合索引里去\",\"示例：\",\"select * from user where age > 20 ; \",\"第一次 取回id，第二次（回表）根据id拿到完整数据\",\"使用联合索引\",\"age,name -> index #执行下面语句 select age from user where age >20 and name like\\\"张%\\\" ; \",\"单列索引升级成了联合索引后，索引的叶子节点存储了节点值，都能够命中，不会回表查询，查询效率也是比较高的\"]},{\"header\":\"25、非聚集索引一定回表查询吗?\",\"slug\":\"_25、非聚集索引一定回表查询吗\",\"contents\":[\"不一定，只要b+树中包含的字段（创建索引的字段），覆盖（包含）想要select 的字段，那么就不会回表查询了。\"]},{\"header\":\"26、为什么要回表查询？直接存储数据不可以吗？\",\"slug\":\"_26、为什么要回表查询-直接存储数据不可以吗\",\"contents\":[\"为了控制非聚簇索引的大小\"]},{\"header\":\"27、如果把一个 InnoDB 表的主键删掉，是不是就没有主键，就没办法进行回表查询了？\",\"slug\":\"_27、如果把一个-innodb-表的主键删掉-是不是就没有主键-就没办法进行回表查询了\",\"contents\":[\"不是，InnoDB会生成rowid辅助回表查询\"]},{\"header\":\"28、什么是联合索引，组合索引，复合索引？\",\"slug\":\"_28、什么是联合索引-组合索引-复合索引\",\"contents\":[\"为c2和c3列建立联合索引，如下所示：\",\"c2，c3 - > index\",\"c3,c2 -> index\",\"where c3=?\",\"全职匹配\",\"最左前缀\"]},{\"header\":\"29、复合索引创建时字段顺序不一样使用效果一样吗？\",\"slug\":\"_29、复合索引创建时字段顺序不一样使用效果一样吗\",\"contents\":[\"我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照 c2和c3列 的大小进行排序，这个包含两层含义：\",\"先把各个记录和页按照c2列进行排序。\",\"在记录的c2列相同的情况下，采用c3列进行排序\",\"B+树叶子节点处的记录由c2列、c3列和主键c1列组成\",\"本质上也是二级索引\",\"create index idx_c2_c3 on user (c2,c3); \"]},{\"header\":\"30、什么是唯一索引？\",\"slug\":\"_30、什么是唯一索引\",\"contents\":[\"随表一起创建索引：\",\"CREATE TABLE customer ( id INT UNSIGNED AUTO_INCREMENT, customer_no VARCHAR(200), customer_name VARCHAR(200), PRIMARY KEY(id), -- 主键索引：列设定为主键后会自动建立索引，唯一且不能为空。 UNIQUE INDEX uk_no (customer_no), -- 唯一索引：索引列值必须唯一，允许有NULL值，且NULL可能会出现多次。 KEY idx_name (customer_name), -- 普通索引：既不是主键，列值也不需要唯一，单纯的为了提高查询速度而创建。 KEY idx_no_name (customer_no,customer_name) -- 复合索引：即一个索引包含多个列。 ); \",\"单独建创索引：\",\"CREATE TABLE customer1 ( id INT UNSIGNED, customer_no VARCHAR(200), customer_name VARCHAR(200) ); ALTER TABLE customer1 ADD PRIMARY KEY customer1(id); -- 主键索引 CREATE UNIQUE INDEX uk_no ON customer1(customer_no); -- 唯一索引 CREATE INDEX idx_name ON customer1(customer_name); -- 普通索引 CREATE INDEX idx_no_name ON customer1(customer_no,customer_name); -- 复合索引 \"]},{\"header\":\"31、唯一索引是否影响性能？\",\"slug\":\"_31、唯一索引是否影响性能\",\"contents\":[\"是\",\"对于读操作而言，跟普通索引没区别\",\"对于写操作来说，唯一索引需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；而普通索引更新记录在 change buffer，语句执行就结束\"]},{\"header\":\"32、什么时候使用唯一索引？\",\"slug\":\"_32、什么时候使用唯一索引\",\"contents\":[\"业务需求唯一字段的时候，一般不考虑性能问题\",\"【强制】业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。 说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明 显的；另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必 然有脏数据产生。\"]},{\"header\":\"33、什么时候适合创建索引，什么时候不适合创建索引？\",\"slug\":\"_33、什么时候适合创建索引-什么时候不适合创建索引\",\"contents\":[\"适合创建索引\",\"频繁作为where条件语句查询字段\",\"关联字段需要建立索引\",\"排序字段可以建立索引\",\"分组字段可以建立索引(因为分组前提是排序)\",\"统计字段可以建立索引（如.count(),max()）\",\"不适合创建索引\",\"频繁更新的字段不适合建立索引\",\"where，分组，排序中用不到的字段不必要建立索引\",\"可以确定表数据非常少不需要建立索引\",\"参与MySQL函数计算的列不适合建索引\",\"创建索引时避免有如下极端误解：\",\"1）宁滥勿缺。认为一个查询就需要建一个索引。\",\"2）宁缺勿滥。认为索引会消耗空间、严重拖慢更新和新增速度。\",\"3）抵制惟一索引。认为业务的惟一性一律需要在应用层通过“先查后插”方式解决。\"]},{\"header\":\"34、什么是索引下推？\",\"slug\":\"_34、什么是索引下推\",\"contents\":[\"5.6之前的版本是没有索引下推这个优化的\",\"**Using index condition：**叫作 Index Condition Pushdown Optimization （索引下推优化）\",\"如果没有索引下推（ICP），那么MySQL在存储引擎层找到满足content1 > 'z'条件的第一条二级索引记录。主键值进行回表，返回完整的记录给server层，server层再判断其他的搜索条件是否成立。如果成立则保留该记录，否则跳过该记录，然后向存储引擎层要下一条记录。\",\"如果使用了索引下推（ICP），那么MySQL在存储引擎层找到满足content1 > 'z'条件的第一条二级索引记录。不着急执行回表，而是在这条记录上先判断一下所有关于idx_content1索引中包含的条件是否成立，也就是content1 > 'z' AND content1 LIKE '%a'是否成立。如果这些条件不成立，则直接跳过该二级索引记录，去找下一条二级索引记录；如果这些条件成立，则执行回表操作，返回完整的记录给server层。\",\"总结：\",\"未开启索引下推：\",\"根据筛选条件在索引树中筛选第一个条件\",\"获得结果集后回表操作\",\"进行其他条件筛选\",\"再次回表查询\",\"开启索引下推：在条件查询时，当前索引树如果满足全部筛选条件，可以在当前树中完成全部筛选过滤，得到比较小的结果集再进行回表操作\"]},{\"header\":\"035 有哪些情况会导致索引失效？\",\"slug\":\"_035-有哪些情况会导致索引失效\",\"contents\":[\"计算、函数导致索引失效\",\"-- 显示查询分析 EXPLAIN SELECT * FROM emp WHERE emp.name LIKE 'abc%'; EXPLAIN SELECT * FROM emp WHERE LEFT(emp.name,3) = 'abc'; --索引失效 \",\"LIKE以%，_ 开头索引失效\",\"拓展：Alibaba《Java开发手册》\",\"【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\",\"EXPLAIN SELECT * FROM emp WHERE name LIKE '%ab%'; --索引失效 \",\"不等于(!= 或者<>)索引失效\",\"EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name = 'abc' ; EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name <> 'abc' ; --索引失效 \",\"IS NOT NULL 失效 和 IS NULL\",\"EXPLAIN SELECT * FROM emp WHERE emp.name IS NULL; EXPLAIN SELECT * FROM emp WHERE emp.name IS NOT NULL; --索引失效 \",\"**注意：**当数据库中的数据的索引列的NULL值达到比较高的比例的时候，即使在IS NOT NULL 的情况下 MySQL的查询优化器会选择使用索引，此时type的值是range（范围查询）\",\"-- 将 id>20000 的数据的 name 值改为 NULL UPDATE emp SET `name` = NULL WHERE `id` > 20000; -- 执行查询分析，可以发现 IS NOT NULL 使用了索引 -- 具体多少条记录的值为NULL可以使索引在IS NOT NULL的情况下生效，由查询优化器的算法决定 EXPLAIN SELECT * FROM emp WHERE emp.name IS NOT NULL \",\"类型转换导致索引失效\",\"EXPLAIN SELECT * FROM emp WHERE name='123'; EXPLAIN SELECT * FROM emp WHERE name= 123; --索引失效 \",\"复合索引未用左列字段失效\",\"如果MySQL觉得全表扫描更快时（数据少）;\"]},{\"header\":\"36 、为什么LIKE以%开头索引会失效？\",\"slug\":\"_36-、为什么like以-开头索引会失效\",\"contents\":[\"id,name,age name 创建索引 \",\"示例1：\",\"SELECT * FROM `user` where name like 'xiaobear'; \",\"select name,id from user where name like '%xiaobear' \",\"查询所有的时候，索引没有生效，会直接type=all\",\"查询索引字段的时候，type=index，表示遍历了索引树，再回表查询\",\"没有高效使用索引是因为字符串索引会逐个转换成accii码，生成b+树时按首个字符串顺序排序，类似复合索引未用左列字段失效一样，跳过开始部分也就无法使用生成的b+树了\"]},{\"header\":\"37 、一个表有多个索引的时候，能否手动选择使用哪个索引？\",\"slug\":\"_37-、一个表有多个索引的时候-能否手动选择使用哪个索引\",\"contents\":[\"不可用手动直接干预，只能通过MySQL优化器自动选择\"]},{\"header\":\"38、如何查看一个表的索引？\",\"slug\":\"_38、如何查看一个表的索引\",\"contents\":[\"show index from t_emp; // 显示表上的索引 explain select * from t_emp where id=1; // 显示可能会用到的索引及最终使用的索引 \"]},{\"header\":\"39、能否查看到索引选择的逻辑？是否使用过optimizer_trace？\",\"slug\":\"_39、能否查看到索引选择的逻辑-是否使用过optimizer-trace\",\"contents\":[\"set session optimizer_trace=\\\"enabled=on\\\",end_markers_in_json=on; SELECT * FROM information_schema.OPTIMIZER_TRACE; set session optimizer_trace=\\\"enabled=off\\\"; \"]},{\"header\":\"40、多个索引优先级是如何匹配的？\",\"slug\":\"_40、多个索引优先级是如何匹配的\",\"contents\":[\"主键（唯一索引）匹配\",\"全值匹配（单值匹配）\",\"最左前缀匹配\",\"范围匹配\",\"索引扫描\",\"全表扫描\",\"一般性建议\",\"Ø 对于单键索引，尽量选择过滤性更好的索引（例如：手机号，邮件，身份证）\",\"Ø 在选择组合索引的时候，过滤性最好的字段在索引字段顺序中，位置越靠前越好。\",\"Ø 选择组合索引时，尽量包含where中更多字段的索引\",\"Ø 组合索引出现范围查询时，尽量把这个字段放在索引次序的最后面\",\"Ø 尽量避免造成索引失效的情况\"]},{\"header\":\"41、使用Order By时能否通过索引排序？\",\"slug\":\"_41、使用order-by时能否通过索引排序\",\"contents\":[\"没有过滤条件不走索引\"]},{\"header\":\"42、通过索引排序内部流程是什么？\",\"slug\":\"_42、通过索引排序内部流程是什么\",\"contents\":[\"select name,id from user where name like '%明' order by name；\",\"select name,id，age from user where name like '%明'\",\"关键配置：\",\"sort_buffer可供排序的内存缓冲区大小\",\"max_length_for_sort_data 单行所有字段总和限制，超过这个大小启动双路排序\",\"通过索引检过滤筛选条件索到需要排序的字段+其他字段（如果是符合索引）\",\"判断索引内容是否覆盖select的字段 \",\"如果覆盖索引，select的字段和排序都在索引上，那么在内存中进行排序，排序后输出结果\",\"如果索引没有覆盖查询字段，接下来计算select的字段是否超过max_length_for_sort_data限制，如果超过，启动双路排序，否则使用单路\"]},{\"header\":\"43、什么是双路排序和单路排序\",\"slug\":\"_43、什么是双路排序和单路排序\",\"contents\":[\"单路排序：一次取出所有字段进行排序，内存不够用的时候会使用磁盘\",\"双路排序：取出排序字段进行排序，排序完成后再次回表查询所需要的其他字段\",\"如果不在索引列上，filesort有两种算法： MySQL就要启动双路排序和单路排序\",\"双路排序（慢）\",\"Select id,age,name from stu order by name;\",\"Ø MySQL 4.1之前是使用双路排序，字面意思就是两次扫描磁盘，最终得到数据， 读取行指针和order by列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出\",\"Ø 从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。\",\"Ø 取一批数据，要对磁盘进行两次扫描，众所周知，I\\\\O是很耗时的，所以在MySQL4.1之后，出现了第二种改进的算法，就是单路排序。\",\"单路排序（快）\",\"从磁盘读取查询需要的所有列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出， 它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间， 因为它把每一行都保存在内存中了。\",\"结论及引申出的问题\",\"但是用单路有问题\",\"在sort_buffer中，单路比多路要多占用很多空间，因为单路是把所有字段都取出, 所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排……从而多次I/O。\",\"单路本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。\",\"优化策略\",\"Ø 增大sort_buffer_size参数的设置\",\"Ø 增大max_length_for_sort_data参数的设置\",\"Ø 减少select 后面的查询的字段。 禁止使用select *\",\"提高Order By的速度\",\"Order by时select * 是一个大忌。只Query需要的字段， 这点非常重要。在这里的影响是：\",\"l 当Query的字段大小总和小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序， 否则用老算法——多路排序。\",\"l 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序算法的风险会更大一些，所以要提高sort_buffer_size。\",\"尝试提高 sort_buffer_size\",\"l 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程（connection）的 1M-8M之间调整。 MySQL5.7和8.0，InnoDB存储引擎默认值是1048576字节，1MB。\",\"SHOW VARIABLES LIKE '%sort_buffer_size%';\",\"​\",\"尝试提高 max_length_for_sort_data\",\"提高这个参数， 会增加用改进算法的概率。\",\"SHOW VARIABLES LIKE '%max_length_for_sort_data%';\",\"#5.7默认1024字节\",\"#8.0默认4096字节\",\"l 但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率。如果需要返回的列的总长度大于max_length_for_sort_data，使用双路算法，否则使用单路算法。1024-8192字节之间调整\"]},{\"header\":\"44、group by 分组和order by在索引使用上有什么区别？\",\"slug\":\"_44、group-by-分组和order-by在索引使用上有什么区别\",\"contents\":[\"group by 使用索引的原则几乎跟order by一致 ，唯一区别：\",\"group by 先排序再分组，遵照索引建的最佳左前缀法则\",\"group by没有过滤条件，也可以用上索引。Order By 必须有过滤条件才能使用上索引。\"]},{\"header\":\"45、如果表中有字段为null，又被经常查询该不该给这个字段创建索引？\",\"slug\":\"_45、如果表中有字段为null-又被经常查询该不该给这个字段创建索引\",\"contents\":[\"应该创建索引，使用的时候尽量使用is null判断。\",\"IS NOT NULL 失效 和 IS NULL\",\"EXPLAIN SELECT * FROM emp WHERE emp.name IS NULL; EXPLAIN SELECT * FROM emp WHERE emp.name IS NOT NULL; --索引失效 \",\"**注意：**当数据库中的数据的索引列的NULL值达到比较高的比例的时候，即使在IS NOT NULL 的情况下 MySQL的查询优化器会选择使用索引，此时type的值是range（范围查询）\",\"-- 将 id>20000 的数据的 name 值改为 NULL UPDATE emp SET `name` = NULL WHERE `id` > 20000; -- 执行查询分析，可以发现 IS NOT NULL 使用了索引 -- 具体多少条记录的值为NULL可以使索引在IS NOT NULL的情况下生效，由查询优化器的算法决定 EXPLAIN SELECT * FROM emp WHERE emp.name IS NOT NULL \"]},{\"header\":\"46、有字段为null索引是否会失效？\",\"slug\":\"_46、有字段为null索引是否会失效\",\"contents\":[\"不一定会失效，每一条sql具体有没有使用索引 可以通过trace追踪一下\",\"最好还是给上默认值\",\"数字类型的给0，字符串给个空串“”，\",\"参考上一题\"]}]},\"/interview/database/150mysql/technical-Architecture.html\":{\"title\":\"MySQL 内部技术架构\",\"contents\":[{\"header\":\"47、MySQL内部支持缓存查询吗？\",\"slug\":\"_47、mysql内部支持缓存查询吗\",\"contents\":[\"当MySQL接收到客户端的查询SQL之后，仅仅只需要对其进行相应的权限验证之后，就会通过Query Cache来查找结果，甚至都不需要经过Optimizer模块进行执行计划的分析优化，更不需要发生任何存储引擎的交互\",\"MySQL5.7支持内部缓存，8.0之后就废弃掉了\"]},{\"header\":\"48、MySQL8为何废弃掉查询缓存？\",\"slug\":\"_48、mysql8为何废弃掉查询缓存\",\"contents\":[\"缓存的意义在于快速查询提升系统性能，可以灵活控制缓存的一致性\",\"MySQL缓存的限制\",\"MySQL基本没有手段灵活的管理缓存失效和生效，尤其对于频繁更新的表\",\"SQL必须完全一致才会导致cache命中\",\"为了节省内存空间，太大的result set不会被cache (< query_cache_limit)；\",\"MySQL缓存在分库分表环境下是不起作用的；\",\"执行SQL里有触发器,自定义函数时，MySQL缓存也是不起作用的；\",\"在表的结构或数据发生改变时，基于该表相关cache立即全部失效。\"]},{\"header\":\"49、MySQL 8废除缓存的替代方案是什么？\",\"slug\":\"_49、mysql-8废除缓存的替代方案是什么\",\"contents\":[\"应用层组织缓存，最简单的是使用redis，ehcached等\"]},{\"header\":\"50、MySQL内部有哪些核心模块组成，作用是什么？\",\"slug\":\"_50、mysql内部有哪些核心模块组成-作用是什么\",\"contents\":[\"Connectors（客户端）\",\"MySQL服务器之外的客户端程序，与具体的语言相关，例如Java中的JDBC，图形用户界面SQLyog等。本质上都是在TCP连接上通过MySQL协议和MySQL服务器进行通信。\",\"MySQL Server（服务器）\",\"第1层：连接层\",\"系统（客户端）访问 MySQL 服务器前，做的第一件事就是建立 TCP 连接。\",\"经过三次握手建立连接成功后， MySQL 服务器对 TCP 传输过来的账号密码做身份认证、权限获取。 \",\"用户名或密码不对，会收到一个Access denied for user错误，客户端程序结束执行\",\"用户名密码认证通过，会从权限表查出账号拥有的权限与连接关联，之后的权限判断逻辑，都将依赖于此时读到的权限\",\"TCP 连接收到请求后，必须要分配给一个线程专门与这个客户端的交互。所以还会有个线程池，去走后面的流程。每一个连接从线程池中获取线程，省去了创建和销毁线程的开销。\",\"第2层：服务层\",\"Management Serveices & Utilities： 系统管理和控制工具\",\"SQL Interface：SQL接口：\",\"接收用户的SQL命令，并且返回用户需要查询的结果。比如SELECT ... FROM就是调用SQL Interface\",\"MySQL支持DML（数据操作语言）、DDL（数据定义语言）、存储过程、视图、触发器、自定义函数等多种SQL语言接口\",\"Parser：解析器：\",\"在SQL命令传递到解析器的时候会被解析器验证和解析。解析器中SQL 语句进行语法分析、语法解析，并为其创建语法树。\",\"语法分析\",\"语法分析主要是把输入转化成若干个tokens，包含key和非key。\",\"在分析之后，会得到4个Token，其中有2个key，它们分别是SELECT、FROM。\",\"key\",\"非key\",\"key\",\"非key\",\"SELECT\",\"age\",\"FROM\",\"user\",\"典型的解析树如下：\",\"Optimizer：查询优化器：\",\"SQL语句在语法解析后、查询前会使用查询优化器对查询进行优化，确定SQL语句的执行路径，生成一个执行计划。\",\"Caches & Buffers： 查询缓存组件：\",\"MySQL内部维持着一些Cache和Buffer，比如Query Cache用来缓存一条SELECT语句的执行结果，如果能够在其中找到对应的查询结果，那么就不必再进行查询解析、查询优化和执行的整个过程了，直接将结果反馈给客户端。\",\"这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等 。\",\"这个查询缓存可以在不同客户端之间共享 。\",\"第3层：引擎层\",\"插件式存储引擎层（ Storage Engines），负责MySQL中数据的存储和提取，对物理服务器级别维护的底层数据执行操作，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，管理的表有不同的存储结构，采用的存取算法也不同，这样我们可以根据自己的实际需要进行选取。例如MyISAM引擎和InnoDB引擎。\",\"存储层\",\"所有的数据、数据库、表的定义、表的每一行的内容、索引，都是存在文件系统 上，以文件的方式存在，并完成与存储引擎的交互。\"]},{\"header\":\"51、一条sql发送给MySQL后，内部是如何执行的？（说一下 MySQL 执行一条查询语句的内部执行过程？）\",\"slug\":\"_51、一条sql发送给mysql后-内部是如何执行的-说一下-mysql-执行一条查询语句的内部执行过程\",\"contents\":[\"1.5、查询流程说明\",\"首先，MySQL客户端通过协议与MySQL服务器建连接，通过SQL接口发送SQL语句，先检查查询缓存，如果命中，直接返回结果，否则进行语句解析。也就是说，在解析查询之前，服务器会先访问查询缓存，如果某个查询结果已经位于缓存中，服务器就不会再对查询进行解析、优化、以及执行。它仅仅将缓存中的结果返回给用户即可，这将大大提高系统的性能。\",\"接下来，MySQL解析器通过关键字将SQL语句进行解析，并生成一棵对应的解析树，解析器使用MySQL语法规则验证和解析SQL语句。例如，它将验证是否使用了错误的关键字，或者使用关键字的顺序是否正确，引号能否前后匹配等；预处理器则根据MySQL规则进一步检查解析树是否合法，例如，这里将检查数据表和数据列是否存在，还会解析名字和别名，看是否有歧义等。然后预处理器会进行查询重写，生成一棵新解析树。\",\"接下来，查询优化器将解析树转化成执行计划。MySQL优化程序会对我们的语句做一些优化，如子查询转换为连接、表达式简化等等。优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引执行查询，以及表之间的连接顺序是啥样，等等。我们可以使用EXPLAIN语句来查看某个语句的执行计划。\",\"最后，进入执行器阶段。完成查询优化后，查询执行引擎会按照生成的执行计划调用存储引擎提供的接口执行SQL查询并将结果返回给客户端。在MySQL8以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存，再返回给客户端。\"]},{\"header\":\"52、MySQL 提示“不存在此列”是执行到哪个节点报出的？\",\"slug\":\"_52、mysql-提示-不存在此列-是执行到哪个节点报出的\",\"contents\":[\"是在Parser：解析器 分析sql语法的时候检查的列。\"]},{\"header\":\"53、如果一张表创建了多个索引，在哪个阶段或模块进行的索引选择？\",\"slug\":\"_53、如果一张表创建了多个索引-在哪个阶段或模块进行的索引选择\",\"contents\":[\"在优化器阶段Optimizer：查询优化器：\"]},{\"header\":\"54、MySQL 支持哪些存储引擎？默认使用哪个？\",\"slug\":\"_54、mysql-支持哪些存储引擎-默认使用哪个\",\"contents\":[\"查看MySQL提供什么存储引擎\",\"SHOW ENGINES; \",\"下面的结果表示MySQL中默认使用的存储引擎是InnoDB，支持事务，行锁，外键，支持分布式事务(XA)，支持保存点(回滚)\",\"也可以通过以下语句查看默认的存储引擎：\",\"SHOW VARIABLES LIKE '%default_storage_engine%'; \"]},{\"header\":\"55、MySQL8.0自带哪些存储引擎？分别是做什么的？\",\"slug\":\"_55、mysql8-0自带哪些存储引擎-分别是做什么的\",\"contents\":[\"1. InnoDB存储引擎\",\"InnoDB是MySQL的默认事务型引擎，它被设计用来处理大量的短期(short-lived)事务。可以确保事务的完整提交(Commit)和回滚(Rollback)。\",\"除非有非常特别的原因需要使用其他的存储引擎，否则应该优先考虑InnoDB引擎。\",\"数据文件结构：\",\"表名.frm 存储表结构（MySQL8.0时，合并在表名.ibd中）\",\"表名.ibd 存储数据和索引\",\"InnoDB不仅缓存索引还要缓存真实数据， 对内存要求较 高 ，而且内存大小对性能有决定性的影响。\",\"2. MyISAM存储引擎\",\"MyISAM提供了大量的特性，包括全文索引、压缩、空间函数(GIS)等，但MyISAM不支持事务和行级锁，有一个毫无疑问的缺陷就是崩溃后无法安全恢复。\",\"优势是访问的 速度快 ，对事务完整性没有要求或者以SELECT、INSERT为主的应用。\",\"数据文件结构：\",\"表名.frm 存储表结构\",\"表名.MYD 存储数据\",\"表名.MYI 存储索引\",\"MyISAM只缓存索引，不缓存真实数据。\",\"3. Archive引擎\",\"Archive档案存储引擎只支持INSERT和SELECT操作。\",\"Archive表适合日志和数据采集（档案）类应用。\",\"根据英文的测试结论来看，Archive表比MyISAM表要小大约75%，比支持事务处理的InnoDB表小大约83%。\",\"4. Blackhole引擎\",\"Blackhole引擎没有实现任何存储机制，它会丢弃所有插入的数据，不做任何保存。\",\"但服务器会记录Blackhole表的日志，所以可以用于复制数据到备库，或者简单地记录到日志。但这种应用方式会碰到很多问题，因此并不推荐。\",\"5. CSV引擎\",\"CSV引擎可以将普通的CSV文件作为MySQL的表来处理，但不支持索引。\",\"CSV引擎可以作为一种数据交换的机制，非常有用。\",\"CSV存储的数据直接可以在操作系统里，用文本编辑器，或者excel读取。\",\"6. Memory引擎\",\"如果需要快速地访问数据，并且这些数据不会被修改，重启以后丢失也没有关系，那么使用Memory表是非常有用。\",\"Memory表至少比MyISAM表要快一个数量级。\",\"7. Federated引擎\",\"Federated引擎是访问其他MySQL服务器的一个代理（跨库关联查询），尽管该引擎看起来提供了一种很好的跨服务器的灵活性，但也经常带来问题，因此默认是禁用的。\"]},{\"header\":\"56、MySQL 存储引擎架构了解吗？\",\"slug\":\"_56、mysql-存储引擎架构了解吗\",\"contents\":[\"https://dev.MySQL.com/doc/refman/5.7/en/innodb-architecture.html\",\"下面是官方的InnoDB引擎结构图，主要分为内存结构和磁盘结构两大部分。\",\"内存区域\",\"Buffer Pool:在InnoDB访问表记录和索引时会在Buffer Pool的页中缓存，以后使用可以减少磁盘IO操作，提升效率。主要用来缓存热的数据页和索引页。\",\"Log Buffer：用来缓存redolog\",\"Adaptive Hash Index：自适应哈希索引\",\"Change Buffer:它是一种应用在非唯一普通索引页（non-unique secondary index page）不在缓冲池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅仅记录缓冲变更（Buffer Changes），等未来数据被读取时，再将数据合并（Merge）恢复到缓冲池中的技术。写缓冲的目的是降低写操作的磁盘IO，提升数据库性能。\",\"磁盘区域\",\"磁盘中的结构分为两大类：表空间和重做日志。\",\"表空间：分为系统表空间(MySQL 目录的 ibdata1 文件)，临时表空间，常规表空间，Undo 表空间以及 file-per-table 表空间(MySQL5.7默认打开file_per_table 配置）。系统表空间又包括了InnoDB数据字典，双写缓冲区(Doublewrite Buffer)，修改缓存(Change Buffer），Undo日志等。\",\"Redo日志：存储的就是 Log Buffer 刷到磁盘的数据。\",\"官方文档：\",\"https://dev.MySQL.com/doc/refman/8.0/en/innodb-storage-engine.html\"]},{\"header\":\"57、能否单独为一张表设置存储引擎？\",\"slug\":\"_57、能否单独为一张表设置存储引擎\",\"contents\":[\"方法1：\",\"设置默认存储引擎：\",\"SET DEFAULT_STORAGE_ENGINE=MyISAM; \",\"方法2：\",\"或者修改 my.cnf 文件：vim /etc/my.cnf 新增一行：default-storage-engine=MyISAM 重启MySQL：systemctl restart MySQLd\",\"方法3：\",\"我们可以为 不同的表设置不同的存储引擎\",\"CREATE TABLE 表名( 建表语句; ) ENGINE = 存储引擎名称; ALTER TABLE 表名 ENGINE = 存储引擎名称; \"]},{\"header\":\"58、阿里、京东等大厂都有自研的存储引擎，如何开发一套自己的？\",\"slug\":\"_58、阿里、京东等大厂都有自研的存储引擎-如何开发一套自己的\",\"contents\":[\"开发存储引擎并不难，难的是开发出来高效的有意义的存储引擎。\",\"简单例子可以看一下官方源码中的示例，可以实现一个什么也没做的存储引擎。\",\"有兴趣可以参考官方文档：https://dev.MySQL.com/doc/dev/MySQL-server/latest/\"]},{\"header\":\"59、MyISAM 和 InnoDB 的区别是什么？\",\"slug\":\"_59、myisam-和-innodb-的区别是什么\",\"contents\":[\"对比项\",\"MyISAM\",\"InnoDB\",\"外键\",\"不支持\",\"支持\",\"事务\",\"不支持\",\"支持\",\"行表锁\",\"表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作\",\"行锁，操作时只锁某一行，不对其它行有影响，适合高并发的操作\",\"缓存\",\"只缓存索引，不缓存真实数据\",\"不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响\",\"关注点\",\"并发查询，节省资源、消耗少、简单业务\",\"并发写、事务、多表关系、更大资源\",\"默认安装\",\"Y\",\"Y\",\"默认使用\",\"N\",\"Y\",\"自带系统表使用\",\"Y\",\"N\"]},{\"header\":\"60、具体说一下如何做技术选型\",\"slug\":\"_60、具体说一下如何做技术选型\",\"contents\":[\"除非几乎没有写操作全部都是高频的读操作可以选择MyISAM作为表的存储引擎，其他业务可以一律使用InnoDB。\"]}]},\"/interview/database/150mysql/transaction.html\":{\"title\":\"MySQL事务\",\"contents\":[{\"header\":\"61、什么是数据库事务？事务的特性是什么？\",\"slug\":\"_61、什么是数据库事务-事务的特性是什么\",\"contents\":[\"事务：\",\"是数据库操作的最小工作单元，是作为单个逻辑工作单元执行的一系列操作；\",\"这些操作作为一个整体一起向系统提交，要么都执行、要么都不执行；\",\"事务是一组不可再分割的操作集合（工作逻辑单元）\",\"事务都有 ACID 特性\"]},{\"header\":\"62、什么是ACID？\",\"slug\":\"_62、什么是acid\",\"contents\":[\"1 、原子性 atomicity\",\"过程的保证\",\"只做一个步骤：给钱 ——> 去买 ——> 交回来\",\"事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做\",\"2 、一致性 consistency\",\"结果的保证\",\"保证要吃完 刚张嘴挂了，失去一致性\",\"事 务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统 运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是 不一致的状态。\",\"3 、隔离性 isolation\",\"并发事务互相干扰\",\"不被干扰 刚张嘴别人塞了东西\",\"一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。\",\"4 、持续性永久性 durability\",\"保存 吃到肚子里\",\"也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。\"]},{\"header\":\"63、并发事务会有哪些问题？\",\"slug\":\"_63、并发事务会有哪些问题\",\"contents\":[\"多个事务并发执行一定会产生相互争夺资源的问题\"]},{\"header\":\"64、什么是脏读\",\"slug\":\"_64、什么是脏读\",\"contents\":[\"脏读（Dirty read）\",\"是一个事务在处理过程中读取了另外一个事务未提交的数据\",\"当一个事务正在访问数据并且对其进行了修改，但是还没提交事务，这时另外一个事务也访问了这个数据，然后使用了这个数据，因为这个数据的修改还没提交到数据库，所以另外一个事务读取的数据就是“脏数据”，这种行为就是“脏读”，依据“脏数据”所做的操作可能是会出现问题的。\"]},{\"header\":\"65、丢失修改\",\"slug\":\"_65、丢失修改\",\"contents\":[\"修改丢失（Lost of modify）：*是指一个事务读取一个数据时，另外一个数据也访问了该数据，那么在第一个事务修改了这个数据之后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，这种情况就被称为**修改丢失\"]},{\"header\":\"66、不可重复读\",\"slug\":\"_66、不可重复读\",\"contents\":[\"不可重复读（Unrepeatableread）：指在一个事务内多次读取同一数据**，在这个事务还没结束时，另外一个事务也访问了这个数据并对这个数据进行了修改，那么就可能造成第一个事务两次读取的数据不一致，这种情况就被称为**不可重复读。\"]},{\"header\":\"67、幻读\",\"slug\":\"_67、幻读\",\"contents\":[\"幻读（Phantom read）\",\"是指同一个事务内多次查询返回的结果集总数不一样（比如增加了或者减少了行记录）。\",\"幻读与不可重复读类似，幻读是指一个事务读取了几行数据，这个事务还没结束，接着另外一个事务插入了一些数据，在随后的查询中，第一个事务读取到的数据就会比原本读取到的多，就好像发生了幻觉一样，所以称为*幻读*。\"]},{\"header\":\"68、不可重复读和幻读有什么区别？\",\"slug\":\"_68、不可重复读和幻读有什么区别\",\"contents\":[\"不可重复读 针对的是一份数据的修改\",\"幻读 针对的是行数修改\"]},{\"header\":\"69、MySQL是如何避免事务并发问题的？\",\"slug\":\"_69、mysql是如何避免事务并发问题的\",\"contents\":[\"避免事务并发问题是需要付出性能代价的，此时和分布式系统设计一样（CAP定理及base理论），为了保证一致性就一定会牺牲性能，要做取舍\",\"在MySQL内部通过加锁的方式实现好了解决方案可供选择，就是配置事务隔离级别\"]},{\"header\":\"70、什么是事务隔离级别？\",\"slug\":\"_70、什么是事务隔离级别\",\"contents\":[\"事务隔离级别 脏读 不可重复读(被修改) 幻读（删减） 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 \"]},{\"header\":\"71、默认的级别是什么？\",\"slug\":\"_71、默认的级别是什么\",\"contents\":[\"MySQL InnoDB存储引擎默认的事务隔离级别是可重复读（REPEATABLE-READ）\",\"MySQL 5.7 SELECT @@tx_isolation; MySQL 8.0 SELECT @@transaction_isolation; \"]},{\"header\":\"72、如何选择事务隔离级别？\",\"slug\":\"_72、如何选择事务隔离级别\",\"contents\":[\"隔离级别越低，事务请求的锁越少相应性能也就越高，如没有特殊要求或有错误发生，使用默认的隔离级别即可，如果系统中有高频读写并且对一致性要求高那么就需要比较高的事务隔离级别甚至串行化。\"]},{\"header\":\"73、靠缓存可以提升高事务隔离级别的性能吗？\",\"slug\":\"_73、靠缓存可以提升高事务隔离级别的性能吗\",\"contents\":[\"提升事务级别的目的本质是提供更高的数据一致性，如果前置有缓存，那么缓存只能提供高效读并不能保证数据及时一致性，相反的我们还需要对缓存管理有额外的开销。\"]},{\"header\":\"74、MySQL事务隔离是如何实现的？\",\"slug\":\"_74、mysql事务隔离是如何实现的\",\"contents\":[\"隔离的实现主要是读写锁和MVCC\"]},{\"header\":\"75、什么是一致性非锁定读和锁定读？\",\"slug\":\"_75、什么是一致性非锁定读和锁定读\",\"contents\":[\"锁定读\",\"使用到了读写锁\",\"读写锁是最简单直接的的事务隔离实现方式\",\"每次读操作需要获取一个共享(读)锁，每次写操作需要获取一个写锁。\",\"共享锁之间不会产生互斥，共享锁和写锁之间、以及写锁与写锁之间会产生互斥。\",\"当产生锁竞争时，需要等待其中一个操作释放锁后，另一个操作才能获取到锁。\",\"锁机制，解决的就是多个事务同时更新数据，此时必须要有一个加锁的机制\",\"行锁（记录锁）：解决的就是多个事务同时更新一行数据\",\"间隙锁：解决的就是多个事务同时更新多行数据\",\"下列操作属于锁定读\",\"select ... lock in share mode select ... for update insert、update、delete \",\"非锁定读\",\"v10 -> age=18\",\"v11 ->age=19\",\"v12 ->age=15\",\"使用mvcc 多版本控制实现\"]},{\"header\":\"76、说一下MVCC内部细节\",\"slug\":\"_76、说一下mvcc内部细节\",\"contents\":[\"https://dev.MySQL.com/doc/refman/5.7/en/innodb-multi-versioning.html\",\"Multi-Version Concurrency Control 多版本并发控制，MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问\",\"InnoDB是一个多版本的存储引擎。它保存有关已更改行的旧版本的信息，以支持并发和回滚等事务特性。这些信息存储在一个称为回滚段的数据结构中的系统表空间或undo表空间中。参见第14.6.3.4节“撤消表空间”。InnoDB使用回滚段中的信息来执行事务回滚所需的撤消操作。它还使用这些信息构建行的早期版本，以实现一致的读取\",\"MVCC 的实现依赖于：隐藏字段、Read View、undo log\",\"隐藏字段\",\"A 6-byte DB_TRX_ID 用来标识最近一次对本行记录做修改 (insert 、update) 的事务的标识符 ，即最后一次修改本行记录的事务 id。 如果是 delete 操作， 在 InnoDB 存储引擎内部也属于一次 update 操作，即更新行中的一个特殊位 ，将行标识为己删除，并非真正删除。\",\"A 7-byte DB_ROLL_PTR 回滚指针，指向该行的 undo log 。如果该行未被更新，则为空.\",\"A 6-byte DB_ROW_ID 如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引.\",\"Read View\",\"不同的事务隔离级别中，当有事物在执行过程中修改了数据（更新版本号），在并发事务时需要判断一下版本链中的哪个版本是当前事务可见的。为此InnoDB有了ReadView的概念，使用ReadView来记录和隔离不同事务并发时此记录的哪些版本是对当前访问事物可见的。\",\"undo log\",\"除了用来回滚数据，还可以读取可见版本的数据。以此实现非锁定读\"]},{\"header\":\"77、MySQL事务一致性，原子性是如何实现的？\",\"slug\":\"_77、mysql事务一致性-原子性是如何实现的\",\"contents\":[\"首先是通过锁和mvcc实现了执行过程中的一致性和原子性\",\"其次是在灾备方面通过Redo log实现，Redo log会把事务在执行过程中对数据库所做的所有修改都记录下来，在之后系统崩溃重启后可以把事务所做的任何修改都恢复出来。\"]},{\"header\":\"78、MySQL事务的持久性是如何实现的？\",\"slug\":\"_78、mysql事务的持久性是如何实现的\",\"contents\":[\"使用Redo log保证了事务的持久性。当事务提交时，必须先将事务的所有日志写入日志文件进行持久化，就是我们常说的WAL(write ahead log)机制，如果出现断电重启便可以从redolog中恢复，如果redolog写入失败那么也就意味着修改失败整个事务也就直接回滚了。\"]},{\"header\":\"79、表级锁和行级锁有什么区别？\",\"slug\":\"_79、表级锁和行级锁有什么区别\",\"contents\":[\"表级锁：串行化（serializable）时，整表加锁，事务访问表数据时需要申请锁，虽然可分为读锁和写锁，但毕竟是锁住整张表，会导致并发能力下降，一般是做ddl处理时使用\",\"行级锁：除了串行化（serializable）时 InnoDB使用的都是行级锁，只锁一行数据，其他行数据不影响，并发能力强。\"]},{\"header\":\"80、什么是行级锁？MySQL如何完成的？\",\"slug\":\"_80、什么是行级锁-mysql如何完成的\",\"contents\":[\"行级锁实现比较复杂不是单纯锁住一行数据，是由mvcc完成的。\"]},{\"header\":\"81、什么是共享锁（读锁）？\",\"slug\":\"_81、什么是共享锁-读锁\",\"contents\":[\"共享锁或S锁，其它事务可以继续加共享锁，但不能加排它锁\"]},{\"header\":\"82、什么是排它锁（写锁/独占锁）？\",\"slug\":\"_82、什么是排它锁-写锁-独占锁\",\"contents\":[\"排它锁或X锁，在进行写操作之前要申请并获得，其它事务不能再获得任何锁。\"]},{\"header\":\"83、什么是意向锁？\",\"slug\":\"_83、什么是意向锁\",\"contents\":[\"它分为意向共享锁（IS）和意向排他锁（IX）\",\"一个事务对一张表的某行添加共享锁前，必须获得对该表一个IS锁或者优先级更高的锁。 一个事务对一张表的某行添加排他锁之前，它必须对该表获取一个IX锁。\",\"意向锁属于表锁，它不与innodb中的行锁冲突，任意两个意向锁之间也不会产生冲突，但是会与表锁（S锁和X锁）产生冲突\"]},{\"header\":\"84、InnoDB支持哪几种锁？\",\"slug\":\"_84、innodb支持哪几种锁\",\"contents\":[\"表锁，行锁，间隙锁，Next-Key锁等\",\"在Serializable中读加共享锁，写加排他锁，读写互斥\",\"两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁）\"]},{\"header\":\"85、当前读和快照读分别是什么？\",\"slug\":\"_85、当前读和快照读分别是什么\",\"contents\":[\"当前读 ：在锁定读（使用锁隔离事物）的时候读到的是最新版本的数据\",\"快照读：可重复读（repeatable-read）下 mvcc生效读取的是数据的快照，并不是最新版本的数据（未提交事物的数据）\"]},{\"header\":\"086 什么是XA协议？\",\"slug\":\"_086-什么是xa协议\",\"contents\":[\"https://dev.MySQL.com/doc/refman/8.0/en/xa.html\",\"AP（Application Program）：应用程序，定义事务边界（定义事务开始和结束）并访问事务边界内的资源。\",\"RM（Resource Manger）资源管理器: 管理共享资源并提供外部访问接口。供外部程序来访问数据库等共享资源。此外，RM还具有事务的回滚能力。\",\"TM（Transaction Manager）事务管理器：TM是分布式事务的协调者，TM与每个RM进行通信，负责管理全局事务，分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚、失败恢复等。\",\"应用程序AP向事务管理器TM发起事务请求\",\"TM调用xa_open()建立同资源管理器的会话\",\"TM调用xa_start()标记一个事务分支的开头\",\"AP访问资源管理器RM并定义操作，比如插入记录操作\",\"TM调用xa_end()标记事务分支的结束\",\"TM调用xa_prepare()通知RM做好事务分支的提交准备工作。其实就是二阶段提交的提交请求阶段。\",\"TM调用xa_commit()通知RM提交事务分支，也就是二阶段提交的提交执行阶段。\",\"TM调用xa_close管理与RM的会话。\",\"这些接口一定要按顺序执行，比如xa_start接口一定要在xa_end之前。此外，这里千万要注意的是事务管理器只是标记事务分支并不执行事务，事务操作最终是由应用程序通知资源管理器完成的。另外，我们来总结下XA的接口\",\"xa_start:负责开启或者恢复一个事务分支，并且管理XID到调用线程\",\"xa_end:负责取消当前线程与事务分支的关系\",\"xa_prepare:负责询问RM 是否准备好了提交事务分支 xa_commit:通知RM提交事务分支\",\"xa_rollback:通知RM回滚事务分支\"]},{\"header\":\"87、什么是MySQL xa事务？\",\"slug\":\"_87、什么是mysql-xa事务\",\"contents\":[\"MySQL的xa事务分为两部分：\",\"InnoDB内部本地普通事务操作协调数据写入与log写入两阶段提交\",\"外部分布式事务\",\"5.7 SHOW VARIABLES LIKE '%innodb_support_xa%'; 8.0 默认开启无法关闭 \",\"XA 事务语法示例如下：\",\"XA START '自定义事务id'; SQL语句... XA END '自定义事务id'; XA PREPARE '自定义事务id'; XA COMMIT\\\\ROLLBACK '自定义事务id'; \",\"XA PREPARE 执行成功后，事务信息将被持久化。即使会话终止甚至应用服务宕机，只要我们将【自定义事务id】记录下来，后续仍然可以使用它对事务进行 rollback 或者 commit。\"]},{\"header\":\"88、XA事务与普通事务区别是什么？\",\"slug\":\"_88、xa事务与普通事务区别是什么\",\"contents\":[\"xa事务可以跨库或跨服务器，属于分布式事务，同时xa事务还支撑了InnoDB内部日志两阶段记录\",\"普通事务只能在单库中执行\"]},{\"header\":\"89、什么是2pc 3pc？\",\"slug\":\"_89、什么是2pc-3pc\",\"contents\":[\"两阶段提交协议与3阶段提交协议，额外增加了参与的角色保证分布式事务完成更完善\"]},{\"header\":\"90、是否使用过select for update？会产生哪些操作？\",\"slug\":\"_90、是否使用过select-for-update-会产生哪些操作\",\"contents\":[\"查询库存 = 100 0 扣减库存 = -1 99 记录日志 = log 提交 commit \",\"select本身是一个查询语句，查询语句是不会产生冲突的一种行为，一般情况下是没有锁的，用select for update 会让select语句产生一个排它锁(X), 这个锁和update的效果一样，会使两个事务无法同时更新一条记录。\",\"https://dev.MySQL.com/doc/refman/8.0/en/innodb-locks-set.html\",\"https://dev.MySQL.com/doc/refman/8.0/en/select.html\",\"for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。\",\"在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。\",\"InnoDB默认是行级别的锁，在筛选条件中当有明确指定主键或唯一索引列的时候，是行级锁。否则是表级别。\",\"示例\",\"SELECT … FOR UPDATE [OF column_list][WAIT n|NOWAIT][SKIP LOCKED]; select * from t for update 会等待行锁释放之后，返回查询结果。 select * from t for update nowait 不等待行锁释放，提示锁冲突，不返回结果 select * from t for update wait 5 等待5秒，若行锁仍未释放，则提示锁冲突，不返回结果 select * from t for update skip locked 查询返回查询结果，但忽略有行锁的记录 \"]},{\"header\":\"091说一下MySQL死锁的原因和处理方法\",\"slug\":\"_091说一下mysql死锁的原因和处理方法\",\"contents\":[\"事务 a 表 t id=100 更新 加行锁 表 t id=200 更新 已加锁 事务 b 表 t id=200 更新 加行锁 表 t id=100 更新 已加锁 \",\"死锁与锁等待是两个概念 \",\"如未开启事务，多个客户端执行的insert操作\",\"当多个事务同时持有和请求同一资源上的锁而产生循环依赖的时候就产生了死锁。\",\"排查：\",\"正在运行的任务 \",\"show full processlist; 找到卡主的进程\",\"解开死锁 \",\"UNLOCK TABLES ；\",\"查看当前运行的事务 \",\"SELECT * FROM information_schema.INNODB_TRX;\",\"当前出现的锁 \",\"SELECT * FROM information_schema.INNODB_LOCKS;\",\"观察错误日志\",\"查看InnoDB锁状态 \",\"show status like \\\"innodb_row_lock%\\\";\",\"lnnodb_row_lock_current_waits:当前正在等待锁定的数量; lnnodb_row_lock_time :从系统启动到现在锁定的总时间长度，单位ms; Innodb_row_lock_time_avg :每次等待所花平均时间; Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间; lnnodb_row_lock_waits :从系统启动到现在总共等待的次数。\",\"kill id 杀死进程\",\"解决：\",\"死锁无法避免，上线前要进行严格的压力测试\",\"快速失败\",\"innodb_lock_wait_timeout 行锁超时时间\",\"拆分sql，严禁大事务\",\"充分利用索引，优化索引，尽量把有风险的事务sql使用上覆盖索，优化where条件前缀匹配，提升查询速度，引减少表锁\",\"无法避免时：\",\"操作多张表时，尽量以相同的顺序来访问避免形成等待环路\",\"单张表时先排序再操作\",\"使用排它锁 比如 for update\"]},{\"header\":\"092 MySQL会产生几种日志？\",\"slug\":\"_092-mysql会产生几种日志\",\"contents\":[\"错误日志（error log）\",\"error log主要记录MySQL在启动、关闭或者运行过程中的错误信息，在MySQL的配置文件my.cnf中，可以通过log-error=/var/log/MySQLd.log 执行MySQL错误日志的位置。\",\"慢查询日志（slow query log）\",\"0.1秒\",\"Ø MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。\",\"Ø long_query_time的默认值为10，意思是运行10秒以上的语句。\",\"Ø 由他来查看哪些SQL超出了我们的最大忍耐时间值，比如一条sql执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒的sql，结合之前explain进行全面分析。\",\"Ø 默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。\",\"Ø 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。\",\"在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具MySQLdumpslow。\",\"一般查询日志（general log）\",\"general log 记录了客户端连接信息以及执行的SQL语句信息，通过MySQL的命令\",\"重写日志（redo log）\",\"回滚日志（undo log）\",\"二进制日志（bin log）****\"]},{\"header\":\"93、bin log作用是什么？\",\"slug\":\"_93、bin-log作用是什么\",\"contents\":[\"MySQL的bin log日志是用来记录MySQL中增删改时的记录日志。\",\"当你的一条sql操作对数据库中的内容进行了更新，就会增加一条bin log日志。查询操作不会记录到bin log中。\",\"bin log最大的用处就是进行主从复制，以及数据库的恢复。\"]},{\"header\":\"94、redo log作用是什么？\",\"slug\":\"_94、redo-log作用是什么\",\"contents\":[\"redo log是一种基于磁盘的数据结构，用来在MySQL宕机情况下将不完整的事务执行数据纠正，redo日志记录事务执行后的状态。\",\"当事务开始后，redo log就开始产生，并且随着事务的执行不断写入redo log file中。redo log file中记录了xxx页做了xx修改的信息，我们都知道数据库的更新操作会在内存中先执行，最后刷入磁盘。\",\"redo log就是为了恢复更新了内存但是由于宕机等原因没有刷入磁盘中的那部分数据。\"]},{\"header\":\"95、undo log作用是什么？\",\"slug\":\"_95、undo-log作用是什么\",\"contents\":[\"undo log主要用来回滚到某一个版本，是一种逻辑日志。\",\"undo log记录的是修改之前的数据，比如：当delete一条记录时，undolog中会记录一条对应的insert记录，从而保证能恢复到数据修改之前。在执行事务回滚的时候，就可以通过undo log中的记录内容并以此进行回滚。\",\"undo log还可以提供多版本并发控制下的读取（MVCC）。\"]},{\"header\":\"96、MySQL日志是否实时写入磁盘？\",\"slug\":\"_96、mysql日志是否实时写入磁盘\",\"contents\":[\"每个日志都不一样，这个得分情况考虑\"]},{\"header\":\"97、bin log刷盘机制是如何实现的？\",\"slug\":\"_97、bin-log刷盘机制是如何实现的\",\"contents\":[\"sync_binlog参数控制着事务提交时binlog写入磁盘的策略\",\"binlog 写入策略：\",\"1、sync_binlog=0 的时候，表示每次提交事务binlog不会马上写入到磁盘，而是先写到page cache,相对于磁盘写入来说写page cache要快得多,不过在MySQL 崩溃的时候会有丢失日志的风险。\",\"2、sync_binlog=1 的时候，表示每次提交事务都会执行 fsync 写入到磁盘 ；\",\"3、sync_binlog的值大于1 的时候，表示每次提交事务都 先写到page cach，只有等到积累了N个事务之后才fsync 写入到磁盘，同样在此设置下MySQL 崩溃的时候会有丢失N个事务日志的风险。\",\"很显然三种模式下，sync_binlog=1 是强一致的选择，选择0或者N的情况下在极端情况下就会有丢失日志的风险，具体选择什么模式还是得看系统对于一致性的要求。\"]},{\"header\":\"98、redo log刷盘机制是如何实现的？\",\"slug\":\"_98、redo-log刷盘机制是如何实现的\",\"contents\":[\"事务执行过程中，InnoDB会先把redo log日志写到InnoDB的log buffer内存中。MySQL支持用户自定义在commit（这里的commit指的是sql中的commit，在具体的两阶段提交中对应的prepare阶段）时将log buffer中的日志刷log file中的策略，通过innodb_flush_log_at_trx_commit参数设置\",\"设置为0：仅将日志写入log file buffer中。该模式下，在事务提交的时候，不会主动触发写入磁盘的操作，仅依靠InnoDB 的后台线程每秒执行一次刷盘操作，即每秒一次write cache和flush disk。\",\"设置为1：每次事务commit时MySQL都会把log buffer的数据立即写入log file的os cache中，并且立即flush刷到磁盘中去。即每次commit都write cache和flush disk，这是默认设置。\",\"设置为2：每次事务commit时MySQL都会把log buffer的数据写入log file的os cache 缓存，但是flush刷到磁盘的操作并不会同时进行，仅依靠InnoDB 的后台线程每秒执行一次真正的刷盘操作。即每次commit都write cache，每秒一次flush disk。\"]},{\"header\":\"99、undo log刷盘机制是如何实现的？\",\"slug\":\"_99、undo-log刷盘机制是如何实现的\",\"contents\":[\"undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。\",\"条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：\",\"通过 trx_id 可以知道该记录是被哪个事务修改的；\",\"通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；\",\"版本链如下图：\",\"undo log 两大作用：\",\"实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。\",\"实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。\",\"时机顺序：\",\"1 开启事务\",\"2 查询数据库中需要更新的字段，加载到内存中 形成数据脏页\",\"3 记录undo log到内存缓冲区（用于回滚和mvcc）并关联redo log -> 可刷盘\",\"4 记录 redo log到内存缓冲区 （用于失败重放）准备提交事务 -> 可刷盘\",\"5 修改内存中的脏页数据\",\"6 提交事务触发redolog刷盘\",\"7 undo log 和脏页 刷盘\",\"8 事务成功\"]},{\"header\":\"100 MySQL的binlog有有几种录入格式？分别有什么区别？\",\"slug\":\"_100-mysql的binlog有有几种录入格式-分别有什么区别\",\"contents\":[\"logbin格式：\",\"binlog_format=STATEMENT（默认）：数据操作的时间，同步时不一致 每一条会修改数据的sql语句会记录到binlog中。优点是并不需要记录每一 条sql语句和每一行的 数据变化，减少了binlog日志量，节约IO，提高性能。缺点是在某些情况下会导致 master-slave 中的数据不一致( 如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会 出 现 问题)\",\"binlog_format=ROW：批量数据操作时，效率低 不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了，修改成什么样 了。而且不会出 现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的 问题。缺 点是会产生大量的日志，尤其是alter table的时候会让日志暴涨。\",\"binlog_format=MIXED：是以上两种level的混合使用，有函数用ROW，没函数用STATEMENT，但是无法识别系统变量\"]},{\"header\":\"101 MySQL集群同步时为什么使用binlog？优缺点是什么？\",\"slug\":\"_101-mysql集群同步时为什么使用binlog-优缺点是什么\",\"contents\":[\"binlog是MySQL提供的日志，所有存储引擎都可用。\",\"支持增量同步\",\"binlog还可以供其他中间件读取，比如同步到hdfs中\",\"如果复制表数据： \",\"不支持某个阶段回放\",\"直接复制数据过程中一旦中断复制（比如断网），很难确定复制的offset\"]}]},\"/interview/sourceCode/java/ArrayList.html\":{\"title\":\"1、ArrayList扩容机制\",\"contents\":[{\"header\":\"1、底层数据结构\",\"slug\":\"_1、底层数据结构\",\"contents\":[\"public class ArrayList<E> extends AbstractList<E> implements List<E>, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * 默认的初始化容量 */ private static final int DEFAULT_CAPACITY = 10; /** *底层采用的是数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * 元素个数 * * @serial */ private int size; } \",\"ArrayList底层采用的是数组\"]},{\"header\":\"2、构造函数\",\"slug\":\"_2、构造函数\",\"contents\":[\"//指定容量构造函数初始化 public ArrayList(int initialCapacity) { if (initialCapacity > 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\\\"Illegal Capacity: \\\"+ initialCapacity); } } //无参构造 初始化为空数组 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } //集合构造初始化 public ArrayList(Collection<? extends E> c) { Object[] a = c.toArray(); if ((size = a.length) != 0) { if (c.getClass() == ArrayList.class) { elementData = a; } else { elementData = Arrays.copyOf(a, size, Object[].class); } } else { // replace with empty array. elementData = EMPTY_ELEMENTDATA; } } \",\"ArrayList() 会使用长度为0的数组\",\"ArrayList(int initialCapacity) 会初始化指定容量的数组\",\"ArrayList(Collection<? extends E> c) 会初始化c的大小作为数组容量\"]},{\"header\":\"3、扩容规则\",\"slug\":\"_3、扩容规则\",\"contents\":[\"private static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { //DEFAULT_CAPACITY = 10 return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) grow(minCapacity); } /** * The maximum size of array to allocate. * Some VMs reserve some header words in an array. * Attempts to allocate larger arrays may result in * OutOfMemoryError: Requested array size exceeds VM limit */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; //扩容后的规则是原来容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity >> 1); if (newCapacity - minCapacity < 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE > 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } private static int hugeCapacity(int minCapacity) { if (minCapacity < 0) // overflow throw new OutOfMemoryError(); return (minCapacity > MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } \"]},{\"header\":\"1、Add(Object o)扩容\",\"slug\":\"_1、add-object-o-扩容\",\"contents\":[\"public boolean add(E e) { //调用上面的扩容规则 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } /** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException {@inheritDoc} */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } \"]},{\"header\":\"add(）详解\",\"slug\":\"add-详解\",\"contents\":[\"初始化ArrayList，size =0，DEFAULT_CAPACITY = 10，elementData = {}，\",\"调用add(Object o)时，调用ensureCapacityInternal(1)\",\"调用calculateCapacity(elementData, 1)，返回数组容量，elementData为空，所以返回初始化容量和指定容量1的最大值Math.max(DEFAULT_CAPACITY, 1)，返回10\",\"调用ensureExplicitCapacity方法，判断是否需要初始化数组容量，若minCapacity - elementData.length > 0，即10 - 1 > 0，调用grow方法，增加容量以确保它至少可以容纳最小容量参数指定的元素数量\",\"grow方法做了哪些事，首先会获取数组的大小oldCapacity，然后进行扩容，采用的移位运算>>，返回一个新的容量newCapacity，然后跟所需的最小容量minCapacity比较 \",\"newCapacity - minCapacity < 0：说明需要的最小容量大，返回minCapacity\",\"newCapacity - MAX_ARRAY_SIZE > 0：大于最大值，返回最大值\",\"最后把容量和元素赋值给数组\"]},{\"header\":\"验证扩容规则\",\"slug\":\"验证扩容规则\",\"contents\":[\"/** * 计算arraylist的扩容规则 * @param n * @return */ private static List<Integer> arrayListGrowRule(int n) { List<Integer> list = new ArrayList<>(); int init = 0; list.add(init); //初始容量 if (n >= 1) { init = 10; list.add(init); } //进行扩容 for (int i = 1; i < n; i++) { init += (init) >> 1; list.add(init); } return list; } \",\"[0, 10, 15, 22, 33, 49, 73, 109, 163, 244, 366] \"]},{\"header\":\"结论\",\"slug\":\"结论\",\"contents\":[\"add(Object o)：首次（空数组的情况下）扩容为10，之后为元容量的1.5倍\"]},{\"header\":\"2、addAll(Collection<? extends E> c)扩容规则\",\"slug\":\"_2、addall-collection-extends-e-c-扩容规则\",\"contents\":[\"/** 按照指定集合的​​迭代器返回的顺序，将指定集合中的所有元素附加到此列表的末尾。如果在操作正在进行时修改了指定的集合，则此操作的行为是未定义的。 （这意味着如果指定的集合是这个列表，并且这个列表是非空的，那么这个调用的行为是未定义的。） 参数：c - 包含要添加到此列表的元素的集合 返回：如果此列表因调用而更改，则为true 抛出：NullPointerException – 如果指定的集合为空 */ public boolean addAll(Collection<? extends E> c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } /** 将指定集合中的所有元素插入此列表，从指定位置开始。将当前位于该位置的元素（如果有）和任何后续元素向右移动（增加它们的索引）。新元素将按照指定集合的​​迭代器返回的顺序出现在列表中。 参数：index – 插入指定集合中第一个元素的索引 c - 包含要添加到此列表的元素的集合 返回：如果此列表因调用而更改，则为true 抛出：IndexOutOfBoundsException –NullPointerException – 如果指定的集合为空 */ public boolean addAll(int index, Collection<? extends E> c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved > 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } \"]},{\"header\":\"addAll详解\",\"slug\":\"addall详解\",\"contents\":[\"初始化ArrayList，size =0，DEFAULT_CAPACITY = 10，elementData = {}，\",\"调用addAll(Object o)时，获取数组长度n，调用ensureCapacityInternal(size+n)\",\"调用calculateCapacity(elementData, size+n)，返回数组容量，elementData为空，所以返回初始化容量和指定容量1的最大值Math.max(DEFAULT_CAPACITY, size+n)，返回10\",\"调用ensureExplicitCapacity方法，判断是否需要初始化数组容量，若minCapacity - elementData.length > 0，即10 - 1 > 0，调用grow方法，增加容量以确保它至少可以容纳最小容量参数指定的元素数量\",\"grow方法做了哪些事，首先会获取数组的大小oldCapacity，然后进行扩容，采用的移位运算>>，返回一个新的容量newCapacity，然后跟所需的最小容量minCapacity比较 \",\"newCapacity - minCapacity < 0：说明需要的最小容量大，返回minCapacity\",\"newCapacity - MAX_ARRAY_SIZE > 0：大于最大值，返回最大值\",\"最后把容量和元素赋值给数组\"]},{\"header\":\"验证扩容规则\",\"slug\":\"验证扩容规则-1\",\"contents\":[\"//空数组情况，测试增加 private static void testAddAllGrowEmpty() { ArrayList<Integer> list = new ArrayList<>(); // list.addAll(Arrays.asList(1,2,3)); list.addAll(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)); System.out.println(length(list)); } //不为空情况下增加 private static void testAddAllGrowNotEmpty() { ArrayList<Integer> list = new ArrayList<>(); for (int i = 0; i < 10; i++) { list.add(i); } // list.addAll(Arrays.asList(1, 2, 3)); list.addAll(Arrays.asList(1, 2, 3, 4, 5, 6)); System.out.println(length(list)); } //反射获取长度 public static int length(ArrayList<Integer> list) { try { Field field = ArrayList.class.getDeclaredField(\\\"elementData\\\"); field.setAccessible(true); return ((Object[]) field.get(list)).length; } catch (Exception e) { e.printStackTrace(); return 0; } } \",\"空数组添加结合 \",\"添加长度小于10，数组的初始容量扩容为10\",\"添加长度大于10，数组的初始容量扩容为Math.max(10,实际个数)\",\"不为空添加集合 \",\"添加长度小于扩容后的容量，数组的容量为原容量的1.5倍\",\"添加长度大于扩容后的容量，数组的容量为Math.max(原容量的1.5倍，实际个数)\"]},{\"header\":\"结论\",\"slug\":\"结论-1\",\"contents\":[\"addAll(Object c)没有元素时，扩容为Math.max(10,实际个数)，有元素时，Math.max(原容量的1.5倍，实际个数)\"]},{\"header\":\"总结\",\"slug\":\"总结\",\"contents\":[\"ArrayList() 会使用长度为0的数组\",\"ArrayList(int initialCapacity) 会初始化指定容量的数组\",\"ArrayList(Collection<? extends E> c) 会初始化c的大小作为数组容量\",\"add(Object o)：首次（空数组的情况下）扩容为10，之后为元容量的1.5倍\",\"addAll(Object c)没有元素时，扩容为Math.max(10,实际个数)，有元素时，Math.max(原容量的1.5倍，实际个数)\"]}]},\"/interview/sourceCode/java/HashMap.html\":{\"title\":\"2、HashMap源码分析\",\"contents\":[{\"header\":\"1、底层结构\",\"slug\":\"_1、底层结构\",\"contents\":[\"相信大家都已经听过很多了，这里就不多阐述了，至于什么时候是数组，什么时候会变成链表，后续会讲解，别急！\",\"JDK版本\",\"数据结构\",\"JDK1.7\",\"数组+链表\",\"JDK1.8\",\"数组 + （链表 | 红黑树）\"]},{\"header\":\"2、树化与退化\",\"slug\":\"_2、树化与退化\",\"contents\":[]},{\"header\":\"1、为什么要用红黑树，为何一上来不树化，树化阈值为何是 8？\",\"slug\":\"_1、为什么要用红黑树-为何一上来不树化-树化阈值为何是-8\",\"contents\":[\"红黑树用来避免 DoS 攻击，防止链表超长时性能下降，树化应当是偶然情况\",\"hash 表的查找，更新的时间复杂度是 O(1)，而红黑树的查找，更新的时间复杂度是 O(log_2n)，TreeNode 占用空间也比普通 Node 的大，如非必要，尽量还是使用链表。\",\"hash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，选择 8 就是为了让树化几率足够小\"]},{\"header\":\"2、何时会树化？\",\"slug\":\"_2、何时会树化\",\"contents\":[\"链表长度超过树化阈值；\",\"数组容量 >= 64\",\"static final int TREEIFY_THRESHOLD = 8; /** * 当桶数组容量小于该值时，优先进行扩容，而不是树化 */ static final int MIN_TREEIFY_CAPACITY = 64; static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> { TreeNode<K,V> parent; // red-black tree links TreeNode<K,V> left; TreeNode<K,V> right; TreeNode<K,V> prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node<K,V> next) { super(hash, key, val, next); } } /** * 将普通节点链表转换成树形节点链表 */ final void treeifyBin(Node<K,V>[] tab, int hash) { int n, index; Node<K,V> e; // 桶数组容量小于 MIN_TREEIFY_CAPACITY，优先进行扩容而不是树化 if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) & hash]) != null) { // hd 为头节点（head），tl 为尾节点（tail） TreeNode<K,V> hd = null, tl = null; do { // 将普通节点替换成树形节点 TreeNode<K,V> p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); // 将普通链表转成由树形节点链表 if ((tab[index] = hd) != null) // 将树形链表转换成红黑树 hd.treeify(tab); } } TreeNode<K,V> replacementTreeNode(Node<K,V> p, Node<K,V> next) { return new TreeNode<>(p.hash, p.key, p.value, next); } \"]},{\"header\":\"3、何时会退化树？\",\"slug\":\"_3、何时会退化树\",\"contents\":[\"在扩容时如果拆分树时，树元素个数 <= 6 则会退化链表\",\"remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表\",\"退化的源码：\",\"final Node<K,V> untreeify(HashMap<K,V> map) { Node<K,V> hd = null, tl = null; for (Node<K,V> q = this; q != null; q = q.next) { Node<K,V> p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } \",\"在扩容时如果拆分树时，树元素个数 <= 6 则会退化链表\",\"static final int UNTREEIFY_THRESHOLD = 6; final void split(HashMap<K,V> map, Node<K,V>[] tab, int index, int bit) { TreeNode<K,V> b = this; // Relink into lo and hi lists, preserving order TreeNode<K,V> loHead = null, loTail = null; TreeNode<K,V> hiHead = null, hiTail = null; int lc = 0, hc = 0; // 遍历计算树节点 for (TreeNode<K,V> e = b, next; e != null; e = next) { next = (TreeNode<K,V>)e.next; e.next = null; if ((e.hash & bit) == 0) { if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; } else { if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; } } if (loHead != null) { //小于树节点6，则链化 if (lc <= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else { tab[index] = loHead; if (hiHead != null) // (else is already treeified) loHead.treeify(tab); } } if (hiHead != null) { //小于树节点6，则链化 if (hc <= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else { tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); } } } \",\"移除节点时\",\"final void removeTreeNode(HashMap<K,V> map, Node<K,V>[] tab, boolean movable) { int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) & hash; TreeNode<K,V> first = (TreeNode<K,V>)tab[index], root = first, rl; TreeNode<K,V> succ = (TreeNode<K,V>)next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); // 若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表 if (root == null || (movable && (root.right == null || (rl = root.left) == null || rl.left == null))) { tab[index] = first.untreeify(map); // too small return; } ...... } \"]},{\"header\":\"3、索引是如何计算的\",\"slug\":\"_3、索引是如何计算的\",\"contents\":[]},{\"header\":\"1、索引计算方法\",\"slug\":\"_1、索引计算方法\",\"contents\":[\"先看源码\",\"static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); //h >>> 16 表示取出h的高16位，具体计算这里就不讲解了 } \",\"首先，如果对象我null，则返回0，否则计算对象的 hashCode()\",\"再进行调用 HashMap 的 hash() 方法进行二次哈希 \",\"二次 hash() 是为了综合高位数据，让哈希分布更为均匀\",\"最后 & (capacity – 1) 得到索引，这里源码并没有看到\",\"关于这里(h = key.hashCode()) ^ (h >>> 16)不懂的可以参考这篇讲解：https://blog.csdn.net/qq_42034205/article/details/90384772\"]},{\"header\":\"2、数组容量为何是 2 的 n 次幂\",\"slug\":\"_2、数组容量为何是-2-的-n-次幂\",\"contents\":[\"计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模\",\"扩容时重新计算索引效率更高： hash & == 0 的元素留在原来位置 ，否则新位置 = 旧位置 + oldCap\",\"注意\",\"二次 hash 是为了配合 容量是 2 的 n 次幂 这一设计前提，如果 hash 表的容量不是 2 的 n 次幂，则不必二次 hash\",\"容量是 2 的 n 次幂 这一设计计算索引效率更好，但 hash 的分散性就不好，需要二次 hash 来作为补偿，没有采用这一设计的典型例子是 HashTable\",\"下面代码演示：容量是 2 的 n 次幂的设计来讲，二次 hash 非常重要\",\"public class Utils { /** * 交换元素 * @param array * @param i * @param j */ public static void swap(int[] array, int i, int j) { int t = array[i]; array[i] = array[j]; array[j] = t; } public static void shuffle(int[] array) { Random rnd = new Random(); int size = array.length; for (int i = size; i > 1; i--) { swap(array, i - 1, rnd.nextInt(i)); } } /** * 随机数组 演示 如果 hashCode 足够随机，容量是否是 2 的 n 次幂影响不大 * @param n * @return */ public static int[] randomArray(int n) { int lastVal = 1; Random r = new Random(); int[] array = new int[n]; for (int i = 0; i < n; i++) { int v = lastVal + Math.max(r.nextInt(10), 1); array[i] = v; lastVal = v; } shuffle(array); return array; } /** * 如果 hashCode 偶数的多，容量是 2 的 n 次幂会导致分布不均匀 * @param n * @return */ public static int[] evenArray(int n) { int[] array = new int[n]; for (int i = 0; i < n; i++) { array[i] = i * 2; } return array; } /** * 如果 hashCode 低位一样的多，容量是 2 的 n 次幂会导致分布不均匀 * @param n * @return */ public static int[] lowSameArray(int n) { int[] array = new int[n]; Random r = new Random(); for (int i = 0; i < n; i++) { array[i] = r.nextInt() & 0x7FFF0002; } return array; } public static void main(String[] args) { System.out.println(Arrays.toString(randomArray(10))); System.out.println(Arrays.toString(lowSameArray(10))); System.out.println(Arrays.toString(evenArray(10))); } } //[30, 53, 12, 41, 24, 23, 17, 9, 45, 35] //[334561280, 277741568, 2014380034, 388300800, 938082306, 2136473600, 1072037890, 1187053570, 320667648, 700907520] //[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] \"]},{\"header\":\"4、put 与扩容\",\"slug\":\"_4、put-与扩容\",\"contents\":[\"先看下面流程\",\"HashMap 是懒惰创建数组的，首次使用才创建数组\",\"计算索引（桶下标）\",\"如果桶下标还没人占用，创建 Node 占位返回\",\"如果桶下标已经有人占用 \",\"已经是 TreeNode 走红黑树的添加或更新逻辑\",\"是普通 Node，走链表的添加或更新逻辑，如果链表长度超过树化阈值，走树化逻辑\",\"返回前检查容量是否超过阈值，一旦超过进行扩容\",\"public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node<K,V>[] tab; Node<K,V> p; int n, i; // 初始化桶数组 table，table 被延迟到插入新数据时再进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果桶中不包含键值对节点引用，则将新键值对节点的引用存入桶中即可 if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node<K,V> e; K k; // 如果键的值以及节点 hash 等于链表中的第一个键值对节点时，则将 e 指向该键值对 if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; // 如果桶中的引用类型为 TreeNode，则调用红黑树的插入方法 else if (p instanceof TreeNode) e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value); else { // 对链表进行遍历，并统计链表长度 for (int binCount = 0; ; ++binCount) { // 链表中不包含要插入的键值对节点时，则将该节点接在链表的最后 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 如果链表长度大于或等于树化阈值，则进行树化操作 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 条件为 true，表示当前链表包含要插入的键值对，终止遍历 if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; p = e; } } // 判断要插入的键值对是否存在 HashMap 中 if (e != null) { // existing mapping for key V oldValue = e.value; // onlyIfAbsent 表示是否仅在 oldValue 为 null 的情况下更新键值对的值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 键值对数量超过阈值时，则进行扩容 if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } \"]},{\"header\":\"如何扩容的呢？\",\"slug\":\"如何扩容的呢\",\"contents\":[\"变量\",\"含义\",\"oldCap\",\"旧桶数组容量\",\"oldThr\",\"旧阈值\",\"newCap\",\"新的桶数组容量\",\"newThr\",\"新的阈值\",\"threshold\",\"阈值\",\"final Node<K,V>[] resize() { Node<K,V>[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果table不为空 if (oldCap > 0) { //table的长度大于最大容量，则不再记性扩容，返回原table if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 按旧容量和阈值的2倍 计算新容量和阈值的大小 else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY && oldCap >= DEFAULT_INITIAL_CAPACITY) newThr = oldThr << 1; // double threshold } //threshold > 0，且桶数组未被初始化 else if (oldThr > 0) // initial capacity was placed in threshold // this.threshold = tableSizeFor(initialCapacity); //将initialCapacity暂存 newCap = oldThr; else { // zero initial threshold signifies using defaults // 调用无参构造方法时，桶数组容量为默认容量， 阈值为默认容量与默认负载因子乘积 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } //如果新阈值为0，按阈值计算公式进行计算 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\\\"rawtypes\\\",\\\"unchecked\\\"}) // 创建新的桶数组，并初始化容量 Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap]; table = newTab; if (oldTab != null) { //如果旧桶数组不为空，则遍历到新桶数组中 for (int j = 0; j < oldCap; ++j) { Node<K,V> e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) // 重新映射时，需要对红黑树进行拆分 ((TreeNode<K,V>)e).split(this, newTab, j, oldCap); else { // preserve order Node<K,V> loHead = null, loTail = null; Node<K,V> hiHead = null, hiTail = null; Node<K,V> next; // 遍历链表，并将链表节点按原顺序进行分组 do { next = e.next; if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 将分组后的链表映射到新桶中 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } \",\"计算新桶数组的容量 newCap 和新阈值 newThr\",\"根据计算出的 newCap 创建新的桶数组，桶数组 table 也是在这里进行初始化的\",\"将键值对节点重新映射到新的桶数组里。如果节点是 TreeNode 类型，则需要拆分红黑树。如果是普通节点，则节点按原顺序进行分组。\"]},{\"header\":\"加载因子为何默认是 0.75f？\",\"slug\":\"加载因子为何默认是-0-75f\",\"contents\":[\"在空间占用与查询时间之间取得较好的权衡\",\"大于这个值，空间节省了，但链表就会比较长影响性能\",\"小于这个值，冲突减少了，但扩容就会更频繁，空间占用也更多\"]},{\"header\":\"5、并发问题\",\"slug\":\"_5、并发问题\",\"contents\":[\"扩容死链（1.7 会存在）\",\"1.7 源码如下：\",\"void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry<K,V> e : table) { while(null != e) { Entry<K,V> next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } } } \",\"e 和 next 都是局部变量，用来指向当前节点和下一个节点\",\"线程1（绿色）的临时变量 e 和 next 刚引用了这俩节点，还未来得及移动节点，发生了线程切换，由线程2（蓝色）完成扩容和迁移\",\"线程2 扩容完成，由于头插法，链表顺序颠倒。但线程1 的临时变量 e 和 next 还引用了这俩节点，还要再来一遍迁移\",\"第一次循环 \",\"循环接着线程切换前运行，注意此时 e 指向的是节点 a，next 指向的是节点 b\",\"e 头插 a 节点，注意图中画了两份 a 节点，但事实上只有一个（为了不让箭头特别乱画了两份）\",\"当循环结束是 e 会指向 next 也就是 b 节点\",\"第二次循环 \",\"next 指向了节点 a\",\"e 头插节点 b\",\"当循环结束时，e 指向 next 也就是节点 a\",\"第三次循环 \",\"next 指向了 null\",\"e 头插节点 a，a 的 next 指向了 b（之前 a.next 一直是 null），b 的 next 指向 a，死链已成\",\"当循环结束时，e 指向 next 也就是 null，因此第四次循环时会正常退出\",\"数据错乱（1.7，1.8 都会存在）\",\"public class HashMapMissData { public static void main(String[] args) throws InterruptedException { HashMap<String, Object> map = new HashMap<>(); Thread t1 = new Thread(() -> { map.put(\\\"a\\\", new Object()); // 97 => 1 }, \\\"t1\\\"); Thread t2 = new Thread(() -> { map.put(\\\"1\\\", new Object()); // 49 => 1 }, \\\"t2\\\"); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(map); } } \",\"操作：多线程情况下，t1线程在put的时候，此时运行t2的线程，完成put操作，t1 put操作就会丢失数据\"]},{\"header\":\"6、key 的设计\",\"slug\":\"_6、key-的设计\",\"contents\":[\"key 的设计要求\",\"HashMap 的 key 可以为 null，但 Map 的其他实现则不然\",\"作为 key 的对象，必须实现 hashCode 和 equals，并且 key 的内容不能修改（不可变）\",\"key 的 hashCode 应该有良好的散列性\",\"如果 key 可变，例如修改了 age 会导致再次查询时查询不到\",\"public class HashMapMutableKey { public static void main(String[] args) { HashMap<Student, Object> map = new HashMap<>(); Student stu = new Student(\\\"张三\\\", 18); map.put(stu, new Object()); System.out.println(map.get(stu)); stu.age = 19; System.out.println(map.get(stu)); } static class Student { String name; int age; public Student(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return age == student.age && Objects.equals(name, student.name); } @Override public int hashCode() { return Objects.hash(name, age); } } } //java.lang.Object@63961c42 //null \",\"String 对象的 hashCode() 设计\",\"目标是达到较为均匀的散列效果，每个字符串的 hashCode 足够独特\",\"字符串中的每个字符都可以表现为一个数字，称为 Si​，其中 i 的范围是 0 ~ n - 1\",\"散列公式为： S0​∗31(n−1)+S1​∗31(n−2)+…Si​∗31(n−1−i)+…S(n−1)​∗310\",\"31 代入公式有较好的散列特性，并且 31 * h 可以被优化为 \",\"即 $32 ∗h -h $\",\"即 25∗h−h\",\"即 h≪5−h\"]}]},\"/interview/sourceCode/spring/Spring-bean.html\":{\"title\":\"Spring bean生命周期\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"bean 的生命周期从调用 beanFactory 的 getBean 开始，到这个 bean 被销毁，可以总结为以下七个阶段：\",\"处理名称，检查缓存\",\"处理父子容器\",\"处理 dependsOn\",\"选择 scope 策略\",\"创建 bean\",\"类型转换处理\",\"销毁 bean\",\"protected <T> T doGetBean(String name, @Nullable Class<T> requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException { //1.处理bean名称解析 String beanName = this.transformedBeanName(name); Object sharedInstance = this.getSingleton(beanName); Object beanInstance; //从单例缓存中判断是否存在这个示例，可能来源于Spring启动的时候，也有可能来源于Spring 启动时初始化了非懒加载的对象 if (sharedInstance != null && args == null) { if (this.logger.isTraceEnabled()) { if (this.isSingletonCurrentlyInCreation(beanName)) { this.logger.trace(\\\"Returning eagerly cached instance of singleton bean '\\\" + beanName + \\\"' that is not fully initialized yet - a consequence of a circular reference\\\"); } else { this.logger.trace(\\\"Returning cached instance of singleton bean '\\\" + beanName + \\\"'\\\"); } } //如果单例工厂存在这个bean，则通过调用getObject方法获取bean实例 beanInstance = this.getObjectForBeanInstance(sharedInstance, name, beanName, (RootBeanDefinition)null); } else { //如果当前线程存在这个实例，则抛出异常 if (this.isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } //2.获取当前bean工厂的父工厂，然后获取bean，此处是递归查找 BeanFactory parentBeanFactory = this.getParentBeanFactory(); if (parentBeanFactory != null && !this.containsBeanDefinition(beanName)) { String nameToLookup = this.originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory)parentBeanFactory).doGetBean(nameToLookup, requiredType, args, typeCheckOnly); } if (args != null) { //调用父工厂的getBean方法 return parentBeanFactory.getBean(nameToLookup, args); } if (requiredType != null) { //根据类型获取bean return parentBeanFactory.getBean(nameToLookup, requiredType); } return parentBeanFactory.getBean(nameToLookup); } //如果参数为检查类型，则设置名称为beanName的bean实例已经创建的标识 if (!typeCheckOnly) { this.markBeanAsCreated(beanName); } StartupStep beanCreation = this.applicationStartup.start(\\\"spring.beans.instantiate\\\").tag(\\\"beanName\\\", name); try { if (requiredType != null) { beanCreation.tag(\\\"beanType\\\", requiredType::toString); } RootBeanDefinition mbd = this.getMergedLocalBeanDefinition(beanName); this.checkMergedBeanDefinition(mbd, beanName, args); //3.获取bean的依赖属性 String[] dependsOn = mbd.getDependsOn(); String[] var12; if (dependsOn != null) { var12 = dependsOn; int var13 = dependsOn.length; for(int var14 = 0; var14 < var13; ++var14) { String dep = var12[var14]; //检查是否存在循环依赖 if (this.isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \\\"Circular depends-on relationship between '\\\" + beanName + \\\"' and '\\\" + dep + \\\"'\\\"); } //向注册器中注册这个依赖的bean this.registerDependentBean(dep, beanName); try { //获取并解决当前bean this.getBean(dep); } catch (NoSuchBeanDefinitionException var31) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \\\"'\\\" + beanName + \\\"' depends on missing bean '\\\" + dep + \\\"'\\\", var31); } } } //根据 scope 处理当前bean if (mbd.isSingleton()) { sharedInstance = this.getSingleton(beanName, () -> { try { return this.createBean(beanName, mbd, args); } catch (BeansException var5) { this.destroySingleton(beanName); throw var5; } }); beanInstance = this.getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { var12 = null; Object prototypeInstance; try { this.beforePrototypeCreation(beanName); prototypeInstance = this.createBean(beanName, mbd, args); } finally { this.afterPrototypeCreation(beanName); } beanInstance = this.getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) { throw new IllegalStateException(\\\"No scope name defined for bean '\\\" + beanName + \\\"'\\\"); } //5.类型转换 Scope scope = (Scope)this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\\\"No Scope registered for scope name '\\\" + scopeName + \\\"'\\\"); } try { Object scopedInstance = scope.get(beanName, () -> { this.beforePrototypeCreation(beanName); Object var4; try { var4 = this.createBean(beanName, mbd, args); } finally { this.afterPrototypeCreation(beanName); } return var4; }); beanInstance = this.getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException var30) { throw new ScopeNotActiveException(beanName, scopeName, var30); } } } catch (BeansException var32) { beanCreation.tag(\\\"exception\\\", var32.getClass().toString()); beanCreation.tag(\\\"message\\\", String.valueOf(var32.getMessage())); //清楚创建bean this.cleanupAfterBeanCreationFailure(beanName); throw var32; } finally { beanCreation.end(); } } return this.adaptBeanInstance(name, beanInstance, requiredType); } \"]},{\"header\":\"1. 处理名称，检查缓存\",\"slug\":\"_1-处理名称-检查缓存\",\"contents\":[\"protected <T> T doGetBean(String name, @Nullable Class<T> requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException { //1.处理bean名称解析 String beanName = this.transformedBeanName(name); //检查是否存在缓存中，一级二级三级 Object sharedInstance = this.getSingleton(beanName); ...... } \",\"处理名称\",\"这一步会处理别名，将别名解析为实际名称，通过调用transformedBeanName处理bean，然后transformedBeanName通过调用BeanFactoryUtils.transformedBeanName(name)对 FactoryBean 也会特殊处理，如果以 & 开头表示要获取 FactoryBean 本身，否则表示要获取其产品\",\"public abstract class BeanFactoryUtils { //对bean名称进行处理 public static String transformedBeanName(String name) { Assert.notNull(name, \\\"'name' must not be null\\\"); return !name.startsWith(\\\"&\\\") ? name : (String)transformedBeanNameCache.computeIfAbsent(name, (beanName) -> { do { beanName = beanName.substring(\\\"&\\\".length()); } while(beanName.startsWith(\\\"&\\\")); return beanName; }); } } \",\"检查缓存\",\"通过调用getSingleton方法，这里针对单例对象会检查一级、二级、三级缓存\",\"singletonFactories 三级缓存，存放单例工厂对象\",\"earlySingletonObjects 二级缓存，存放单例工厂的产品对象 \",\"如果发生循环依赖，产品是代理；无循环依赖，产品是原始对象\",\"singletonObjects 一级缓存，存放单例成品对象\",\"//singletonObjects 一级缓存，存放单例成品对象 private final Map<String, Object> singletonObjects = new ConcurrentHashMap(256); //singletonFactories 三级缓存，存放单例工厂对象 private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap(16); //earlySingletonObjects 二级缓存，存放单例工厂的产品对象 private final Map<String, Object> earlySingletonObjects = new ConcurrentHashMap(16); @Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null && this.isSingletonCurrentlyInCreation(beanName)) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { synchronized(this.singletonObjects) { singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null) { ObjectFactory<?> singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } } } return singletonObject; } \"]},{\"header\":\"2. 处理父子容器\",\"slug\":\"_2-处理父子容器\",\"contents\":[\"如果当前容器根据名字找不到这个 bean，此时若父容器存在，则执行父容器的 getBean 流程\",\"父子容器的 bean 名称可以重复\"]},{\"header\":\"3. 处理 dependsOn\",\"slug\":\"_3-处理-dependson\",\"contents\":[\"如果当前 bean 有通过 dependsOn 指定了非显式依赖的 bean，这一步会提前创建这些 dependsOn 的 bean\",\"所谓非显式依赖，就是指两个 bean 之间不存在直接依赖关系，但需要控制它们的创建先后顺序\"]},{\"header\":\"4. 选择 scope 策略\",\"slug\":\"_4-选择-scope-策略\",\"contents\":[\"if (mbd.isSingleton()) { //如果是单例类型的bean,通过调用createBean方法尽心创建， sharedInstance = this.getSingleton(beanName, () -> { try { return this.createBean(beanName, mbd, args); } catch (BeansException var5) { //如果创建bean发生异常，则由于当前bean可能被添加在单例池中，调用销毁方法销毁当前bean this.destroySingleton(beanName); throw var5; } }); beanInstance = this.getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { var12 = null; //如果当前bean是多例的，则实时创建 Object prototypeInstance; try { //在threadlocal中设置这个bean的标志位，防止同一个线程重复创建bean造成资源浪费 this.beforePrototypeCreation(beanName); //创建bean实例 prototypeInstance = this.createBean(beanName, mbd, args); } finally { //清除threadlocal标志位 this.afterPrototypeCreation(beanName); } beanInstance = this.getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { //处理bean的作用范围为 request、session的bean String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) { throw new IllegalStateException(\\\"No scope name defined for bean '\\\" + beanName + \\\"'\\\"); } Scope scope = (Scope)this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\\\"No Scope registered for scope name '\\\" + scopeName + \\\"'\\\"); } try { Object scopedInstance = scope.get(beanName, () -> { this.beforePrototypeCreation(beanName); Object var4; try { var4 = this.createBean(beanName, mbd, args); } finally { this.afterPrototypeCreation(beanName); } return var4; }); beanInstance = this.getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException var30) { throw new ScopeNotActiveException(beanName, scopeName, var30); } } \",\"源码部分已经分析完了，接下来我们看看对于Scope的生命周期进行讨论？\",\"前面一节我们讲了Spring refresh的过程，在解析源码的时候发现调用refresh方法会调用getBean方法从而创建bean，通过下面代码测试看看？\",\"public class TestScope { public static void main(String[] args) { testSingletonScope(); System.out.println(\\\"---------------------------------\\\"); testRequestScope(); System.out.println(\\\"---------------------------------\\\"); testPrototypeScope(); } /** * 单例 bean 从 refresh 被创建, 到 close 被销毁, BeanFactory 会记录哪些 bean 要调用销毁方法 */ private static void testSingletonScope() { GenericApplicationContext context = new GenericApplicationContext(); context.registerBean(\\\"bean1\\\", Bean1.class); context.registerBean(CommonAnnotationBeanPostProcessor.class); context.refresh(); // 调用getBean context.close(); // 调用各自的销毁方法 } /** * 多例 bean 从首次 getBean 被创建, 到调用 BeanFactory 的 destroyBean 被销毁 */ private static void testPrototypeScope() { GenericApplicationContext context = new GenericApplicationContext(); context.registerBean(\\\"bean1\\\", Bean1.class, bd -> bd.setScope(\\\"prototype\\\")); context.registerBean(CommonAnnotationBeanPostProcessor.class); context.refresh(); Bean1 bean = context.getBean(Bean1.class); // 没谁记录该 bean 要调用销毁方法, 需要我们自行调用 context.getDefaultListableBeanFactory().destroyBean(bean); context.close(); } /** * request bean 从首次 getBean 被创建, 到 request 结束前被销毁 */ private static void testRequestScope() { GenericApplicationContext context = new GenericApplicationContext(); context.getDefaultListableBeanFactory().registerScope(\\\"request\\\", new RequestScope()); context.registerBean(\\\"bean1\\\", Bean1.class, bd -> bd.setScope(\\\"request\\\")); context.registerBean(CommonAnnotationBeanPostProcessor.class); context.refresh(); for (int i = 0; i < 2; i++) { new Thread(() -> { MockHttpServletRequest request = new MockHttpServletRequest(); // 每个 webRequest 对象会记录哪些 bean 要调用销毁方法 ServletWebRequest webRequest = new ServletWebRequest(request); RequestContextHolder.setRequestAttributes(webRequest); Bean1 bean = context.getBean(Bean1.class); LoggerUtils.get().debug(\\\"{}\\\", bean); LoggerUtils.get().debug(\\\"{}\\\", request.getAttribute(\\\"bean1\\\")); // request 请求结束前调用这些销毁方法 webRequest.requestCompleted(); }).start(); } } static class Bean1 { @PostConstruct public void init() { LoggerUtils.get().debug(\\\"{} - init\\\", this); } @PreDestroy public void destroy() { LoggerUtils.get().debug(\\\"{} - destroy\\\", this); } } } \",\"对于测试单例bean\",\"20:40:25.907 [main] DEBUG org.springframework.context.support.GenericApplicationContext - Refreshing org.springframework.context.support.GenericApplicationContext@5aaa6d82 20:40:25.974 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.CommonAnnotationBeanPostProcessor' 20:40:26.034 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'bean1' 20:40:26.072 [main] DEBUG G - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@6325a3ee - init 20:40:26.131 [main] DEBUG org.springframework.context.support.GenericApplicationContext - Closing org.springframework.context.support.GenericApplicationContext@5aaa6d82, started on Sun Dec 18 20:40:25 CST 2022 20:40:26.132 [main] DEBUG G - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@6325a3ee - destroy \",\"从日志中可以看到，调用refresh方法的时候就会创建bean，而调动close方法的时候，就会调用自身的销毁方法\",\"测试多例bean\",\"20:48:45.742 [main] DEBUG org.springframework.context.support.GenericApplicationContext - Refreshing org.springframework.context.support.GenericApplicationContext@5aaa6d82 20:48:45.804 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.CommonAnnotationBeanPostProcessor' 20:48:45.948 [main] DEBUG G - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@2833cc44 - init 20:48:45.951 [main] DEBUG G - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@2833cc44 - destroy 20:48:45.957 [main] DEBUG org.springframework.context.support.GenericApplicationContext - Closing org.springframework.context.support.GenericApplicationContext@5aaa6d82, started on Sun Dec 18 20:48:45 CST 2022 \",\"多例测试bean，从上面代码中可以看出，我们多调用了一个方法destroyBean，因为没有谁去记录该 bean，要调用销毁方法, 需要我们自行调用\",\"测试request Bean\",\"21:09:41.477 [Thread-0] DEBUG F - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@7390e871 - init 21:09:41.477 [Thread-1] DEBUG A - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@bdd9a7f - init 21:09:41.480 [Thread-1] DEBUG A - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@bdd9a7f 21:09:41.480 [Thread-0] DEBUG F - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@7390e871 21:09:41.480 [Thread-1] DEBUG A - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@bdd9a7f 21:09:41.480 [Thread-0] DEBUG F - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@7390e871 21:09:41.480 [Thread-0] DEBUG F - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@7390e871 - destroy 21:09:41.480 [Thread-1] DEBUG A - com.xiaobear.sourcecode.framework.spring.bean.TestScope$Bean1@bdd9a7f - destroy \",\"对于request bean，我们模拟了多线程同时访问，请求结束时，需要调用requestCompleted方法来销毁bean\",\"总结：\",\"对于 singleton scope，首先到单例池去获取 bean，如果有则直接返回，没有再进入创建流程\",\"对于 prototype scope表示不缓存bean，每次都会进入创建流程\",\"对于自定义 scope，例如 request，首先到 request 域获取 bean，如果有则直接返回，没有再进入创建流程\"]},{\"header\":\"5、创建 bean\",\"slug\":\"_5、创建-bean\",\"contents\":[\"protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Creating instance of bean '\\\" + beanName + \\\"'\\\"); } RootBeanDefinition mbdToUse = mbd; //从bean的描述信息中解析出bean的class,为创建实例做准备，并复制一个新的RootBeanDefinition对象来使用，防止多线程篡改原来的对象 Class<?> resolvedClass = this.resolveBeanClass(mbd, beanName, new Class[0]); if (resolvedClass != null && !mbd.hasBeanClass() && mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } //为防止重写 try { mbdToUse.prepareMethodOverrides(); } catch (BeanDefinitionValidationException var9) { throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, \\\"Validation of method overrides failed\\\", var9); } Object beanInstance; try { //获取并调用bean的后置处理器的子接口 beanInstance = this.resolveBeforeInstantiation(beanName, mbdToUse); //如果bean实例不为空，说明后置处理器实例化了当前bean，则不再创建bean，也说明了bean的创建不全是反射实现的，后置处理器也可 if (beanInstance != null) { return beanInstance; } } catch (Throwable var10) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \\\"BeanPostProcessor before instantiation of bean failed\\\", var10); } try { //正式创建bean beanInstance = this.doCreateBean(beanName, mbdToUse, args); if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Finished creating instance of bean '\\\" + beanName + \\\"'\\\"); } return beanInstance; } catch (ImplicitlyAppearedSingletonException | BeanCreationException var7) { throw var7; } catch (Throwable var8) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \\\"Unexpected exception during bean creation\\\", var8); } } /** *bean的创建 **/ protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { BeanWrapper instanceWrapper = null; //如果bean是单例的，并且在单例Map（用来存储单例bean的集合）已经存在，则不再实例化bean，若不存在，则实例化后添加到map中 if (mbd.isSingleton()) { instanceWrapper = (BeanWrapper)this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { //创建bean实例 instanceWrapper = this.createBeanInstance(beanName, mbd, args); } Object bean = instanceWrapper.getWrappedInstance(); Class<?> beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) { mbd.resolvedTargetType = beanType; } //调用这个bean的merge bean的后置处理器方法，例如检查自动注入时的成员变量 synchronized(mbd.postProcessingLock) { if (!mbd.postProcessed) { try { this.applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable var17) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \\\"Post-processing of merged bean definition failed\\\", var17); } mbd.postProcessed = true; } } //创建该单例bean实例，并解决循环依赖问题 boolean earlySingletonExposure = mbd.isSingleton() && this.allowCircularReferences && this.isSingletonCurrentlyInCreation(beanName); if (earlySingletonExposure) { if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Eagerly caching bean '\\\" + beanName + \\\"' to allow for resolving potential circular references\\\"); } this.addSingletonFactory(beanName, () -> { return this.getEarlyBeanReference(beanName, mbd, bean); }); } Object exposedObject = bean; try { //处理bean的相关属性和注入 this.populateBean(beanName, mbd, instanceWrapper); //初始化当前bean exposedObject = this.initializeBean(beanName, exposedObject, mbd); } catch (Throwable var18) { if (var18 instanceof BeanCreationException && beanName.equals(((BeanCreationException)var18).getBeanName())) { throw (BeanCreationException)var18; } throw new BeanCreationException(mbd.getResourceDescription(), beanName, \\\"Initialization of bean failed\\\", var18); } //为bean的循环依赖的处理及提早注册实现的部分 if (earlySingletonExposure) { Object earlySingletonReference = this.getSingleton(beanName, false); if (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } else if (!this.allowRawInjectionDespiteWrapping && this.hasDependentBean(beanName)) { String[] dependentBeans = this.getDependentBeans(beanName); Set<String> actualDependentBeans = new LinkedHashSet(dependentBeans.length); String[] var12 = dependentBeans; int var13 = dependentBeans.length; for(int var14 = 0; var14 < var13; ++var14) { String dependentBean = var12[var14]; if (!this.removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \\\"Bean with name '\\\" + beanName + \\\"' has been injected into other beans [\\\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \\\"] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using 'getBeanNamesForType' with the 'allowEagerInit' flag turned off, for example.\\\"); } } } } //深入源码发现，当前bean的生命周期不是多例，需要spring来管理bean的生命周期，此时会把bean的destory方法注册到spring上下文中，当spring启动异常时，则会调用销毁回调方法，处理已经生成的bean,来释放bean占用的资源 try { this.registerDisposableBeanIfNecessary(beanName, bean, mbd); return exposedObject; } catch (BeanDefinitionValidationException var16) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \\\"Invalid destruction signature\\\", var16); } } \",\"创建阶段-> 依赖注入阶段 -> 初始化阶段 -> 注册可销毁 bean\"]},{\"header\":\"创建 bean 实例\",\"slug\":\"创建-bean-实例\",\"contents\":[\"要点\",\"总结\",\"有自定义 TargetSource 的情况\",\"由 AnnotationAwareAspectJAutoProxyCreator 创建代理返回\",\"Supplier 方式创建 bean 实例\",\"为 Spring 5.0 新增功能，方便编程方式创建 bean 实例\",\"FactoryMethod 方式 创建 bean 实例\",\"① 分成静态工厂与实例工厂；② 工厂方法若有参数，需要对工厂方法参数进行解析，利用 resolveDependency；③ 如果有多个工厂方法候选者，还要进一步按权重筛选\",\"AutowiredAnnotationBeanPostProcessor\",\"① 优先选择带 @Autowired 注解的构造；② 若有唯一的带参构造，也会入选\",\"mbd.getPreferredConstructors\",\"选择所有公共构造，这些构造之间按权重筛选\",\"采用默认构造\",\"如果上面的后处理器和 BeanDefiniation 都没找到构造，采用默认构造，即使是私有的\"]},{\"header\":\"创建 bean - 依赖注入\",\"slug\":\"创建-bean-依赖注入\",\"contents\":[\"要点\",\"总结\",\"AutowiredAnnotationBeanPostProcessor(注解匹配)\",\"识别 @Autowired 及 @Value 标注的成员，封装为 InjectionMetadata 进行依赖注入\",\"CommonAnnotationBeanPostProcessor(注解匹配)\",\"识别 @Resource 标注的成员，封装为 InjectionMetadata 进行依赖注入\",\"resolveDependency\",\"用来查找要装配的值，可以识别：① Optional；② ObjectFactory 及 ObjectProvider；③ @Lazy 注解；④ @Value 注解（${ }, #{ }, 类型转换）；⑤ 集合类型（Collection，Map，数组等）；⑥ 泛型和 @Qualifier（用来区分类型歧义）；⑦ primary 及名字匹配（用来区分类型歧义）\",\"AUTOWIRE_BY_NAME（根据名字匹配）\",\"根据成员名字找 bean 对象，修改 mbd 的 propertyValues，不会考虑简单类型的成员\",\"AUTOWIRE_BY_TYPE（根据类型匹配）\",\"根据成员类型执行 resolveDependency 找到依赖注入的值，修改 mbd 的 propertyValues\",\"applyPropertyValues（即xml）\",\"根据 mbd 的 propertyValues 进行依赖注入（即xml中 `<property name ref\"]},{\"header\":\"创建 bean - 初始化\",\"slug\":\"创建-bean-初始化\",\"contents\":[\"protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) { //初始化bean，先调用beanAware接口注入beanName，classLoader和beanFactory,为后续初始化数据做准备 if (System.getSecurityManager() != null) { AccessController.doPrivileged(() -> { this.invokeAwareMethods(beanName, bean); return null; }, this.getAccessControlContext()); } else { this.invokeAwareMethods(beanName, bean); } //调用bean的初始化后置处理器 Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { wrappedBean = this.applyBeanPostProcessorsBeforeInitialization(bean, beanName); } //调用初始化方法 try { this.invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable var6) { throw new BeanCreationException(mbd != null ? mbd.getResourceDescription() : null, beanName, \\\"Invocation of init method failed\\\", var6); } if (mbd == null || !mbd.isSynthetic()) { wrappedBean = this.applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } \",\"要点\",\"总结\",\"内置 Aware 接口的装配\",\"包括 BeanNameAware，BeanFactoryAware 等\",\"扩展 Aware 接口的装配\",\"由 ApplicationContextAwareProcessor 解析，执行时机在 postProcessBeforeInitialization\",\"@PostConstruct\",\"由 CommonAnnotationBeanPostProcessor 解析，执行时机在 postProcessBeforeInitialization\",\"InitializingBean\",\"通过接口回调执行初始化\",\"initMethod\",\"根据 BeanDefinition 得到的初始化方法执行初始化，即 <bean init-method> 或 @Bean(initMethod)\",\"创建 aop 代理\",\"由 AnnotationAwareAspectJAutoProxyCreator 创建，执行时机在 postProcessAfterInitialization\"]},{\"header\":\"创建 bean - 注册可销毁 bean\",\"slug\":\"创建-bean-注册可销毁-bean\",\"contents\":[\"在这一步判断并登记可销毁 bean\",\"判断依据 \",\"如果实现了 DisposableBean 或 AutoCloseable 接口，则为可销毁 bean\",\"如果自定义了 destroyMethod，则为可销毁 bean\",\"如果采用 @Bean 没有指定 destroyMethod，则采用自动推断方式获取销毁方法名（close，shutdown）\",\"如果有 @PreDestroy 标注的方法\",\"存储位置 \",\"singleton scope 的可销毁 bean 会存储于 beanFactory 的成员当中\",\"自定义 scope 的可销毁 bean 会存储于对应的域对象当中\",\"prototype scope 不会存储，需要自己找到此对象销毁\",\"存储时都会封装为 DisposableBeanAdapter 类型对销毁方法的调用进行适配\"]},{\"header\":\"6. 类型转换处理\",\"slug\":\"_6-类型转换处理\",\"contents\":[\"如果 getBean 的 requiredType 参数与实际得到的对象类型不同，会尝试进行类型转换\"]},{\"header\":\"7. 销毁 bean\",\"slug\":\"_7-销毁-bean\",\"contents\":[\"销毁时机 \",\"singleton bean 的销毁在 ApplicationContext.close 时，此时会找到所有 DisposableBean 的名字，逐一销毁\",\"自定义 scope bean 的销毁在作用域对象生命周期结束时\",\"prototype bean 的销毁可以通过自己手动调用 AutowireCapableBeanFactory.destroyBean 方法执行销毁\",\"同一 bean 中不同形式销毁方法的调用次序 \",\"优先后处理器销毁，即 @PreDestroy\",\"其次 DisposableBean 接口销毁\",\"最后 destroyMethod 销毁（包括自定义名称，推断名称，AutoCloseable 接口 多选一）\"]}]},\"/interview/sourceCode/spring/Spring-refresh.html\":{\"title\":\"Spring refresh过程\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"refresh 是 AbstractApplicationContext 中的核心方法，负责初始化 ApplicationContext 容器，容器必须调用 refresh 才能正常工作。\",\"public void refresh() throws BeansException, IllegalStateException { //使用互斥锁，防止启动、关闭及注册函调函数的重复调用，保证上下文对象状态 synchronized(this.startupShutdownMonitor) { StartupStep contextRefresh = this.applicationStartup.start(\\\"spring.context.refresh\\\"); //1.提前准备启动参数，预处理 this.prepareRefresh(); //2.获取BeanFactory；默认实现是DefaultListableBeanFactory，在创建容器的时候创建的 ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory(); //3.初始化和设置BeanFactory的参数 this.prepareBeanFactory(beanFactory); try { //4.调用BeanFactory的后置处理器， this.postProcessBeanFactory(beanFactory); StartupStep beanPostProcess = this.applicationStartup.start(\\\"spring.context.beans.post-process\\\"); //5.执行BeanFactoryPostProcessor的方法； this.invokeBeanFactoryPostProcessors(beanFactory); //6.在bean工厂中注册bean的后置处理器，bean的代理的生成也是由它实现的 this.registerBeanPostProcessors(beanFactory); beanPostProcess.end(); //7.初始化消息源 this.initMessageSource(); //8.初始化消息推送器 this.initApplicationEventMulticaster(); //9.调用子类重写当前方法，是子类实现的扩展 this.onRefresh(); //10.注册应用的监听器。就是注册实现了ApplicationListener接口的监听器bean，这些监听器是注册到ApplicationEventMulticaster中的 this.registerListeners(); //11.把所有非延迟加载的bean初始化并设置冻结标志位 this.finishBeanFactoryInitialization(beanFactory); //12. this.finishRefresh(); } catch (BeansException var10) { if (this.logger.isWarnEnabled()) { this.logger.warn(\\\"Exception encountered during context initialization - cancelling refresh attempt: \\\" + var10); } //销毁已经存在的bean this.destroyBeans(); //释放标志位 this.cancelRefresh(var10); throw var10; } finally { //清除与反射相关的缓存，例如反射的方法、字段、类型解析已经类加载等 this.resetCommonCaches(); contextRefresh.end(); } } } \",\"它的内部主要会调用 12 个方法，我们把它们称为 refresh 的 12 个步骤：\",\"prepareRefresh\",\"obtainFreshBeanFactory\",\"prepareBeanFactory\",\"postProcessBeanFactory\",\"invokeBeanFactoryPostProcessors\",\"registerBeanPostProcessors\",\"initMessageSource\",\"initApplicationEventMulticaster\",\"onRefresh\",\"registerListeners\",\"finishBeanFactoryInitialization\",\"finishRefresh\",\"功能分类\",\"1 为准备环境\",\"2 3 4 5 6 为准备 BeanFactory\",\"7 8 9 10 12 为准备 ApplicationContext\",\"11 为初始化 BeanFactory 中非延迟单例 bean\"]},{\"header\":\"1. prepareRefresh\",\"slug\":\"_1-preparerefresh\",\"contents\":[\"protected void prepareRefresh() { //设置启动时间 this.startupDate = System.currentTimeMillis(); //撤销关闭状态 this.closed.set(false); //开启活跃状态 this.active.set(true); if (this.logger.isDebugEnabled()) { if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Refreshing \\\" + this); } else { this.logger.debug(\\\"Refreshing \\\" + this.getDisplayName()); } } //初始化上下文信息 this.initPropertySources(); //验证属性 this.getEnvironment().validateRequiredProperties(); if (this.earlyApplicationListeners == null) { this.earlyApplicationListeners = new LinkedHashSet(this.applicationListeners); } else { this.applicationListeners.clear(); this.applicationListeners.addAll(this.earlyApplicationListeners); } this.earlyApplicationEvents = new LinkedHashSet(); } \",\"这一步创建和准备了 Environment 对象，它作为 ApplicationContext 的一个成员变量\",\"Environment 对象的作用之一是为后续 @Value，值注入时提供键值\",\"Environment 分成三个主要部分\",\"systemProperties - 保存 java 环境键值\",\"systemEnvironment - 保存系统环境键值\",\"自定义 PropertySource - 保存自定义键值，例如来自于 *.properties 文件的键值\"]},{\"header\":\"2. obtainFreshBeanFactory\",\"slug\":\"_2-obtainfreshbeanfactory\",\"contents\":[\"protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { //刷新beanFactory this.refreshBeanFactory(); return this.getBeanFactory(); } \",\"这一步获取（或创建） BeanFactory，它也是作为 ApplicationContext 的一个成员变量\",\"BeanFactory 的作用是负责 bean 的创建、依赖注入和初始化，bean 的各项特征由 BeanDefinition 定义 \",\"BeanDefinition 作为 bean 的设计蓝图，规定了 bean 的特征，如单例多例、依赖关系、初始销毁方法等\",\"BeanDefinition 的来源有多种多样，可以是通过 xml 获得、配置类获得、组件扫描获得，也可以是编程添加\",\"所有的 BeanDefinition 会存入 BeanFactory 中的 beanDefinitionMap 集合\"]},{\"header\":\"3. prepareBeanFactory\",\"slug\":\"_3-preparebeanfactory\",\"contents\":[\"protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { //设置类加载器 beanFactory.setBeanClassLoader(this.getClassLoader()); if (!shouldIgnoreSpel) { //设置 支持解析 SpEL的解析器 beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); } beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, this.getEnvironment())); //设置添加一个 tApplicationContextAwareProcessor 后置处理器 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); //设置忽略的自动装配的接口,就是设置这些接口的实现类不能通过这些接口实现自动注入 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); beanFactory.ignoreDependencyInterface(ApplicationStartupAware.class); //注册可以解析的自动装配,假设想要使用@Autowired 注解将Spring提供的 BeanFactory beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); //设置添加一个ApplicationListenerDetector后置处理器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); //添加编译时支持AspectJ if (!NativeDetector.inNativeImage() && beanFactory.containsBean(\\\"loadTimeWeaver\\\")) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } if (!beanFactory.containsLocalBean(\\\"environment\\\")) { //注册单例bean environment,存储默认的属性 beanFactory.registerSingleton(\\\"environment\\\", this.getEnvironment()); } if (!beanFactory.containsLocalBean(\\\"systemProperties\\\")) { //注册单例bean systemProperties,存储系统属性 beanFactory.registerSingleton(\\\"systemProperties\\\", this.getEnvironment().getSystemProperties()); } if (!beanFactory.containsLocalBean(\\\"systemEnvironment\\\")) { //注册单例bean systemEnvironment,存储系统环境信息 beanFactory.registerSingleton(\\\"systemEnvironment\\\", this.getEnvironment().getSystemEnvironment()); } if (!beanFactory.containsLocalBean(\\\"applicationStartup\\\")) { //注册单例bean applicationStartup,系统启动 beanFactory.registerSingleton(\\\"applicationStartup\\\", this.getApplicationStartup()); } } \",\"这一步会进一步完善 BeanFactory，为它的各项成员变量赋值\",\"beanExpressionResolver 用来解析 SpEL，常见实现为 StandardBeanExpressionResolver\",\"propertyEditorRegistrars 会注册类型转换器 \",\"它在这里使用了 ResourceEditorRegistrar 实现类\",\"并应用 ApplicationContext 提供的 Environment 完成 ${ } 解析\",\"registerResolvableDependency 来注册 beanFactory 以及 ApplicationContext，让它们也能用于依赖注入\",\"beanPostProcessors 是 bean 后处理器集合，会工作在 bean 的生命周期各个阶段，此处会添加两个： \",\"ApplicationContextAwareProcessor 用来解析 Aware 接口\",\"ApplicationListenerDetector 用来识别容器中 ApplicationListener 类型的 bean\"]},{\"header\":\"4. postProcessBeanFactory\",\"slug\":\"_4-postprocessbeanfactory\",\"contents\":[\"protected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) { } \",\"这一步是空实现，留给子类扩展。 \",\"一般 Web 环境的 ApplicationContext 都要利用它注册新的 Scope，完善 Web 下的 BeanFactory\",\"这里体现的是模板方法设计模式\"]},{\"header\":\"5. invokeBeanFactoryPostProcessors\",\"slug\":\"_5-invokebeanfactorypostprocessors\",\"contents\":[\"protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) { //执行beanFactory后置处理器中的方法,该方法中获取所有BeanFactoryPostProcessor,遍历判断 //对不同的BeanFactoryPostProcessor进行排序,因为先后执行的顺序不同, //PriorityOrdered>BeanDefinitionRegistryPostProcessor>BeanFactoryPostProcessor //然后执行后置处理器中定义的初始化 beanFactory 后要执行的方法 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, this.getBeanFactoryPostProcessors()); if (!NativeDetector.inNativeImage() && beanFactory.getTempClassLoader() == null && beanFactory.containsBean(\\\"loadTimeWeaver\\\")) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } \",\"这一步会调用 beanFactory 后处理器\",\"beanFactory 后处理器，充当 beanFactory 的扩展点，可以用来补充或修改 BeanDefinition\",\"常见的 beanFactory 后处理器有 \",\"ConfigurationClassPostProcessor – 解析 @Configuration、@Bean、@Import、@PropertySource 等\",\"PropertySourcesPlaceHolderConfigurer – 替换 BeanDefinition 中的 $\",\"MapperScannerConfigurer – 补充 Mapper 接口对应的 BeanDefinition\"]},{\"header\":\"6. registerBeanPostProcessors\",\"slug\":\"_6-registerbeanpostprocessors\",\"contents\":[\"protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) { PostProcessorRegistrationDelegate.registerBeanPostProcessors(beanFactory, this); } \",\"这一步是继续从 beanFactory 中找出 bean 后处理器，添加至 beanPostProcessors 集合中\",\"bean 后处理器，充当 bean 的扩展点，可以工作在 bean 的实例化、依赖注入、初始化阶段，常见的有： \",\"AutowiredAnnotationBeanPostProcessor 功能有：解析 @Autowired，@Value 注解\",\"CommonAnnotationBeanPostProcessor 功能有：解析 @Resource，@PostConstruct，@PreDestroy\",\"AnnotationAwareAspectJAutoProxyCreator 功能有：为符合切点的目标 bean 自动创建代理\"]},{\"header\":\"7. initMessageSource\",\"slug\":\"_7-initmessagesource\",\"contents\":[\"protected void initMessageSource() { //获取beanFactory ConfigurableListableBeanFactory beanFactory = this.getBeanFactory(); //是否包含‘messageSource’的bean if (beanFactory.containsLocalBean(\\\"messageSource\\\")) { //获取这个bean并进行赋值 this.messageSource = (MessageSource)beanFactory.getBean(\\\"messageSource\\\", MessageSource.class); if (this.parent != null && this.messageSource instanceof HierarchicalMessageSource) { HierarchicalMessageSource hms = (HierarchicalMessageSource)this.messageSource; if (hms.getParentMessageSource() == null) { hms.setParentMessageSource(this.getInternalParentMessageSource()); } } if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Using MessageSource [\\\" + this.messageSource + \\\"]\\\"); } } else { //没有就创建一个 DelegatingMessageSource dms = new DelegatingMessageSource(); // 注册到容器中(以后获取国际化配置文件的相关信息,可以通过@Autowired在Spring dms.setParentMessageSource(this.getInternalParentMessageSource()); this.messageSource = dms; beanFactory.registerSingleton(\\\"messageSource\\\", this.messageSource); if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"No 'messageSource' bean, using [\\\" + this.messageSource + \\\"]\\\"); } } } \",\"这一步是为 ApplicationContext 添加 messageSource 成员，实现国际化功能\",\"去 beanFactory 内找名为 messageSource 的 bean，如果没有，则提供空的 MessageSource 实现\"]},{\"header\":\"8. initApplicationContextEventMulticaster\",\"slug\":\"_8-initapplicationcontexteventmulticaster\",\"contents\":[\"protected void initApplicationEventMulticaster() { //获取beanFactory ConfigurableListableBeanFactory beanFactory = this.getBeanFactory(); if (beanFactory.containsLocalBean(\\\"applicationEventMulticaster\\\")) { this.applicationEventMulticaster = (ApplicationEventMulticaster)beanFactory.getBean(\\\"applicationEventMulticaster\\\", ApplicationEventMulticaster.class); if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"Using ApplicationEventMulticaster [\\\" + this.applicationEventMulticaster + \\\"]\\\"); } } else { //没有就创建一个 this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); //注册到bean容器中 beanFactory.registerSingleton(\\\"applicationEventMulticaster\\\", this.applicationEventMulticaster); if (this.logger.isTraceEnabled()) { this.logger.trace(\\\"No 'applicationEventMulticaster' bean, using [\\\" + this.applicationEventMulticaster.getClass().getSimpleName() + \\\"]\\\"); } } } \",\"这一步为 ApplicationContext 添加事件广播器成员，即 applicationContextEventMulticaster\",\"它的作用是发布事件给监听器\",\"去 beanFactory 找名为 applicationEventMulticaster 的 bean 作为事件广播器，若没有，会创建默认的事件广播器\",\"之后就可以调用 ApplicationContext.publishEvent(事件对象) 来发布事件\"]},{\"header\":\"9. onRefresh\",\"slug\":\"_9-onrefresh\",\"contents\":[\"protected void onRefresh() throws BeansException { } \",\"这一步是空实现，留给子类扩展\",\"SpringBoot 中的子类在这里准备了 WebServer，即内嵌 web 容器\",\"体现的是模板方法设计模式\"]},{\"header\":\"10. registerListeners\",\"slug\":\"_10-registerlisteners\",\"contents\":[\"protected void registerListeners() { //获取容器中的监听器 Iterator var1 = this.getApplicationListeners().iterator(); while(var1.hasNext()) { ApplicationListener<?> listener = (ApplicationListener)var1.next(); //添加至事件applicationEventMulticaster this.getApplicationEventMulticaster().addApplicationListener(listener); } //获取没有创建的监听器 String[] listenerBeanNames = this.getBeanNamesForType(ApplicationListener.class, true, false); String[] var7 = listenerBeanNames; int var3 = listenerBeanNames.length; //添加至事件applicationEventMulticaster for(int var4 = 0; var4 < var3; ++var4) { String listenerBeanName = var7[var4]; this.getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); } //获取早期设置的事件(派发之前的事件) Set<ApplicationEvent> earlyEventsToProcess = this.earlyApplicationEvents; this.earlyApplicationEvents = null; if (!CollectionUtils.isEmpty(earlyEventsToProcess)) { Iterator var9 = earlyEventsToProcess.iterator(); while(var9.hasNext()) { ApplicationEvent earlyEvent = (ApplicationEvent)var9.next(); //将早期的事件派发出去 this.getApplicationEventMulticaster().multicastEvent(earlyEvent); } } } \",\"这一步会从多种途径找到事件监听器，并添加至 applicationEventMulticaster\",\"事件监听器顾名思义，用来接收事件广播器发布的事件，有如下来源 \",\"事先编程添加的\",\"来自容器中的 bean\",\"来自于 @EventListener 的解析\",\"要实现事件监听器，只需要实现 ApplicationListener 接口，重写其中 onApplicationEvent(E e) 方法即可\"]},{\"header\":\"11. finishBeanFactoryInitialization\",\"slug\":\"_11-finishbeanfactoryinitialization\",\"contents\":[\"protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { //如果beanFactory包含“conversionService” if (beanFactory.containsBean(\\\"conversionService\\\") && beanFactory.isTypeMatch(\\\"conversionService\\\", ConversionService.class)) { beanFactory.setConversionService((ConversionService)beanFactory.getBean(\\\"conversionService\\\", ConversionService.class)); } //如果beanFactory之前没有注册嵌入值解析器，则注册默认的嵌入值解析器：主要用于注解属性值的解析 if (!beanFactory.hasEmbeddedValueResolver()) { beanFactory.addEmbeddedValueResolver((strVal) -> { return this.getEnvironment().resolvePlaceholders(strVal); }); } //初始化LoadTimeWeaverAware Bean实例对象 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); String[] var3 = weaverAwareNames; int var4 = weaverAwareNames.length; for(int var5 = 0; var5 < var4; ++var5) { String weaverAwareName = var3[var5]; this.getBean(weaverAwareName); } beanFactory.setTempClassLoader((ClassLoader)null); //冻结所有bean定义，注册的bean定义不会被修改或进一步后处理，因为马上要创建 Bean 实例对象了 beanFactory.freezeConfiguration(); //实例化所有剩余（非懒加载）单例对象 beanFactory.preInstantiateSingletons(); } \",\"这一步会将 beanFactory 的成员补充完毕，并初始化所有非延迟单例 bean\",\"conversionService 也是一套转换机制，作为对 PropertyEditor 的补充\",\"embeddedValueResolvers 即内嵌值解析器，用来解析 @Value 中的 ${ }，借用的是 Environment 的功能\",\"singletonObjects 即单例池，缓存所有单例对象 \",\"对象的创建都分三个阶段，每一阶段都有不同的 bean 后处理器参与进来，扩展功能\"]},{\"header\":\"12. finishRefresh\",\"slug\":\"_12-finishrefresh\",\"contents\":[\"protected void finishRefresh() { //清除资源缓存 this.clearResourceCaches(); //初始化跟生命周期有关的后置处理器,在容器中获取LifecycleProcessor this.initLifecycleProcessor(); //获取到生命周期后，调用onRefresh容器刷新完成方法 this.getLifecycleProcessor().onRefresh(); //刷新事件 this.publishEvent((ApplicationEvent)(new ContextRefreshedEvent(this))); if (!NativeDetector.inNativeImage()) { LiveBeansView.registerApplicationContext(this); } } \",\"这一步会为 ApplicationContext 添加 lifecycleProcessor 成员，用来控制容器内需要生命周期管理的 bean\",\"如果容器中有名称为 lifecycleProcessor 的 bean 就用它，否则创建默认的生命周期管理器\",\"准备好生命周期管理器，就可以实现 \",\"调用 context 的 start，即可触发所有实现 LifeCycle 接口 bean 的 start\",\"调用 context 的 stop，即可触发所有实现 LifeCycle 接口 bean 的 stop\",\"发布 ContextRefreshed 事件，整个 refresh 执行完成\"]},{\"header\":\"问题回答\",\"slug\":\"问题回答\",\"contents\":[\"你知道 @Value(\\\"${key}\\\") 注入的值是从哪儿来的吗，谁去解析 ${ } ？\",\"是 ApplicationContext 完成了 bean 的创建和依赖组装吗？\",\"ApplicationContext 与 BeanFactory 之间的关系是什么？\",\"国际化属于谁的功能\",\"SpEL 谁来解析\",\"类型谁来转换\",\"为什么实现了 XxxAware 接口，就能自动装配 Xxx 类型\",\"@Bean 和 <bean> 是等价的吗？\",\"@Autowired，@Resource 它们由谁来解析，原始的 BeanFactory 提供了这些解析功能吗\",\"Spring 中的事件驱动开发是怎么回事，谁来发事件，谁来收事件\",\"LifeCycle 生命周期和 Spring 的生命周期是一回事吗？\"]}]},\"/interview/sourceCode/spring/Spring-tx.html\":{\"title\":\"Spring事务失效的8个场景\",\"contents\":[{\"header\":\"1. 抛出检查异常导致事务不能正确回滚\",\"slug\":\"_1-抛出检查异常导致事务不能正确回滚\",\"contents\":[\"@Service public class Service1 { @Autowired private AccountMapper accountMapper; @Transactional public void transfer(int from, int to, int amount) throws FileNotFoundException { int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); new FileInputStream(\\\"aaa\\\"); accountMapper.update(to, amount); } } } \",\"原因：Spring 默认只会回滚非检查异常\",\"解法：配置 rollbackFor 属性\",\"@Transactional(rollbackFor = Exception.class)\"]},{\"header\":\"2. 业务方法内自己 try-catch 异常导致事务不能正确回滚\",\"slug\":\"_2-业务方法内自己-try-catch-异常导致事务不能正确回滚\",\"contents\":[\"@Service public class Service2 { @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) { try { int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); new FileInputStream(\\\"aaa\\\"); accountMapper.update(to, amount); } } catch (FileNotFoundException e) { e.printStackTrace(); } } } \",\"原因：事务通知只有捉到了目标抛出的异常，才能进行后续的回滚处理，如果目标自己处理掉异常，事务通知无法知悉\",\"解法1：异常原样抛出\",\"在 catch 块添加 throw new RuntimeException(e);\",\"解法2：手动设置 TransactionStatus.setRollbackOnly()\",\"在 catch 块添加 TransactionInterceptor.currentTransactionStatus().setRollbackOnly();\"]},{\"header\":\"3. aop 切面顺序导致导致事务不能正确回滚\",\"slug\":\"_3-aop-切面顺序导致导致事务不能正确回滚\",\"contents\":[\"@Service public class Service3 { @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) throws FileNotFoundException { int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); new FileInputStream(\\\"aaa\\\"); accountMapper.update(to, amount); } } } \",\"@Aspect public class MyAspect { @Around(\\\"execution(* transfer(..))\\\") public Object around(ProceedingJoinPoint pjp) throws Throwable { LoggerUtils.get().debug(\\\"log:{}\\\", pjp.getTarget()); try { return pjp.proceed(); } catch (Throwable e) { e.printStackTrace(); return null; } } } \",\"原因：事务切面优先级最低，但如果自定义的切面优先级和他一样，则还是自定义切面在内层，这时若自定义切面没有正确抛出异常…\",\"解法1、2：同情况2 中的解法:1、2\",\"解法3：调整切面顺序，在 MyAspect 上添加 @Order(Ordered.LOWEST_PRECEDENCE - 1) （不推荐）\"]},{\"header\":\"4. 非 public 方法导致的事务失效\",\"slug\":\"_4-非-public-方法导致的事务失效\",\"contents\":[\"@Service public class Service4 { @Autowired private AccountMapper accountMapper; @Transactional void transfer(int from, int to, int amount) throws FileNotFoundException { int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); } } } \",\"原因：Spring 为方法创建代理、添加事务通知、前提条件都是该方法是 public 的\",\"解法1：改为 public 方法\",\"解法2：添加 bean 配置如下（不推荐）\",\"@Bean public TransactionAttributeSource transactionAttributeSource() { return new AnnotationTransactionAttributeSource(false); } \"]},{\"header\":\"5. 父子容器导致的事务失效\",\"slug\":\"_5-父子容器导致的事务失效\",\"contents\":[\"package day04.tx.app.service; // ... @Service public class Service5 { @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) throws FileNotFoundException { int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); } } } \",\"控制器类\",\"package day04.tx.app.controller; // ... @Controller public class AccountController { @Autowired public Service5 service; public void transfer(int from, int to, int amount) throws FileNotFoundException { service.transfer(from, to, amount); } } \",\"App 配置类\",\"@Configuration @ComponentScan(\\\"day04.tx.app.service\\\") @EnableTransactionManagement // ... public class AppConfig { // ... 有事务相关配置 } \",\"Web 配置类\",\"@Configuration @ComponentScan(\\\"day04.tx.app\\\") // ... public class WebConfig { // ... 无事务配置 } \",\"现在配置了父子容器，WebConfig 对应子容器，AppConfig 对应父容器，发现事务依然失效\",\"原因：子容器扫描范围过大，把未加事务配置的 service 扫描进来\",\"解法1：各扫描各的，不要图简便\",\"解法2：不要用父子容器，所有 bean 放在同一容器\"]},{\"header\":\"6. 调用本类方法导致传播行为失效\",\"slug\":\"_6-调用本类方法导致传播行为失效\",\"contents\":[\"@Service public class Service6 { @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException { LoggerUtils.get().debug(\\\"foo\\\"); bar(); } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException { LoggerUtils.get().debug(\\\"bar\\\"); } } \",\"原因：本类方法调用不经过代理，因此无法增强\",\"解法1：依赖注入自己（代理）来调用\",\"解法2：通过 AopContext 拿到代理对象，来调用\",\"解法3：通过 CTW，LTW 实现功能增强\",\"解法1\",\"@Service public class Service6 { @Autowired private Service6 proxy; // 本质上是一种循环依赖 @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException { LoggerUtils.get().debug(\\\"foo\\\"); System.out.println(proxy.getClass()); proxy.bar(); } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException { LoggerUtils.get().debug(\\\"bar\\\"); } } \",\"解法2，还需要在 AppConfig 上添加 @EnableAspectJAutoProxy(exposeProxy = true)\",\"@Service public class Service6 { @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException { LoggerUtils.get().debug(\\\"foo\\\"); ((Service6) AopContext.currentProxy()).bar(); } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException { LoggerUtils.get().debug(\\\"bar\\\"); } } \"]},{\"header\":\"7. @Transactional 没有保证原子行为\",\"slug\":\"_7-transactional-没有保证原子行为\",\"contents\":[\"@Service public class Service7 { private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) { int fromBalance = accountMapper.findBalanceBy(from); logger.debug(\\\"更新前查询余额为: {}\\\", fromBalance); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); } } public int findBalance(int accountNo) { return accountMapper.findBalanceBy(accountNo); } } \",\"上面的代码实际上是有 bug 的，假设 from 余额为 1000，两个线程都来转账 1000，可能会出现扣减为负数的情况\",\"原因：事务的原子性仅涵盖 insert、update、delete、select … for update 语句，select 方法并不阻塞\",\"如上图所示，红色线程和蓝色线程的查询都发生在扣减之前，都以为自己有足够的余额做扣减\"]},{\"header\":\"8. @Transactional 方法导致的 synchronized 失效\",\"slug\":\"_8-transactional-方法导致的-synchronized-失效\",\"contents\":[\"针对上面的问题，能否在方法上加 synchronized 锁来解决呢？\",\"@Service public class Service7 { private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public synchronized void transfer(int from, int to, int amount) { int fromBalance = accountMapper.findBalanceBy(from); logger.debug(\\\"更新前查询余额为: {}\\\", fromBalance); if (fromBalance - amount >= 0) { accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); } } public int findBalance(int accountNo) { return accountMapper.findBalanceBy(accountNo); } } \",\"答案是不行，原因如下：\",\"synchronized 保证的仅是目标方法的原子性，环绕目标方法的还有 commit 等操作，它们并未处于 sync 块内\",\"可以参考下图发现，蓝色线程的查询只要在红色线程提交之前执行，那么依然会查询到有 1000 足够余额来转账\",\"解法1：synchronized 范围应扩大至代理方法调用\",\"解法2：使用 select … for update 替换 select\"]}]},\"/lick-brick-java/avoid/code/http.html\":{\"title\":\"2、HTTP调用：你考虑到超时、重试、并发了吗？\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"今天，我们一起聊聊进行 HTTP 调用需要注意的超时、重试、并发等问题。\",\"与执行本地方法不同，进行 HTTP 调用本质上是通过 HTTP 协议进行一次网络请求。网络请求必然有超时的可能性，因此我们必须考虑到这三点：\",\"首先，框架设置的默认超时是否合理；\",\"其次，考虑到网络的不稳定，超时后的请求重试是一个不错的选择，但需要考虑服务端接口的幂等性设计是否允许我们重试；\",\"最后，需要考虑框架是否会像浏览器那样限制并发连接数，以免在服务并发很大的情况下，HTTP 调用的并发数限制成为瓶颈。\",\"Spring Cloud 是 Java 微服务架构的代表性框架。如果使用 Spring Cloud 进行微服务开发，就会使用 Feign 进行声明式的服务调用。如果不使用 Spring Cloud，而直接使用 Spring Boot 进行微服务开发的话，可能会直接使用 Java 中最常用的 HTTP 客户端 Apache HttpClient 进行服务调用。\",\"接下来，我们就看看使用 Feign 和 Apache HttpClient 进行 HTTP 接口调用时，可能会遇到的超时、重试和并发方面的坑。\"]},{\"header\":\"1、配置连接超时和读取超时参数的学问\",\"slug\":\"_1、配置连接超时和读取超时参数的学问\",\"contents\":[\"对于 HTTP 调用，虽然应用层走的是 HTTP 协议，但网络层面始终是 TCP/IP 协议。TCP/IP 是面向连接的协议，在传输数据之前需要建立连接。几乎所有的网络框架都会提供这么两个超时参数：\",\"连接超时参数 ConnectTimeout，让用户配置建连阶段的最长等待时间；\",\"读取超时参数 ReadTimeout，用来控制从 Socket 上读取数据的最长等待时间。\",\"这两个参数看似是网络层偏底层的配置参数，不足以引起开发同学的重视。但，正确理解和配置这两个参数，对业务应用特别重要，毕竟超时不是单方面的事情，需要客户端和服务端对超时有一致的估计，协同配合方能平衡吞吐量和错误率。\",\"连接超时参数和连接超时的误区有这么两个：\",\"**连接超时配置得特别长，比如 60 秒。**一般来说，TCP 三次握手建立连接需要的时间非常短，通常在毫秒级最多到秒级，不可能需要十几秒甚至几十秒。如果很久都无法建连，很可能是网络或防火墙配置的问题。这种情况下，如果几秒连接不上，那么可能永远也连接不上。因此，设置特别长的连接超时意义不大，将其配置得短一些（比如 1~5 秒）即可。如果是纯内网调用的话，这个参数可以设置得更短，在下游服务离线无法连接的时候，可以快速失败。\",\"**排查连接超时问题，却没理清连的是哪里。**通常情况下，我们的服务会有多个节点，如果别的客户端通过客户端负载均衡技术来连接服务端，那么客户端和服务端会直接建立连接，此时出现连接超时大概率是服务端的问题；而如果服务端通过类似 Nginx 的反向代理来负载均衡，客户端连接的其实是 Nginx，而不是服务端，此时出现连接超时应该排查 Nginx。\",\"读取超时参数和读取超时则会有更多的误区，我将其归纳为如下三个。\",\"**第一个误区：**认为出现了读取超时，服务端的执行就会中断。\",\"我们来简单测试下。定义一个 client 接口，内部通过 HttpClient 调用服务端接口 server，客户端读取超时 2 秒，服务端接口执行耗时 5 秒。\",\"@RestController @RequestMapping(\\\"clientreadtimeout\\\") @Slf4j public class ClientReadTimeoutController { private String getResponse(String url, int connectTimeout, int readTimeout) throws IOException { return Request.Get(\\\"http://localhost:45678/clientreadtimeout\\\" + url) .connectTimeout(connectTimeout) .socketTimeout(readTimeout) .execute() .returnContent() .asString(); } @GetMapping(\\\"client\\\") public String client() throws IOException { log.info(\\\"client1 called\\\"); //服务端5s超时，客户端读取超时2秒 return getResponse(\\\"/server?timeout=5000\\\", 1000, 2000); } @GetMapping(\\\"server\\\") public void server(@RequestParam(\\\"timeout\\\") int timeout) throws InterruptedException { log.info(\\\"server called\\\"); TimeUnit.MILLISECONDS.sleep(timeout); log.info(\\\"Done\\\"); } } \",\"调用 client 接口后，从日志中可以看到，客户端 2 秒后出现了 SocketTimeoutException，原因是读取超时，服务端却丝毫没受影响在 3 秒后执行完成。\",\"[11:35:11.943] [http-nio-45678-exec-1] [INFO ] [.t.c.c.d.ClientReadTimeoutController:29 ] - client1 called [11:35:12.032] [http-nio-45678-exec-2] [INFO ] [.t.c.c.d.ClientReadTimeoutController:36 ] - server called [11:35:14.042] [http-nio-45678-exec-1] [ERROR] [.a.c.c.C.[.[.[/].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) ... [11:35:17.036] [http-nio-45678-exec-2] [INFO ] [.t.c.c.d.ClientReadTimeoutController:38 ] - Done \",\"我们知道，类似 Tomcat 的 Web 服务器都是把服务端请求提交到线程池处理的，只要服务端收到了请求，网络层面的超时和断开便不会影响服务端的执行。因此，出现读取超时不能随意假设服务端的处理情况，需要根据业务状态考虑如何进行后续处理。\",\"**第二个误区：**认为读取超时只是 Socket 网络层面的概念，是数据传输的最长耗时，故将其配置得非常短，比如 100 毫秒。\",\"其实，发生了读取超时，网络层面无法区分是服务端没有把数据返回给客户端，还是数据在网络上耗时较久或丢包。\",\"但，因为 TCP 是先建立连接后传输数据，对于网络情况不是特别糟糕的服务调用，通常可以认为出现连接超时是网络问题或服务不在线，而出现读取超时是服务处理超时。确切地说，读取超时指的是，向 Socket 写入数据后，我们等到 Socket 返回数据的超时时间，其中包含的时间或者说绝大部分的时间，是服务端处理业务逻辑的时间。\",\"**第三个误区：**认为超时时间越长任务接口成功率就越高，将读取超时参数配置得太长。\",\"进行 HTTP 请求一般是需要获得结果的，属于同步调用。如果超时时间很长，在等待服务端返回数据的同时，客户端线程（通常是 Tomcat 线程）也在等待，当下游服务出现大量超时的时候，程序可能也会受到拖累创建大量线程，最终崩溃。\",\"对定时任务或异步任务来说，读取超时配置得长些问题不大。但面向用户响应的请求或是微服务短平快的同步接口调用，并发量一般较大，我们应该设置一个较短的读取超时时间，以防止被下游服务拖慢，通常不会设置超过 30 秒的读取超时。\",\"你可能会说，如果把读取超时设置为 2 秒，服务端接口需要 3 秒，岂不是永远都拿不到执行结果了？的确是这样，因此设置读取超时一定要根据实际情况，过长可能会让下游抖动影响到自己，过短又可能影响成功率。甚至，有些时候我们还要根据下游服务的 SLA，为不同的服务端接口设置不同的客户端读取超时。\"]},{\"header\":\"2、Feign 和 Ribbon 配合使用，你知道怎么配置超时吗？\",\"slug\":\"_2、feign-和-ribbon-配合使用-你知道怎么配置超时吗\",\"contents\":[\"刚才我强调了根据自己的需求配置连接超时和读取超时的重要性，你是否尝试过为 Spring Cloud 的 Feign 配置超时参数呢，有没有被网上的各种资料绕晕呢？\",\"在我看来，为 Feign 配置超时参数的复杂之处在于，Feign 自己有两个超时参数，它使用的负载均衡组件 Ribbon 本身还有相关配置。那么，这些配置的优先级是怎样的，又哪些什么坑呢？接下来，我们做一些实验吧。\",\"为测试服务端的超时，假设有这么一个服务端接口，什么都不干只休眠 10 分钟：\",\"@PostMapping(\\\"/server\\\") public void server() throws InterruptedException { //睡眠10分钟 TimeUnit.MINUTES.sleep(10); } \",\"首先，定义一个 Feign 来调用这个接口：\",\"@FeignClient(name = \\\"clientsdk\\\") public interface Client { @PostMapping(\\\"/feignandribbon/server\\\") void server(); } \",\"然后，通过 Feign Client 进行接口调用：\",\"@GetMapping(\\\"client\\\") public void timeout() { long begin=System.currentTimeMillis(); try{ client.server(); }catch (Exception ex){ log.warn(\\\"执行耗时：{}ms 错误：{}\\\", System.currentTimeMillis() - begin, ex.getMessage()); } } \",\"在配置文件仅指定服务端地址的情况下：\",\"clientsdk.ribbon.listOfServers=localhost:45678 \",\"结果如下：\",\"[15:40:16.094] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：1007ms 错误：Read timed out executing POST http://clientsdk/feignandribbon/server \",\"从这个输出中，我们可以得到结论一，默认情况下 Feign 的读取超时是 1 秒，如此短的读取超时算是坑点一。\",\"我们来分析一下源码。打开 RibbonClientConfiguration 类后，会看到 DefaultClientConfigImpl 被创建出来之后，ReadTimeout 和 ConnectTimeout 被设置为 1 s：\",\"** * Ribbon client default connect timeout. */ public static final int DEFAULT_CONNECT_TIMEOUT = 1000; /** * Ribbon client default read timeout. */ public static final int DEFAULT_READ_TIMEOUT = 1000; @Bean @ConditionalOnMissingBean public IClientConfig ribbonClientConfig() { DefaultClientConfigImpl config = new DefaultClientConfigImpl(); config.loadProperties(this.name); config.set(CommonClientConfigKey.ConnectTimeout, DEFAULT_CONNECT_TIMEOUT); config.set(CommonClientConfigKey.ReadTimeout, DEFAULT_READ_TIMEOUT); config.set(CommonClientConfigKey.GZipPayload, DEFAULT_GZIP_PAYLOAD); return config; } \",\"如果要修改 Feign 客户端默认的两个全局超时时间，你可以设置 feign.client.config.default.readTimeout 和 feign.client.config.default.connectTimeout 参数：\",\"feign.client.config.default.readTimeout=3000 feign.client.config.default.connectTimeout=3000 \",\"可见，3 秒读取超时生效了。注意：这里有一个大坑，如果你希望只修改读取超时，可能会只配置这么一行：\",\"feign.client.config.default.readTimeout=3000 \",\"测试一下你就会发现，这样的配置是无法生效的！\",\"结论二，也是坑点二，如果要配置 Feign 的读取超时，就必须同时配置连接超时，才能生效。\",\"打开 FeignClientFactoryBean 可以看到，只有同时设置 ConnectTimeout 和 ReadTimeout，Request.Options 才会被覆盖：\",\"if (config.getConnectTimeout() != null && config.getReadTimeout() != null) { builder.options(new Request.Options(config.getConnectTimeout(), config.getReadTimeout())); } \",\"更进一步，如果你希望针对单独的 Feign Client 设置超时时间，可以把 default 替换为 Client 的 name：\",\"feign.client.config.default.readTimeout=3000 feign.client.config.default.connectTimeout=3000 feign.client.config.clientsdk.readTimeout=2000 feign.client.config.clientsdk.connectTimeout=2000 \",\"可以得出结论三，单独的超时可以覆盖全局超时，这符合预期，不算坑：\",\"[15:45:51.708] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：2006ms 错误：Read timed out executing POST http://clientsdk/feignandribbon/server \",\"结论四，除了可以配置 Feign，也可以配置 Ribbon 组件的参数来修改两个超时时间。这里的坑点三是，参数首字母要大写，和 Feign 的配置不同。\",\"ribbon.ReadTimeout=4000 ribbon.ConnectTimeout=4000 \",\"可以通过日志证明参数生效：\",\"[15:55:18.019] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：4003ms 错误：Read timed out executing POST http://clientsdk/feignandribbon/server \",\"最后，我们来看看同时配置 Feign 和 Ribbon 的参数，最终谁会生效？如下代码的参数配置：\",\"clientsdk.ribbon.listOfServers=localhost:45678 feign.client.config.default.readTimeout=3000 feign.client.config.default.connectTimeout=3000 ribbon.ReadTimeout=4000 ribbon.ConnectTimeout=4000 \",\"日志输出证明，最终生效的是 Feign 的超时：\",\"[16:01:19.972] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：3006ms 错误：Read timed out executing POST http://clientsdk/feignandribbon/server \",\"结论五，同时配置 Feign 和 Ribbon 的超时，以 Feign 为准。这有点反直觉，因为 Ribbon 更底层所以你会觉得后者的配置会生效，但其实不是这样的。\",\"在 LoadBalancerFeignClient 源码中可以看到，如果 Request.Options 不是默认值，就会创建一个 FeignOptionsClientConfig 代替原来 Ribbon 的 DefaultClientConfigImpl，导致 Ribbon 的配置被 Feign 覆盖：\",\"IClientConfig getClientConfig(Request.Options options, String clientName) { IClientConfig requestConfig; if (options == DEFAULT_OPTIONS) { requestConfig = this.clientFactory.getClientConfig(clientName); } else { requestConfig = new FeignOptionsClientConfig(options); } return requestConfig; } \",\"但如果这么配置最终生效的还是 Ribbon 的超时（4 秒），这容易让人产生 Ribbon 覆盖了 Feign 的错觉，其实这还是因为坑二所致，单独配置 Feign 的读取超时并不能生效：\",\"clientsdk.ribbon.listOfServers=localhost:45678 feign.client.config.default.readTimeout=3000 feign.client.config.clientsdk.readTimeout=2000 ribbon.ReadTimeout=4000 \"]},{\"header\":\"3、 Ribbon 会自动重试请求，了解吗？\",\"slug\":\"_3、-ribbon-会自动重试请求-了解吗\",\"contents\":[\"一些 HTTP 客户端往往会内置一些重试策略，其初衷是好的，毕竟因为网络问题导致丢包虽然频繁但持续时间短，往往重试下第二次就能成功，但一定要小心这种自作主张是否符合我们的预期。\",\"之前遇到过一个短信重复发送的问题，但短信服务的调用方用户服务，反复确认代码里没有重试逻辑。那问题究竟出在哪里了？我们来重现一下这个案例。\",\"首先，定义一个 Get 请求的发送短信接口，里面没有任何逻辑，休眠 2 秒模拟耗时：\",\"@RestController @RequestMapping(\\\"ribbonretryissueserver\\\") @Slf4j public class RibbonRetryIssueServerController { @GetMapping(\\\"sms\\\") public void sendSmsWrong(@RequestParam(\\\"mobile\\\") String mobile, @RequestParam(\\\"message\\\") String message, HttpServletRequest request) throws InterruptedException { //输出调用参数后休眠2秒 log.info(\\\"{} is called, {}=>{}\\\", request.getRequestURL().toString(), mobile, message); TimeUnit.SECONDS.sleep(2); } } \",\"配置一个 Feign 供客户端调用：\",\"@FeignClient(name = \\\"SmsClient\\\") public interface SmsClient { @GetMapping(\\\"/ribbonretryissueserver/sms\\\") void sendSmsWrong(@RequestParam(\\\"mobile\\\") String mobile, @RequestParam(\\\"message\\\") String message); } \",\"Feign 内部有一个 Ribbon 组件负责客户端负载均衡，通过配置文件设置其调用的服务端为两个节点：\",\"SmsClient.ribbon.listOfServers=localhost:45679,localhost:45678 \",\"编写一个客户端接口，通过 Feign 调用服务端：\",\"@RestController @RequestMapping(\\\"ribbonretryissueclient\\\") @Slf4j public class RibbonRetryIssueClientController { @Autowired private SmsClient smsClient; @GetMapping(\\\"wrong\\\") public String wrong() { log.info(\\\"client is called\\\"); try{ //通过Feign调用发送短信接口 smsClient.sendSmsWrong(\\\"13600000000\\\", UUID.randomUUID().toString()); } catch (Exception ex) { //捕获可能出现的网络错误 log.error(\\\"send sms failed : {}\\\", ex.getMessage()); } return \\\"done\\\"; } } \",\"在 45678 和 45679 两个端口上分别启动服务端，然后访问 45678 的客户端接口进行测试。因为客户端和服务端控制器在一个应用中，所以 45678 同时扮演了客户端和服务端的角色。\",\"在 45678 日志中可以看到，29 秒时客户端收到请求开始调用服务端接口发短信，同时服务端收到了请求，2 秒后（注意对比第一条日志和第三条日志）客户端输出了读取超时的错误信息：\",\"[12:49:29.020] [http-nio-45678-exec-4] [INFO ] [c.d.RibbonRetryIssueClientController:23 ] - client is called [12:49:29.026] [http-nio-45678-exec-5] [INFO ] [c.d.RibbonRetryIssueServerController:16 ] - http://localhost:45678/ribbonretryissueserver/sms is called, 13600000000=>a2aa1b32-a044-40e9-8950-7f0189582418 [12:49:31.029] [http-nio-45678-exec-4] [ERROR] [c.d.RibbonRetryIssueClientController:27 ] - send sms failed : Read timed out executing GET http://SmsClient/ribbonretryissueserver/sms?mobile=13600000000&message=a2aa1b32-a044-40e9-8950-7f0189582418 \",\"而在另一个服务端 45679 的日志中还可以看到一条请求，30 秒时收到请求，也就是客户端接口调用后的 1 秒：\",\"[12:49:30.029] [http-nio-45679-exec-2] [INFO ] [c.d.RibbonRetryIssueServerController:16 ] - http://localhost:45679/ribbonretryissueserver/sms is called, 13600000000=>a2aa1b32-a044-40e9-8950-7f0189582418 \",\"客户端接口被调用的日志只输出了一次，而服务端的日志输出了两次。虽然 Feign 的默认读取超时时间是 1 秒，但客户端 2 秒后才出现超时错误。显然，这说明客户端自作主张进行了一次重试，导致短信重复发送。\",\"翻看 Ribbon 的源码可以发现，MaxAutoRetriesNextServer 参数默认为 1，也就是 Get 请求在某个服务端节点出现问题（比如读取超时）时，Ribbon 会自动重试一次：\",\"// DefaultClientConfigImpl public static final int DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER = 1; public static final int DEFAULT_MAX_AUTO_RETRIES = 0; // RibbonLoadBalancedRetryPolicy public boolean canRetry(LoadBalancedRetryContext context) { HttpMethod method = context.getRequest().getMethod(); return HttpMethod.GET == method || lbContext.isOkToRetryOnAllOperations(); } @Override public boolean canRetrySameServer(LoadBalancedRetryContext context) { return sameServerCount < lbContext.getRetryHandler().getMaxRetriesOnSameServer() && canRetry(context); } @Override public boolean canRetryNextServer(LoadBalancedRetryContext context) { // this will be called after a failure occurs and we increment the counter // so we check that the count is less than or equals to too make sure // we try the next server the right number of times return nextServerCount <= lbContext.getRetryHandler().getMaxRetriesOnNextServer() && canRetry(context); } \",\"解决办法有两个：\",\"一是，把发短信接口从 Get 改为 Post。其实，这里还有一个 API 设计问题，有状态的 API 接口不应该定义为 Get。根据 HTTP 协议的规范，Get 请求用于数据查询，而 Post 才是把数据提交到服务端用于修改或新增。选择 Get 还是 Post 的依据，应该是 API 的行为，而不是参数大小。这里的一个误区是，Get 请求的参数包含在 Url QueryString 中，会受浏览器长度限制，所以一些同学会选择使用 JSON 以 Post 提交大参数，使用 Get 提交小参数。\",\"二是，将 MaxAutoRetriesNextServer 参数配置为 0，禁用服务调用失败后在下一个服务端节点的自动重试。在配置文件中添加一行即可：\",\"ribbon.MaxAutoRetriesNextServer=0 \",\"看到这里，你觉得问题出在用户服务还是短信服务呢？\",\"在我看来，双方都有问题。就像之前说的，Get 请求应该是无状态或者幂等的，短信接口可以设计为支持幂等调用的；而用户服务的开发同学，如果对 Ribbon 的重试机制有所了解的话，或许就能在排查问题上少走些弯路。\"]},{\"header\":\"4、并发限制了爬虫的抓取能力\",\"slug\":\"_4、并发限制了爬虫的抓取能力\",\"contents\":[\"除了超时和重试的坑，进行 HTTP 请求调用还有一个常见的问题是，并发数的限制导致程序的处理能力上不去。\",\"我之前遇到过一个爬虫项目，整体爬取数据的效率很低，增加线程池数量也无济于事，只能堆更多的机器做分布式的爬虫。现在，我们就来模拟下这个场景，看看问题出在了哪里。\",\"假设要爬取的服务端是这样的一个简单实现，休眠 1 秒返回数字 1：\",\"@GetMapping(\\\"server\\\") public int server() throws InterruptedException { TimeUnit.SECONDS.sleep(1); return 1; } \",\"爬虫需要多次调用这个接口进行数据抓取，为了确保线程池不是并发的瓶颈，我们使用一个没有线程上限的 newCachedThreadPool 作为爬取任务的线程池（再次强调，除非你非常清楚自己的需求，否则一般不要使用没有线程数量上限的线程池），然后使用 HttpClient 实现 HTTP 请求，把请求任务循环提交到线程池处理，最后等待所有任务执行完成后输出执行耗时：\",\"private int sendRequest(int count, Supplier<CloseableHttpClient> client) throws InterruptedException { //用于计数发送的请求个数 AtomicInteger atomicInteger = new AtomicInteger(); //使用HttpClient从server接口查询数据的任务提交到线程池并行处理 ExecutorService threadPool = Executors.newCachedThreadPool(); long begin = System.currentTimeMillis(); IntStream.rangeClosed(1, count).forEach(i -> { threadPool.execute(() -> { try (CloseableHttpResponse response = client.get().execute(new HttpGet(\\\"http://127.0.0.1:45678/routelimit/server\\\"))) { atomicInteger.addAndGet(Integer.parseInt(EntityUtils.toString(response.getEntity()))); } catch (Exception ex) { ex.printStackTrace(); } }); }); //等到count个任务全部执行完毕 threadPool.shutdown(); threadPool.awaitTermination(1, TimeUnit.HOURS); log.info(\\\"发送 {} 次请求，耗时 {} ms\\\", atomicInteger.get(), System.currentTimeMillis() - begin); return atomicInteger.get(); } \",\"首先，使用默认的 PoolingHttpClientConnectionManager 构造的 CloseableHttpClient，测试一下爬取 10 次的耗时：\",\"static CloseableHttpClient httpClient1; static { httpClient1 = HttpClients.custom().setConnectionManager(new PoolingHttpClientConnectionManager()).build(); } @GetMapping(\\\"wrong\\\") public int wrong(@RequestParam(value = \\\"count\\\", defaultValue = \\\"10\\\") int count) throws InterruptedException { return sendRequest(count, () -> httpClient1); } \",\"虽然一个请求需要 1 秒执行完成，但我们的线程池是可以扩张使用任意数量线程的。按道理说，10 个请求并发处理的时间基本相当于 1 个请求的处理时间，也就是 1 秒，但日志中显示实际耗时 5 秒：\",\"[12:48:48.122] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.h.r.RouteLimitController :54 ] - 发送 10 次请求，耗时 5265 ms \",\"查看 PoolingHttpClientConnectionManager 源码，可以注意到有两个重要参数：\",\"defaultMaxPerRoute=2，也就是同一个主机 / 域名的最大并发请求数为 2。我们的爬虫需要 10 个并发，显然是默认值太小限制了爬虫的效率。\",\"maxTotal=20，也就是所有主机整体最大并发为 20，这也是 HttpClient 整体的并发度。目前，我们请求数是 10 最大并发是 10，20 不会成为瓶颈。举一个例子，使用同一个 HttpClient 访问 10 个域名，defaultMaxPerRoute 设置为 10，为确保每一个域名都能达到 10 并发，需要把 maxTotal 设置为 100。\",\"public PoolingHttpClientConnectionManager( final HttpClientConnectionOperator httpClientConnectionOperator, final HttpConnectionFactory<HttpRoute, ManagedHttpClientConnection> connFactory, final long timeToLive, final TimeUnit timeUnit) { ... this.pool = new CPool(new InternalConnectionFactory( this.configData, connFactory), 2, 20, timeToLive, timeUnit); ... } public CPool( final ConnFactory<HttpRoute, ManagedHttpClientConnection> connFactory, final int defaultMaxPerRoute, final int maxTotal, final long timeToLive, final TimeUnit timeUnit) { ... }} \",\"HttpClient 是 Java 非常常用的 HTTP 客户端，这个问题经常出现。你可能会问，为什么默认值限制得这么小。\",\"其实，这不能完全怪 HttpClient，很多早期的浏览器也限制了同一个域名两个并发请求。对于同一个域名并发连接的限制，其实是 HTTP 1.1 协议要求的，这里有这么一段话：\",\"Clients that use persistent connections SHOULD limit the number of simultaneous connections that they maintain to a given server. A single-user client SHOULD NOT maintain more than 2 connections with any server or proxy. A proxy SHOULD use up to 2*N connections to another server or proxy, where N is the number of simultaneously active users. These guidelines are intended to improve HTTP response times and avoid congestion. \",\"HTTP 1.1 协议是 20 年前制定的，现在 HTTP 服务器的能力强很多了，所以有些新的浏览器没有完全遵从 2 并发这个限制，放开并发数到了 8 甚至更大。如果需要通过 HTTP 客户端发起大量并发请求，不管使用什么客户端，请务必确认客户端的实现默认的并发度是否满足需求。\",\"既然知道了问题所在，我们就尝试声明一个新的 HttpClient 放开相关限制，设置 maxPerRoute 为 50、maxTotal 为 100，然后修改一下刚才的 wrong 方法，使用新的客户端进行测试：\",\"httpClient2 = HttpClients.custom().setMaxConnPerRoute(10).setMaxConnTotal(20).build(); \",\"输出如下，10 次请求在 1 秒左右执行完成。可以看到，因为放开了一个 Host 2 个并发的默认限制，爬虫效率得到了大幅提升：\",\"发送 10 次请求，耗时 1023 ms \"]},{\"header\":\"5、总结\",\"slug\":\"_5、总结\",\"contents\":[\"今天，我和你分享了 HTTP 调用最常遇到的超时、重试和并发问题。\",\"连接超时代表建立 TCP 连接的时间，读取超时代表了等待远端返回数据的时间，也包括远端程序处理的时间。在解决连接超时问题时，我们要搞清楚连的是谁；在遇到读取超时问题的时候，我们要综合考虑下游服务的服务标准和自己的服务标准，设置合适的读取超时时间。此外，在使用诸如 Spring Cloud Feign 等框架时务必确认，连接和读取超时参数的配置是否正确生效。\",\"对于重试，因为 HTTP 协议认为 Get 请求是数据查询操作，是无状态的，又考虑到网络出现丢包是比较常见的事情，有些 HTTP 客户端或代理服务器会自动重试 Get/Head 请求。如果你的接口设计不支持幂等，需要关闭自动重试。但，更好的解决方案是，遵从 HTTP 协议的建议来使用合适的 HTTP 方法。\",\"最后我们看到，包括 HttpClient 在内的 HTTP 客户端以及浏览器，都会限制客户端调用的最大并发数。如果你的客户端有比较大的请求调用并发，比如做爬虫，或是扮演类似代理的角色，又或者是程序本身并发较高，如此小的默认值很容易成为吞吐量的瓶颈，需要及时调整。\"]}]},\"/lick-brick-java/avoid/code/thread-safe.html\":{\"title\":\"1、使用了并发工具类库，线程安全就高枕无忧了吗？\",\"contents\":[{\"header\":\"1、说明\",\"slug\":\"_1、说明\",\"contents\":[\"在代码审核讨论的时候，我们有时会听到有关线程安全和并发工具的一些片面的观点和结论，比如“把 HashMap 改为 ConcurrentHashMap，就可以解决并发问题了呀”，“要不我们试试无锁的 CopyOnWriteArrayList 吧，性能更好”。事实上，这些说法都不太准确。\",\"的确，为了方便开发者进行多线程编程，现代编程语言会提供各种并发工具类。但如果我们没有充分了解它们的使用场景、解决的问题，以及最佳实践的话，盲目使用就可能会导致一些坑，小则损失性能，大则无法确保多线程情况下业务逻辑的正确性。\",\"需要先说明下，这里的并发工具类是指用来解决多线程环境下并发问题的工具类库。一般而言并发工具包括同步器和容器两大类，业务代码中使用并发容器的情况会多一些，我今天分享的例子也会侧重并发容器。\",\"接下来，我们就看看在使用并发工具时，最常遇到哪些坑，以及如何解决、避免这些坑吧。\"]},{\"header\":\"2、没有意识到线程重用导致用户信息错乱的 Bug\",\"slug\":\"_2、没有意识到线程重用导致用户信息错乱的-bug\",\"contents\":[\"之前有业务同学和我反馈，在生产上遇到一个诡异的问题，有时获取到的用户信息是别人的。查看代码后，我发现他使用了 ThreadLocal 来缓存获取到的用户信息。\",\"我们知道，ThreadLocal 适用于变量在线程间隔离，而在方法或类间共享的场景。如果用户信息的获取比较昂贵（比如从数据库查询用户信息），那么在 ThreadLocal 中缓存数据是比较合适的做法。但，这么做为什么会出现用户信息错乱的 Bug 呢？\",\"先看一个具体的案例：\",\"场景：使用 Spring Boot 创建一个 Web 应用程序，使用 ThreadLocal 存放一个 Integer 的值，来暂且代表需要在线程中保存的用户信息，这个值初始是 null。在业务逻辑中，我先从ThreadLocal 获取一次值，然后把外部传入的参数设置到 ThreadLocal 中，来模拟从当前上下文获取到用户信息的逻辑，随后再获取一次值，最后输出两次获得的值和线程名称。\",\"新建CaseController\",\"@RestController @RequestMapping(\\\"/threadlocal\\\") public class CaseController { private ThreadLocal<Integer> currentUser = ThreadLocal.withInitial(() -> null); @GetMapping(\\\"/wrong\\\") public Map wrong(@RequestParam(\\\"userId\\\") Integer userId){ //设置用户信息之前先查询一次ThreadLocal中的用户信息 String before = Thread.currentThread().getName() + \\\":\\\" + currentUser.get(); //设置用户信息到ThreadLocal currentUser.set(userId); //设置用户信息之后再查询一次ThreadLocal中的用户信息 String after = Thread.currentThread().getName() + \\\":\\\" + currentUser.get(); //汇总输出两次查询结果 Map result = new HashMap(2); result.put(\\\"before\\\", before); result.put(\\\"after\\\", after); return result; } } \",\"从代码层面理解，在设置用户信息之前第一次获取的值始终应该是 null，但我们要意识到，程序运行在 Tomcat 中，执行程序的线程是 Tomcat 的工作线程，而 Tomcat 的工作线程是基于线程池的。\",\"顾名思义，线程池会重用固定的几个线程，一旦线程重用，那么很可能首次从ThreadLocal 获取的值是之前其他用户的请求遗留的值。这时，ThreadLocal 中的用户信息就是其他用户的信息。\",\"请求路径：http://localhost:1688/threadlocal/wrong?userId=1\",\"请求结果：\",\"这里测试没有设置最大线程数，线程是在变化的\",\"为了更快地重现这个问题，我在配置文件中设置一下 Tomcat 的参数，把工作线程池最大线程数设置为 1，这样始终是同一个线程在处理请求：\",\"# 设置最大线程为1 server.tomcat.threads.max=1 # 设置端口 server.port=1688 \",\"运行程序后先让用户 1 来请求接口，可以看到第一和第二次获取到用户 ID 分别是 null 和1，符合预期：\",\"随后用户 2 来请求接口，这次就出现了 Bug，第一和第二次获取到用户 ID 分别是 1 和2，显然第一次获取到了用户 1 的信息，原因就是 Tomcat 的线程池重用了线程。从图中可以看到，两次请求的线程都是同一个线程：http-nio-1688-exec-1\",\"这个场景也是会经常出现的，在一些小型项目中跟，同时告诉我们，在写业务代码时，首先要理解代码会跑在什么线程上\",\"我们可能会抱怨学多线程没用，因为代码里没有开启使用多线程。但其实，可能只是我们没有意识到，在 Tomcat 这种 Web 服务器下跑的业务代码，本来就运行在一个多线程环境（否则接口也不可能支持这么高的并发），并不能认为没有显式开启多线程就不会有线程安全问题。\",\"因为线程的创建比较昂贵，所以 Web 服务器往往会使用线程池来处理请求，这就意味着线程会被重用。这时，使用类似 ThreadLocal 工具来存放一些数据时，需要特别注意在代码运行完后，显式地去清空设置的数据。如果在代码中使用了自定义的线程池，也同样会遇到这个问题。\",\"理解了这个知识点后，我们修正这段代码的方案是：在代码的 finally 代码块中，显式清除ThreadLocal 中的数据。这样一来，新的请求过来即使使用了之前的线程也不会获取到错误的用户信息了。修正后的代码如下：\",\"@GetMapping(\\\"/right\\\") public Map right(@RequestParam(\\\"userId\\\") Integer userId){ //设置用户信息之前先查询一次ThreadLocal中的用户信息 String before = Thread.currentThread().getName() + \\\":\\\" + currentUser.get(); //设置用户信息到ThreadLocal currentUser.set(userId); try { //设置用户信息之后再查询一次ThreadLocal中的用户信息 String after = Thread.currentThread().getName() + \\\":\\\" + currentUser.get(); //汇总输出两次查询结果 Map result = new HashMap(2); result.put(\\\"before\\\", before); result.put(\\\"after\\\", after); return result; } catch (Exception e) { throw new RuntimeException(e); } finally { //在finally代码块中删除ThreadLocal中的数据，确保数据不串 currentUser.remove(); } } \",\"重新运行程序可以验证，再也不会出现第一次查询用户信息查询到之前用户请求的 Bug：\",\"ThreadLocal 是利用独占资源的方式，来解决线程安全问题，那如果我们确实需要有资源在线程之前共享，应该怎么办呢？这时，我们可能就需要用到线程安全的容器了。\"]},{\"header\":\"3、使用了线程安全的并发工具，并不代表解决了所有线程安全问题\",\"slug\":\"_3、使用了线程安全的并发工具-并不代表解决了所有线程安全问题\",\"contents\":[\"JDK 1.5 后推出的 ConcurrentHashMap，是一个高性能的线程安全的哈希表容器。“线程安全”这四个字特别容易让人误解，因为 ConcurrentHashMap 只能保证提供的原子性读写操作是线程安全的。\",\"我在相当多的业务代码中看到过这个误区，比如下面这个场景：\",\"有一个含 900 个元素的Map，现在再补充 100 个元素进去，这个补充操作由 10 个线程并发进行。\",\"开发人员误以为使用了 ConcurrentHashMap 就不会有线程安全问题，于是不加思索地写出了下面的代码：在每一个线程的代码逻辑中先通过 size 方法拿到当前元素数量，计算 ConcurrentHashMap 目前还需要补充多少元素，并在日志中输出了这个值，然后通过 putAll 方法把缺少的元素添加进去。\",\"为方便观察问题，我们输出了这个 Map 一开始和最后的元素个数。\",\"@RestController @RequestMapping(\\\"/threadlocal/case02\\\") public class Case02Controller { private final Logger log = LoggerFactory.getLogger(getClass()); //线程个数 private static int THREAD_COUNT = 10; //总元素数量 private static int ITEM_COUNT = 1000; /** * 用来获得一个指定元素数量模拟数据的ConcurrentHashMap * @param count * @return */ private ConcurrentHashMap<String, Long> getData(int count) { return LongStream.rangeClosed(1, count). boxed(). collect(Collectors.toConcurrentMap(i -> UUID.randomUUID().toString(), Function.identity(), (o1, o2) -> o1, ConcurrentHashMap::new)); } @GetMapping(\\\"/wrong\\\") public String wrong() throws InterruptedException { ConcurrentHashMap<String, Long> concurrentHashMap = getData(ITEM_COUNT - 100); //初始900个元素 log.info(\\\"init size:{}\\\", concurrentHashMap.size()); ForkJoinPool forkJoinPool = new ForkJoinPool(THREAD_COUNT); //使用线程池并发处理逻辑 forkJoinPool.execute(() -> IntStream.rangeClosed(1, 10).parallel().forEach(i ->{ //查询还需要补充多少个元素 int gap = ITEM_COUNT - concurrentHashMap.size(); log.info(\\\"gap size:{}\\\", gap); //补充元素 concurrentHashMap.putAll(getData(gap)); })); //等待所有任务完成 forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); //最后元素个数会是1000吗？ log.info(\\\"finish size:{}\\\", concurrentHashMap.size()); return \\\"OK\\\"; } } \",\"访问接口：localhost:1688/threadlocal/case02/wrong\",\"后程序输出的日志内容如下：\",\"2023-06-07T10:24:58.088+08:00 INFO 15000 --- [nio-1688-exec-2] c.j.t.c.controller.Case02Controller : init size:900 2023-06-07T10:24:58.090+08:00 INFO 15000 --- [Pool-2-worker-1] c.j.t.c.controller.Case02Controller : gap size:100 2023-06-07T10:24:58.091+08:00 INFO 15000 --- [Pool-2-worker-2] c.j.t.c.controller.Case02Controller : gap size:100 2023-06-07T10:24:58.091+08:00 INFO 15000 --- [Pool-2-worker-1] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.091+08:00 INFO 15000 --- [Pool-2-worker-4] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.091+08:00 INFO 15000 --- [Pool-2-worker-3] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.092+08:00 INFO 15000 --- [Pool-2-worker-5] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.092+08:00 INFO 15000 --- [Pool-2-worker-1] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.092+08:00 INFO 15000 --- [Pool-2-worker-1] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.092+08:00 INFO 15000 --- [Pool-2-worker-4] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.092+08:00 INFO 15000 --- [Pool-2-worker-3] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T10:24:58.093+08:00 INFO 15000 --- [nio-1688-exec-2] c.j.t.c.controller.Case02Controller : finish size:1100 \",\"从日志中可以看到： 初始大小 900 符合预期，还需要填充 100 个元素。\",\"worker1，worker2线程查询到当前需要填充的元素为 100，最后 HashMap 的总项目数是 1100，显然不符合填充满 1000 的预期。\",\"针对这个场景，我们可以举一个形象的例子。ConcurrentHashMap 就像是一个大篮子，现在这个篮子里有 900 个桔子，我们期望把这个篮子装满 1000 个桔子，也就是再装 100个桔子。有 10 个工人来干这件事儿，大家先后到岗后会计算还需要补多少个桔子进去，最后把桔子装入篮子。\",\"ConcurrentHashMap 这个篮子本身，可以确保多个工人在装东西进去时，不会相互影响干扰，但无法确保工人 A 看到还需要装 100 个桔子但是还未装的时候，工人 B 就看不到篮子中的桔子数量。更值得注意的是，你往这个篮子装 100 个桔子的操作不是原子性的，在别人看来可能会有一个瞬间篮子里有 964 个桔子，还需要补 36 个桔子。\",\"回到 ConcurrentHashMap，我们需要注意 ConcurrentHashMap 对外提供的方法或能力的限制：\",\"使用了 ConcurrentHashMap，不代表对它的多个操作之间的状态是一致的，是没有其他线程在操作它的，如果需要确保需要手动加锁。\",\"诸如 size、isEmpty 和 containsValue 等聚合方法，在并发情况下可能会反映ConcurrentHashMap 的中间状态。因此在并发情况下，这些方法的返回值只能用作参考，而不能用于流程控制。显然，利用 size 方法计算差异值，是一个流程控制。\",\"诸如 putAll 这样的聚合方法也不能确保原子性，在 putAll 的过程中去获取数据可能会获取到部分数据。\",\"代码的修改方案很简单，整段逻辑加锁即可：\",\"@GetMapping(\\\"/right\\\") public String right() throws InterruptedException { ConcurrentHashMap<String, Long> concurrentHashMap = getData(ITEM_COUNT - 100); //初始900个元素 log.info(\\\"init size:{}\\\", concurrentHashMap.size()); ForkJoinPool forkJoinPool = new ForkJoinPool(THREAD_COUNT); //使用线程池并发处理逻辑 forkJoinPool.execute(() -> IntStream.rangeClosed(1, 10).parallel().forEach(i ->{ //加锁处理 synchronized (concurrentHashMap){ //查询还需要补充多少个元素 int gap = ITEM_COUNT - concurrentHashMap.size(); log.info(\\\"gap size:{}\\\", gap); //补充元素 concurrentHashMap.putAll(getData(gap)); } })); //等待所有任务完成 forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); //最后元素个数会是1000吗？ log.info(\\\"finish size:{}\\\", concurrentHashMap.size()); return \\\"OK\\\"; } \",\"重新调用接口，程序的日志输出结果符合预期：\",\"2023-06-07T11:56:27.530+08:00 INFO 26284 --- [nio-1688-exec-1] c.j.t.c.controller.Case02Controller : init size:900 2023-06-07T11:56:27.545+08:00 INFO 26284 --- [Pool-1-worker-1] c.j.t.c.controller.Case02Controller : gap size:100 2023-06-07T11:56:27.547+08:00 INFO 26284 --- [Pool-1-worker-2] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.547+08:00 INFO 26284 --- [Pool-1-worker-1] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.548+08:00 INFO 26284 --- [Pool-1-worker-4] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.548+08:00 INFO 26284 --- [Pool-1-worker-3] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.548+08:00 INFO 26284 --- [Pool-1-worker-2] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.548+08:00 INFO 26284 --- [Pool-1-worker-4] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.549+08:00 INFO 26284 --- [Pool-1-worker-3] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.549+08:00 INFO 26284 --- [Pool-1-worker-5] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.549+08:00 INFO 26284 --- [Pool-1-worker-1] c.j.t.c.controller.Case02Controller : gap size:0 2023-06-07T11:56:27.551+08:00 INFO 26284 --- [nio-1688-exec-1] c.j.t.c.controller.Case02Controller : finish size:1000 \",\"可以看到，只有一个线程查询到了需要补 100 个元素，其他 9 个线程查询到不需要补元素，最后 Map 大小为 1000。 到了这里，你可能又要问了，使用 ConcurrentHashMap 全程加锁，还不如使用普通的HashMap 呢。其实不完全是这样。\",\"ConcurrentHashMap 提供了一些原子性的简单复合逻辑方法，用好这些方法就可以发挥其威力。这就引申出代码中常见的另一个问题：在使用一些类库提供的高级工具类时，开发人员可能还是按照旧的方式去使用这些新类，因为没有使用其特性，所以无法发挥其威力。\"]},{\"header\":\"4、没有充分了解并发工具的特性，从而无法发挥其威力\",\"slug\":\"_4、没有充分了解并发工具的特性-从而无法发挥其威力\",\"contents\":[\"我们来看一个使用 Map 来统计 Key 出现次数的场景吧，这个逻辑在业务代码中非常常见。\",\"使用 ConcurrentHashMap 来统计，Key 的范围是 10。\",\"使用最多 10 个并发，循环操作 1000 万次，每次操作累加随机的 Key。\",\"如果 Key 不存在的话，首次设置值为 1。\",\"@RestController @RequestMapping(\\\"/case03\\\") public class Case03Controller { //循环次数 private static int LOOP_COUNT = 10000000; //线程数量 private static int THREAD_COUNT = 10; //元素数量 private static int ITEM_COUNT = 1000; @GetMapping(\\\"/normaluse\\\") private Map<String, Long> normalUse() throws InterruptedException { ConcurrentHashMap<String, Long> freqs = new ConcurrentHashMap<>(ITEM_COUNT); ForkJoinPool forkJoinPool = new ForkJoinPool(THREAD_COUNT); forkJoinPool.execute(() -> IntStream.rangeClosed(1, LOOP_COUNT).parallel().forEach(i -> { //获得一个随机的Key String key = \\\"item\\\" + ThreadLocalRandom.current().nextInt(ITEM_COUNT); synchronized (freqs) { if (freqs.containsKey(key)) { //Key存在则+1 freqs.put(key, freqs.get(key) + 1); } else { //Key不存在则初始化为1 freqs.put(key, 1L); } } } )); forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); return freqs; } } \",\"我们吸取之前的教训，直接通过锁的方式锁住 Map，然后做判断、读取现在的累计值、加 1、保存累加后值的逻辑。这段代码在功能上没有问题，但无法充分发挥 ConcurrentHashMap 的威力，改进后的代码如下：\",\"@GetMapping(\\\"rightuse\\\") private Map<String, Long> gooduse() throws InterruptedException { ConcurrentHashMap<String, LongAdder> freqs = new ConcurrentHashMap<>(ITEM_COUNT); ForkJoinPool forkJoinPool = new ForkJoinPool(THREAD_COUNT); forkJoinPool.execute(() -> IntStream.rangeClosed(1, LOOP_COUNT).parallel().forEach(i -> { String key = \\\"item\\\" + ThreadLocalRandom.current().nextInt(ITEM_COUNT); //利用computeIfAbsent()方法来实例化LongAdder，然后利用LongAdder来进行线程安全计数 freqs.computeIfAbsent(key, k -> new LongAdder()).increment(); } )); forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); //因为我们的Value是LongAdder而不是Long，所以需要做一次转换才能返回 return freqs.entrySet().stream() .collect(Collectors.toMap( e -> e.getKey(), e -> e.getValue().longValue()) ); } \",\"在这段改进后的代码中，我们巧妙利用了下面两点：\",\"使用 ConcurrentHashMap 的原子性方法 computeIfAbsent 来做复合逻辑操作，判断 Key 是否存在 Value，如果不存在则把 Lambda 表达式运行后的结果放入 Map 作为 Value，也就是新创建一个 LongAdder 对象，最后返回 Value。\",\"由于 computeIfAbsent 方法返回的 Value 是 LongAdder，是一个线程安全的累加器，因此可以直接调用其 increment 方法进行累加。\",\"这样在确保线程安全的情况下达到极致性能，把之前 7 行代码替换为了 1 行。\",\"我们通过一个简单的测试比较一下修改前后两段代码的性能：\",\"@GetMapping(\\\"good\\\") public String good() throws InterruptedException { StopWatch stopWatch = new StopWatch(); stopWatch.start(\\\"normaluse\\\"); Map<String, Long> normaluse = normalUse(); stopWatch.stop(); //校验元素数量 Assert.isTrue(normaluse.size() == ITEM_COUNT, \\\"normaluse size error\\\"); //校验累计总数 Assert.isTrue(normaluse.entrySet().stream() .mapToLong(item -> item.getValue()).reduce(0, Long::sum) == LOOP_COUNT , \\\"normaluse count error\\\"); stopWatch.start(\\\"gooduse\\\"); Map<String, Long> gooduse = gooduse(); stopWatch.stop(); Assert.isTrue(gooduse.size() == ITEM_COUNT, \\\"gooduse size error\\\"); Assert.isTrue(gooduse.entrySet().stream() .mapToLong(item -> item.getValue()) .reduce(0, Long::sum) == LOOP_COUNT , \\\"gooduse count error\\\"); log.info(stopWatch.prettyPrint()); return \\\"OK\\\"; } \",\"这段测试代码并无特殊之处，使用 StopWatch 来测试两段代码的性能，最后跟了一个断言判断 Map 中元素的个数以及所有 Value 的和，是否符合预期来校验代码的正确性。测试结果如下：\",\"可以看到，优化后的代码，相比使用锁来操作 ConcurrentHashMap 的方式，性能提升了 10 倍。\",\"你可能会问，computeIfAbsent 为什么如此高效呢？\",\"答案就在源码最核心的部分，也就是 Java 自带的 Unsafe 实现的 CAS。它在虚拟机层面确保了写入数据的原子性，比加锁的效率高得多：\",\" static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i, Node<K,V> c, Node<K,V> v) { return U.compareAndSetObject(tab, ((long)i << ASHIFT) + ABASE, c, v); } \",\"像 ConcurrentHashMap 这样的高级并发工具的确提供了一些高级 API，只有充分了解其特性才能最大化其威力，而不能因为其足够高级、酷炫盲目使用。\"]},{\"header\":\"5、没有认清并发工具的使用场景，因而导致性能问题\",\"slug\":\"_5、没有认清并发工具的使用场景-因而导致性能问题\",\"contents\":[\"除了 ConcurrentHashMap 这样通用的并发工具类之外，我们的工具包中还有些针对特殊场景实现的生面孔。一般来说，针对通用场景的通用解决方案，在所有场景下性能都还可以，属于“万金油”；而针对特殊场景的特殊实现，会有比通用解决方案更高的性能，但一定要在它针对的场景下使用，否则可能会产生性能问题甚至是 Bug。\",\"之前在排查一个生产性能问题时，我们发现一段简单的非数据库操作的业务逻辑，消耗了超出预期的时间，在修改数据时操作本地缓存比回写数据库慢许多。查看代码发现，开发同学使用了 CopyOnWriteArrayList 来缓存大量的数据，而数据变化又比较频繁。\",\"CopyOnWrite 是一个时髦的技术，不管是 Linux 还是 Redis 都会用到。在 Java 中，CopyOnWriteArrayList 虽然是一个线程安全的 ArrayList，但因为其实现方式是，每次修改数据时都会复制一份数据出来，所以有明显的适用场景，即读多写少或者说希望无锁读的场景。\",\"如果我们要使用 CopyOnWriteArrayList，那一定是因为场景需要而不是因为足够酷炫。如果读写比例均衡或者有大量写操作的话，使用 CopyOnWriteArrayList 的性能会非常糟糕。\",\"我们写一段测试代码，来比较下使用 CopyOnWriteArrayList 和普通加锁方式 ArrayList 的读写性能吧。在这段代码中我们针对并发读和并发写分别写了一个测试方法，测试两者一定次数的写或读操作的耗时。\",\"//测试并发写的性能 @GetMapping(\\\"write\\\") public Map testWrite() { List<Integer> copyOnWriteArrayList = new CopyOnWriteArrayList<>(); List<Integer> synchronizedList = Collections.synchronizedList(new ArrayList<>()); StopWatch stopWatch = new StopWatch(); int loopCount = 100000; stopWatch.start(\\\"Write:copyOnWriteArrayList\\\"); //循环100000次并发往CopyOnWriteArrayList写入随机元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -> copyOnWriteArrayList.add(ThreadLocalRandom.current().nextInt(loopCount))); stopWatch.stop(); stopWatch.start(\\\"Write:synchronizedList\\\"); //循环100000次并发往加锁的ArrayList写入随机元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -> synchronizedList.add(ThreadLocalRandom.current().nextInt(loopCount))); stopWatch.stop(); log.info(stopWatch.prettyPrint()); Map result = new HashMap(); result.put(\\\"copyOnWriteArrayList\\\", copyOnWriteArrayList.size()); result.put(\\\"synchronizedList\\\", synchronizedList.size()); return result; } //帮助方法用来填充List private void addAll(List<Integer> list) { list.addAll(IntStream.rangeClosed(1, 1000000).boxed().collect(Collectors.toList())); } //测试并发读的性能 @GetMapping(\\\"read\\\") public Map testRead() { //创建两个测试对象 List<Integer> copyOnWriteArrayList = new CopyOnWriteArrayList<>(); List<Integer> synchronizedList = Collections.synchronizedList(new ArrayList<>()); //填充数据 addAll(copyOnWriteArrayList); addAll(synchronizedList); StopWatch stopWatch = new StopWatch(); int loopCount = 1000000; int count = copyOnWriteArrayList.size(); stopWatch.start(\\\"Read:copyOnWriteArrayList\\\"); //循环1000000次并发从CopyOnWriteArrayList随机查询元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -> copyOnWriteArrayList.get(ThreadLocalRandom.current().nextInt(count))); stopWatch.stop(); stopWatch.start(\\\"Read:synchronizedList\\\"); //循环1000000次并发从加锁的ArrayList随机查询元素 IntStream.range(0, loopCount).parallel().forEach(__ -> synchronizedList.get(ThreadLocalRandom.current().nextInt(count))); stopWatch.stop(); log.info(stopWatch.prettyPrint()); Map result = new HashMap(); result.put(\\\"copyOnWriteArrayList\\\", copyOnWriteArrayList.size()); result.put(\\\"synchronizedList\\\", synchronizedList.size()); return result; } \",\"运行程序可以看到，大量写的场景（10 万次 add 操作），CopyOnWriteArray 几乎比同步的 ArrayList 慢一百倍：\",\"而在大量读的场景下（100 万次 get 操作），CopyOnWriteArray 又比同步的 ArrayList 快五倍以上：\",\"你可能会问，为何在大量写的场景下，CopyOnWriteArrayList 会这么慢呢？\",\"答案就在源码中。以 add 方法为例，每次 add 时，都会用 Arrays.copyOf 创建一个新数组，频繁 add 时内存的申请释放消耗会很大：\",\" /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e) { synchronized (lock) { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } } \"]},{\"header\":\"6、总结\",\"slug\":\"_6、总结\",\"contents\":[\"开发人员使用并发工具来解决线程安全问题时容易犯的四类错误，大家要注意避坑！\",\"一是，只知道使用并发工具，但并不清楚当前线程的来龙去脉，解决多线程问题却不了解线程。比如，使用 ThreadLocal 来缓存数据，以为 ThreadLocal 在线程之间做了隔离不会有线程安全问题，没想到线程重用导致数据串了。请务必记得，在业务逻辑结束之前清理 ThreadLocal 中的数据。\",\"二是，误以为使用了并发工具就可以解决一切线程安全问题，期望通过把线程不安全的类替换为线程安全的类来一键解决问题。比如，认为使用了 ConcurrentHashMap 就可以解决线程安全问题，没对复合逻辑加锁导致业务逻辑错误。如果你希望在一整段业务逻辑中，对容器的操作都保持整体一致性的话，需要加锁处理。\",\"三是，没有充分了解并发工具的特性，还是按照老方式使用新工具导致无法发挥其性能。比如，使用了 ConcurrentHashMap，但没有充分利用其提供的基于 CAS 安全的方法，还是使用锁的方式来实现逻辑。你可以阅读一下ConcurrentHashMap 的文档，看一下相关原子性操作 API 是否可以满足业务需求，如果可以则优先考虑使用。\",\"四是，没有了解清楚工具的适用场景，在不合适的场景下使用了错误的工具导致性能更差。比如，没有理解 CopyOnWriteArrayList 的适用场景，把它用在了读写均衡或者大量写操作的场景下，导致性能问题。对于这种场景，你可以考虑是用普通的 List。\",\"其实，这四类坑之所以容易踩到，原因可以归结为，我们在使用并发工具的时候，并没有充分理解其可能存在的问题、适用场景等。所以最后，我还要和你分享两点建议：\",\"一定要认真阅读官方文档（比如 Oracle JDK 文档）。充分阅读官方文档，理解工具的适用场景及其 API 的用法，并做一些小实验。了解之后再去使用，就可以避免大部分坑。\",\"如果你的代码运行在多线程环境下，那么就会有并发问题，并发问题不那么容易重现，可能需要使用压力测试模拟并发场景，来发现其中的 Bug 或性能问题。\"]}]},\"/lick-brick-java/java-dev/AlibabaDevelopmentManual/integer-cache.html\":{\"title\":\"1、Integer缓存问题引发的分析\",\"contents\":[{\"header\":\"1、引言\",\"slug\":\"_1、引言\",\"contents\":[\"阿里巴巴Java开发手册在第一章节，编程规约中OOP规约的第7条提到：\",\"**【强制】**所有整型包装类对象之间值的比较，全部使用equals方法比较。 说明：对于Integer var = ? 在-128至127之间的赋值，Integer对象是在 IntegerCache.cache产生，会复用已有对象，这个区间内的Integer值可以直接使用==进行判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，推荐使用equals方法进行判断。\",\"这条建议非常值得大家关注， 而且该问题在 Java 面试中十分常见，看到这条建议，大家是否有以下疑问：\",\"如何知道对于Integer var = ? 会缓存-128至127之间的赋值？\",\"为什么会缓存这个范围的值，而不是其他呢？\"]},{\"header\":\"2、分析\",\"slug\":\"_2、分析\",\"contents\":[\"我们先看下面测试代码：\",\"public class IntegerTest { public static void main(String[] args) { Integer a = 99, b = 99, c = 188, d = 188; System.out.println(a == b); System.out.println(c == d); } } \",\"这个结果应该一眼就可以看出来，分别是true、false\",\"为什么是这样的结果呢？\",\"相信很多人都知道，Integer的值范围为-128到127，超过就会创建一个新的Integer对象，其实这里的值范围在jdk源码中是指缓存的值范围\",\"那么为什么会缓存这一段区间的数值？缓存的区间可以修改吗？其它的包装类型有没有类似缓存？\"]},{\"header\":\"1、源码分析\",\"slug\":\"_1、源码分析\",\"contents\":[\"我们知道， Integer var = ? 形式声明变量，会通过 java.lang.Integer#valueOf(int) 来构造 Integer 对象。\",\"/** * 返回表示指定的 {@code Integer} 实例 * {@code int} 值。 如果一个新的 {@code Integer} 实例不是 * 必需，通常应优先使用此方法 * 构造函数{@link #Integer(int)}，因为这个方法很可能 * 产生明显更好的空间和时间性能 *缓存经常请求的值。 * * 此方法将始终缓存 -128 到 127 范围内的值， * 包括在内，并且可能缓存此范围之外的其他值。 * * @param i 一个 {@code int} 值。 * @return 一个代表 {@code i} 的 {@code Integer} 实例。 * @since 1.5 */ public static Integer valueOf(int i) { if (i >= IntegerCache.low && i <= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } \",\"通过源码可以看出，如果用 Ineger.valueOf(int) 来创建整数对象，参数大于等于整数缓存的最小值（ IntegerCache.low ）并小于等于整数缓存的最大值（ IntegerCache.high）, 会直接从缓存数组 ( java.lang.Integer.IntegerCache#cache ) 中提取整数对象；否则会 new 一个整数对象。\"]},{\"header\":\"1、为什么会缓存这一段区间的整数对象呢？\",\"slug\":\"_1、为什么会缓存这一段区间的整数对象呢\",\"contents\":[\"通过注释我们可以得知：如果不要求必须新建一个整型对象，缓存最常用的值（提前构造缓存范围内的整型对象），会更省空间，速度也更快。\",\"这给我们一个非常重要的启发：如果想减少内存占用，提高程序运行的效率，可以将常用的对象提前缓存起来，需要时直接从缓存中提取。\"]},{\"header\":\"2、下一个问题：缓存的区间可以修改吗？\",\"slug\":\"_2、下一个问题-缓存的区间可以修改吗\",\"contents\":[\"上述的源码看不出来，接着划到IntegerCache静态类源码的部分\",\"/** *缓存以支持自动装箱的对象标识语义之间的值 * JLS 要求的 -128 和 127（含）。 * * 缓存在第一次使用时初始化。 缓存的大小 * 可以由 {@code -XX:AutoBoxCacheMax=<size>} 选项控制。 * VM初始化时，java.lang.Integer.IntegerCache.high属性 * 可以设置并保存在私有系统属性中 * sun.misc.VM 类。 */ private static class IntegerCache { //最小的值 static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; //缓存的最大值是可以通过虚拟机参数 -XX:AutoBoxCacheMax=<size>} 或 -Djava.lang.Integer.IntegerCache.high=<value> 来设置的 String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\\\"java.lang.Integer.IntegerCache.high\\\"); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE MAX_VALUE = 0x7fffffff; h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k < cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high >= 127; } private IntegerCache() {} } \",\"从上述源码中可以得知：\",\"最小的值是确定的，为low = -128\",\"最大值并不是固定的 \",\"缓存的最大值是可以通过虚拟机参数 -XX:AutoBoxCacheMax=size} 或 -Djava.lang.Integer.IntegerCache.high=value来设置的\",\"当integerCacheHighPropValue不为null时，首先会取一个中间值i = Math.max(i, 127);然后将中间值与Integer的最大值进行比较，取最小的那个赋值给最大值\",\"因此，可以通过虚拟机参数修改Integer的最大值，让缓存的最大值大于160，则最开始的测试代码全为true\",\"现在回答我们的问题\",\"注释上也解释了，是为了自动装箱时可以复用这些对象 ，这也是 JLS2 的要求。我们可以参考 JLS 的 Boxing Conversion 部分的相关描述。 if the value p being boxed is an integer literal of type int between -128 and 127 inclusive (§3.10.1), or the boolean literal true or false (§3.10.3), or a character literal between '\\\\u0000' and '\\\\u007f' inclusive (§3.10.4), then let a and b be the results of any two boxing conversions of p . It is always the case that a == b .\",\"在 -128 到 127 （含）之间的 int 类型的值，或者 boolean 类型的 true 或 false， 以及范围在’\\\\u0000’和’\\\\u007f’ （含）之间的 char 类型的数值 p， 自动包装成 a 和 b 两个对象时， 可以使用 a == b 判断 a 和 b 的值是否相等。\"]},{\"header\":\"2、反编译\",\"slug\":\"_2、反编译\",\"contents\":[\"首先编译源代码： javac IntegerTest.java\",\"然后需要对代码进行反汇编，执行： javap -c IntegerTest\",\"如果想了解 javap 的用法，直接输入 javap -help 查看用法提示（很多命令行工具都支持 -help 或 --help 给出用法提示）。\",\"反编译后，我们得到以下代码：\",\"Compiled from \\\"IntegerTest.java\\\" public class com.javaxiaobear.oop.IntegerTest { public com.javaxiaobear.oop.IntegerTest(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\\\"<init>\\\":()V 4: return public static void main(java.lang.String[]); Code: 0: bipush 99 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: bipush 99 8: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 11: astore_2 12: sipush 188 15: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 18: astore_3 19: sipush 188 22: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 25: astore 4 27: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 30: aload_1 31: aload_2 32: if_acmpne 39 35: iconst_1 36: goto 40 39: iconst_0 40: invokevirtual #4 // Method java/io/PrintStream.println:(Z)V 43: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 46: aload_3 47: aload 4 49: if_acmpne 56 52: iconst_1 53: goto 57 56: iconst_0 57: invokevirtual #4 // Method java/io/PrintStream.println:(Z)V 60: return } \",\"可以明确得 \\\"看到\\\" 这四个 ``Integer var = ? 形式声明的变量的确是通过 java.lang.Integer#valueOf(int) 来构造 Integer` 对象的。\",\"接下来对汇编后的代码进行详细分析，如果看不懂可略过：\",\"根据《Java Virtual Machine Specification : Java SE 8 Edition》3，后缩写为 JVMS , 第 6 章 虚拟机指令集的相关 描述以及《深入理解 Java 虚拟机》4 414-149 页的 附录 B “虚拟机字节码指令表”。 我们对上述指令进行解读：\",\"偏移为 0 的指令为： bipush 99，其含义是将单字节整型常量 99推入操作数栈的栈顶；\",\"偏移为 2 的指令为： invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 表示调用一个 static 函 数，即 java.lang.Integer#valueOf(int) ；\",\"偏移为 5 的指令为： astore_1 ，其含义是从操作数栈中弹出对象引用，然后将其存到第 1 个局部变量 Slot 中；\",\"偏移 6 到 25 的指令和上面类似；\",\"偏移为 30 的指令为 aload_1 ，其含义是从第 1 个局部变量 Slot 取出对象引用（即 a），并将其压入栈；\",\"偏移为 31 的指令为 aload_2 ，其含义是从第 2 个局部变量 Slot 取出对象引用（即 b），并将其压入栈；\",\"偏移为 32 的指令为 if_acmpn，该指令为条件跳转指令， if_ 后以 a 开头表示对象的引用比较。 由于该指令有以下特性： if_acmpeq 比较栈两个引用类型数值，相等则跳转 if_acmpne 比较栈两个引用类型数值，不相等则跳转\",\"由于 Integer 的缓存问题，所以 a 和 b 引用指向同一个地址，因此此条件不成立（成立则跳转到偏移为 39 的指令 处），执行偏移为 35 的指令。\",\"偏移为 35 的指令: iconst_1 ，其含义为将常量 1 压栈（ Java 虚拟机中 boolean 类型的运算类型为 int ，其中 true 用 1 表示，详见 2.11.1 数据类型和 Java 虚拟机。\",\"然后执行偏移为 36 的 goto 指令，跳转到偏移为 40 的指令。\",\"偏移为 40 的指令： invokevirtual #4 // Method java/io/PrintStream.println:(Z)V 。 可知参数描述符为 Z ，返回值描述符为 V 。\",\"根据 4.3.2 字段描述符 ，可知 FieldType 的字符为 Z 表示 boolean 类型， 值为 true 或 false 。\",\"根据 4.3.3 字段描述符 ，可知返回值为 void 。\",\"因此可以知，最终调用了 java.io.PrintStream#println(boolean) 函数打印栈顶常量即 true 。\",\"然后比较执行偏移 43 到 57 之间的指令，比较 c 和 d， 打印 false 。\",\"执行偏移为 60 的指令，即 retrun ，程序结束。\",\"可能看这个有些生疏和抵触，一开始都是这样的，多编译几次看几次，就好了。我们重点不是是分析和研究问题，看懂核心逻辑即可，不要过分纠结于细节。\",\"如果想深入学习 java 反汇编，强烈建议结合官方的 JVMS 或其中文版:《Java 虚拟机规范》这本书进行拓展学习。\",\"如果大家不喜欢命令行的方式进行 Java 的反汇编，这里推荐一个简单易用的可视化工具：classpy ，大家可以自行了解学习。\"]},{\"header\":\"3、Long缓存问题\",\"slug\":\"_3、long缓存问题\",\"contents\":[\"前面我们分析了Integer缓存问题，举一反三，对Long也进行研究，看看二者有何不同\"]},{\"header\":\"1、源码分析\",\"slug\":\"_1、源码分析-1\",\"contents\":[\"类似的，我们接下来分析 java.lang.Long#valueOf(long) 的源码：\",\"/** * 返回代表指定的 {@code Long} 实例 * {@code long} 值。 * 如果不需要新的 {@code Long} 实例，则此方法 * 通常应该优先于构造函数使用 * {@link #Long(long)}，因为这种方法可能会产生 * 通过缓存显着提高空间和时间性能 * 经常要求的值。 * * 请注意，与 {@linkplain Integer#valueOf(int) * {@code Integer}类中对应的方法}，这个方法 * <em>不需要</em>在特定的缓存中缓存值 * 范围。 * * @param l 长值。 * @return 代表 {@code l} 的 {@code Long} 实例。 * @since 1.5 */ public static Long valueOf(long l) { final int offset = 128; if (l >= -128 && l <= 127) { // will cache return LongCache.cache[(int)l + offset]; } return new Long(l); } \",\"从源码中可以看到，如果long类型的变量值在[-128,127]之间的话，会直接从缓存对象中提取\",\"而且注释同样也提到了：缓存的目的是为了提高性能。\",\"Note that unlike the {@linkplain Integer#valueOf(int) corresponding method} in the {@code Integer} class, this method is not required to cache values within a particular range.\",\"注意：和 Ineger.valueOf(int) 不同的是，此方法并没有被要求缓存特定范围的值。\",\"这也正是上面源码中缓存范围判断的注释为何用 // will cache 的原因（可以对比一下上面 Integer 的缓存的注释）。\",\"因此我们可知，虽然此处采用了缓存，但应该不是 JLS 的要求。那么 Long 类型的缓存是如何构造的呢？\",\"接着继续往下看，查看缓存数组的构造\",\"private static class LongCache { private LongCache(){} static final Long cache[] = new Long[-(-128) + 127 + 1]; static { for(int i = 0; i < cache.length; i++) cache[i] = new Long(i - 128); } } \",\"可以看到，它是在静态代码块中填充缓存数组的\"]},{\"header\":\"2、反编译\",\"slug\":\"_2、反编译-1\",\"contents\":[\"public class com.javaxiaobear.oop.LongTest { public com.javaxiaobear.oop.LongTest(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\\\"<init>\\\":()V 4: return public static void main(java.lang.String[]); Code: 0: ldc2_w #7 // long -128l 3: invokestatic #9 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 6: astore_1 7: ldc2_w #7 // long -128l 10: invokestatic #9 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 13: astore_2 14: ldc2_w #15 // long 1001l 17: invokestatic #9 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 20: astore_3 21: ldc2_w #15 // long 1001l 24: invokestatic #9 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 51: if_acmpne 58 54: iconst_1 55: goto 59 58: iconst_0 59: invokevirtual #23 // Method java/io/PrintStream.println:(Z)V 62: return } \",\"我们从上述代码中发现 Long var = ? 的确是通过 java.lang.Long#valueOf(long) 来构造对象的。\"]},{\"header\":\"4、其他基本类型的缓存\",\"slug\":\"_4、其他基本类型的缓存\",\"contents\":[\"//boolean原生类型自动装箱成Boolean public static Boolean valueOf(boolean b) { return (b ? TRUE : FALSE); } //byte原生类型自动装箱成Byte public static Byte valueOf(byte b) { final int offset = 128; return ByteCache.cache[(int)b + offset]; } //byte原生类型自动装箱成Byte public static Short valueOf(short s) { final int offset = 128; int sAsInt = s; if (sAsInt >= -128 && sAsInt <= 127) { // must cache return ShortCache.cache[sAsInt + offset]; } return new Short(s); } //char原生类型自动装箱成Character public static Character valueOf(char c) { if (c <= 127) { // must cache return CharacterCache.cache[(int)c]; } return new Character(c); } //int原生类型自动装箱成Integer public static Integer valueOf(int i) { if (i >= IntegerCache.low && i <= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } //int原生类型自动装箱成Long public static Long valueOf(long l) { final int offset = 128; if (l >= -128 && l <= 127) { // will cache return LongCache.cache[(int)l + offset]; } return new Long(l); } //double原生类型自动装箱成Double public static Double valueOf(double d) { return new Double(d); } //float原生类型自动装箱成Float public static Float valueOf(float f) { return new Float(f); } \",\"从valueOf方法就可以看得出来，除了Boolean、Double和Float没有缓存以外，其余的包装类型都有缓存实现。各个包装类型对应的缓存值范围如下：\",\"基本类型\",\"大小\",\"包装类型\",\"缓存范围\",\"是否支持自定义缓存范围\",\"boolean\",\"6bit\",\"Bloolean\",\"/\",\"/\",\"char\",\"8bit\",\"Character\",\"0~127\",\"否\",\"byte\",\"8bit\",\"Byte\",\"-128~127\",\"否\",\"short\",\"16bit\",\"Short\",\"-128~127\",\"否\",\"int\",\"32bit\",\"Integer\",\"-128~127\",\"支持，通过虚拟机参数设定\",\"long\",\"64bit\",\"Long\",\"-128~127\",\"否\",\"float\",\"32bit\",\"Float\",\"/\",\"/\",\"double\",\"64bit\",\"Double\",\"/\",\"/\"]},{\"header\":\"5、总结\",\"slug\":\"_5、总结\",\"contents\":[\"我们通过源码分析和反编译进行Integer和Long的缓存分析，其他基本类型的缓存分析结果也一并分析并展示了\",\"总的来说，缓存的目的：就是为了提高性能，复用这些对象\",\"这就跟我们项目中大部分用到缓存的目的是一样的\"]},{\"header\":\"6、参考资料\",\"slug\":\"_6、参考资料\",\"contents\":[\"阿里巴巴与 Java 社区开发者.《 Java 开发手册 1.7.0》崇山版.\",\"James Gosling, Bill Joy, Guy Steele, Gilad Bracha, Alex Buckley.《Java Language Specification: Java SE 8Edition》. 2015\",\"周志明.《深入理解 Java 虚拟机》. 机械工业出版社. 2018\"]}]},\"/lick-brick-java/java-dev/AlibabaDevelopmentManual/serialization.html\":{\"title\":\"2、Java序列化引发的血案\",\"contents\":[{\"header\":\"1、引言\",\"slug\":\"_1、引言\",\"contents\":[\"阿里巴巴Java开发手册在第一章节，编程规约中OOP规约的第15条提到：\",\"**【强制】**序列化类新增属性时，请不要修改serialVersionUID字段，避免反序列失败；如果完全不兼容升级，避免反序列化混乱，那么请修改serialVersionUID值。\",\"说明：注意serialVersionUID不一致会抛出序列化运行时异常\",\"如果没接触过序列化的人，应该会有以下疑问：\",\"序列化和反序列化到底是什么？\",\"它的主要使用场景有哪些？\",\"Java 序列化常见的方案有哪些？\",\"各种常见序列化方案的区别有哪些？\",\"实际的业务开发中有哪些坑点？\"]},{\"header\":\"2、什么是序列化和反序列化\",\"slug\":\"_2、什么是序列化和反序列化\",\"contents\":[\"序列化是将内存中的对象信息转化成可以存储或者传输的数据到临时或永久存储的过程。在Java中其实就是把Java对象转换为二进制内容，其本质就是一个byte[]数组 反序列化是从临时或永久存储中读取序列化的数据并转化成内存对象的过程。在Java中就是将一个byte[]转换为Java对象的过程\"]},{\"header\":\"3、为什么需要序列化和反序列化呢？\",\"slug\":\"_3、为什么需要序列化和反序列化呢\",\"contents\":[\"大家可以回忆一下，平时都是如果将文字文件、图片文件、视频文件、软件安装包等传给小伙伴时，这些资源在计算机中存储的方式是怎样的。 进而再思考，Java 中的对象如果需要存储或者传输应该通过什么形式呢？\",\"我们都知道，一个文件通常是一个 m 个字节的序列：B0, B1, …, Bk, …, Bm-1。所有的 I/O 设备（例如网络、磁盘和终端）都被模型化为文件，而所有的输入和输出都被当作对应文件的读和写来执行。\",\"因此本质上讲，文本文件，图片、视频和安装包等文件底层都被转化为二进制字节流来传输的，对方得文件就需要对文件进行解析，因此就需要有能够根据不同的文件类型来解码出文件的内容的程序。\",\"大家试想一个典型的场景：如果要实现 Java 远程方法调用，就需要将调用结果通过网路传输给调用方，如果调用方和服务提供方不在一台机器上就很难共享内存，就需要将 Java 对象进行传输。而想要将 Java 中的对象进行网络传输或存储到文件中，就需要将对象转化为二进制字节流，这就是所谓的序列化。存储或传输之后必然就需要将二进制流读取并解析成 Java 对象，这就是所谓的反序列化。\",\"序列化的主要目的是：方便存储到文件系统、数据库系统或网络传输等。\"]},{\"header\":\"4、序列化和反序列化的使用场景\",\"slug\":\"_4、序列化和反序列化的使用场景\",\"contents\":[\"远程方法调用（RPC）的框架里会用到序列化\",\"将对象存储到文件中时，需要用到序列化\",\"将对象存储到缓存数据库（如 Redis）时需要用到序列化\",\"通过序列化和反序列化的方式实现对象的深拷贝\"]},{\"header\":\"5、常见的序列化方式\",\"slug\":\"_5、常见的序列化方式\",\"contents\":[\"常见的序列化方式包括 Java 原生序列化、Hessian 序列化、Kryo 序列化、JSON 序列化等。\"]},{\"header\":\"1、Java原生序列化\",\"slug\":\"_1、java原生序列化\",\"contents\":[\"学习的最好方式就是查看源码，我们接下来查看一下Serializable的源码\",\"public interface Serializable { } \",\"源码非常简单，什么方法都没有，但是注释很长，其核心就是：\",\"Java 原生序列化需要实现 Serializable 接口。序列化接口不包含任何方法和属性等，它只起到序列化标识作用。\",\"一个类实现序列化接口则其子类型也会继承序列化能力，但是实现序列化接口的类中有其他对象的引用，则其他对象也要实现序列化接口。序列化时如果抛出 NotSerializableException 异常，说明该对象没有实现 Serializable 接口。\",\"每个序列化类都有一个叫 serialVersionUID 的版本号，反序列化时会校验待反射的类的序列化版本号和加载的序列化字节流中的版本号是否一致，如果序列化号不一致则会抛出 InvalidClassException 异常。\",\"强烈推荐每个序列化类都手动指定其 serialVersionUID ，如果不手动指定，那么编译器会动态生成默认的序列化号，因为这个默认的序列化号和类的特征以及编译器的实现都有关系，很容易在反序列化时抛出 InvalidClassException 异常。建议将这个序列化版本号声明为私有，以避免运行时被修改。\",\"实现序列化接口的类可以提供自定义的函数修改默认的序列化和反序列化行为。\",\"上面注释也说明，建议序列化版本号声明为私有，以避免运行时被修改。\",\"如果一个类文件序列化到文件后，类的结构发生了改变，是否能被正确的反序列化？\",\"这个答案是不确定的。 通常我们是通过加密算法对文件进行前面，根据签名判断文件是否被修改；但Java序列化的场景并不适用于上述的方案，如果在类文件的某个地方加个空格，执行等符号类的结构，没有发生变化，这个时候签名就不应该发生变；还有一个类新增一个属性，之前的属性都是有值的，之前都被序列化到对象文件中，有些场景下还希望反序列化时可以正常解析，怎么办呢？\",\"序列化测试代码：\",\"public class SerializationTest { public static void main(String[] args) throws IOException { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); try (ObjectOutputStream output = new ObjectOutputStream(buffer)) { // 写入byte: output.writeBytes(\\\"小熊学Java\\\"); // 写入String: output.writeUTF(\\\"Hello\\\"); // 写入Object: output.writeObject(\\\"javaxiaobear\\\"); } System.out.println(Arrays.toString(buffer.toByteArray())); } } \"]},{\"header\":\"2、Hessian 序列化\",\"slug\":\"_2、hessian-序列化\",\"contents\":[\"Hessian 是一个动态类型，二进制序列化，也是一个基于对象传输的网络协议。Hessian 是一种跨语言的序列化方案，序列化后的字节数更少，效率更高。Hessian 序列化会把复杂对象的属性映射到 Map 中再进行序列化。 官方介绍👉Hessian 2.0 Serialization Protocol\",\"和JDK自带的序列化方式类似，Hessian采用的也是二进制协议，只不过Hessian序列化之后，字节数更小，性能更优。目前Hessian已经出到2.0版本，相较于1.0的Hessian性能更优。相较于JDK自带的序列化，Hessian的设计目标更明确👇\",\"Hessian 是动态类型的、紧凑的，并且可以跨语言移植。Hessian 协议有以下设计目标：\",\"它必须是单次可读或可写的。\",\"它必须尽可能紧凑。\",\"它必须简单，以便可以有效地测试和实施。\",\"它必须尽可能快。\",\"它必须支持 Unicode 字符串。\",\"它必须支持 8 位二进制数据而不转义或使用附件。\",\"它必须支持加密、压缩、签名和事务上下文信封。\",\"Hessian的序列化速度相较于JDK序列化才更快。只不过Java序列化会把要序列化的对象类的元数据和业务数据全部序列化从字节流，并且会保留完整的继承关系，因此相较于Hessian序列化更加可靠。\",\"不过相较于JDK的序列化，Hessian另一个优势在于，这是一个跨语言的序列化方式，这意味着序列化后的数据可以被其他语言使用，兼容性更好。\",\"基础使用\",\"引入pom依赖\",\"<!-- https://mvnrepository.com/artifact/com.caucho/hessian --> <dependency> <groupId>com.caucho</groupId> <artifactId>hessian</artifactId> <version>4.0.65</version> </dependency> \",\"不服，咱跑个分\",\"public class SerializationTest { public static void main(String[] args) throws IOException { String javaxiaobear = \\\"小熊学Java\\\"; System.out.println(\\\"JDK序列化长度：\\\" + jdkSerialize(javaxiaobear).length); System.out.println(\\\"hessian序列化长度：\\\" + hessianSerialize(javaxiaobear).length); } /** * jdk序列化测试 * @param str * @return * @param <T> */ public static <T> byte[] jdkSerialize(T str){ byte[] data = null; try{ ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream output = new ObjectOutputStream(byteArrayOutputStream); output.writeObject(str); output.flush(); output.close(); data = byteArrayOutputStream.toByteArray(); }catch (Exception e){ e.printStackTrace(); } return data; } /** * hessian序列化测试 * @param str * @return * @param <T> */ public static <T> byte[] hessianSerialize(T str){ byte[] data = null; try{ ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Hessian2Output output = new Hessian2Output(byteArrayOutputStream); output.writeObject(str); output.flush(); output.close(); data = byteArrayOutputStream.toByteArray(); }catch (Exception e){ e.printStackTrace(); } return data; } } \",\"输出结果： //JDK序列化长度：20 //hessian序列化长度：14\"]},{\"header\":\"3、Kryo 序列化\",\"slug\":\"_3、kryo-序列化\",\"contents\":[\"Kryo 是一个快速高效的 Java 序列化和克隆工具。Kryo 的目标是快速、字节少和易用。Kryo 还可以自动进行深拷贝或者浅拷贝。Kryo 的拷贝是对象到对象的拷贝而不是对象到字节，再从字节到对象的恢复。Kryo 为了保证序列化的高效率，会提前加载需要的类，这会带一些消耗，但是这是序列化后文件较小且反序列化非常快的重要原因。 官方地址：kryo\",\"基础使用 这里只作为基础使用，不作为重点讲解，需要了解的可以去查看官方文档哈\",\"引入pom依赖，这里需要JDK11编译哦\",\"<dependency> <groupId>com.esotericsoftware</groupId> <artifactId>kryo</artifactId> <version>5.4.0</version> </dependency> \",\"测试demo\",\"public static void main(String[] args) throws IOException { String javaxiaobear = \\\"小熊学Java\\\"; System.out.println(\\\"JDK序列化长度：\\\" + jdkSerialize(javaxiaobear).length); System.out.println(\\\"hessian序列化长度：\\\" + hessianSerialize(javaxiaobear).length); User user = new User(\\\"小熊学Java\\\"); byte[] bytes = kryoSerialize(user); System.out.println(\\\"kryo序列化的长度：\\\" + bytes.length); } /** * kryo序列化 * @param user * @return */ public static byte[] kryoSerialize(User user) { Kryo kryo = new Kryo(); kryo.register(user.getClass()); ByteArrayOutputStream bos = new ByteArrayOutputStream(); Output output = new Output(bos); //写入null时会报错 kryo.writeObject(output,user); output.close(); return bos.toByteArray(); } \",\"结果：kryo序列化的长度：14\"]},{\"header\":\"4、JSON 序列化\",\"slug\":\"_4、json-序列化\",\"contents\":[\"JSON (JavaScript Object Notation) 是一种轻量级的数据交换方式。JSON 序列化是基于 JSON 这种结构来实现的。JSON 序列化将对象转化成 JSON 字符串，JSON 反序列化则是将 JSON 字符串转回对象的过程。常用的JSON 序列化和反序列化的库有 Jackson、GSON、Fastjson 等。\"]},{\"header\":\"1、GSON\",\"slug\":\"_1、gson\",\"contents\":[\"Gson提供了fromJson() 和toJson() 两个直接用于解析和生成的方法，前者实现反序列化，后者实现了序列化；同时每个方法都提供了重载方法。\",\"跑个demo\",\"/** * Gson 序列化 与反序列化 * @param user */ public static void gsonSerialize(User user){ //gson序列化 String userJson = new Gson().toJson(user); System.out.println(\\\"gson序列化后的值：\\\" + userJson); //gson反序列化 User user1 = new Gson().fromJson(userJson, User.class); System.out.println(\\\"gson反序列化后：\\\" + user1.toString()); } \"]},{\"header\":\"6、Java 常见的序列化方案对比\",\"slug\":\"_6、java-常见的序列化方案对比\",\"contents\":[\"实验的版本：kryo-shaded 使用 5.4.0版本，gson 使用 2.8.5 版本，hessian 用 4.0.65 版本。 实验的数据：构造 50 万 User 对象运行多次。 大致得出一个结论：\",\"从二进制流大小来讲：JSON 序列化 > Java 序列化 > Hessian2 序列化 > Kryo 序列化 > Kryo 序列化注册模式；\",\"从序列化耗时而言来讲：GSON 序列化 > Java 序列化 > Kryo 序列化 > Hessian2 序列化 > Kryo 序列化注册模式；\",\"从反序列化耗时而言来讲：GSON 序列化 > Java 序列化 > Hessian2 序列化 > Kryo 序列化注册模式 > Kryo序列化；\",\"从总耗时而言：Kryo 序列化注册模式耗时最短。\"]},{\"header\":\"7、序列化引发的一个血案\",\"slug\":\"_7、序列化引发的一个血案\",\"contents\":[\"我们看下面的一个案例\",\"前端调用服务 A，服务 A 调用服务 B，服务 B 首次接到请求会查 DB，然后缓存到 Redis（缓存 1 个小时）。服务 A 根据服务 B 返回的数据后执行一些处理逻辑，处理后形成新的对象存到 Redis（缓存 2 个小时）。服务 A 通过 Dubbo 来调用服务 B，A 和 B 之间数据通过 Map<String,Object> 类型传输，服务 B 使用Fastjson 来实现 JSON 的序列化和反序列化。\",\"服务 B 的接口返回的 Map 值中存在一个 Long 类型的 id 字段，服务 A 获取到 Map ，取出 id 字段并强转为 Long 类型使用。\",\"执行流程如下：\",\"通过分析我们发现，服务 A 和服务 B 的 RPC 调用使用 Java 序列化，因此类型信息不会丢失。\",\"但是由于服务 B 采用 JSON 序列化进行缓存，第一次访问没啥问题，其执行流程如下：\",\"如果服务 A开启了缓存 ，服务 A 在第一次请求服务 B 后，缓存了运算结果，且服务 A 缓存时间比服务 B 长，因此不会出现错误。\",\"如果服务 A 不开启缓存 ，服务 A 会请求服务 B ，由于首次请求时，服务 B 已经缓存了数据，服务 B 从Redis（B）中反序列化得到 Map 。流程如下图所示：\",\"然而问题来了： 服务 A 从 Map 取出此 Id 字段，强转为 Long 时会出现类型转换异常。\",\"最后定位到原因是 Json 反序列化 Map 时如果原始值小于 Int 最大值，反序列化后原本为 Long 类型的字段，变为了 Integer 类型，服务 B 的同学紧急修复。\",\"服务 A 开启缓存时， 虽然采用了 JSON 序列化存入缓存，但是采用 DTO 对象而不是 Map 来存放属性，所以JSON 反序列化没有问题。 因此大家使用二方或者三方服务时，当对方返回的是 Map<String,Object> 类型的数据时要特别注意这个问题。\",\"作为服务提供方，可以采用 JDK 或者 Hessian 等序列化方式；\",\"作为服务的使用方，我们不要从 Map 中一个字段一个字段获取和转换，可以使用 JSON 库直接将 Map 映射成所需的对象，这样做不仅代码更简洁还可以避免强转失败。\",\"来个demo\",\"@Test public void testFastJsonObject() { Map<String, Object> map = new HashMap<>(); final String name = \\\"name\\\"; final String id = \\\"id\\\"; map.put(name, \\\"张三\\\"); map.put(id, 20L); String fastJsonString = FastJsonUtil.getJsonString(map); // 模拟拿到服务B的数据 Map<String, Object> mapFastJson = FastJsonUtil.parseJson(fastJsonString,map.getClass()); // 转成强类型属性的对象而不是使用map 单个取值 User user = new JSONObject(mapFastJson).toJavaObject(User.class); // 正确 Assert.assertEquals(map.get(name), user.getName()); // 正确 Assert.assertEquals(map.get(id), user.getId()); } \"]},{\"header\":\"8、总结\",\"slug\":\"_8、总结\",\"contents\":[\"主要描述了Java序列化的场景和使用，以及案例分析，在开发中我们还是要注意细节，避开趟坑！\"]}]},\"/study-tutorial/components/report/easypoi.html\":{\"title\":\"6、EasyPOI\",\"contents\":[{\"header\":\"1、简介\",\"slug\":\"_1、简介\",\"contents\":[\"原文档地址：https://easypoi.mydoc.io/ （比较推荐）\",\"新地址：http://www.wupaas.com/\",\"easypoi功能如同名字easy,主打的功能就是容易,让一个没见接触过poi的人员 就可以方便的写出Excel导出,Excel模板导出,Excel导入,Word模板导出,通过简单的注解和模板 语言(熟悉的表达式语法),完成以前复杂的写法\",\"Easypoi的目标是什么\",\"Easypoi的目标不是替代poi,而是让一个不懂导入导出的快速使用poi完成Excel和word的各种操作,而不是看很多api才可以完成这样工作\",\"独特的功能\",\"基于注解的导入导出,修改注解就可以修改Excel\",\"支持常用的样式自定义\",\"基于map可以灵活定义的表头字段\",\"支持一堆多的导出,导入\",\"支持模板的导出,一些常见的标签,自定义标签\",\"支持HTML/Excel转换,如果模板还不能满足用户的变态需求,请用这个功能\",\"支持word的导出,支持图片,Excel\",\"需要的依赖\",\"把项目中的poi的依赖去除\",\"<dependency> <groupId>cn.afterturn</groupId> <artifactId>easypoi-base</artifactId> <version>4.1.0</version> </dependency> <dependency> <groupId>cn.afterturn</groupId> <artifactId>easypoi-web</artifactId> <version>4.1.0</version> </dependency> <dependency> <groupId>cn.afterturn</groupId> <artifactId>easypoi-annotation</artifactId> <version>4.1.0</version> </dependency> \",\"SpringBoot版本\",\"<dependency> <groupId>cn.afterturn</groupId> <artifactId>easypoi-spring-boot-starter</artifactId> <version>4.1.0</version> </dependency> \"]},{\"header\":\"2、Excel操作\",\"slug\":\"_2、excel操作\",\"contents\":[]},{\"header\":\"1、导出\",\"slug\":\"_1、导出\",\"contents\":[\"注解方式导出\",\"修改实体类，添加注解，用到的是@Excel注解，使用如下，必须要有空构造函数，否则会报“对象创建错误”\",\"| 属性 | 类型 | 类型 | 说明 | | -------------- | -------- | ---------------- | ------------------------------------------------------------ | | name | String | null | 列名 | | needMerge | boolean | fasle | 纵向合并单元格 | | orderNum | String | \\\"0\\\" | 列的排序,支持name_id | | replace | String[] | {} | 值得替换 导出是{a_id,b_id} 导入反过来 | | savePath | String | \\\"upload\\\" | 导入文件保存路径 | | type | int | 1 | 导出类型 1 是文本 2 是图片,3 是函数,10 是数字 默认是文本 | | width | double | 10 | 列宽 | | height | double | 10 | 列高,后期打算统一使用@ExcelTarget的height,这个会被废弃,注意 | | isStatistics | boolean | fasle | 自动统计数据,在追加一行统计,把所有数据都和输出这个处理会吞没异常,请注意这一点 | | isHyperlink | boolean | false | 超链接,如果是需要实现接口返回对象 | | isImportField | boolean | true | 校验字段,看看这个字段是不是导入的Excel中有,如果没有说明是错误的Excel,读取失败,支持name_id | | exportFormat | String | \\\"\\\" | 导出的时间格式,以这个是否为空来判断是否需要格式化日期 | | importFormat | String | \\\"\\\" | 导入的时间格式,以这个是否为空来判断是否需要格式化日期 | | format | String | \\\"\\\" | 时间格式,相当于同时设置了exportFormat 和 importFormat | | databaseFormat | String | \\\"yyyyMMddHHmmss\\\" | 导出时间设置,如果字段是Date类型则不需要设置 数据库如果是string类型,这个需要设置这个数据库格式,用以转换时间格式输出 | | numFormat | String | \\\"\\\" | 数字格式化,参数是Pattern,使用的对象是DecimalFormat | | imageType | int | 1 | 导出类型 1 从file读取 2 是从数据库中读取 默认是文件 同样导入也是一样的 | | suffix | String | \\\"\\\" | 文字后缀,如% 90 变成90% | | isWrap | boolean | true | 是否换行 即支持\\\\n | | mergeRely | int[] | {} | 合并单元格依赖关系,比如第二列合并是基于第一列 则{1}就可以了 | | mergeVertical | boolean | fasle | 纵向合并内容相同的单元格 |\",\"@Data @Table(name=\\\"tb_user\\\") public class User { @Id @KeySql(useGeneratedKeys = true) @Excel(name = \\\"编号\\\",orderNum = \\\"0\\\",width = 5) private Long id; //主键 @Excel(name = \\\"姓名\\\",orderNum = \\\"1\\\",width = 10) private String userName; //员工名 @Excel(name = \\\"手机号\\\",orderNum = \\\"2\\\",width = 15) private String phone; //手机号 @Excel(name = \\\"省份名\\\",orderNum = \\\"3\\\",width = 15) private String province; //省份名 @Excel(name = \\\"城市名\\\",orderNum = \\\"4\\\",width = 15) private String city; //城市名 @Excel(name = \\\"工资\\\",orderNum = \\\"5\\\",width = 5) private Integer salary; // 工资 @JsonFormat(pattern=\\\"yyyy-MM-dd\\\") @Excel(name = \\\"入职日期\\\",format = \\\"yyyy-MM-dd\\\",orderNum = \\\"6\\\",width = 15) private Date hireDate; // 入职日期 private String deptId; //部门id @Excel(name = \\\"出生日期\\\",format = \\\"yyyy-MM-dd\\\",orderNum = \\\"7\\\",width = 15) private Date birthday; //出生日期 @Excel(name = \\\"一寸照片\\\",orderNum = \\\"8\\\",width = 15, type = 2) private String photo; //一寸照片 @Excel(name = \\\"现在居住地址\\\",orderNum = \\\"9\\\",width = 15) private String address; //现在居住地址 private List<Resource> resourceList; //办公用品 } \",\"实现方法\",\"/** * easypoi导出excel * @param response * @throws Exception */ public void exportExcelByEasyPOI(HttpServletResponse response) throws Exception { //设置标题以及sheetName，excel导出类型 ExportParams params = new ExportParams(\\\"员工信息\\\",\\\"员工数据\\\", ExcelType.XSSF); //获取数据源 List<User> users = userMapper.selectAll(); org.apache.poi.ss.usermodel.Workbook workbook = ExcelExportUtil.exportExcel(params, User.class, users); String fileName = \\\"easypoi导出用户数据列表.xlsx\\\"; response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;filename=\\\"+new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(response.getOutputStream()); } \"]},{\"header\":\"2、导入\",\"slug\":\"_2、导入\",\"contents\":[\"有导出就应该有导入，我们就把刚才导出的数据库导入到表中\",\"Excel导入时需要的参数类ImportParams常用设置说明\",\"读取指定的sheet 比如要读取上传得第二个sheet 那么需要把startSheetIndex = 1 就可以了\",\"读取几个sheet 比如读取前2个sheet,那么 sheetNum=2 就可以了\",\"读取第二个到第五个sheet 设置 startSheetIndex = 1 然后sheetNum = 4\",\"读取全部的sheet sheetNum 设置大点就可以了\",\"保存Excel 设置 needVerfiy = true,默认保存的路径为upload/excelUpload/Test/yyyyMMddHHmss 保存名称上传时间五位随机数 如果自定义路径 修改下saveUrl 就可以了,同时saveUrl也是图片上传时候的保存的路径\",\"判断一个Excel是不是合法的Excel importFields 设置下值,就是表示表头必须至少包含的字段,如果缺一个就是不合法的excel,不导入\",\"图片的导入\",\"有图片的导出就有图片的导入,导入的配置和导出是一样的,但是需要设置保存路径 1.设置保存路径saveUrl 默认为\\\"upload/excelUpload\\\" 可以手动修改 ImportParams 修改下就可以了\",\"修改实体类，表明哪些需要导入，设置isImportField = \\\"true\\\"\",\"@Data @Table(name=\\\"tb_user\\\") public class User { @Id @KeySql(useGeneratedKeys = true) @Excel(name = \\\"编号\\\",orderNum = \\\"0\\\",width = 5, isImportField = \\\"true\\\") private Long id; //主键 @Excel(name = \\\"姓名\\\",orderNum = \\\"1\\\",width = 10, isImportField = \\\"true\\\") private String userName; //员工名 @Excel(name = \\\"手机号\\\",orderNum = \\\"2\\\",width = 15, isImportField = \\\"true\\\") private String phone; //手机号 @Excel(name = \\\"省份名\\\",orderNum = \\\"3\\\",width = 15, isImportField = \\\"true\\\") private String province; //省份名 @Excel(name = \\\"城市名\\\",orderNum = \\\"4\\\",width = 15, isImportField = \\\"true\\\") private String city; //城市名 @Excel(name = \\\"工资\\\",orderNum = \\\"5\\\",width = 5, isImportField = \\\"true\\\") private Integer salary; // 工资 @JsonFormat(pattern=\\\"yyyy-MM-dd\\\") @Excel(name = \\\"入职日期\\\",format = \\\"yyyy-MM-dd\\\",orderNum = \\\"6\\\",width = 15, isImportField = \\\"true\\\") private Date hireDate; // 入职日期 private String deptId; //部门id @Excel(name = \\\"出生日期\\\",format = \\\"yyyy-MM-dd\\\",orderNum = \\\"7\\\",width = 15, isImportField = \\\"true\\\") private Date birthday; //出生日期 @Excel(name = \\\"一寸照片\\\",orderNum = \\\"8\\\",width = 15, type = 2, isImportField = \\\"true\\\", savePath = \\\"D:\\\\\\\\28611\\\\\\\\idea-workspace\\\\\\\\xiaobear\\\\\\\\xiaobear-report\\\\\\\\xiaobear-report\\\\\\\\src\\\\\\\\main\\\\\\\\resources\\\\\\\\static\\\\\\\\user_photos\\\\\\\\\\\") private String photo; //一寸照片 @Excel(name = \\\"现在居住地址\\\",orderNum = \\\"9\\\",width = 15, isImportField = \\\"true\\\") private String address; //现在居住地址 private List<Resource> resourceList; //办公用品 } \",\"代码实现\",\"/** * easypoi导入 * @param file */ public void importUserByEasyPoi(MultipartFile file) throws Exception{ ImportParams importParams = new ImportParams(); //设置有一行标题 importParams.setTitleRows(1); //设置有一行行头 importParams.setHeadRows(1); List<User> users = ExcelImportUtil.importExcel(file.getInputStream(), User.class, importParams); for (User user : users) { user.setId(null); userMapper.insert(user); } } \"]},{\"header\":\"3、使用模板导出数据\",\"slug\":\"_3、使用模板导出数据\",\"contents\":[\"模板是处理复杂Excel的简单方法，复杂的Excel样式，可以用Excel直接编辑，完美的避开了代码编写样式的雷区，同时指令的支持，也提了模板的有效性；采用的写法是{{}}代表表达式，然后根据表达式里面的数据取值\",\"关于样式问题：easypoi不会改变excel原有的样式\",\"制作模板\",\"放入项目中\",\"代码实现\",\"/** * 通过easypoi导出word模板 * @param id * @param request * @param response */ public void exportUserInfoWithTemplateByEasyPoi(Long id, HttpServletRequest request, HttpServletResponse response) throws Exception { //获取模板的路径 SpringBoot项目获取根目录的方式 File rootPath = new File(ResourceUtils.getURL(\\\"classpath:\\\").getPath()); File templatePath = new File(rootPath.getAbsolutePath(),\\\"/excel_template/userInfo3.xlsx\\\"); //读取模板文件 TemplateExportParams params = new TemplateExportParams(templatePath.getPath(), true); User user = userMapper.selectByPrimaryKey(id); //将对象转成map Map<String, Object> map = EntityUtils.entityToMap(user); //设置图片 ImageEntity imageEntity = new ImageEntity(); /* imageEntity.setWidth(50); imageEntity.setHeight(100);*/ imageEntity.setColspan(2); imageEntity.setRowspan(4); imageEntity.setUrl(user.getPhoto()); map.put(\\\"photo\\\",imageEntity); //进行导出 org.apache.poi.ss.usermodel.Workbook workbook = ExcelExportUtil.exportExcel(params, map); //导出的文件名称 String filename=\\\"用户详细信息数据.xlsx\\\"; //设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(outputStream); } \"]},{\"header\":\"4、导出csv文件\",\"slug\":\"_4、导出csv文件\",\"contents\":[\"csv的导出基本上和excel的导出一致,大体参数也是一致的\",\"CsvExportParams 的参数描述如下：\",\"属性\",\"类型\",\"默认值\",\"功能\",\"encoding\",\"String\",\"UTF8\",\"文件编码\",\"spiltMark\",\"String\",\",\",\"分隔符\",\"textMark\",\"String\",\"“\",\"字符串识别,可以去掉,需要前后一致\",\"titleRows\",\"int\",\"0\",\"表格头,忽略\",\"headRows\",\"int\",\"1\",\"标题\",\"exclusions\",\"String[]\",\"0\",\"忽略的字段\",\"代码：\",\"/** * easypoi导出csv文件 * @param response */ public void downLoadCSVWithEasyPOI(HttpServletResponse response) throws Exception{ ServletOutputStream outputStream = response.getOutputStream(); String fileName = \\\"用户数据.csv\\\"; response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;filename=\\\"+new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/csv\\\"); CsvExportParams csvExportParams = new CsvExportParams(); csvExportParams.setExclusions(new String[]{\\\"照片\\\"}); List<User> users = userMapper.selectAll(); CsvExportUtil.exportCsv(csvExportParams, User.class, users, outputStream); } \"]},{\"header\":\"3、word操作\",\"slug\":\"_3、word操作\",\"contents\":[]},{\"header\":\"导出word\",\"slug\":\"导出word\",\"contents\":[\"Word模板和Excel模板用法基本一致，支持的标签也是一致的，仅仅支持07版本的word也是只能生成后缀是docx的文档，poi对doc支持不好所以easyPOI中就没有支持doc，我们就拿docx做导出\",\"模板中标签的用法：\",\"指令\",\"作用\",\"{{test ? obj:obj2}}\",\"三元运算\",\"n:\",\"表示 这个cell是数值类型 {{n:}} \",\"le:\",\"代表长度{{le:()}} 在if/else 运用{{le:() > 8 ? obj1 : obj2}}\",\"fd:\",\"格式化时间 {{fd:(obj;yyyy-MM-dd)}} \",\"fn:\",\"格式化数字 {{fn:(obj;###.00)}} \",\"fe:\",\"遍历数据,创建row\",\"!fe:\",\"遍历数据不创建row\",\"$fe:\",\"下移插入,把当前行,下面的行全部下移.size()行,然后插入\",\"#fe:\",\"横向遍历\",\"v_fe:\",\"横向遍历值\",\"!if:\",\"删除当前列 {{!if:(test)}} \",\"‘’\",\"单引号表示常量值 ‘’ 比如’1’ 那么输出的就是 1\",\"&NULL&\",\"空格\",\"&INDEX&\",\"表示循环中的序号,自动添加\",\"]]\",\"换行符 多行遍历导出\",\"sum：\",\"统计数据\",\"根据指令，制作模板\",\"放入项目中\",\"代码实现\",\"/** * easyPOI导出word模板 * @param id * @param response */ public void exportWordByEasyPOI(Long id, HttpServletResponse response) throws Exception{ //获取模板 File root = new File(ResourceUtils.getFile(\\\"classpath:\\\").getPath()); File templatePath = new File(root.getAbsolutePath(), \\\"/word_template/contract_template2.docx\\\"); //获取数据 User user = this.findById(id); Map<String,Object> map = new HashMap<>(); map.put(\\\"userName\\\",user.getUserName()); map.put(\\\"hireDate\\\",sd.format(user.getHireDate())); map.put(\\\"address\\\",user.getAddress()); //正文照片 ImageEntity imageEntity1 = new ImageEntity(); imageEntity1.setHeight(180); imageEntity1.setWidth(240); imageEntity1.setUrl(root.getPath()+user.getPhoto()); map.put(\\\"photo\\\",imageEntity1); //处理word表格 List<Map> list = new ArrayList<>(); Map<String, Object> map1; for (com.xiaobear.pojo.Resource resource : user.getResourceList()) { map1 = new HashMap<>(); map1.put(\\\"name\\\",resource.getName()); map1.put(\\\"price\\\",resource.getPrice()); map1.put(\\\"needReturn\\\",resource.getNeedReturn()); //表格照片 ImageEntity imageEntity = new ImageEntity(); imageEntity.setHeight(180); imageEntity.setWidth(240); imageEntity.setUrl(root.getPath()+\\\"\\\\\\\\static\\\"+resource.getPhoto()); map1.put(\\\"photo\\\",imageEntity); list.add(map1); } map.put(\\\"list\\\",list); XWPFDocument document = WordExportUtil.exportWord07(templatePath.getPath(), map); //导出 String filename=user.getUserName()+\\\"_合同.docx\\\"; // 设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\\\"); document.write(outputStream); } \"]}]},\"/study-tutorial/components/report/excel.html\":{\"title\":\"2、Excel\",\"contents\":[{\"header\":\"1、Excel简介\",\"slug\":\"_1、excel简介\",\"contents\":[\"在企业级应用开发中，Excel报表是一种最常见的报表需求。Excel报表开发一般分为两种形式： 1、为了方便操作，基于Excel的报表批量上传数据，也就是把Excel中的数据导入到系统中。 2、通过java代码生成Excel报表。也就是把系统中的数据导出到Excel中，方便查阅。\"]},{\"header\":\"1、Excel版本\",\"slug\":\"_1、excel版本\",\"contents\":[\"目前世面上的Excel分为两个大的版本Excel2003和Excel2007及以上两个版本；\",\"Excel2003\",\"Excel2007\",\"后缀\",\"xls\",\"xlsx\",\"结构\",\"二进制格式，核心是复合文档类型结构\",\"XML类型结构\",\"单sheet数量\",\"行：65525，列：256\",\"行：1048576，列：16384\",\"特点\",\"存储容量有限\",\"基于xml压缩，占用空间小，操作效率高\"]},{\"header\":\"2、常见的Excel操作工具\",\"slug\":\"_2、常见的excel操作工具\",\"contents\":[\"Java中常见的用来操作Excel的方式一般有2种：JXL和POI。\"]},{\"header\":\"1、JXL\",\"slug\":\"_1、jxl\",\"contents\":[\"JXL只能对Excel进行操作,属于比较老的框架，它只支持到Excel 95-2000的版本。现在已经停止更新和维护.\"]},{\"header\":\"2、POI\",\"slug\":\"_2、poi\",\"contents\":[\"POI是apache的项目,可对微软的Word,Excel,PPT进行操作,包括office2003和2007,Excle2003和2007。poi现在一直有更新。所以现在主流使用POI。\",\"Apache POI是Apache软件基金会的开源项目，由Java编写的免费开源的跨平台的 Java API，ApachePOI提供API给Java语言操作Microsoft Office的功能。\",\"API对象介绍\",\"Excle2003\",\"Excle2007\",\"工作簿（WorkBook）\",\"HSSFWordBook\",\"XSSFWorkBook\",\"工作表（Sheet）\",\"HSSFSheet\",\"XSSFSheet\",\"行（Row）\",\"HSSFRow\",\"XSSFRow\",\"单元格（Cell）\",\"HSSFCell\",\"XSSCell\"]},{\"header\":\"2、JXL导出excel\",\"slug\":\"_2、jxl导出excel\",\"contents\":[]},{\"header\":\"1、JXL导出基本知识点\",\"slug\":\"_1、jxl导出基本知识点\",\"contents\":[\"通过WritableWorkbook，WritableSheet，Label这三个对象我们就可以实现Excel文件的导出工作。\",\"1、 创建可写入的Excel工作薄\",\"WritableWorkbook workbook= Workbook.createWorkbook(输出流); \",\"2、创建工作表\",\"WritableSheet sheet= workbook.createSheet(工作表的名称, 工作表的索引值); \",\"3、创建单元格，添加文本类单元格\",\"Label labelC = new Label(列索引值, 行索引值, \\\"单元格中的内容\\\"); sheet.addCell(labelC); \",\"4、写入到文件\",\"workbook.write();// 写入数据 \",\"5、释放资源：\",\"workbook.close();// 关闭文件 \"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"/** * 通过JSL进行下载 * @param response */ public void downLoadByJxl(HttpServletResponse response){ try{ //输出流 ServletOutputStream outputStream = response.getOutputStream(); //创建一个工作簿 WritableWorkbook wb = Workbook.createWorkbook(outputStream); //创建一个sheet页 s：sheet名 i:sheet号 WritableSheet wbSheet = wb.createSheet(\\\"xiaobear第一个报表文件\\\", 0); //设置列宽 col:列 wid:宽d wbSheet.setColumnView(0,5); wbSheet.setColumnView(1,8); wbSheet.setColumnView(2,8); wbSheet.setColumnView(3,8); wbSheet.setColumnView(4,10); wbSheet.setColumnView(5,30); //创建单元格 // Label labelC = new Label(列索引值, 行索引值, \\\"单元格中的内容\\\"); //sheet.addCell(labelC); Label label = null; String[] titles = new String[]{\\\"编号\\\",\\\"名称\\\",\\\"电话\\\",\\\"生日\\\",\\\"入职日期\\\",\\\"住址\\\"}; //处理标题 for (int i = 0; i < titles.length; i++) { label = new Label(i, 0, titles[i]); wbSheet.addCell(label); } //查询数据进行处理 List<User> users = findAll(); //行初始化值 int rowIndex = 1; for (User user : users) { //编号 label = new Label(0, rowIndex, user.getId().toString()); wbSheet.addCell(label); //名称 label = new Label(1, rowIndex, user.getUserName()); wbSheet.addCell(label); //电话 label = new Label(2, rowIndex, user.getPhone()); wbSheet.addCell(label); //生日 label = new Label(3, rowIndex, sd.format(user.getBirthday())); wbSheet.addCell(label); //入职日期 label = new Label(4, rowIndex, sd.format(user.getHireDate())); wbSheet.addCell(label); //地址 label = new Label(5, rowIndex, user.getAddress()); wbSheet.addCell(label); //行+1 rowIndex++; } //导出的文件名称 String fileName = \\\"JXL导出示例.xls\\\"; // 设置文件的打开方式和mime类型 response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;filename=\\\" + new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.ms-excel\\\"); //导出 wb.write(); //关闭资源 wb.close(); outputStream.close(); }catch (Exception e){ e.printStackTrace(); } } \"]},{\"header\":\"3、POI操作Excel\",\"slug\":\"_3、poi操作excel\",\"contents\":[\"<!--poi所需要的依赖--> <dependency> <groupId>org.apache.poi</groupId> <artifactId>poi</artifactId> <version>4.0.1</version> </dependency> <dependency> <groupId>org.apache.poi</groupId> <artifactId>poi-ooxml</artifactId> <version>4.0.1</version> </dependency> <dependency> <groupId>org.apache.poi</groupId> <artifactId>poi-ooxml-schemas</artifactId> <version>4.0.1</version> </dependency> \"]},{\"header\":\"1、版本之间的区别\",\"slug\":\"_1、版本之间的区别\",\"contents\":[\"在POI包中有如下几个主要对象和excel的几个对象对应：\",\"对应excel名称\",\"低版本中的类名\",\"高版本中的类名\",\"工作簿\",\"HSSFWorkbook\",\"XSSFWorkbook\",\"工作表\",\"HSSFSheet\",\"XSSFSheet\",\"行\",\"HSSFRow\",\"XSSFRow\",\"单元格\",\"HSSFCell\",\"XSSFCell\",\"单元格样式\",\"HSSFCellStyle\",\"XSSFCellStyle\"]},{\"header\":\"1、2003操作excel\",\"slug\":\"_1、2003操作excel\",\"contents\":[\"/** * 通过低版本创建excel */ public static void CreateExcelBy2003() throws IOException { //创建一个工作簿 HSSFWorkbook hssfWorkbook = new HSSFWorkbook(); //创建一个sheet页 HSSFSheet sheet = hssfWorkbook.createSheet(\\\"低版本\\\"); //创建行 HSSFRow row = sheet.createRow(0); //创建列 HSSFCell cell = row.createCell(0); //创建单元格 cell.setCellValue(\\\"xiaobear so nice\\\"); hssfWorkbook.write(new FileOutputStream(\\\"D://test.xls\\\")); } \"]},{\"header\":\"2、2007操作excel\",\"slug\":\"_2、2007操作excel\",\"contents\":[\"/** * 通过高版本创建excel */ public static void CreateExcelBy2007() throws IOException { //创建一个工作簿 Workbook workbook = new XSSFWorkbook(); //创建一个sheet页 Sheet sheet = workbook.createSheet(\\\"低版本\\\"); //创建行 Row row = sheet.createRow(0); //创建列 Cell cell = row.createCell(0); //创建单元格 cell.setCellValue(\\\"xiaobear so nice\\\"); workbook.write(new FileOutputStream(\\\"D://test.xls\\\")); } \"]},{\"header\":\"2、数据导入\",\"slug\":\"_2、数据导入\",\"contents\":[\"数据的导入就是读取excel中的内容，转成对象插入到数据库中\",\"导入上图的数据\"]},{\"header\":\"1、思路\",\"slug\":\"_1、思路\",\"contents\":[\"一般来说，即将导入的文件，每个列代表什么意思基本上都是固定的，比如第1列就是用户姓名，最后一列就是用户的现住址，并且在做excel时对每个列的类型都是有要求的，这样就可以给我们开发带来很大的简便。\",\"最终的目标就是读取每一行数据，把数据转成用户的对象，保存到表中\",\"步骤：\",\"根据上传的文件创建Workbook\",\"获取到第一个sheet工作表\",\"从第二行开始读取数据\",\"读取每一个单元格，把内容放入到用户对象的相关的属性中\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-1\",\"contents\":[\"/** * 用户导入数据 * @param file * @throws IOException * @throws ParseException */ public void importUserByExcel(MultipartFile file) throws IOException, ParseException { //通过文件流返回一个工作薄 XSSFWorkbook wb = new XSSFWorkbook(file.getInputStream()); //获取第一个sheet页 XSSFSheet sheet = wb.getSheetAt(0); //获取最后一行 int lastRowNum = sheet.getLastRowNum(); //遍历获取数据 开始循环每行，获取每行的单元格中的值，放入到user属性中 User user; for (int i = 1; i <= lastRowNum ; i++) { user = new User(); String userName = sheet.getRow(i).getCell(0).getStringCellValue(); user.setUserName(userName); //手机号 String phone = null; try { phone = sheet.getRow(i).getCell(1).getStringCellValue(); } catch (IllegalStateException e) { phone = sheet.getRow(i).getCell(1).getNumericCellValue()+\\\"\\\"; } user.setPhone(phone); //省份 String province = sheet.getRow(i).getCell(2).getStringCellValue(); //城市 String city = sheet.getRow(i).getCell(3).getStringCellValue(); user.setCity(city); // 因为在填写excel中的数据时就可以约定这个列只能填写数值，所以可以直接用getNumericCellValue方法 //工资 Integer salary = ((Double)sheet.getRow(i).getCell(4).getNumericCellValue()).intValue(); user.setSalary(salary); //入职日期 String hireDateStr = sheet.getRow(i).getCell(5).getStringCellValue(); Date hireDate = sd.parse(hireDateStr); user.setHireDate(hireDate); //出生日期 String birthdayStr = sheet.getRow(i).getCell(6).getStringCellValue(); Date birthday = sd.parse(birthdayStr); user.setBirthday(birthday); //现住地址 String address = sheet.getRow(i).getCell(7).getStringCellValue(); user.setProvince(province); user.setAddress(address); userMapper.insert(user); } } \"]},{\"header\":\"3、数据导出\",\"slug\":\"_3、数据导出\",\"contents\":[\"用户导出跟JXL导出差不多\",\"/** * 用户导出 * @param response * @throws IOException */ public void exportUser(HttpServletResponse response) throws IOException { // 创建一个空的工作薄 org.apache.poi.ss.usermodel.Workbook workbook = new XSSFWorkbook(); // 在工作薄中创建一个工作表 Sheet sheet = workbook.createSheet(\\\"测试\\\"); // 设置列宽 sheet.setColumnWidth(0,5*256); sheet.setColumnWidth(1,12*256); sheet.setColumnWidth(2,15*256); sheet.setColumnWidth(3,15*256); sheet.setColumnWidth(4,30*256); // 处理标题 String[] titles = new String[]{\\\"编号\\\",\\\"姓名\\\",\\\"手机号\\\",\\\"入职日期\\\",\\\"现住址\\\"}; // 创建标题行 Row titleRow = sheet.createRow(0); Cell cell = null; for (int i = 0; i < titles.length; i++) { cell = titleRow.createCell(i); cell.setCellValue(titles[i]); } // 处理内容 List<User> userList = this.findAll(); int rowIndex = 1; Row row = null; for (User user : userList) { row = sheet.createRow(rowIndex); cell = row.createCell(0); cell.setCellStyle(contentStyle); cell.setCellValue(user.getId()); cell = row.createCell(1); cell.setCellStyle(contentStyle); cell.setCellValue(user.getUserName()); cell = row.createCell(2); cell.setCellStyle(contentStyle); cell.setCellValue(user.getPhone()); cell = row.createCell(3); cell.setCellStyle(contentStyle); cell.setCellValue(sd.format(user.getHireDate())); cell = row.createCell(4); cell.setCellStyle(contentStyle); cell.setCellValue(user.getAddress()); rowIndex++; } // 导出的文件名称 String filename=\\\"员工数据.xlsx\\\"; // 设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(outputStream); } \"]},{\"header\":\"4、设置样式\",\"slug\":\"_4、设置样式\",\"contents\":[\"注：所有样式需在单元格被创建之后才能设置，否则就会报空指针\"]},{\"header\":\"1、设置边框\",\"slug\":\"_1、设置边框\",\"contents\":[\"CellStyle contentStyle = workbook.createCellStyle(); contentStyle.setBorderBottom(BorderStyle.THIN); contentStyle.setBorderTop(BorderStyle.THIN); contentStyle.setBorderLeft(BorderStyle.THIN); contentStyle.setBorderRight(BorderStyle.THIN); \"]},{\"header\":\"2、对齐方式\",\"slug\":\"_2、对齐方式\",\"contents\":[\"//设置居中对齐 contentStyle.setAlignment(HorizontalAlignment.CENTER); contentStyle.setVerticalAlignment(VerticalAlignment.CENTER); \"]},{\"header\":\"3、合并单元格\",\"slug\":\"_3、合并单元格\",\"contents\":[\"//合并单元格 起始行, 结束行, 起始列, 结束列 sheet.addMergedRegion(new CellRangeAddress(0,0,0,4)); \"]},{\"header\":\"4、设置行高\",\"slug\":\"_4、设置行高\",\"contents\":[\"//设置行高 row1.setHeightInPoints((short) 52); \"]},{\"header\":\"5、设置字体样式\",\"slug\":\"_5、设置字体样式\",\"contents\":[\"//设置字体 CellStyle titleStyle = workbook.createCellStyle(); Font font = workbook.createFont(); font.setBold(true); font.setFontName(\\\"黑体\\\"); font.setFontHeightInPoints((short) 16); titleStyle.setFont(font); \"]},{\"header\":\"5、完整导出代码\",\"slug\":\"_5、完整导出代码\",\"contents\":[\"/** * 用户导出 * @param response * @throws IOException */ public void exportUser(HttpServletResponse response) throws IOException { // 创建一个空的工作薄 org.apache.poi.ss.usermodel.Workbook workbook = new XSSFWorkbook(); // 在工作薄中创建一个工作表 Sheet sheet = workbook.createSheet(\\\"测试\\\"); Row row1 = sheet.createRow(0); //设置行高 row1.setHeightInPoints((short) 52); //合并单元格 sheet.addMergedRegion(new CellRangeAddress(0,0,0,4)); // 设置列宽 sheet.setColumnWidth(0,5*256); sheet.setColumnWidth(1,12*256); sheet.setColumnWidth(2,15*256); sheet.setColumnWidth(3,15*256); sheet.setColumnWidth(4,30*256); /** * 设置框线 */ CellStyle contentStyle = workbook.createCellStyle(); contentStyle.setBorderBottom(BorderStyle.THIN); contentStyle.setBorderTop(BorderStyle.THIN); contentStyle.setBorderLeft(BorderStyle.THIN); contentStyle.setBorderRight(BorderStyle.THIN); //设置居中对齐 contentStyle.setAlignment(HorizontalAlignment.CENTER); contentStyle.setVerticalAlignment(VerticalAlignment.CENTER); //设置字体 CellStyle titleStyle = workbook.createCellStyle(); Font font = workbook.createFont(); font.setBold(true); font.setFontHeightInPoints((short) 16); titleStyle.setFont(font); titleStyle.setBorderBottom(BorderStyle.THIN); titleStyle.setBorderLeft(BorderStyle.THIN); titleStyle.setBorderRight(BorderStyle.THIN); titleStyle.setBorderTop(BorderStyle.THIN); //设置居中对齐 titleStyle.setAlignment(HorizontalAlignment.CENTER); titleStyle.setVerticalAlignment(VerticalAlignment.CENTER); //合并样式 for (int i = 0; i < 5; i++) { Cell cell = row1.createCell(i); cell.setCellStyle(titleStyle); } row1.getCell(0).setCellValue(\\\"用户测试数据\\\"); // 处理标题 String[] titles = new String[]{\\\"编号\\\",\\\"姓名\\\",\\\"手机号\\\",\\\"入职日期\\\",\\\"现住址\\\"}; // 创建标题行 Row titleRow = sheet.createRow(1); Cell cell = null; for (int i = 0; i < titles.length; i++) { cell = titleRow.createCell(i); cell.setCellValue(titles[i]); cell.setCellStyle(titleStyle); } // 处理内容 List<User> userList = this.findAll(); int rowIndex = 2; Row row = null; for (User user : userList) { row = sheet.createRow(rowIndex); cell = row.createCell(0); cell.setCellStyle(contentStyle); cell.setCellValue(user.getId()); cell = row.createCell(1); cell.setCellStyle(contentStyle); cell.setCellValue(user.getUserName()); cell = row.createCell(2); cell.setCellStyle(contentStyle); cell.setCellValue(user.getPhone()); cell = row.createCell(3); cell.setCellStyle(contentStyle); cell.setCellValue(sd.format(user.getHireDate())); cell = row.createCell(4); cell.setCellStyle(contentStyle); cell.setCellValue(user.getAddress()); rowIndex++; } // 导出的文件名称 String filename=\\\"员工数据.xlsx\\\"; // 设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(outputStream); } \"]}]},\"/study-tutorial/components/report/opencsv.html\":{\"title\":\"4、opencsv操作csv文件\",\"contents\":[{\"header\":\"1、csv文件概述\",\"slug\":\"_1、csv文件概述\",\"contents\":[\"CSV文件：Comma-Separated Values，中文叫逗号分隔值或者字符分割值，其文件以纯文本的形式存储表格数据。该文件是一个字符序列，可以由任意数目的记录组成，记录间以某种换行符分割。每条记录由字段组成，字段间的分隔符是其他字符或者字符串。所有的记录都有完全相同的字段序列，相当于一个结构化表的纯文本形式。 用文本文件、excel或者类似与文本文件的编辑器都可以打开CSV文件。\",\"为了简化开发，我们可以使用opencsv类库来导出csv文件\",\"<dependency> <groupId>com.opencsv</groupId> <artifactId>opencsv</artifactId> <version>4.5</version> </dependency> \"]},{\"header\":\"2、opencsv常用API\",\"slug\":\"_2、opencsv常用api\",\"contents\":[]},{\"header\":\"3、导出csv文件\",\"slug\":\"_3、导出csv文件\",\"contents\":[\"/** * 导出csv文件 * @param response */ public void downloadCSV(HttpServletResponse response) throws IOException { //获取输出流 ServletOutputStream outputStream = response.getOutputStream(); String fileName = \\\"用户数据.csv\\\"; response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;filename\\\"+ new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"text/csv\\\"); //创建一个用来写入到csv文件的write CSVWriter csvWriter = new CSVWriter(new OutputStreamWriter(outputStream, \\\"utf-8\\\")); //设置标题 csvWriter.writeNext(new String[]{\\\"编号\\\",\\\"姓名\\\",\\\"手机号\\\",\\\"入职日期\\\",\\\"现住址\\\"}); //获取数据源 List<User> users = userMapper.selectAll(); if(!CollectionUtils.isEmpty(users)){ for (User user : users) { csvWriter.writeNext(new String[]{user.getId().toString(),user.getUserName(),user.getPhone(),sd.format(user.getHireDate()),user.getAddress()}); } csvWriter.flush(); } csvWriter.close(); } \"]},{\"header\":\"4、导入csv文件\",\"slug\":\"_4、导入csv文件\",\"contents\":[\"public class ImportCSV { private static SimpleDateFormat sdf = new SimpleDateFormat(\\\"yyyy-MM-dd\\\"); public static void main(String[] args) throws Exception { CSVReader reader = new CSVReader(new FileReader(\\\"C:\\\\\\\\Users\\\\\\\\28611\\\\\\\\Downloads\\\\\\\\downLoadCSV\\\")); //读取第一行数据 标题 String[] readNext = reader.readNext(); User user; while (true){ user = new User(); String[] next = reader.readNext(); if(null == next){ break; } user.setId(Long.parseLong(next[0])); user.setUserName(next[1]); user.setPhoto(next[2]); user.setHireDate(sdf.parse(next[3])); user.setAddress(next[4]); System.out.println(user); } } } \"]}]},\"/study-tutorial/components/report/overview.html\":{\"title\":\"1、概述\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"简单的说：报表就是用表格、图表等格式来动态显示数据，可以用公式表示为：“报表 = 多样的格式 + 动态的数据”。\"]},{\"header\":\"1、环境搭建\",\"slug\":\"_1、环境搭建\",\"contents\":[\"功能说明：整个案例我们操作用户表，做一个企业员工（用户）数据的导入导出。\",\"我们使用SpringBoot+通用mapper+vue方式搭建开发环境。\"]},{\"header\":\"1、数据库准备\",\"slug\":\"_1、数据库准备\",\"contents\":[\"CREATE DATABASE /*!32312 IF NOT EXISTS*/`xiaobear-report` /*!40100 DEFAULT CHARACTER SET utf8 */; USE `xiaobear-report`; SET FOREIGN_KEY_CHECKS=0; -- ---------------------------- -- Table structure for tb_dept -- ---------------------------- DROP TABLE IF EXISTS `tb_dept`; CREATE TABLE `tb_dept` ( `id` bigint(20) DEFAULT NULL COMMENT '部门编号', `dept_name` varchar(100) DEFAULT NULL COMMENT '部门编号' ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of tb_dept -- ---------------------------- INSERT INTO `tb_dept` VALUES ('5', '资产管理部'); INSERT INTO `tb_dept` VALUES ('6', '质量监察部'); INSERT INTO `tb_dept` VALUES ('7', '营销部'); INSERT INTO `tb_dept` VALUES ('1', '销售部'); INSERT INTO `tb_dept` VALUES ('2', '人事部'); INSERT INTO `tb_dept` VALUES ('3', '财务部'); INSERT INTO `tb_dept` VALUES ('4', '技术部'); -- ---------------------------- -- Table structure for tb_province -- ---------------------------- DROP TABLE IF EXISTS `tb_province`; CREATE TABLE `tb_province` ( `id` bigint(50) NOT NULL, `name` varchar(100) DEFAULT NULL COMMENT '省份或直辖市或特别行政区名称', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of tb_province -- ---------------------------- INSERT INTO `tb_province` VALUES ('1', '北京市'); INSERT INTO `tb_province` VALUES ('2', '天津市'); INSERT INTO `tb_province` VALUES ('3', '上海市'); INSERT INTO `tb_province` VALUES ('4', '重庆市'); INSERT INTO `tb_province` VALUES ('5', '河北省'); INSERT INTO `tb_province` VALUES ('6', '山西省'); INSERT INTO `tb_province` VALUES ('7', '辽宁省'); INSERT INTO `tb_province` VALUES ('8', '吉林省'); INSERT INTO `tb_province` VALUES ('9', '黑龙江省'); INSERT INTO `tb_province` VALUES ('10', '江苏省'); INSERT INTO `tb_province` VALUES ('11', '浙江省'); INSERT INTO `tb_province` VALUES ('12', '安徽省'); INSERT INTO `tb_province` VALUES ('13', '福建省'); INSERT INTO `tb_province` VALUES ('14', '江西省'); INSERT INTO `tb_province` VALUES ('15', '山东省'); INSERT INTO `tb_province` VALUES ('16', '河南省'); INSERT INTO `tb_province` VALUES ('17', '湖北省'); INSERT INTO `tb_province` VALUES ('18', '湖南省'); INSERT INTO `tb_province` VALUES ('19', '广东省'); INSERT INTO `tb_province` VALUES ('20', '海南省'); INSERT INTO `tb_province` VALUES ('21', '四川省'); INSERT INTO `tb_province` VALUES ('22', '贵州省'); INSERT INTO `tb_province` VALUES ('23', '云南省'); INSERT INTO `tb_province` VALUES ('24', '陕西省'); INSERT INTO `tb_province` VALUES ('25', '甘肃省'); INSERT INTO `tb_province` VALUES ('26', '青海省'); INSERT INTO `tb_province` VALUES ('27', '台湾省'); INSERT INTO `tb_province` VALUES ('28', '内蒙古自治区'); INSERT INTO `tb_province` VALUES ('29', '广西壮族自治区'); INSERT INTO `tb_province` VALUES ('30', '西藏自治区'); INSERT INTO `tb_province` VALUES ('31', '宁夏回族自治区'); INSERT INTO `tb_province` VALUES ('32', '新疆维吾尔自治区'); INSERT INTO `tb_province` VALUES ('33', '香港特别行政区'); INSERT INTO `tb_province` VALUES ('34', '澳门特别行政区'); -- ---------------------------- -- Table structure for tb_resource -- ---------------------------- DROP TABLE IF EXISTS `tb_resource`; CREATE TABLE `tb_resource` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `price` double(10,1) DEFAULT NULL, `user_id` bigint(20) DEFAULT NULL, `need_return` tinyint(1) DEFAULT NULL COMMENT '是否需要归还', `photo` varchar(200) DEFAULT NULL COMMENT '照片', PRIMARY KEY (`id`), KEY `fk_user_id` (`user_id`), CONSTRAINT `fk_user_id` FOREIGN KEY (`user_id`) REFERENCES `tb_user` (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of tb_resource -- ---------------------------- INSERT INTO `tb_resource` VALUES ('1', '记录本', '2.0', '3', '0', '\\\\\\\\resource_photos\\\\\\\\3\\\\\\\\1.jpg'); INSERT INTO `tb_resource` VALUES ('2', '笔记本电脑', '7000.0', '3', '1', '\\\\\\\\resource_photos\\\\\\\\3\\\\\\\\2.jpg'); INSERT INTO `tb_resource` VALUES ('3', '办公桌', '1000.0', '3', '1', '\\\\\\\\resource_photos\\\\\\\\3\\\\\\\\3.jpg'); INSERT INTO `tb_resource` VALUES ('4', '订书机', '50.0', '4', '1', '\\\\\\\\resource_photos\\\\\\\\4\\\\\\\\1.jpg'); INSERT INTO `tb_resource` VALUES ('5', '双面胶带', '5.0', '4', '0', '\\\\\\\\resource_photos\\\\\\\\4\\\\\\\\2.jpg'); INSERT INTO `tb_resource` VALUES ('6', '资料文件夹', '10.0', '4', '0', '\\\\\\\\resource_photos\\\\\\\\4\\\\\\\\3.jpg'); INSERT INTO `tb_resource` VALUES ('7', '打印机', '1200.0', '4', '1', '\\\\\\\\resource_photos\\\\\\\\4\\\\\\\\4.jpg'); -- ---------------------------- -- Table structure for tb_user -- ---------------------------- DROP TABLE IF EXISTS `tb_user`; CREATE TABLE `tb_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '用户ID', `user_name` varchar(100) DEFAULT NULL COMMENT '姓名', `phone` varchar(15) DEFAULT NULL COMMENT '手机号', `province` varchar(50) DEFAULT NULL COMMENT '省份', `city` varchar(50) DEFAULT NULL COMMENT '城市', `salary` int(10) DEFAULT NULL, `hire_date` datetime DEFAULT NULL COMMENT '入职日期', `dept_id` bigint(20) DEFAULT NULL COMMENT '部门编号', `birthday` datetime DEFAULT NULL COMMENT '出生日期', `photo` varchar(200) DEFAULT NULL COMMENT '照片路径', `address` varchar(300) DEFAULT NULL COMMENT '现在住址', PRIMARY KEY (`id`), KEY `fk_dept` (`dept_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of tb_user -- ---------------------------- INSERT INTO `tb_user` VALUES ('1', '大一', '13800000001', '北京市', '北京市', '11000', '2001-01-01 21:18:29', '1', '1981-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\1.jpg', '北京市西城区宣武大街1号院'); INSERT INTO `tb_user` VALUES ('2', '不二', '13800000002', '河北省', '石家庄市', '12000', '2002-01-02 21:18:29', '2', '1982-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\2.jpg', '北京市西城区宣武大街2号院'); INSERT INTO `tb_user` VALUES ('3', '张三', '13800000003', '河北省', '石家庄市', '13000', '2003-03-03 21:18:29', '3', '1983-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\3.jpg', '北京市西城区宣武大街3号院'); INSERT INTO `tb_user` VALUES ('4', '李四', '13800000004', '河北省', '石家庄市', '14000', '2004-02-04 21:18:29', '4', '1984-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\4.jpg', '北京市西城区宣武大街4号院'); INSERT INTO `tb_user` VALUES ('5', '王五', '13800000005', '河北省', '唐山市', '15000', '2005-03-05 21:18:29', '5', '1985-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\5.jpg', '北京市西城区宣武大街5号院'); INSERT INTO `tb_user` VALUES ('6', '赵六', '13800000006', '河北省', '承德市省', '16000', '2006-04-06 21:18:29', '6', '1986-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\6.jpg', '北京市西城区宣武大街6号院'); INSERT INTO `tb_user` VALUES ('7', '沈七', '13800000007', '河北省', '秦皇岛市', '17000', '2007-06-07 21:18:29', '7', '1987-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\7.jpg', '北京市西城区宣武大街7号院'); INSERT INTO `tb_user` VALUES ('8', '酒八', '13800000008', '河北省', '秦皇岛市', '18000', '2008-07-08 21:18:29', '6', '1988-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\8.jpg', '北京市西城区宣武大街8号院'); INSERT INTO `tb_user` VALUES ('9', '第九', '13800000009', '山东省', '德州市', '19000', '2009-03-09 21:18:29', '1', '1989-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\9.jpg', '北京市西城区宣武大街9号院'); INSERT INTO `tb_user` VALUES ('10', '石十', '13800000010', '山东省', '青岛市', '20000', '2010-07-10 21:18:29', '4', '1990-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\10.jpg', '北京市西城区宣武大街10号院'); INSERT INTO `tb_user` VALUES ('11', '肖十一', '13800000011', '山东省', '青岛市', '21000', '2011-12-11 21:18:29', '4', '1991-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\11.jpg', '北京市西城区宣武大街11号院'); INSERT INTO `tb_user` VALUES ('12', '星十二', '13800000012', '山东省', '青岛市', '22000', '2012-05-12 21:18:29', '4', '1992-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\12.jpg', '北京市西城区宣武大街12号院'); INSERT INTO `tb_user` VALUES ('13', '钗十三', '13800000013', '山东省', '济南市', '23000', '2013-06-13 21:18:29', '3', '1993-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\13.jpg', '北京市西城区宣武大街13号院'); INSERT INTO `tb_user` VALUES ('14', '贾十四', '13800000014', '山东省', '威海市', '24000', '2014-06-14 21:18:29', '2', '1994-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\14.jpg', '北京市西城区宣武大街14号院'); INSERT INTO `tb_user` VALUES ('15', '甄世武', '13800000015', '山东省', '济南市', '25000', '2015-06-15 21:18:29', '4', '1995-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\15.jpg', '北京市西城区宣武大街15号院'); -- ---------------------------- -- Table structure for tb_month -- ---------------------------- DROP TABLE IF EXISTS `tb_month`; CREATE TABLE `tb_month` ( `name` varchar(2) DEFAULT NULL COMMENT '月份' ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of tb_month -- ---------------------------- INSERT INTO `tb_month` VALUES ('01'); INSERT INTO `tb_month` VALUES ('02'); INSERT INTO `tb_month` VALUES ('03'); INSERT INTO `tb_month` VALUES ('04'); INSERT INTO `tb_month` VALUES ('05'); INSERT INTO `tb_month` VALUES ('06'); INSERT INTO `tb_month` VALUES ('07'); INSERT INTO `tb_month` VALUES ('08'); INSERT INTO `tb_month` VALUES ('09'); INSERT INTO `tb_month` VALUES ('10'); INSERT INTO `tb_month` VALUES ('11'); INSERT INTO `tb_month` VALUES ('12'); \"]},{\"header\":\"2、页面搭建\",\"slug\":\"_2、页面搭建\",\"contents\":[\"整个页面都是已经搭建好的，主要还是后台的实现\"]},{\"header\":\"3、启动项目\",\"slug\":\"_3、启动项目\",\"contents\":[\"访问地址：http://localhost:8080/list.html\"]}]},\"/study-tutorial/components/report/report.html\":{\"title\":\"7、图表报表\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在大数据时代，人们需要对大量的数据进行分析，帮助用户或公司领导更直观的察觉差异，做出判断，减少时间成本，而在web项目中除了表格显示数据外，还可以通过图表来表现数据，这种图表形式表现数据使人看的清楚明白且更加直观。对于web项目展示图形报表使用最多的技术就是基于js的前端报表框架，目前前端市场使用最多的图形报表框架有：JfreeChart、ECharts、Highcharts、FusionCharts、amCharts等。\",\"因为这里主要是说后端操作，所以主要讲JFreeChart\"]},{\"header\":\"1、JFreeChart\",\"slug\":\"_1、jfreechart\",\"contents\":[]},{\"header\":\"1、简介\",\"slug\":\"_1、简介\",\"contents\":[\"JFreeChart是JAVA平台上的一个开放的图表绘制类库。它完全使用JAVA语言编写，可生成饼图（pie charts）、柱状图（bar charts）、散点图（scatter plots）、时序图（time series）、甘特图（Gantt charts）等等多种图表，并且可以产生PNG和JPEG格式的输出，还可以与PDF或EXCEL关联。\",\"需要的依赖：\",\"<dependency> <groupId>org.jfree</groupId> <artifactId>jfreechart</artifactId> <version>1.5.0</version> </dependency> \"]},{\"header\":\"2、生成饼图\",\"slug\":\"_2、生成饼图\",\"contents\":[\"public class PieChart { public static void main(String[] args) throws Exception { //数据集 DefaultPieDataset dataset = new DefaultPieDataset(); dataset.setValue(\\\"语文\\\",110); dataset.setValue(\\\"数学\\\",119); dataset.setValue(\\\"英语\\\",108); dataset.setValue(\\\"物理\\\",90); //设置字体以及中文乱码 StandardChartTheme chartTheme = new StandardChartTheme(\\\"CN\\\"); //设置标题字体 chartTheme.setExtraLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,20)); //设置图例的字体 chartTheme.setRegularFont(new Font(\\\"华文宋体\\\", Font.BOLD,15)); //设置轴向的字体 chartTheme.setLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,15)); //应用主题样式 ChartFactory.setChartTheme(chartTheme); //绘制3d饼形图 JFreeChart pieChart3D = ChartFactory.createPieChart3D(\\\"成绩表\\\", dataset, true, true, true); //绘制饼形图 JFreeChart pieChart = ChartFactory.createPieChart(\\\"成绩表\\\", dataset, true, true, true); //保存图片 ChartUtils.saveChartAsPNG(new File(\\\"D:\\\\\\\\pieChart.png\\\"), pieChart, 400, 400); } } \"]},{\"header\":\"3、生成折线图\",\"slug\":\"_3、生成折线图\",\"contents\":[\"public class LineChart { public static void main(String[] args) throws Exception{ //数据集 DefaultCategoryDataset dataset = new DefaultCategoryDataset(); dataset.addValue(110,\\\"语文\\\",\\\"2021年\\\"); dataset.addValue(119,\\\"数学\\\",\\\"2021年\\\"); dataset.addValue(108,\\\"英语\\\",\\\"2021年\\\"); dataset.addValue(90,\\\"物理\\\",\\\"2021年\\\"); dataset.addValue(106,\\\"语文\\\",\\\"2020年\\\"); dataset.addValue(142,\\\"数学\\\",\\\"2020年\\\"); dataset.addValue(142,\\\"英语\\\",\\\"2020年\\\"); dataset.addValue(100,\\\"物理\\\",\\\"2020年\\\"); dataset.addValue(130,\\\"语文\\\",\\\"2019年\\\"); dataset.addValue(129,\\\"数学\\\",\\\"2019年\\\"); dataset.addValue(118,\\\"英语\\\",\\\"2019年\\\"); dataset.addValue(98,\\\"物理\\\",\\\"2019年\\\"); dataset.addValue(120,\\\"语文\\\",\\\"2018年\\\"); dataset.addValue(149,\\\"数学\\\",\\\"2018年\\\"); dataset.addValue(148,\\\"英语\\\",\\\"2018年\\\"); dataset.addValue(99,\\\"物理\\\",\\\"2018年\\\"); //设置字体以及中文乱码 StandardChartTheme chartTheme = new StandardChartTheme(\\\"CN\\\"); //设置标题字体 chartTheme.setExtraLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,20)); //设置图例的字体 chartTheme.setRegularFont(new Font(\\\"华文宋体\\\", Font.BOLD,15)); //设置轴向的字体 chartTheme.setLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,15)); //应用主题样式 ChartFactory.setChartTheme(chartTheme); JFreeChart lineChart = ChartFactory.createLineChart(\\\"成绩\\\", \\\"年度\\\", \\\"分数\\\", dataset); ChartUtils.saveChartAsPNG(new File(\\\"D:\\\\\\\\lineChart.png\\\"), lineChart, 400, 400); } } \"]},{\"header\":\"4、生成柱状图\",\"slug\":\"_4、生成柱状图\",\"contents\":[\"public class BarChart { public static void main(String[] args) throws Exception{ //数据集 DefaultCategoryDataset dataset = new DefaultCategoryDataset(); dataset.addValue(110,\\\"语文\\\",\\\"2021年\\\"); dataset.addValue(119,\\\"数学\\\",\\\"2021年\\\"); dataset.addValue(108,\\\"英语\\\",\\\"2021年\\\"); dataset.addValue(90,\\\"物理\\\",\\\"2021年\\\"); dataset.addValue(106,\\\"语文\\\",\\\"2020年\\\"); dataset.addValue(142,\\\"数学\\\",\\\"2020年\\\"); dataset.addValue(142,\\\"英语\\\",\\\"2020年\\\"); dataset.addValue(100,\\\"物理\\\",\\\"2020年\\\"); dataset.addValue(130,\\\"语文\\\",\\\"2019年\\\"); dataset.addValue(129,\\\"数学\\\",\\\"2019年\\\"); dataset.addValue(118,\\\"英语\\\",\\\"2019年\\\"); dataset.addValue(98,\\\"物理\\\",\\\"2019年\\\"); dataset.addValue(120,\\\"语文\\\",\\\"2018年\\\"); dataset.addValue(149,\\\"数学\\\",\\\"2018年\\\"); dataset.addValue(148,\\\"英语\\\",\\\"2018年\\\"); dataset.addValue(99,\\\"物理\\\",\\\"2018年\\\"); //设置字体以及中文乱码 StandardChartTheme chartTheme = new StandardChartTheme(\\\"CN\\\"); //设置标题字体 chartTheme.setExtraLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,20)); //设置图例的字体 chartTheme.setRegularFont(new Font(\\\"华文宋体\\\", Font.BOLD,15)); //设置轴向的字体 chartTheme.setLargeFont(new Font(\\\"华文宋体\\\",Font.BOLD,15)); //应用主题样式 ChartFactory.setChartTheme(chartTheme); JFreeChart barChart = ChartFactory.createBarChart(\\\"成绩\\\", \\\"年度\\\", \\\"分数\\\", dataset); ChartUtils.saveChartAsPNG(new File(\\\"D:\\\\\\\\barChart.png\\\"), barChart, 400, 400); } } \"]}]},\"/study-tutorial/components/report/word.html\":{\"title\":\"5、POI操作word\",\"contents\":[{\"header\":\"1、API介绍\",\"slug\":\"_1、api介绍\",\"contents\":[]},{\"header\":\"1、操作Word正文\",\"slug\":\"_1、操作word正文\",\"contents\":[\"XWPFDocument代表一个docx文档，其可以用来读docx文档，也可以用来写docx文档\",\"一个文档包含多个段落，一个段落包含多个Runs文本，一个Runs包含多个Run，Run是文档的最小单元\",\"获取所有段落：\",\"List<XWPFParagraph> paragraphs = word.getParagraphs(); \",\"获取一个段落中的所有片段Runs：\",\"List<XWPFRun> xwpfRuns = xwpfParagraph.getRuns(); \",\"获取一个Runs中的一个Run：\",\"XWPFRun run = xwpfRuns.get(index); \"]},{\"header\":\"2、操作Word表格\",\"slug\":\"_2、操作word表格\",\"contents\":[\"一个文档包含多个表格，一个表格包含多行，一行包含多列单元格\",\"获取所有表格：\",\"List<XWPFTable> xwpfTables = doc.getTables(); \",\"获取一个表格中的所有行：\",\"List<XWPFTableRow> xwpfTableRows = xwpfTable.getRows(); \",\"获取一行中的所有列：\",\"List<XWPFTableCell> xwpfTableCells = xwpfTableRow.getTableCells(); \",\"获取一格里的内容：\",\"List<XWPFParagraph> paragraphs = xwpfTableCell.getParagraphs(); \"]},{\"header\":\"1、word工具类\",\"slug\":\"_1、word工具类\",\"contents\":[\"public class WordUtils { /** * 导出word工具类 * @param response * @param user * @throws Exception */ public static void exportWord(HttpServletResponse response, User user) throws Exception{ File classpath = new File(ResourceUtils.getFile(\\\"classpath:\\\").getPath()); File file = new File(classpath.getAbsolutePath(), \\\"/word_template/contract_template.docx\\\"); //读取模板信息 XWPFDocument document = new XWPFDocument(new FileInputStream(file)); Map<String, String> map = new HashMap<>(10); map.put(\\\"userName\\\", user.getUserName()); map.put(\\\"hireDate\\\",new SimpleDateFormat(\\\"yyyy-MM-dd\\\").format(user.getHireDate())); map.put(\\\"address\\\", user.getAddress()); //处理数据 获取所有段落 List<XWPFParagraph> paragraphs = document.getParagraphs(); for (XWPFParagraph paragraph : paragraphs) { //获取所有片段 List<XWPFRun> runs = paragraph.getRuns(); for (XWPFRun run : runs) { String text = run.getText(0); for (String s : map.keySet()) { if (text.contains(s)){ //替换字段 run.setText(text.replaceAll(s,map.get(s)),0); } } } } //处理表格 List<com.xiaobear.pojo.Resource> resourceList = user.getResourceList(); //获取第一个表格 XWPFTable table = document.getTables().get(0); //获取第一行 XWPFTableRow row = table.getRow(0); int rowIndex = 1; for (com.xiaobear.pojo.Resource resource : resourceList) { //增加行 addBlankRow(table,row,rowIndex); XWPFTableRow tableRow = table.getRow(rowIndex); tableRow.getCell(0).setText(resource.getName()); tableRow.getCell(1).setText(resource.getPrice().toString()); tableRow.getCell(2).setText(resource.getNeedReturn()?\\\"需要\\\":\\\"不需要\\\"); File imageFile = new File(classpath,\\\"/static\\\"+resource.getPhoto()); setCellImage(tableRow.getCell(3),imageFile); rowIndex++; } // 处理表格开始结束 // 4、导出word String filename = \\\"员工(\\\" + user.getUserName() + \\\")合同.docx\\\"; response.setHeader(\\\"content-disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(), \\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); document.write(response.getOutputStream()); } /** * 向单元格中写入图片 * @param cell * @param imageFile */ private static void setCellImage(XWPFTableCell cell, File imageFile) { XWPFRun run = cell.getParagraphs().get(0).createRun(); // InputStream pictureData, int pictureType, String filename, int width, int height try(FileInputStream inputStream = new FileInputStream(imageFile)) { run.addPicture(inputStream,XWPFDocument.PICTURE_TYPE_JPEG,imageFile.getName(), Units.toEMU(100),Units.toEMU(50)); } catch (Exception e) { e.printStackTrace(); } } /** * 增加一个空行 * @param table * @param row * @param rowIndex */ public static void addBlankRow(XWPFTable table, XWPFTableRow row, int rowIndex){ //插入一行 ps:没有样式 XWPFTableRow xwpfTableRow = table.insertNewTableRow(rowIndex); xwpfTableRow.getCtRow().setTrPr(row.getCtRow().getTrPr()); //获取原来row的单元格 List<XWPFTableCell> tableCells = row.getTableCells(); if (CollectionUtils.isEmpty(tableCells)) { } XWPFTableCell temp; for (XWPFTableCell cell : tableCells) { temp = xwpfTableRow.addNewTableCell(); //单元格的样式、属性 temp.getCTTc().setTcPr(cell.getCTTc().getTcPr()); temp.getParagraphs().get(0).getCTP().setPPr(cell.getParagraphs().get(0).getCTP().getPPr()); } } } \"]}]},\"/study-tutorial/components/report/write_data.html\":{\"title\":\"3、写入数据\",\"contents\":[{\"header\":\"1、基于模板导出列表数据\",\"slug\":\"_1、基于模板导出列表数据\",\"contents\":[\"首先准备一个excel模板，这个模板把复杂的样式和固定的内容先准备好并且放入到项目中，然后读取到模板后向里面放入数据。\"]},{\"header\":\"1、准备工作\",\"slug\":\"_1、准备工作\",\"contents\":[\"准备模板内容\",\"第二个sheet页\",\"把这个模板改一个英文名称比如：userList.xlsx,放入到项目中\"]},{\"header\":\"2、步骤\",\"slug\":\"_2、步骤\",\"contents\":[\"获取模板路径\",\"获取我们设置的样式\",\"处理数据\",\"导出\"]},{\"header\":\"3、实现\",\"slug\":\"_3、实现\",\"contents\":[\" /** * 通过模板下载 * @param request * @param response * @throws Exception */ public void exportUserByTemplate(HttpServletRequest request, HttpServletResponse response) throws Exception{ //获取根目录路径 SpringBoot项目获取根目录的方式 String classpath = ResourceUtils.getFile(\\\"classpath:\\\").getPath(); File file = new File(classpath); //模板路径 File templatePath = new File(file.getAbsolutePath(), \\\"/excel_template/userList.xlsx\\\"); //读取模板文件产生workbook对象,这个workbook是一个有内容的工作薄 org.apache.poi.ss.usermodel.Workbook workbook = new XSSFWorkbook(templatePath); Sheet sheet = workbook.getSheetAt(0); //获取我们设置好的样式 CellStyle contentStyle = workbook.getSheetAt(1).getRow(0).getCell(0).getCellStyle(); // 处理内容 List<User> userList = this.findAll(); int rowIndex = 2; Row row = null; Cell cell; for (User user : userList) { row = sheet.createRow(rowIndex); row.setHeightInPoints(15); cell = row.createCell(0); cell.setCellStyle(contentStyle); cell.setCellValue(user.getId()); cell = row.createCell(1); cell.setCellStyle(contentStyle); cell.setCellValue(user.getUserName()); cell = row.createCell(2); cell.setCellStyle(contentStyle); cell.setCellValue(user.getPhone()); cell = row.createCell(3); cell.setCellStyle(contentStyle); cell.setCellValue(sd.format(user.getHireDate())); cell = row.createCell(4); cell.setCellStyle(contentStyle); cell.setCellValue(user.getAddress()); rowIndex++; } //删除之前多余的sheet workbook.removeSheetAt(1); // 导出的文件名称 String filename=\\\"员工数据.xlsx\\\"; // 设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(outputStream); } \"]},{\"header\":\"2、基于模板导出详细数据\",\"slug\":\"_2、基于模板导出详细数据\",\"contents\":[\"要做成这样的\"]},{\"header\":\"1、准备工作\",\"slug\":\"_1、准备工作-1\",\"contents\":[\"制作一个excel导出模板，如下\",\"制作好的模板放入到项目中\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"/** * 通过模板下载 * @param id * @param request * @param response * @throws IOException * @throws InvalidFormatException */ public void downLoadUserInfoByTemplate(Long id, HttpServletRequest request, HttpServletResponse response) throws IOException, InvalidFormatException { //获取根目录 File root = new File(ResourceUtils.getFile(\\\"classpath:\\\").getPath()); //获取模板路径 File file = new File(root.getAbsolutePath(), \\\"/excel_template/userInfo.xlsx\\\"); //获取工作薄 org.apache.poi.ss.usermodel.Workbook workbook = new XSSFWorkbook(file); //获取sheet页 Sheet sheet = workbook.getSheetAt(0); //获取数据源 User user = userMapper.selectByPrimaryKey(id); //设置用户名 sheet.getRow(1).getCell(1).setCellValue(user.getUserName()); //设置手机号 sheet.getRow(2).getCell(1).setCellValue(user.getPhone()); // 生日 第4行第2列 日期转成字符串 sheet.getRow(3).getCell(1).setCellValue(sd.format(user.getBirthday())); // 工资 第5行第2列 sheet.getRow(4).getCell(1).setCellValue(user.getSalary()); // 工资 第6行第2列 sheet.getRow(5).getCell(1).setCellValue(sd.format(user.getHireDate())); //省份 第7行第2列 sheet.getRow(6).getCell(1).setCellValue(user.getProvince()); //现住址 第8行第2列 sheet.getRow(7).getCell(1).setCellValue(user.getAddress()); //司龄 第6行第4列暂时先不考虑 //城市 第7行第4列 sheet.getRow(6).getCell(3).setCellValue(user.getCity()); String fileName = \\\"用户详细数据导出测试.xlsx\\\"; response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;fileName=\\\" + new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(response.getOutputStream()); } \"]},{\"header\":\"3、导出数据带图片以及公式\",\"slug\":\"_3、导出数据带图片以及公式\",\"contents\":[]},{\"header\":\"1、图片\",\"slug\":\"_1、图片\",\"contents\":[\"POI主要提供了两个类来处理照片，这两个类是Patriarch和ClientAnchor前者负责在表中创建图片，后者负责设置图片的大小位置。\",\"//用户头像数据 创建一个字节输出流 ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); //获取图片信息 BufferedImage是一个带缓冲区图像类,主要作用是将一幅图片加载到内存中 BufferedImage image = ImageIO.read(new File(root, user.getPhoto())); //把读取图片放入输出流中 ImageIO.write(image,\\\"jpg\\\",byteArrayOutputStream); // 创建一个绘图控制类，负责画图 Drawing<?> patriarch = sheet.createDrawingPatriarch(); // 指定把图片放到哪个位置 XSSFClientAnchor clientAnchor = new XSSFClientAnchor(0, 0, 0, 0, 2, 1, 4, 5); // 开始把图片写入到sheet指定的位置 patriarch.createPicture(clientAnchor, workbook.addPicture(byteArrayOutputStream.toByteArray(), org.apache.poi.ss.usermodel.Workbook.PICTURE_TYPE_JPEG)); \",\"关于XSSFClientAnchor的8个参数说明：\",\"dx1 - the x coordinate within the first cell.//定义了图片在第一个cell内的偏移x坐标，既左上角所在cell的偏移x坐标，一般可设0 dy1 - the y coordinate within the first cell.//定义了图片在第一个cell的偏移y坐标，既左上角所在cell的偏移y坐标，一般可设0 dx2 - the x coordinate within the second cell.//定义了图片在第二个cell的偏移x坐标，既右下角所在cell的偏移x坐标，一般可设0 dy2 - the y coordinate within the second cell.//定义了图片在第二个cell的偏移y坐标，既右下角所在cell的偏移y坐标，一般可设0 col1 - the column (0 based) of the first cell.//第一个cell所在列，既图片左上角所在列 row1 - the row (0 based) of the first cell.//图片左上角所在行 col2 - the column (0 based) of the second cell.//图片右下角所在列 row2 - the row (0 based) of the second cell.//图片右下角所在行 \"]},{\"header\":\"2、公式\",\"slug\":\"_2、公式\",\"contents\":[\"关于POI支持公式详见官网： https://poi.apache.org/components/spreadsheet/eval-devguide.html\",\"ps:其实在正常开发时应该在模板中直接设置好公式，这样打开直接导出的excel文档时公式会直接运行出我们想要的结果\",\"sheet.getRow(5).getCell(3).setCellFormula(\\\"DATEDIF(B6,TODAY(),\\\\\\\"y\\\\\\\")\\\"); \"]},{\"header\":\"4、基于模板引擎(按照字段写入)\",\"slug\":\"_4、基于模板引擎-按照字段写入\",\"contents\":[]},{\"header\":\"1、说明\",\"slug\":\"_1、说明\",\"contents\":[\"看我们刚才导出时写的代码，必须要提前知道要导出数据在哪一行哪一个单元格，但是如果模板一旦发生调整，那么我们的java代码必须要修改，我们可以自定义个导出的引擎，有了这个引擎即使模板修改了我们的java代码也不用修改\"]},{\"header\":\"2、思路\",\"slug\":\"_2、思路\",\"contents\":[\"在制作模板时，在需要插入数据的位置我们坐上标记，在导出时，对象的属性要和标记做对应，如果对应匹配一样，就把值赋值到相应的位置。\",\"标记：通常是我们导出的字段，这样检查也好一点\"]},{\"header\":\"3、实现\",\"slug\":\"_3、实现-1\",\"contents\":[\"制作模板\",\"添加到项目中\"]},{\"header\":\"4、代码实现\",\"slug\":\"_4、代码实现\",\"contents\":[\"模板引擎的工具类：\",\"public class ExcelEngineUtils { private static SimpleDateFormat sdf = new SimpleDateFormat(\\\"yyyy-MM-dd\\\"); /** * * @param object * @param workbook * @param photoPath * @return */ public static Workbook writeToExcel(Object object, Workbook workbook, String photoPath) throws IOException { //先把bean转成map Map<String, Object> map = EntityUtils.entityToMap(object); //循环遍历每一对数据，把日期型的转成字符串 for (String s : map.keySet()) { Object o = map.get(s); if (o instanceof Date){ map.put(s,sdf.format(o)); } } //获取第一个sheet Sheet sheet = workbook.getSheetAt(0); Cell cell; Row row; for (int i = 0; i < 100; i++) { //不知道数据有多少行的情况下，当获取row为null，退出循环 row = sheet.getRow(i); if (null == row){ break; }else { //不为空，获取单元格的数据 for (int j = 0; j < 100; j++) { cell = row.getCell(j); //写入单元格 if(null != cell){ writeCell(cell,map); } } } } if (StringUtils.isNotBlank(photoPath)){ //获取根目录 File root = new File(ResourceUtils.getFile(\\\"classpath:\\\").getAbsolutePath()); //创建一个字节输出流 ByteArrayOutputStream arrayOutputStream = new ByteArrayOutputStream(); //获取图片信息 BufferedImage是一个带缓冲区图像类,主要作用是将一幅图片加载到内存中 BufferedImage image = ImageIO.read(new File(root, photoPath)); // 把读取图片放入输出流中 ImageIO.write(image,\\\"jpg\\\",arrayOutputStream); Drawing<?> drawingPatriarch = sheet.createDrawingPatriarch(); //获取第二个sheet页的坐标 Sheet sheet1 = workbook.getSheetAt(1); Row row0 = sheet1.getRow(0); int col1 = ((Double) row0.getCell(0).getNumericCellValue()).intValue(); int row1 = ((Double) row0.getCell(1).getNumericCellValue()).intValue(); int col2 = ((Double) row0.getCell(2).getNumericCellValue()).intValue(); int row2 = ((Double) row0.getCell(3).getNumericCellValue()).intValue(); XSSFClientAnchor clientAnchor = new XSSFClientAnchor(0, 0, 0, 0, col1, row1, col2, row2); drawingPatriarch.createPicture(clientAnchor,workbook.addPicture(arrayOutputStream.toByteArray(), Workbook.PICTURE_TYPE_JPEG)); workbook.removeSheetAt(1); } return workbook; } /** * 写入单元格信息 * @param cell * @param map */ public static void writeCell(Cell cell, Map<String, Object> map){ //获取单元格类型 CellType cellType = cell.getCellType(); switch (cellType){ case FORMULA:{ break; } default:{ String cellValue = cell.getStringCellValue(); //就是判断一下获取到单元格中的值是否和map中的key保持一致 if(StringUtils.isNotBlank(cellValue)){ for (String key : map.keySet()) { if(key.equals(cellValue)){ cell.setCellValue(map.get(key).toString()); } } } } } } } \",\"调用\",\"/** * 通过模板引擎导出 * @param id * @param request * @param response */ public void downLoadUserInfoByTemplateEngine(Long id, HttpServletRequest request, HttpServletResponse response) throws IOException, InvalidFormatException { File root = new File(ResourceUtils.getFile(\\\"classpath:\\\").getPath()); //获取图片路径 File file = new File(root.getAbsolutePath(), \\\"/excel_template/userInfo2.xlsx\\\"); //获取工作薄 org.apache.poi.ss.usermodel.Workbook workbook = new XSSFWorkbook(file); //查询用户信息 User user = userMapper.selectByPrimaryKey(id); org.apache.poi.ss.usermodel.Workbook wb = ExcelEngineUtils.writeToExcel(user, workbook, user.getPhoto()); String fileName = \\\"用户模板引擎导出测试.xlsx\\\"; response.setHeader(\\\"Content-Disposition\\\",\\\"attachment;filename=\\\" + new String(fileName.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); workbook.write(response.getOutputStream()); } \"]},{\"header\":\"5、百万数据导出\",\"slug\":\"_5、百万数据导出\",\"contents\":[]},{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"我们都知道Excel可以分为早期的Excel2003版本（使用POI的HSSF对象操作）和Excel2007版本（使用POI的XSSF操作），两者对百万数据的支持如下：\",\"Excel 2003：在POI中使用HSSF对象时，excel 2003最多只允许存储65536条数据，一般用来处理较少的数据量。这时对于百万级别数据，Excel肯定容纳不了。\",\"Excel 2007：当POI升级到XSSF对象时，它可以直接支持excel2007以上版本，因为它采用ooxml格式。这时excel可以支持1048576条数据，单个sheet表就支持近百万条数据。但实际运行时还可能存在问题，原因是执行POI报表所产生的行对象，单元格对象，字体对象，他们都不会销毁，这就导致OOM的风险。\"]},{\"header\":\"2、思路\",\"slug\":\"_2、思路-1\",\"contents\":[\"​ 对于百万数据量的Excel导入导出，只讨论基于Excel2007的解决方法。在ApachePoi 官方提供了对操作大数据量的导入导出的工具和解决办法，操作Excel2007使用XSSF对象，可以分为三种模式：\",\"java代码解析xml\",\"dom4j:一次性加载xml文件再解析\",\"SAX:逐行加载，逐行解析\",\"**用户模式：**用户模式有许多封装好的方法操作简单，但创建太多的对象，非常耗内存（之前使用的方法）\",\"**事件模式：**基于SAX方式解析XML，SAX全称Simple API for XML，它是一个接口，也是一个软件包。它是一种XML解析的替代方法，不同于DOM解析XML文档时把所有内容一次性加载到内存中的方式，它逐行扫描文档，一边扫描，一边解析。\",\"SXSSF对象：是用来生成海量excel数据文件，主要原理是借助临时存储空间生成excel\"]},{\"header\":\"3、导出\",\"slug\":\"_3、导出\",\"contents\":[]},{\"header\":\"1、模拟数据\",\"slug\":\"_1、模拟数据\",\"contents\":[\"创建表\",\"CREATE TABLE `tb_user2` ( `id` bigint(20) NOT NULL COMMENT '用户ID', `user_name` varchar(100) DEFAULT NULL COMMENT '姓名', `phone` varchar(15) DEFAULT NULL COMMENT '手机号', `province` varchar(50) DEFAULT NULL COMMENT '省份', `city` varchar(50) DEFAULT NULL COMMENT '城市', `salary` int(10) DEFAULT NULL, `hire_date` datetime DEFAULT NULL COMMENT '入职日期', `dept_id` bigint(20) DEFAULT NULL COMMENT '部门编号', `birthday` datetime DEFAULT NULL COMMENT '出生日期', `photo` varchar(200) DEFAULT NULL COMMENT '照片路径', `address` varchar(300) DEFAULT NULL COMMENT '现在住址' ) ENGINE=InnoDB DEFAULT CHARSET=utf8; \",\"创建存储过程\",\"DELIMITER $ -- 重新定义“;”分号 DROP PROCEDURE IF EXISTS test_insert $ -- 如果有test_insert这个存储过程就删除 CREATE PROCEDURE test_insert() -- 创建存储过程 BEGIN DECLARE n int DEFAULT 1; -- 定义变量n=1 SET AUTOCOMMIT=0; -- 取消自动提交 while n <= 5000000 do INSERT INTO `tb_user2` VALUES ( n, CONCAT('测试', n), '13800000001', '北京市', '北京市', '11000', '2001-03-01 21:18:29', '1', '1981-03-02 00:00:00', '\\\\\\\\static\\\\\\\\user_photos\\\\\\\\1.jpg', '北京市西城区宣武大街1号院'); SET n=n+1; END while; COMMIT; END $ \",\"开始执行\",\"CALL test_insert(); \",\"插入500W数据大概需要200至300秒左右\",\" /** * 百万数据导出 * @param request * @param response * @throws IOException */ public void downLoadMillionData(HttpServletRequest request, HttpServletResponse response) throws IOException { //创建工作薄 org.apache.poi.ss.usermodel.Workbook sxssfWorkbook = new SXSSFWorkbook(); int page = 1; int pageSize = 200000; Sheet sheet = null; int rowIndex = 1; int num = 0; //总数据量 Cell cell; Row row; while(true){ //用户分页 1000000为一页 List<User> userList = this.findPage(page, pageSize); if(CollectionUtils.isEmpty(userList)){ break; } if(num % 1000000 == 0){ rowIndex = 1; sheet = sxssfWorkbook.createSheet(\\\"第\\\" + num/1000000 + \\\"个工作表\\\"); //设置列宽 sheet.setColumnWidth(0,8*256); sheet.setColumnWidth(1,12*256); sheet.setColumnWidth(2,15*256); sheet.setColumnWidth(3,15*256); sheet.setColumnWidth(4,30*256); // 处理标题 String[] titles = new String[]{\\\"编号\\\",\\\"姓名\\\",\\\"手机号\\\",\\\"入职日期\\\",\\\"现住址\\\"}; // 创建标题行 Row titleRow = sheet.createRow(0); for (int i = 0; i < titles.length; i++) { cell = titleRow.createCell(i); cell.setCellValue(titles[i]); } } //处理数据 for (User user : userList) { row = sheet.createRow(rowIndex); cell = row.createCell(0); cell.setCellValue(user.getId()); row = sheet.createRow(rowIndex); cell = row.createCell(1); cell.setCellValue(user.getUserName()); row = sheet.createRow(rowIndex); cell = row.createCell(2); cell.setCellValue(user.getPhone()); row = sheet.createRow(rowIndex); cell = row.createCell(3); cell.setCellValue(sd.format(user.getHireDate())); row = sheet.createRow(rowIndex); cell = row.createCell(4); cell.setCellValue(user.getAddress()); num++; rowIndex++; } page++; } // 导出的文件名称 String filename=\\\"百万数据.xlsx\\\"; // 设置文件的打开方式和mime类型 ServletOutputStream outputStream = response.getOutputStream(); response.setHeader( \\\"Content-Disposition\\\", \\\"attachment;filename=\\\" + new String(filename.getBytes(),\\\"ISO8859-1\\\")); response.setContentType(\\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"); sxssfWorkbook.write(outputStream); } \"]},{\"header\":\"6、百万数据导入\",\"slug\":\"_6、百万数据导入\",\"contents\":[]},{\"header\":\"1、思路分析\",\"slug\":\"_1、思路分析\",\"contents\":[\"**用户模式：**加载并读取Excel时，是通过一次性的将所有数据加载到内存中再去解析每个单元格内容。当Excel数据量较大时，由于不同的运行环境可能会造成内存不足甚至OOM异常。\",\"**事件模式：**它逐行扫描文档，一边扫描一边解析。由于应用程序只是在读取数据时检查数据，因此不需要将数据存储在内存中，这对于大型文档的解析是个巨大优势。\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[]},{\"header\":\"1、步骤分析\",\"slug\":\"_1、步骤分析\",\"contents\":[\"设置POI的事件模式\",\"根据Excel获取文件流 根据文件流创建OPCPackage 用来组合读取到的xml 组合出来的数据占用的空间更小 创建XSSFReader对象\",\"Sax解析\",\"自定义Sheet处理器 创建Sax的XmlReader对象 设置Sheet的事件处理器 逐行读取\"]},{\"header\":\"2、自定义处理器\",\"slug\":\"_2、自定义处理器\",\"contents\":[\"public class SheetHandle extends XSSFSheetXMLHandler implements XSSFSheetXMLHandler.SheetContentsHandler { private User user; public SheetHandle(Styles styles, Comments comments, SharedStrings strings, SheetContentsHandler sheetContentsHandler, DataFormatter dataFormatter, boolean formulasNotResults) { super(styles, comments, strings, sheetContentsHandler, dataFormatter, formulasNotResults); } /** * 每一行的开始 * @param i 代表的是每一个sheet的行索引 */ @Override public void startRow(int i) { if (0 == i){ user = null; }else { user = new User(); } } /** * 每一行的结束 * @param i */ @Override public void endRow(int i) { if(0 != i){ System.out.println(user); } } /** * 处理每一行的单元格 * @param s 单元格名称 A1 B2 * @param s1 单元格的值 * @param xssfComment */ @SneakyThrows @Override public void cell(String s, String s1, XSSFComment xssfComment) { if (user != null) { //获取单元格首个字符 String substring = s.substring(0, 1); switch (substring){ case \\\"A\\\":{ user.setId(Long.parseLong(s1)); break; } case \\\"B\\\":{ user.setUserName(s1); break; } case \\\"C\\\":{ user.setPhone(s1); break; } case \\\"D\\\":{ user.setHireDate(new SimpleDateFormat(\\\"yyyy-MM-dd\\\").parse(s1)); break; }case \\\"E\\\":{ user.setAddress(s1); break; } } } } } \"]},{\"header\":\"3、自定义解析\",\"slug\":\"_3、自定义解析\",\"contents\":[\"public class ExcelParser { public static void parse(String path) throws Exception { //1.根据Excel获取OPCPackage对象 OPCPackage pkg = OPCPackage.open(path, PackageAccess.READ); try { //2.创建XSSFReader对象 XSSFReader reader = new XSSFReader(pkg); //3.获取SharedStringsTable对象 SharedStringsTable sst = reader.getSharedStringsTable(); //4.获取StylesTable对象 StylesTable styles = reader.getStylesTable(); XMLReader parser = XMLReaderFactory.createXMLReader(); // 处理公共属性：Sheet名，Sheet合并单元格 parser.setContentHandler(new XSSFSheetXMLHandler(styles,sst, new SheetHandle(styles,null,sst,null,null,false), false)); XSSFReader.SheetIterator sheets = (XSSFReader.SheetIterator) reader.getSheetsData(); while (sheets.hasNext()) { InputStream sheetstream = sheets.next(); InputSource sheetSource = new InputSource(sheetstream); try { parser.parse(sheetSource); } finally { sheetstream.close(); } } } finally { pkg.close(); } } } \"]}]},\"/study-tutorial/basic/features/java8.html\":{\"title\":\"Java 8新特性\",\"contents\":[{\"header\":\"1、Lambda表达式\",\"slug\":\"_1、lambda表达式\",\"contents\":[]},{\"header\":\"1、初体验\",\"slug\":\"_1、初体验\",\"contents\":[\"目标：了解使用匿名内部类存在的问题，体验Lambda\",\"匿名内部类存在的问题：当需要启动一个线程去完成任务时，通常会通过Runnable 接口来定义任务内容，并使用Thread 类来启动该线程。\"]},{\"header\":\"1、传统写法\",\"slug\":\"_1、传统写法\",\"contents\":[\"public class LambdaIntro01 { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { System.out.println(\\\"启动一个线程\\\"); } }).start(); } } \",\"代码分析：\",\"由于面向对象的语法要求，首先创建一个Runnable 接口的匿名内部类对象来指定线程要执行的任务内容，再将其交给一个线程来启动。\",\"对于Runnable 的匿名内部类用法，可以分析出几点内容：\",\"Thread 类需要Runnable 接口作为参数，其中的抽象run 方法是用来指定线程任务内容的核心\",\"为了指定run 的方法体，不得不需要Runnable 接口的实现类\",\"为了省去定义一个Runnable 实现类的麻烦，不得不使用匿名内部类\",\"必须覆盖重写抽象run 方法，所以方法名称、方法参数、方法返回值不得不再写一遍，且不能写错\",\"而实际上，似乎只有方法体才是关键所在。\"]},{\"header\":\"2、lambda写法\",\"slug\":\"_2、lambda写法\",\"contents\":[\"Lambda是一个匿名函数，可以理解为一段可以传递的代码。\",\"public class LambdaIntro01 { public static void main(String[] args) { new Thread(() ->{ System.out.println(\\\"启动一个线程\\\"); }).start(); } } \",\"代码分析：\",\"这段代码和刚才的执行效果是完全一样的，可以在JDK 8或更高的编译级别下通过。从代码的语义中可以看出：我们启动了一个线程，而线程任务的内容以一种更加简洁的形式被指定。 我们只需要将要执行的代码放到一个Lambda表达式中，不需要定义类，不需要创建对象。\"]},{\"header\":\"2、Lambda的标准格式\",\"slug\":\"_2、lambda的标准格式\",\"contents\":[]},{\"header\":\"1、格式\",\"slug\":\"_1、格式\",\"contents\":[\"Lambda省去面向对象的条条框框，Lambda的标准格式格式由3个部分组成：\",\"(参数类型 参数名称) -> { 代码体; } \",\"格式说明：\",\"(参数类型 参数名称)：参数列表\",\"{代码体;}：方法体\",\"-> ：箭头，分隔参数列表和方法体\"]},{\"header\":\"2、无参数无返回值的Lambda\",\"slug\":\"_2、无参数无返回值的lambda\",\"contents\":[\"public interface PhoneStore { public abstract void buy(); } \",\"public class LambdaUse02 { public static void main(String[] args) { goStore(new PhoneStore() { @Override public void buy() { System.out.println(\\\"买华为手机\\\"); } }); goStore(() -> System.out.println(\\\"买小米手机\\\")); } public static void goStore(PhoneStore phoneStore){ phoneStore.buy(); } } \"]},{\"header\":\"3、有参数有返回值的Lambda\",\"slug\":\"_3、有参数有返回值的lambda\",\"contents\":[\"下面举例演示java.util.Comparator<T> 接口的使用场景代码，其中的抽象方法定义为：\",\"public abstract int compare(T o1, T o2); \",\"当需要对一个对象集合进行排序时， Collections.sort 方法需要一个Comparator 接口实例来指定排序的规则。\",\"实体类\",\"public class Person { private String name; private Integer age; private Date birthday; public Person(String name, Integer age, Date birthday) { this.name = name; this.age = age; this.birthday = birthday; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public Date getBirthday() { return birthday; } public void setBirthday(Date birthday) { this.birthday = birthday; } @Override public String toString() { return new StringJoiner(\\\", \\\", Person.class.getSimpleName() + \\\"[\\\", \\\"]\\\") .add(\\\"name='\\\" + name + \\\"'\\\") .add(\\\"age=\\\" + age) .add(\\\"birthday=\\\" + birthday) .toString(); } } \",\"传统写法VS Lambda写法\",\"传统写法\",\"public class Lambda03 { public static void main(String[] args) { ArrayList<Person> persons = new ArrayList<>(); persons.add(new Person(\\\"刘德华\\\", 58, new Date())); persons.add(new Person(\\\"张学友\\\", 58, new Date())); persons.add(new Person(\\\"刘德华\\\", 54, new Date())); persons.add(new Person(\\\"黎明\\\", 53, new Date())); Collections.sort(persons, new Comparator<Person>() { @Override public int compare(Person o1, Person o2) { return o1.getAge() - o2.getAge(); } }); for (Person person : persons) { System.out.println(person); } } } \",\"Lambda写法\",\"public class Lambda03 { public static void main(String[] args) { ArrayList<Person> persons = new ArrayList<>(); persons.add(new Person(\\\"刘德华\\\", 58, new Date())); persons.add(new Person(\\\"张学友\\\", 58, new Date())); persons.add(new Person(\\\"刘德华\\\", 54, new Date())); persons.add(new Person(\\\"黎明\\\", 53, new Date())); Collections.sort(persons,(o1, o2)->{ return o1.getAge() - o2.getAge(); }); //另一种写法 Collections.sort(persons, Comparator.comparingInt(Person::getAge)); for (Person person : persons) { System.out.println(person); } } } \"]},{\"header\":\"3、省略格式\",\"slug\":\"_3、省略格式\",\"contents\":[\"在Lambda标准格式的基础上，使用省略写法的规则为：\",\"小括号内参数的类型可以省略\",\"如果小括号内有且仅有一个参数，则小括号可以省略\",\"如果大括号内有且仅有一个语句，可以同时省略大括号、return关键字及语句分号\",\"(int a) ->{ return new Person(); } \",\"省略后\",\"a -> new Person(); \"]},{\"header\":\"4、前提条件\",\"slug\":\"_4、前提条件\",\"contents\":[\"Lambda的语法非常简洁，但是Lambda表达式不是随便使用的，使用时有几个条件要特别注意：\",\"方法的参数或局部变量类型必须为接口才能使用Lambda\",\"接口中有且仅有一个抽象方法\",\"public interface Flyable { public abstract void flying(); } \",\"public class Lambda04 { public static void main(String[] args) { test(() ->{}); Flyable flyable = new Flyable() { @Override public void flying() { } }; Flyable flyable1 = () ->{ }; } public static void test(Flyable flyable){ flyable.flying(); } } \"]},{\"header\":\"5、函数式接口\",\"slug\":\"_5、函数式接口\",\"contents\":[\"函数式接口在Java中是指：有且仅有一个抽象方法的接口。\",\"函数式接口，即适用于函数式编程场景的接口。而Java中的函数式编程体现就是Lambda，所以函数式接口就是可以适用于Lambda使用的接口。只有确保接口中有且仅有一个抽象方法，Java中的Lambda才能顺利地进行推导。\",\"FunctionalInterface注解\",\"与@Override 注解的作用类似，Java 8中专门为函数式接口引入了一个新的注解： @FunctionalInterface 。该注解可用于一个接口的定义上：\",\"@FunctionalInterface public interface Operator { void myMethod(); } \",\"一旦使用该注解来定义接口，编译器将会强制检查该接口是否确实有且仅有一个抽象方法，否则将会报错。不过，即使不使用该注解，只要满足函数式接口的定义，这仍然是一个函数式接口，使用起来都一样。\"]},{\"header\":\"6、Lambda和匿名内部类对比\",\"slug\":\"_6、lambda和匿名内部类对比\",\"contents\":[\"不同\",\"Lambda\",\"匿名内部类\",\"所需的类型\",\"需要的类型必须是接口\",\"需要的类型可以是类,抽象类,接口\",\"抽象方法的数量\",\"所需的接口只能有一个抽象方法\",\"所需的接口中抽象方法的数量随意\",\"实现原理\",\"是在程序运行的时候动态生成class\",\"是在编译后会形成class\",\"总结：当接口中只有一个抽象方法时,建议使用Lambda表达式,其他其他情况还是需要使用匿名内部类\"]},{\"header\":\"2、接口静态方法\",\"slug\":\"_2、接口静态方法\",\"contents\":[\"interface 接口名 { 修饰符 static 返回值类型 方法名() { 代码; } } \"]},{\"header\":\"1、接口静态方法的使用\",\"slug\":\"_1、接口静态方法的使用\",\"contents\":[\"直接使用接口名调用即可：接口名.静态方法名();\",\"public class UseStaticFunction05 { public static void main(String[] args) { A.test(); } interface A{ public static void test(){ System.out.println(\\\"接口静态方法\\\"); } } class B implements A{ //静态方法不能重写 } } \"]},{\"header\":\"2、接口默认方法和静态方法的区别\",\"slug\":\"_2、接口默认方法和静态方法的区别\",\"contents\":[\"默认方法通过实例调用，静态方法通过接口名调用。\",\"默认方法可以被继承，实现类可以直接使用接口默认方法，也可以重写接口默认方法。\",\"静态方法不能被继承，实现类不能重写接口静态方法，只能使用接口名调用。\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"如何选择默认方法和静态方法？\",\"如果这个方法需要被实现类继承或者重写，则使用默认方法；不需要被继承就是用静态方法\"]},{\"header\":\"3、内置函数式接口\",\"slug\":\"_3、内置函数式接口\",\"contents\":[]},{\"header\":\"1、函数式接口的由来\",\"slug\":\"_1、函数式接口的由来\",\"contents\":[\"我们知道使用Lambda表达式的前提是需要有函数式接口。而Lambda使用时不关心接口名，抽象方法名，只关心抽象方法的参数列表和返回值类型。因此为了让我们使用Lambda方便，JDK提供了大量常用的函数式接口。\",\"public class UserFunctionalInterface06 { public static void main(String[] args) { method((arr -> { int sum = 0; for (int i : arr) { sum += i; } return sum; })); } public static void method(Operator operator){ int[] arr = {1, 3, 4, 5}; int sum = operator.getSum(arr); System.out.println(\\\"sum = \\\" + sum); } interface Operator{ public abstract int getSum(int[] arr); } } \"]},{\"header\":\"2、常用内置函数式接口\",\"slug\":\"_2、常用内置函数式接口\",\"contents\":[\"它们主要在java.util.function 包中\",\"Supplier接口\",\"@FunctionalInterface public interface Supplier<T> { public abstract T get(); } \",\"Consumer接口\",\"@FunctionalInterface public interface Consumer<T> { public abstract void accept(T t); } \",\"Function接口\",\"@FunctionalInterface public interface Function<T, R> { public abstract R apply(T t); } \",\"Predicate接口\",\"@FunctionalInterface public interface Predicate<T> { public abstract boolean test(T t); } //Predicate接口用于做判断,返回boolean类型的值 \"]},{\"header\":\"1、Supplier接口\",\"slug\":\"_1、supplier接口\",\"contents\":[\"java.util.function.Supplier<T> 接口，它意味着\\\"供给\\\" , 对应的Lambda表达式需要“对外提供”一个符合泛型类型的对象数据。\",\"@FunctionalInterface public interface Supplier<T> { public abstract T get(); } \",\"供给型接口，通过Supplier接口中的get方法可以得到一个值，无参有返回的接口。\",\"使用Lambda表达式返回数组元素最大值\",\"public class SupplierDemo07 { public static void main(String[] args) { getMax(() ->{ int[] arr = {10,20,99,100,1001,1004}; Arrays.sort(arr); return arr[arr.length - 1]; }); } private static void getMax(Supplier<Integer> supplier){ Integer max = supplier.get(); System.out.println(\\\"max = \\\" + max); } } \"]},{\"header\":\"2、Consumer接口\",\"slug\":\"_2、consumer接口\",\"contents\":[\"java.util.function.Consumer<T> 接口则正好相反，它不是生产一个数据，而是消费一个数据，其数据类型由泛型参数决定。\",\"@FunctionalInterface public interface Consumer<T> { public abstract void accept(T t); } \",\"使用Lambda表达式将一个字符串转成大写和小写的字符串\",\"Consumer消费型接口，可以拿到accept方法参数传递过来的数据进行处理, 有参无返回的接口。基本使用如：\",\"public class ConsumerDemo08 { public static void main(String[] args) { test((String s) ->{ System.out.println(s.toUpperCase(Locale.ROOT)); }); } public static void test(Consumer<String> comsumer){ comsumer.accept(\\\"hello xiaobear\\\"); } } \",\"默认方法：andThen\",\"如果一个方法的参数和返回值全都是Consumer 类型，那么就可以实现效果：消费一个数据的时候，首先做一个操作，然后再做一个操作，实现组合。而这个方法就是Consumer 接口中的default方法andThen 。下面是JDK的源代码：\",\"default Consumer<T> andThen(Consumer<? super T> after) { Objects.requireNonNull(after); return (T t) -> { accept(t); after.accept(t); }; } \",\"备注： java.util.Objects 的requireNonNull 静态方法将会在参数为null时主动抛出NullPointerException 异常。这省去了重复编写if语句和抛出空指针异常的麻烦。\",\"要想实现组合，需要两个或多个Lambda表达式即可，而andThen 的语义正是“一步接一步”操作。例如两个步骤组合的情况：\",\"public class ConsumerAndThen09 { public static void main(String[] args) { //lambda表达式 test((String s) ->{ System.out.println(s.toUpperCase(Locale.ROOT)); }, (String s) ->{ System.out.println(s.toLowerCase(Locale.ROOT)); }); //简写 test(s ->{ System.out.println(s.toUpperCase(Locale.ROOT)); }, s ->{ System.out.println(s.toLowerCase(Locale.ROOT)); }); } public static void test(Consumer<String> c1, Consumer<String> c2){ String str = \\\"hello world\\\"; c1.andThen(c2).accept(str); c2.andThen(c1).accept(str); } } \"]},{\"header\":\"3、Function接口\",\"slug\":\"_3、function接口\",\"contents\":[\"java.util.function.Function<T,R> 接口用来根据一个类型的数据得到另一个类型的数据，前者称为前置条件，后者称为后置条件。有参数有返回值。\",\"@FunctionalInterface public interface Function<T, R> { public abstract R apply(T t); } \",\"使用Lambda表达式将字符串转成数字\",\"Function转换型接口，对apply方法传入的T类型数据进行处理，返回R类型的结果，有参有返回的接口。使用的场景 例如：将String 类型转换为Integer 类型。\",\"public class Function10 { public static void main(String[] args) { test(s -> { return Integer.parseInt(s); }); //简写 test(Integer::parseInt); } public static void test(Function<String, Integer> func){ Integer apply = func.apply(\\\"10\\\"); System.out.println(apply); } } \",\"默认方法：andThen Function 接口中有一个默认的andThen 方法，用来进行组合操作。JDK源代码如：\",\"default <V> Function<T, V> andThen(Function<? super R, ? extends V> after) { Objects.requireNonNull(after); return (T t) -> after.apply(apply(t)); } \",\"该方法同样用于“先做什么，再做什么”的场景，和Consumer 中的andThen 差不多：\",\"public class FunctionAndThen11 { public static void main(String[] args) { test(Integer::parseInt, integer -> { return integer * 10; }); } public static void test(Function<String, Integer> f1, Function<Integer, Integer> f2){ Integer apply = f1.andThen(f2).apply(\\\"66\\\"); System.out.println(apply); } } \",\"第一个操作是将字符串解析成为int数字，第二个操作是乘以10。两个操作通过andThen 按照前后顺序组合到了一起。\",\"请注意，Function的前置条件泛型和后置条件泛型可以相同。\"]},{\"header\":\"4、Predicate接口\",\"slug\":\"_4、predicate接口\",\"contents\":[\"有时候我们需要对某种类型的数据进行判断，从而得到一个boolean值结果。这时可以使用java.util.function.Predicate<T> 接口。\",\"@FunctionalInterface public interface Predicate<T> { public abstract boolean test(T t); } //Predicate接口用于做判断,返回boolean类型的值 \",\"使用Lambda判断一个人名如果超过3个字就认为是很长的名字\",\"对test方法的参数T进行判断，返回boolean类型的结果。用于条件判断的场景：\",\"public class Predicate12 { public static void main(String[] args) { test(s -> s.length() > 3,\\\"华总的小熊\\\"); } public static void test(Predicate<String> predicate, String str){ boolean test = predicate.test(str); System.out.println(\\\"你爱她吗：\\\" + test); } } \",\"条件判断的标准是传入的Lambda表达式逻辑，只要名称长度大于3则认为很长。\",\"默认方法：and\",\"既然是条件判断，就会存在与、或、非三种常见的逻辑关系。其中将两个Predicate 条件使用“与”逻辑连接起来实 现“并且”的效果时，可以使用default方法and 。其JDK源码为：\",\"default Predicate<T> and(Predicate<? super T> other) { Objects.requireNonNull(other); return (t) -> test(t) && other.test(t); } \",\"测试内容：\",\"使用Lambda表达式判断一个字符串中即包含W,也包含H 使用Lambda表达式判断一个字符串中包含W或者包含H 使用Lambda表达式判断一个字符串中即不包含W\",\"如果要判断一个字符串既要包含大写“H”，又要包含大写“W”\",\"public class Predicate_And_Or_Negate13 { public static void main(String[] args) { test(s -> s.contains(\\\"H\\\"),s -> s.contains(\\\"W\\\")); } public static void test(Predicate<String> p1, Predicate<String> p2) { String str = \\\"HelloWorld\\\"; boolean b1 = p1.test(str); // 判断包含大写“H” boolean b2 = p2.test(str); // 判断包含大写“W” if (b1 && b2) { System.out.println(\\\"即包含W,也包含H\\\"); } boolean bb = p1.and(p2).test(str); if (bb) { System.out.println(\\\"即包含W,也包含H\\\"); } } } \",\"默认方法：or 使用Lambda表达式判断一个字符串中包含W或者包含H 与and 的“与”类似，默认方法or 实现逻辑关系中的“或”。JDK源码为：\",\"default Predicate<T> or(Predicate<? super T> other) { Objects.requireNonNull(other); return (t) -> test(t) || other.test(t); } \",\"如果希望实现逻辑“字符串包含大写H或者包含大写W”，那么代码只需要将“and”修改为“or”名称即可，其他都不变：\",\"public class Predicate_And_Or_Negate13 { public static void main(String[] args) { test1(s -> s.contains(\\\"H\\\"),s -> s.contains(\\\"W\\\")); } public static void test1(Predicate<String> p1, Predicate<String> p2){ String str = \\\"HelloWorld\\\"; boolean b1 = p1.test(str); // 判断包含大写“H” boolean b2 = p2.test(str); // 判断包含大写“W” if (b1 || b2){ System.out.println(\\\"有H,或者W\\\"); } boolean test = p1.or(p2).test(str); if(test){ System.out.println(\\\"有H,或者W\\\"); } } } \",\"默认方法：negate 使用Lambda表达式判断一个字符串中即不包含W “与”、“或”已经了解了，剩下的“非”（取反）也会简单。默认方法negate 的JDK源代码为：\",\"default Predicate<T> negate() { return (t) -> !test(t); } \",\"从实现中很容易看出，它是执行了test方法之后，对结果boolean值进行“!”取反而已。一定要在test 方法调用之前调用negate 方法，正如and 和or 方法一样：\",\"public class Predicate_And_Or_Negate13 { public static void main(String[] args) { test2(s -> s.contains(\\\"H\\\"),s -> s.contains(\\\"W\\\")); } public static void test2(Predicate<String> p1, Predicate<String> p2){ String str = \\\"HelloWorld\\\"; boolean b1 = p1.test(str); // 判断包含大写“H” boolean b2 = p2.test(str); // 判断包含大写“W” if (!b1){ System.out.println(\\\"没有H\\\"); } boolean test = p1.negate().test(str); if (test){ System.out.println(\\\"没有H\\\"); } } } \"]},{\"header\":\"4、引用\",\"slug\":\"_4、引用\",\"contents\":[]},{\"header\":\"1、方法引用\",\"slug\":\"_1、方法引用\",\"contents\":[\"符号表示 : ::\",\"符号说明 : 双冒号为方法引用运算符，而它所在的表达式被称为方法引用。\",\"应用场景 : 如果Lambda所要实现的方案 , 已经有其他方法存在相同方案，那么则可以使用方法引用。\"]},{\"header\":\"2、常见的引用方式\",\"slug\":\"_2、常见的引用方式\",\"contents\":[\"instanceName::methodName 对象::方法名\",\"ClassName::staticMethodName 类名::静态方法\",\"ClassName::methodName 类名::普通方法\",\"ClassName::new 类名::new 调用的构造器\",\"TypeName[]::new String[]::new 调用数组的构造器\"]},{\"header\":\"3、对象::方法名\",\"slug\":\"_3、对象-方法名\",\"contents\":[\"这是最常见的一种用法，与上例相同。如果一个类中已经存在了一个成员方法，则可以通过对象名引用成员方法，代码为：\",\"public class Quote01 { public static void main(String[] args) { Date date = new Date(); Supplier<Long> longCallable = () -> date.getTime(); System.out.println(longCallable.get()); //对象::实例方法 Supplier<Long> time = date::getTime; System.out.println(time.get()); } } \",\"方法引用的注意事项\",\"被引用的方法，参数要和接口中抽象方法的参数一样\",\"当接口抽象方法有返回值时，被引用的方法也必须有返回值\"]},{\"header\":\"4、类名::引用静态方法\",\"slug\":\"_4、类名-引用静态方法\",\"contents\":[\"由于在java.lang.System 类中已经存在了静态方法currentTimeMillis ，所以当我们需要通过Lambda来调用该方法时,可以使用方法引用 , 写法是：\",\"public class Quote02 { public static void main(String[] args) { Supplier<Long> time = () -> System.currentTimeMillis(); System.out.println(time.get()); //类名::静态方法 Supplier<Long> timeMillis = System::currentTimeMillis; System.out.println(timeMillis.get()); } } \"]},{\"header\":\"5、类名::引用实例方法\",\"slug\":\"_5、类名-引用实例方法\",\"contents\":[\"Java面向对象中，类名只能调用静态方法，类名引用实例方法是有前提的，实际上是拿第一个参数作为方法的调用者。\",\"public class Quote03 { public static void main(String[] args) { Function<String, Integer> f1 = (s) -> { return s.length(); }; System.out.println(f1.apply(\\\"abc\\\")); Function<String, Integer> f2 = String::length; System.out.println(f2.apply(\\\"def\\\")); BiFunction<String, Integer, String> bif = String::substring; String hello = bif.apply(\\\"hello\\\", 2); System.out.println(\\\"hello = \\\" + hello); } } \"]},{\"header\":\"6、类名::new引用构造器\",\"slug\":\"_6、类名-new引用构造器\",\"contents\":[\"由于构造器的名称与类名完全一样。所以构造器引用使用类名称::new 的格式表示。首先是一个简单的Person 类：\",\"public class Quote04 { public static void main(String[] args) { Supplier<Person> person = () -> { return new Person(); }; System.out.println(person.get()); Supplier<Person> person1 = Person::new; System.out.println(person1.get()); } } \"]},{\"header\":\"7、数组::new 引用数组构造器\",\"slug\":\"_7、数组-new-引用数组构造器\",\"contents\":[\"数组也是Object 的子类对象，所以同样具有构造器，只是语法稍有不同。\",\"public class Quote05 { public static void main(String[] args) { Function<Integer, String[]> fun = (len) -> { return new String[len]; }; String[] arr1 = fun.apply(10); System.out.println(arr1 + \\\", \\\" + arr1.length); Function<Integer, String[]> fun2 = String[]::new; String[] arr2 = fun.apply(5); System.out.println(arr2 + \\\", \\\" + arr2.length); } } \",\"总结\",\"方法引用是对Lambda表达式符合特定情况下的一种缩写，它使得我们的Lambda表达式更加的精简，也可以理解为Lambda表达式的缩写形式 , 不过要注意的是方法引用只能\\\"引用\\\"已经存在的方法!\"]},{\"header\":\"5、Stream流\",\"slug\":\"_5、stream流\",\"contents\":[]},{\"header\":\"1、思想概述\",\"slug\":\"_1、思想概述\",\"contents\":[\"Stream和IO流(InputStream/OutputStream)没有任何关系，请暂时忘记对传统IO流的固有印象！\",\"Stream流式思想类似于工厂车间的“生产流水线”，Stream流不是一种数据结构，不保存数据，而是对数据进行加工处理。Stream可以看作是流水线上的一个工序。在流水线上，通过多个工序让一个原材料加工成一个商品。\",\"Stream API能让我们快速完成许多复杂的操作，如筛选、切片、映射、查找、去除重复，统计，匹配和归约。\"]},{\"header\":\"2、获取流的两种方式\",\"slug\":\"_2、获取流的两种方式\",\"contents\":[\"java.util.stream.Stream<T> 是JDK 8新加入的流接口。\",\"所有的Collection 集合都可以通过stream 默认方法获取流；\",\"Stream 接口的静态方法of 可以获取数组对应的流。\"]},{\"header\":\"1、根据Collection获取流\",\"slug\":\"_1、根据collection获取流\",\"contents\":[\"首先， java.util.Collection 接口中加入了default方法stream 用来获取流，所以其所有实现类均可获取流。\",\"public class GetStream01 { public static void main(String[] args) { ArrayList<String> list = new ArrayList<>(); Stream<String> stream = list.stream(); Set<String> set = new HashSet<>(); Stream<String> stream2 = set.stream(); Vector<String> vector = new Vector<>(); Stream<String> stream3 = vector.stream(); // Map获取流 Map<String, String> map = new HashMap<>(); //java.util.Map 接口不是Collection 的子接口，所以获取对应的流需要分key、value或entry等情况 Stream<String> keyStream = map.keySet().stream(); Stream<String> valueStream = map.values().stream(); Stream<Map.Entry<String, String>> entryStream = map.entrySet().stream(); } } \"]},{\"header\":\"2、Stream中的静态方法of获取流\",\"slug\":\"_2、stream中的静态方法of获取流\",\"contents\":[\"由于数组对象不可能添加默认方法，所以Stream 接口中提供了静态方法of ，使用很简单。\",\"public class GetStream02 { public static void main(String[] args) { // Stream中的静态方法: static Stream of(T... values) Stream<String> stringStream = Stream.of(\\\"aa\\\", \\\"bb\\\", \\\"cc\\\"); String[] arr = {\\\"aa\\\", \\\"bb\\\", \\\"cc\\\"}; Stream<String> stream7 = Stream.of(arr); Integer[] arr2 = {11, 22, 33}; Stream<Integer> stream8 = Stream.of(arr2); // 注意:基本数据类型的数组不行 int[] arr3 = {11, 22, 33}; Stream<int[]> stream9 = Stream.of(arr3); } } \",\"备注： of 方法的参数其实是一个可变参数，所以支持数组。\"]},{\"header\":\"3、Stream常用方法和注意事项\",\"slug\":\"_3、stream常用方法和注意事项\",\"contents\":[]},{\"header\":\"1、常用用法\",\"slug\":\"_1、常用用法\",\"contents\":[\"Stream流模型的操作很丰富，这里介绍一些常用的API。这些方法可以被分成两种：\",\"方法名\",\"方法作用\",\"返回值类型\",\"方法种类\",\"count\",\"统计个数\",\"long\",\"终结\",\"forEach\",\"逐个处理\",\"void\",\"终结\",\"filter\",\"过滤\",\"Stream\",\"函数拼接\",\"limit\",\"取前几个\",\"Stream\",\"函数拼接\",\"skip\",\"跳过前几个\",\"Stream\",\"函数拼接\",\"map\",\"映射\",\"Stream\",\"函数拼接\",\"concat\",\"组合\",\"Stream\",\"函数拼接\",\"终结方法：返回值类型不再是Stream 类型的方法，不再支持链式调用。本小节中，终结方法包括count 和forEach 方法。\",\"非终结方法：返回值类型仍然是Stream 类型的方法，支持链式调用。（除了终结方法外，其余方法均为非终结方法。）\",\"​\"]},{\"header\":\"2、注意事项\",\"slug\":\"_2、注意事项\",\"contents\":[\"Stream只能操作一次\",\"Stream方法返回的是新的流\",\"Stream不调用终结方法，中间的操作不会执行\"]},{\"header\":\"3、forEach方法\",\"slug\":\"_3、foreach方法\",\"contents\":[\"forEach 用来遍历流中的数据\",\"void forEach(Consumer<? super T> action); \",\"该方法接收一个Consumer 接口函数，会将每一个流元素交给该函数进行处理。例如：\",\"public class StreamMethodForEach { public static void main(String[] args) { List<String> one = new ArrayList<>(); Collections.addAll(one, \\\"迪丽热巴\\\", \\\"宋远桥\\\", \\\"苏星河\\\", \\\"老子\\\", \\\"庄子\\\", \\\"孙子\\\"); one.forEach(System.out::println); } } \"]},{\"header\":\"4、count方法\",\"slug\":\"_4、count方法\",\"contents\":[\"Stream流提供count 方法来统计其中的元素个数\",\"long count(); \",\"该方法返回一个long值代表元素个数。基本使用：\",\"public class StreamMethodCount { public static void main(String[] args) { List<String> one = new ArrayList<>(); Collections.addAll(one, \\\"迪丽热巴\\\", \\\"宋远桥\\\", \\\"苏星河\\\", \\\"老子\\\", \\\"庄子\\\", \\\"孙子\\\"); System.out.println(one.stream().count()); } } \"]},{\"header\":\"5、filter方法\",\"slug\":\"_5、filter方法\",\"contents\":[\"filter用于过滤数据，返回符合过滤条件的数据\",\"可以通过filter 方法将一个流转换成另一个子集流。方法声明：\",\"Stream<T> filter(Predicate<? super T> predicate); \",\"该接口接收一个Predicate 函数式接口参数（可以是一个Lambda或方法引用）作为筛选条件。\",\"Stream流中的filter 方法基本使用的代码如：\",\"public class StreamMethodFilter { public static void main(String[] args) { List<String> one = new ArrayList<>(); Collections.addAll(one, \\\"迪丽热巴\\\", \\\"宋远桥\\\", \\\"苏星河\\\", \\\"老子\\\", \\\"庄子\\\", \\\"孙子\\\"); //筛选出姓名长度为2个字。 one.stream().filter(o -> 2 == o.length()).forEach(System.out::println); } \"]},{\"header\":\"6、limit方法\",\"slug\":\"_6、limit方法\",\"contents\":[\"limit 方法可以对流进行截取，只取用前n个。\",\"Stream<T> limit(long maxSize); \",\"参数是一个long型，如果集合当前长度大于参数则进行截取。否则不进行操作。基本使用：\",\"public class StreamMethodLimit { public static void main(String[] args) { List<String> one = new ArrayList<>(); Collections.addAll(one, \\\"迪丽热巴\\\", \\\"宋远桥\\\", \\\"苏星河\\\", \\\"老子\\\", \\\"庄子\\\", \\\"孙子\\\"); //筛选出姓名为2个字，并且取出前两个并打印 one.stream().filter(o -> 2 == o.length()).limit(2).forEach(System.out::println); } } \"]},{\"header\":\"7、skip方法\",\"slug\":\"_7、skip方法\",\"contents\":[\"如果希望跳过前几个元素，可以使用skip 方法获取一个截取之后的新流：\",\"Stream<T> skip(long n); \",\"如果流的当前长度大于n，则跳过前n个；否则将会得到一个长度为0的空流。基本使用：\",\"public class StreamMethodSkip { public static void main(String[] args) { List<String> one = new ArrayList<>(); Collections.addAll(one, \\\"迪丽热巴\\\", \\\"宋远桥\\\", \\\"苏星河\\\", \\\"老子\\\", \\\"庄子\\\", \\\"孙子\\\"); //筛选出姓名为2个字，并且取出前两个然后跳过第一个并打印 one.stream().filter(o -> 2 == o.length()).limit(2).skip(1).forEach(System.out::println); } } \"]},{\"header\":\"8、map方法\",\"slug\":\"_8、map方法\",\"contents\":[\"如果需要将流中的元素映射到另一个流中，可以使用map 方法。方法签名：\",\"<R> Stream<R> map(Function<? super T, ? extends R> mapper); \",\"该接口需要一个Function 函数式接口参数，可以将当前流中的T类型数据转换为另一种R类型的流。\",\"public class StreamMethodMap { public static void main(String[] args) { Stream<String> original = Stream.of(\\\"11\\\", \\\"22\\\", \\\"33\\\"); Stream<Integer> result = original.map(Integer::parseInt); result.forEach(s -> System.out.println(s + 10)); } } \"]},{\"header\":\"9、sorted方法\",\"slug\":\"_9、sorted方法\",\"contents\":[\"如果需要将数据排序，可以使用sorted 方法。\",\"Stream<T> sorted(); Stream<T> sorted(Comparator<? super T> comparator); \",\"sorted 方法根据元素的自然顺序排序，也可以指定比较器排序。\",\"public class StreamMethodSort { public static void main(String[] args) { Stream.of(33, 22, 11, 55) .sorted() .sorted((o1, o2) -> o2 - o1) .forEach(System.out::println); } } \"]},{\"header\":\"10、distinct方法\",\"slug\":\"_10、distinct方法\",\"contents\":[\"如果需要去除重复数据，可以使用distinct 方法。方法签名：\",\"Stream<T> distinct(); \",\"public class StreamMethodDistinct { public static void main(String[] args) { //去重并排序 Stream.of(22, 33, 22, 11, 33) .distinct() .sorted(((o1, o2) -> o2 - o1)) .forEach(System.out::println); } } \"]},{\"header\":\"11、match方法\",\"slug\":\"_11、match方法\",\"contents\":[\"如果需要判断数据是否匹配指定的条件，可以使用Match 相关方法。\",\"boolean allMatch(Predicate<? super T> predicate); boolean anyMatch(Predicate<? super T> predicate); boolean noneMatch(Predicate<? super T> predicate); \",\"public class StreamMethodMatch { public static void main(String[] args) { boolean b = Stream.of(5, 3, 6, 1) // allMatch: 元素是否全部满足条件 // .allMatch(e -> e > 0); // anyMatch: 元素是否任意有一个满足条件 // .anyMatch(e -> e > 5); .noneMatch(e -> e < 0); // noneMatch: 元素是否全部不满足条件 System.out.println(\\\"b = \\\" + b); } } \"]},{\"header\":\"12、find方法\",\"slug\":\"_12、find方法\",\"contents\":[\"如果需要找到某些数据，可以使用find 相关方法。\",\"Optional<T> findFirst(); Optional<T> findAny(); \",\"public class StreamMethodFind { public static void main(String[] args) { Optional<Integer> first = Stream.of(5, 3, 6, 1).findFirst(); System.out.println(\\\"first = \\\" + first.get()); Optional<Integer> any = Stream.of(5, 3, 6, 1).findAny(); System.out.println(\\\"any = \\\" + any.get()); } } \"]},{\"header\":\"13、max和min方法\",\"slug\":\"_13、max和min方法\",\"contents\":[\"如果需要获取最大和最小值，可以使用max 和min 方法。\",\"Optional<T> max(Comparator<? super T> comparator); Optional<T> min(Comparator<? super T> comparator); \",\"public class StreamMethodMaxAndMin { public static void main(String[] args) { Optional<Integer> max = Stream.of(5, 3, 6, 1).max(Comparator.comparingInt(o -> o)); System.out.println(\\\"first = \\\" + max.get()); Optional<Integer> min = Stream.of(5, 3, 6, 1).min(Comparator.comparingInt(o -> o)); System.out.println(\\\"any = \\\" + min.get()); } } \"]},{\"header\":\"14、reduce方法\",\"slug\":\"_14、reduce方法\",\"contents\":[\"如果需要将所有数据归纳得到一个数据，可以使用reduce 方法。\",\"T reduce(T identity, BinaryOperator<T> accumulator); \",\"public class StreamMethodReduce { public static void main(String[] args) { int reduce = Stream.of(4, 5, 3, 9) .reduce(0, (a, b) -> { System.out.println(\\\"a = \\\" + a + \\\", b = \\\" + b); return a + b; }); // reduce: // 第一次将默认做赋值给x, 取出第一个元素赋值给y,进行操作 // 第二次,将第一次的结果赋值给x, 取出二个元素赋值给y,进行操作 // 第三次,将第二次的结果赋值给x, 取出三个元素赋值给y,进行操作 // 第四次,将第三次的结果赋值给x, 取出四个元素赋值给y,进行操作 System.out.println(\\\"reduce = \\\" + reduce); int reduce2 = Stream.of(4, 5, 3, 9) .reduce(0, Integer::sum); System.out.println(\\\"reduce2 = \\\" + reduce2); int reduce3 = Stream.of(4, 5, 3, 9).reduce(0, Integer::sum); int max = Stream.of(4, 5, 3, 9) .reduce(0, (x, y) -> x > y ? x : y); System.out.println(\\\"max = \\\" + max); } } \"]},{\"header\":\"15、map和reduce的组合使用\",\"slug\":\"_15、map和reduce的组合使用\",\"contents\":[\"public class StreamMethodReduceAndMap { public static void main(String[] args) { Stream<Person> personStream = Stream.of( new Person(\\\"1111\\\", 18, new Date()), new Person(\\\"2222\\\", 19, new Date()), new Person(\\\"33333\\\", 20, new Date()), new Person(\\\"4444\\\", 21, new Date())); Integer totalAge = personStream.map(Person::getAge).reduce(0, (x, y) -> x + y); System.out.println(\\\"总年龄为：\\\" + totalAge); Integer maxAge = personStream.map(Person::getAge).reduce(0, (x, y) -> x > y ? x : y); System.out.println(\\\"最大年龄为：\\\" + maxAge); } } \"]},{\"header\":\"16、mapToInt方法\",\"slug\":\"_16、maptoint方法\",\"contents\":[\"如果需要将Stream中的Integer类型数据转成int类型，可以使用mapToInt 方法。方法签名：\",\"IntStream mapToInt(ToIntFunction<? super T> mapper); \",\"public class StreamMethodMapToInt { public static void main(String[] args) { Stream<Integer> stream = Arrays.stream(new Integer[]{1, 2, 3, 4, 5}); //注释的原因是因为流只能使用一次 // System.out.println(stream.filter(o -> o > 3) // .reduce(0, Integer::sum)); IntStream intStream = stream.mapToInt(Integer::intValue); // System.out.println(intStream.filter(o -> o < 3) // .reduce(0, Integer::sum)); System.out.println(intStream.filter(o -> o > 3).summaryStatistics().getSum()); } } \"]},{\"header\":\"17、concat方法\",\"slug\":\"_17、concat方法\",\"contents\":[\"如果有两个流，希望合并成为一个流，那么可以使用Stream 接口的静态方法concat ：\",\"static <T> Stream<T> concat(Stream<? extends T> a, Stream<? extends T> b) \",\"这是一个静态方法，与java.lang.String 当中的concat 方法是不同的。\",\"public class StreamMethodConcat { public static void main(String[] args) { Stream<String> stringStream = Stream.of(\\\"1111\\\"); Stream<String> stringStream1 = Stream.of(\\\"2222\\\"); Stream<String> concat = Stream.concat(stringStream, stringStream1); concat.forEach(System.out::println); } } \"]},{\"header\":\"4、收集流中的结果\",\"slug\":\"_4、收集流中的结果\",\"contents\":[]},{\"header\":\"1、收集到数组中\",\"slug\":\"_1、收集到数组中\",\"contents\":[\"Stream提供toArray 方法来将结果放到一个数组中，返回值类型是Object[]的：\",\"Object[] toArray(); \",\"public class StreamToArray { public static void main(String[] args) { Stream<String> aa = Stream.of(\\\"aa\\\", \\\"bb\\\", \\\"cc\\\"); // Object[] toArray = aa.toArray(); // for (Object o : toArray) { // System.out.println(o); // } String[] strings = aa.toArray(String[]::new); for (String s : strings) { System.out.println(s); } } } \"]},{\"header\":\"2、收集到集合中\",\"slug\":\"_2、收集到集合中\",\"contents\":[\"Stream流提供collect 方法，其参数需要一个java.util.stream.Collector<T,A, R> 接口对象来指定收集到哪种集合中。java.util.stream.Collectors 类提供一些方法，可以作为Collector`接口的实例：\",\"public static <T> Collector<T, ?, List<T>> toList() ：转换为List 集合。\",\"public static <T> Collector<T, ?, Set<T>> toSet() ：转换为Set 集合。\",\"public class StreamToCollection { public static void main(String[] args) { Stream<String> stream = Stream.of(\\\"11\\\", \\\"22\\\", \\\"11\\\"); List<String> list = stream.collect(Collectors.toList()); list.forEach(System.out::println); Set<String> set = stream.collect(Collectors.toSet()); set.forEach(System.out::println); ArrayList<String> list1 = stream.collect(Collectors.toCollection(ArrayList::new)); HashSet<String> set1 = stream.collect(Collectors.toCollection(HashSet::new)); } } \"]},{\"header\":\"3、对流中数据进行聚合计算\",\"slug\":\"_3、对流中数据进行聚合计算\",\"contents\":[\"当我们使用Stream流处理数据后，可以像数据库的聚合函数一样对某个字段进行操作。比如获取最大值，获取最小值，求总和，平均值，统计数量。\",\"public class StreamToOther { public static void main(String[] args) { Stream<Integer> stream = Stream.of(12, 16, 15, 86, 35, 54, 76, 54); IntSummaryStatistics statistics = stream.mapToInt(Integer::intValue).summaryStatistics(); //获取最大值 int max = statistics.getMax(); double average = statistics.getAverage(); int min = statistics.getMin(); long sum = statistics.getSum(); } } \"]},{\"header\":\"4、对数据进行分组\",\"slug\":\"_4、对数据进行分组\",\"contents\":[\"public class StreamToGroup { public static void main(String[] args) { Stream<Person> personStream = Stream.of( new Person(\\\"1111\\\", 18, new Date()), new Person(\\\"2222\\\", 19, new Date()), new Person(\\\"33333\\\", 20, new Date()), new Person(\\\"4444\\\", 21, new Date())); //Map<Integer, List<Person>> map = personStream.collect(Collectors.groupingBy(Person::getAge)); Map<String, List<Person>> map1 = personStream.collect(Collectors.groupingBy((s) -> { if (s.getAge() > 18) { return \\\"成年了\\\"; } else { return \\\"刚好成年\\\"; } })); map1.forEach((k,v) ->{ System.out.println(k + \\\"::\\\" + v); }); } \"]},{\"header\":\"5、对流中数据进行多级分组\",\"slug\":\"_5、对流中数据进行多级分组\",\"contents\":[\"public class StreamToGradeGrouping { public static void main(String[] args) { Stream<Person> personStream = Stream.of( new Person(\\\"1111\\\", 18, new Date()), new Person(\\\"2222\\\", 19, new Date()), new Person(\\\"33333\\\", 20, new Date()), new Person(\\\"4444\\\", 21, new Date())); //现根据年龄进行分组，再根据年龄大小进行分组 Map<Integer, Map<String, List<Person>>> mapMap = personStream.collect(Collectors.groupingBy(s -> s.getAge(), Collectors.groupingBy(s -> { if (s.getAge() > 18) { return \\\"成年了\\\"; } else { return \\\"刚好成年\\\"; } }))); mapMap.forEach((k,v) ->{ System.out.println(k + \\\":\\\" + v); }); } } \"]},{\"header\":\"6、对数据进行分区\",\"slug\":\"_6、对数据进行分区\",\"contents\":[\"Collectors.partitioningBy 会根据值是否为true，把集合分割为两个列表，一个true列表，一个false列表。\",\"public class StreamToPartitioningBy { public static void main(String[] args) { Stream<Person> personStream = Stream.of( new Person(\\\"1111\\\", 18, new Date()), new Person(\\\"2222\\\", 19, new Date()), new Person(\\\"33333\\\", 20, new Date()), new Person(\\\"4444\\\", 21, new Date())); Map<Boolean, List<Person>> map = personStream.collect(Collectors.partitioningBy(s -> s.getAge() > 20)); map.forEach((k,v) ->{ System.out.println(k + \\\":\\\" + v); }); } \"]},{\"header\":\"7、对数据进行拼接\",\"slug\":\"_7、对数据进行拼接\",\"contents\":[\"public class StreamToJoining { public static void main(String[] args) { Stream<Person> personStream = Stream.of( new Person(\\\"1111\\\", 18, new Date()), new Person(\\\"2222\\\", 19, new Date()), new Person(\\\"33333\\\", 20, new Date()), new Person(\\\"4444\\\", 21, new Date())); String collect = personStream.map(Person::getName).collect(Collectors.joining(\\\">_<\\\", \\\"^_^\\\", \\\"^v^\\\")); System.out.println(collect); } } \"]},{\"header\":\"6、并行流\",\"slug\":\"_6、并行流\",\"contents\":[]},{\"header\":\"1、初体验\",\"slug\":\"_1、初体验-1\",\"contents\":[]},{\"header\":\"1、串行的Stream流\",\"slug\":\"_1、串行的stream流\",\"contents\":[\"简单的说，就是在一个线程上执行。\",\"public class StreamDemo1 { public static void main(String[] args) { long count = Stream.of(4, 5, 3, 9, 1, 2, 6) .filter(s -> { System.out.println(Thread.currentThread() + \\\", s = \\\" + s); return true; }) .count(); System.out.println(\\\"count = \\\" + count); } } \"]},{\"header\":\"2、并行的Stream流\",\"slug\":\"_2、并行的stream流\",\"contents\":[\"parallelStream其实就是一个并行执行的流。它通过默认的ForkJoinPool，可能提高多线程任务的速度。\",\"public class StreamDemo2 { public static void main(String[] args) { long count = Stream.of(4, 5, 3, 9, 1, 2, 6) .parallel() .filter(s -> { System.out.println(Thread.currentThread() + \\\", s = \\\" + s); return true; }) .count(); System.out.println(\\\"count = \\\" + count); } } \",\"获取并行流有两种方式:\",\"直接获取并行流: parallelStream()\",\"将串行流转成并行流: parallel()\"]},{\"header\":\"3、并行和串行Stream流的效率对比\",\"slug\":\"_3、并行和串行stream流的效率对比\",\"contents\":[\"使用for循环，串行Stream流，并行Stream流来对5亿个数字求和。看消耗的时间。\",\"public class StreamDemo3 { private static final long TIMES = 50000000000L; public static void main(String[] args) { //正常处理 for long startFor = System.currentTimeMillis(); long result = 0; for (long i = 0; i < TIMES; i++) { result += i; } System.out.println(\\\"for循环执行时间：\\\"+(System.currentTimeMillis()-startFor)); //串行流处理时间 long startStream = System.currentTimeMillis(); LongStream.rangeClosed(0, TIMES).reduce(0, Long::sum); System.out.println(\\\"stream执行时间：\\\"+(System.currentTimeMillis()-startStream)); //并行流处理时间 long startParallelStream = System.currentTimeMillis(); LongStream.rangeClosed(0,TIMES).parallel().reduce(0,Long::sum); System.out.println(\\\"parallelStream执行时间：\\\"+(System.currentTimeMillis()-startParallelStream)); } } \",\"输出结果为：\",\"for循环执行时间：13591 stream执行时间：18298 parallelStream执行时间：5039 \",\"Stream并行处理的过程会分而治之，也就是将一个大任务切分成多个小任务，这表示每个任务都是一个操作。\"]},{\"header\":\"2、parallelStream线程安全问题\",\"slug\":\"_2、parallelstream线程安全问题\",\"contents\":[\"public class ParallelStreamNotice { public static void main(String[] args) { ArrayList<Integer> list = new ArrayList<Integer>(); for (int i = 0; i < 1000; i++) { list.add(i); } System.out.println(list.size()); //并行流创建 List<Integer> newList = new ArrayList<>(); list.parallelStream() .forEach(newList::add); System.out.println(newList.size()); } } \",\"输出结构为\",\"1000 990 \",\"我们明明是往集合中添加1000个元素，而实际上只有903个元素。 解决方法： 加锁、使用线程安全的集合或者调用Stream的toArray() / collect() 操作就是满足线程安全的了。\",\"public class ParallelStreamNotice { public static void main(String[] args) { ArrayList<Integer> list = new ArrayList<Integer>(); for (int i = 0; i < 1000; i++) { list.add(i); } System.out.println(list.size()); //并行流创建 List<Integer> newList = new ArrayList<>(); //解决方法1 // List<Integer> newList = new Vector<>(); //解决方法2 // list.parallelStream() // .forEach(newList::add); // System.out.println(newList.size()); //解决方法3 // Arrays.stream(list.toArray()).forEach(o ->{ // newList.add((Integer) o); // }); newList = list.parallelStream().collect(Collectors.toList()); System.out.println(newList.size()); } } \"]},{\"header\":\"3、parallelStream背后的技术\",\"slug\":\"_3、parallelstream背后的技术\",\"contents\":[]},{\"header\":\"1、Fork/Join框架介绍\",\"slug\":\"_1、fork-join框架介绍\",\"contents\":[\"parallelStream使用的是Fork/Join框架。Fork/Join框架自JDK 7引入。Fork/Join框架可以将一个大任务拆分为很多小任务来异步执行。 Fork/Join框架主要包含三个模块：\",\"线程池：ForkJoinPool\",\"任务对象：ForkJoinTask\",\"执行任务的线程：ForkJoinWorkerThread\"]},{\"header\":\"2、Fork/Join原理-分治法\",\"slug\":\"_2、fork-join原理-分治法\",\"contents\":[\"ForkJoinPool主要用来使用分治法(Divide-and-Conquer Algorithm)来解决问题。典型的应用比如快速排序算法，ForkJoinPool需要使用相对少的线程来处理大量的任务。比如要对1000万个数据进行排序，那么会将这个任务分割成两个500万的排序任务和一个针对这两组500万数据的合并任务。以此类推，对于500万的数据也会做出同样的分割处理，到最后会设置一个阈值来规定当数据规模到多少时，停止这样的分割处理。比如，当元素的数量小于10时，会停止分割，转而使用插入排序对它们进行排序。那么到最后，所有的任务加起来会有大概2000000+个。问题的关键在于，对于一个任务而言，只有当它所有的子任务完成之后，它才能够被执行。\"]},{\"header\":\"3、Fork/Join原理-工作窃取算法\",\"slug\":\"_3、fork-join原理-工作窃取算法\",\"contents\":[\"Fork/Join最核心的地方就是利用了现代硬件设备多核，在一个操作时候会有空闲的cpu，那么如何利用好这个空闲的cpu就成了提高性能的关键，而这里我们要提到的工作窃取（work-stealing）算法就是整个Fork/Join框架的核心理念Fork/Join工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。\",\"那么为什么需要使用工作窃取算法呢？假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如A线程负责处理A队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。\",\"工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 上文中已经提到了在Java 8引入了自动并行化的概念。它能够让一部分Java代码自动地以并行的方式执行，也就是我们使用了ForkJoinPool的ParallelStream。\",\"对于ForkJoinPool通用线程池的线程数量，通常使用默认值就可以了，即运行时计算机的处理器数量。可以通过设置系统属性：java.util.concurrent.ForkJoinPool.common.parallelism=N （N为线程数量），来调整ForkJoinPool的线程数量，可以尝试调整成不同的参数来观察每次的输出结果。\"]},{\"header\":\"4、Fork/Join案例\",\"slug\":\"_4、fork-join案例\",\"contents\":[\"需求：使用Fork/Join计算1-10000的和，当一个任务的计算数量大于3000时拆分任务，数量小于3000时计算。\",\"public class ForkJoinDemo4 { public static void main(String[] args) { long start = System.currentTimeMillis(); ForkJoinPool pool = new ForkJoinPool(); SumRecursiveTask recursiveTask = new SumRecursiveTask(1, 10000L); Long result = pool.invoke(recursiveTask); System.out.println(\\\"result = \\\" + result); long end = System.currentTimeMillis(); System.out.println(\\\"消耗的时间为: \\\" + (end - start)); } static class SumRecursiveTask extends RecursiveTask<Long> { private static final long THRESHOLD = 3000L; private final long start; private final long end; public SumRecursiveTask(long start, long end) { this.start = start; this.end = end; } @Override protected Long compute() { long length = end - start; if (length <= THRESHOLD) { // 任务不用再拆分了.可以计算了 long sum = 0; for (long i = start; i <= end; i++) { sum += i; } System.out.println(\\\"计算: \\\" + start + \\\" -> \\\" + end + \\\",结果为: \\\" + sum); return sum; } else { // 数量大于预定的数量,任务还需要再拆分 long middle = (start + end) / 2; System.out.println(\\\"拆分: 左边 \\\" + start + \\\" -> \\\" + middle + \\\", 右边 \\\" + (middle + 1) + \\\" -> \\\" + end); SumRecursiveTask left = new SumRecursiveTask(start, middle); left.fork(); SumRecursiveTask right = new SumRecursiveTask(middle + 1, end); right.fork(); return left.join() + right.join(); } } } } \"]},{\"header\":\"7、Optional类\",\"slug\":\"_7、optional类\",\"contents\":[]},{\"header\":\"1、初体验\",\"slug\":\"_1、初体验-2\",\"contents\":[]},{\"header\":\"1、以前对null的处理方式\",\"slug\":\"_1、以前对null的处理方式\",\"contents\":[\"public class OptionalDemo1 { public static void main(String[] args) { String userName = \\\"xiaobear\\\"; if (userName != null) { System.out.println(\\\"用户名为:\\\" + userName); }else { System.out.println(\\\"用户名不存在！\\\"); } } } \"]},{\"header\":\"2、Option的使用\",\"slug\":\"_2、option的使用\",\"contents\":[\"public class OptionalDemo2 { public static void main(String[] args) { Optional<String> xiaobear = Optional.of(\\\"xiaobear\\\"); // Optional<String> xiaobear = Optional.of(null); // Optional<Object> xiaobear = Optional.ofNullable(null); //判断是否包含值 if(xiaobear.isPresent()){ String s = xiaobear.get(); System.out.println(\\\"用户名为:\\\" + s); }else { System.out.println(\\\"用户名不存在！\\\"); } } } \"]},{\"header\":\"2、Optional的基本使用\",\"slug\":\"_2、optional的基本使用\",\"contents\":[\"Optional是一个没有子类的工具类，Optional是一个可以为null的容器对象。它的作用主要就是为了解决避免Null检查，防止NullPointerException。\",\"Optional类的创建方式：\",\"Optional.of(T t) : 创建一个 Optional 实例 Optional.empty() : 创建一个空的 Optional 实例 Optional.ofNullable(T t):若 t 不为 null,创建 Optional 实例,否则创建空实例 \",\"Optional类的常用方法：\",\"isPresent() : 判断是否包含值,包含值返回true，不包含值返回false get() : 如果Optional有值则将其返回，否则抛出NoSuchElementException orElse(T t) : 如果调用对象包含值，返回该值，否则返回参数t orElseGet(Supplier s) :如果调用对象包含值，返回该值，否则返回 s 获取的值 map(Function f): 如果有值对其处理，并返回处理后的Optional，否则返回 Optional.empty() \"]},{\"header\":\"3、Optional的高级使用\",\"slug\":\"_3、optional的高级使用\",\"contents\":[\"public class OptionalDemo3 { public static void main(String[] args) { Optional<String> optional = Optional.of(\\\"张三\\\"); // Optional<Object> optional = Optional.empty(); //存在则输出 optional.ifPresent(System.out::println); //如果empty包含值，则返回该值，不包含则为空 System.out.println(optional.orElse(null)); optional.orElseGet(() -> {return \\\"未知的用户名\\\";}); } } \"]},{\"header\":\"8、日期和时间\",\"slug\":\"_8、日期和时间\",\"contents\":[]},{\"header\":\"1、旧版日期时间API存在的问题\",\"slug\":\"_1、旧版日期时间api存在的问题\",\"contents\":[\"设计很差： 在java.util和java.sql的包中都有日期类，java.util.Date同时包含日期和时间，而java.sql.Date仅包 含日期。此外用于格式化和解析的类在java.text包中定义。\",\"非线程安全：java.util.Date 是非线程安全的，所有的日期类都是可变的，这是Java日期类最大的问题之一。\",\"时区处理麻烦：日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。\"]},{\"header\":\"2、新版的日期和时间API\",\"slug\":\"_2、新版的日期和时间api\",\"contents\":[\"JDK 8中增加了一套全新的日期时间API，这套API设计合理，是线程安全的。新的日期及时间API位于 java.time 包中\",\"API\",\"描述\",\"LocalDate\",\"表示日期，包含年月日，格式为 2019-10-16\",\"LocalTime\",\"表示时间，包含时分秒，格式为 16:38:54.158549300\",\"LocalDateTime\",\"表示日期时间，包含年月日，时分秒，格式为 2018-09-06T15:33:56.750\",\"DateTimeFormatter\",\"日期时间格式化类\",\"Instant\",\"时间戳，表示一个特定的时间瞬间\",\"Duration\",\"用于计算2个时间(LocalTime，时分秒)的距离\",\"Period\",\"用于计算2个日期(LocalDate，年月日)的距离\",\"ZonedDateTime\",\"包含时区的时间\",\"Java中使用的历法是ISO 8601日历系统，它是世界民用历法，也就是我们所说的公历。平年有365天，闰年是366 天。此外Java 8还提供了4套其他历法，分别是：\",\"ThaiBuddhistDate：泰国佛教历\",\"MinguoDate：中华民国历\",\"JapaneseDate：日本历\",\"HijrahDate：伊斯兰历\"]},{\"header\":\"3、日期和时间类\",\"slug\":\"_3、日期和时间类\",\"contents\":[\"LocalDate、LocalTime、LocalDateTime类的实例是不可变的对象，分别表示使用 ISO-8601 日历系统的日期、时间、日期和时间。它们提供了简单的日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。\",\"public class LocalDateTimeDemo1 { public static void main(String[] args) { LocalDate date = LocalDate.of(1999, 10, 01); System.out.println(\\\"创建的日期为：\\\" + date); LocalDate now = LocalDate.now(); System.out.println(\\\"当前日期为：\\\" + now); //获取日期信息 System.out.println(now.getYear()); System.out.println(now.getMonthValue()); System.out.println(now.getDayOfMonth()); System.out.println(now.getDayOfWeek()); LocalDateTime nowDateTime = LocalDateTime.now(); System.out.println(\\\"当前时间为\\\" + nowDateTime); System.out.println(nowDateTime.getYear()); System.out.println(nowDateTime.getMonthValue()); System.out.println(nowDateTime.getDayOfMonth()); System.out.println(nowDateTime.getHour()); System.out.println(nowDateTime.getMinute()); System.out.println(nowDateTime.getSecond()); System.out.println(nowDateTime.getNano()); } } \",\"对日期时间的修改，对已存在的LocalDate对象，创建它的修改版，最简单的方式是使用withAttribute方法。withAttribute方法会创建对象的一个副本，并按照需要修改它的属性。以下所有的方法都返回了一个修改属性的对象，他们不会影响原来的对象。\",\"public class LocalDateTimeDemo2 { public static void main(String[] args) { LocalDateTime now = LocalDateTime.now(); System.out.println(\\\"当前时间为：\\\"+now); //修改当前时间 LocalDateTime with = now.withYear(2099); System.out.println(\\\"修改年份为：\\\"+with); System.out.println(\\\"修改后是否相等：\\\" + (now == with)); System.out.println(\\\"修改月份: \\\" + now.withMonth(6)); System.out.println(\\\"修改小时: \\\" + now.withHour(9)); System.out.println(\\\"修改分钟: \\\" + now.withMinute(11)); // 再当前对象的基础上加上或减去指定的时间 LocalDateTime localDateTime = now.plusDays(5); System.out.println(\\\"5天后: \\\" + localDateTime); System.out.println(\\\"now == localDateTime: \\\" + (now == localDateTime)); System.out.println(\\\"10年后: \\\" + now.plusYears(10)); System.out.println(\\\"20月后: \\\" + now.plusMonths(20)); System.out.println(\\\"20年前: \\\" + now.minusYears(20)); System.out.println(\\\"5月前: \\\" + now.minusMonths(5)); System.out.println(\\\"100天前: \\\" + now.minusDays(100)); } } \"]},{\"header\":\"4、时间格式化与解析\",\"slug\":\"_4、时间格式化与解析\",\"contents\":[\"通过java.time.format.DateTimeFormatter 类可以进行日期时间解析与格式化。\",\"public class LocalDateTimeDemo3 { public static void main(String[] args) { LocalDateTime now = LocalDateTime.now(); DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern(\\\"yyyy-MM-dd HH:mm:ss\\\"); //将日期格式化为字符串 String format = now.format(timeFormatter); System.out.println(format); // 将字符串解析为日期时间 LocalDateTime parse = LocalDateTime.parse(\\\"1985-09-23 10:12:22\\\", timeFormatter); System.out.println(\\\"parse = \\\" + parse); } } \"]},{\"header\":\"5、Instant 类\",\"slug\":\"_5、instant-类\",\"contents\":[\"Instant 时间戳/时间线，内部保存了从1970年1月1日 00:00:00以来的秒和纳秒。\",\"public class InstantDemo { public static void main(String[] args) { Instant now = Instant.now(); System.out.println(\\\"当前时间戳为：\\\" + now); // 获取从1970年1月1日 00:00:00的秒 System.out.println(now.getNano()); System.out.println(now.getEpochSecond()); System.out.println(now.toEpochMilli()); System.out.println(System.currentTimeMillis()); Instant instant = Instant.ofEpochSecond(5); System.out.println(instant); } } \"]},{\"header\":\"6、计算日期时间差类\",\"slug\":\"_6、计算日期时间差类\",\"contents\":[\"Duration/Period类: 计算日期时间差。\",\"Duration：用于计算2个时间(LocalTime，时分秒)的距离\",\"Period：用于计算2个日期(LocalDate，年月日)的距离\",\"public class DurationDemo { public static void main(String[] args) { LocalTime now = LocalTime.now(); LocalTime of = LocalTime.of(14, 15, 20); //计算时间差 Duration between = Duration.between(now, of); System.out.println(\\\"相差的天数：\\\" + between.toDays()); System.out.println(\\\"相差的小时：\\\" + between.toHours()); System.out.println(\\\"相差的分钟：\\\" + between.toMinutes()); System.out.println(\\\"相差的秒数：\\\" + between.getSeconds()); LocalDate localDate = LocalDate.now(); LocalDate date = LocalDate.of(2021, 8, 31); //比较日期差 Period period = Period.between(localDate, date); System.out.println(\\\"相差的年份：\\\" + period.getYears()); System.out.println(\\\"相差的月份：\\\" + period.getMonths()); System.out.println(\\\"相差的天数：\\\" + period.getDays()); } } \"]},{\"header\":\"7、时间校正器\",\"slug\":\"_7、时间校正器\",\"contents\":[\"有时我们可能需要获取例如：将日期调整到“下一个月的第一天”等操作。可以通过时间校正器来进行。\",\"TemporalAdjuster : 时间校正器。\",\"TemporalAdjusters : 该类通过静态方法提供了大量的常用TemporalAdjuster的实现。\",\"public class TemporalAdjusterDemo { public static void main(String[] args) { LocalDateTime now = LocalDateTime.now(); //得到下个月的第一天 TemporalAdjuster temporalAdjuster = temporal -> { LocalDateTime dateTime = (LocalDateTime) temporal; LocalDateTime nextMonth = dateTime.plusMonths(1).withDayOfMonth(1); System.out.println(\\\"nextMonth = \\\" + nextMonth); return nextMonth; }; LocalDateTime with = now.with(temporalAdjuster); System.out.println(\\\"下个月为：\\\"+with); } } \"]},{\"header\":\"8、设置日期时间的时区\",\"slug\":\"_8、设置日期时间的时区\",\"contents\":[\"Java8 中加入了对时区的支持，LocalDate、LocalTime、LocalDateTime是不带时区的，带时区的日期时间类分别为：ZonedDate、ZonedTime、ZonedDateTime。\",\"其中每个时区都对应着 ID，ID的格式为 “区域/城市” 。例如 ：Asia/Shanghai 等。\",\"ZoneId：该类中包含了所有的时区信息。\",\"public class SetTimeZoneDemo { public static void main(String[] args) { //获取所有时区ID ZoneId.getAvailableZoneIds().forEach(System.out::println); //不带时间，获取计算机的当前时间 LocalDateTime now = LocalDateTime.now(); System.out.println(\\\"now = \\\" + now); //操作带时区的类 ZonedDateTime zonedDateTime = ZonedDateTime.now(Clock.systemUTC()); System.out.println(\\\"zonedDateTime = \\\" + zonedDateTime); //使用计算机的默认的时区,创建日期时间 ZonedDateTime dateTime = ZonedDateTime.now(); System.out.println(\\\"dateTime = \\\" + dateTime); // 使用指定的时区创建日期时间 ZonedDateTime now2 = ZonedDateTime.now(ZoneId.of(\\\"America/Vancouver\\\")); System.out.println(\\\"now2 = \\\" + now2); } } \"]},{\"header\":\"总结\",\"slug\":\"总结\",\"contents\":[\"LocalDate表示日期,包含年月日\",\"LocalTime表示时间,包含时分秒\",\"LocalDateTime = LocalDate + LocalTime 时间的格式化和解析,通过DateTimeFormatter类型进行\",\"学习了Instant类,方便操作秒和纳秒,一般是给程序使用的.学习Duration/Period计算日期或时间的距离,还使用时间调整器方便的调整时间,学习了带时区的3个类ZoneDate/ZoneTime/ZoneDateTime\",\"JDK 8新的日期和时间 API的优势：\",\"新版的日期和时间API中，日期和时间对象是不可变的。操纵的日期不会影响老值，而是新生成一个实例。\",\"新的API提供了两种不同的时间表示方式，有效地区分了人和机器的不同需求。\",\"TemporalAdjuster可以更精确的操纵日期，还可以自定义日期调整器。\",\"是线程安全的\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\"]}]},\"/study-tutorial/basic/dataAndAlgorithm/advancedsort.html\":{\"title\":\"4、高级排序\",\"contents\":[{\"header\":\"1、希尔排序\",\"slug\":\"_1、希尔排序\",\"contents\":[\"希尔排序，也称递减增量排序算法，是插入排序的一种更高效的改进版本。但希尔排序是非稳定排序算法。\",\"希尔排序是基于插入排序的以下两点性质而提出改进方法的：\",\"插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率；\",\"但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位；\",\"希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录\\\"基本有序\\\"时，再对全体记录进行依次直接插入排序。\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤\",\"contents\":[\"选定一个增长量h，按照增长量h作为数据分组的依据，对数据进行分组；\",\"对分好组的每一组数据完成插入排序；\",\"减小增长量，最小减为1，重复第二步操作。\",\"增长量h的确定：\",\"先假设h=1\",\"如果h的值小于数组长度的一半，则h = 2h+1，直到大于数组长度的一半，即为增长量的值*\",\"每次循环之后，h变为原来的一半\",\"int h=1 while(h<5){ h=2h+1；//3,7 } //循环结束后我们就可以确定h的最大值； //h的减小规则为： h=h/2 \"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"public int[] hillSort(int[] nums){ //先确定增长量的值 int n = nums.length; int index = 1; while(index < n/2){ index = 2*index + 1; } while(1 <= index){ for (int i = index; i < n; i++) { for (int j = i; j >= index; j -= index) { if(nums[j] < nums[j - index]){ int temp = nums[j]; nums[j] = nums[j-index]; nums[j-index] = temp; }else { break; } } } index = index/2; } return nums; } \"]},{\"header\":\"2、归并排序\",\"slug\":\"_2、归并排序\",\"contents\":[\"归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。\",\"作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法：\",\"自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）；\",\"自下而上的迭代；\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤-1\",\"contents\":[\"尽可能的一组数据拆分成两个元素相等的子组，并对每一个子组继续拆分，直到拆分后的每个子组的元素个数是1为止。\",\"将相邻的两个子组进行合并成一个有序的大组；\",\"不断的重复步骤2，直到最终只有一个组为止。\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-1\",\"contents\":[\"Arrays.copyOfRange：主要用于对一个已有的数组进行截取复制，复制出一个左闭右开区间的数组。\",\"public int[] sort(int[] nums){ if(1 == nums.length){ return nums; } int mid = nums.length/2; //将原来数组拆分成左右两个数组 int[] left = Arrays.copyOfRange(nums,0,mid); int[] right = Arrays.copyOfRange(nums,mid,nums.length); return mergeSort(sort(left),sort(right)); } /** * * @param left 左边数组 * @param right 右边数组 * @return */ public int[] mergeSort(int[] left,int[] right){ //初始化结果数组 int[] result = new int[left.length+right.length]; int i = 0; while(0 < left.length && 0 < right.length){ if(left[0] <= right[0]){ result[i++] = left[0]; left = Arrays.copyOfRange(left,1,left.length); }else { result[i++] = right[0]; right = Arrays.copyOfRange(right,1,right.length); } } //这里主要是把落单的元素排到最后面 while(0 < left.length){ result[i++] = left[0]; left = Arrays.copyOfRange(left,1,left.length); } while(0 < right.length){ result[i++] = right[0]; right = Arrays.copyOfRange(right,1,right.length); } return result; } \"]},{\"header\":\"3、快速排序\",\"slug\":\"_3、快速排序\",\"contents\":[\"快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。\",\"快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。\",\"它是处理大数据最快的排序算法之一。\",\"快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤-2\",\"contents\":[\"首先设定一个分界值，通过该分界值将数组分成左右两部分；\",\"将大于或等于分界值的数据放到到数组右边，小于分界值的数据放到数组的左边。此时左边部分中各元素都小于或等于分界值，而右边部分中各元素都大于或等于分界值；\",\"然后，左边和右边的数据可以独立排序。对于左侧的数组数据，又可以取一个分界值，将该部分数据分成左右两部分，同样在左边放置较小值，右边放置较大值。右侧的数组数据也可以做类似处理。\",\"重复上述过程，可以看出，这是一个递归定义。通过递归将左侧部分排好序后，再递归排好右侧部分的顺序。当左侧和右侧两个部分的数据排完序后，整个数组的排序也就完成了。\",\"切分原理：\",\"找一个基准值，用两个指针分别指向数组的头部和尾部；\",\"先从尾部向头部开始搜索一个比基准值小的元素，搜索到即停止，并记录指针的位置；\",\"再从头部向尾部开始搜索一个比基准值大的元素，搜索到即停止，并记录指针的位置；\",\"交换当前左边指针位置和右边指针位置的元素；\",\"重复2,3,4步骤，直到左边指针的值大于右边指针的值停止。\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-2\",\"contents\":[\"public int[] quickSort(int[] arr, int left, int right){ if(left < right){ //获取基准值 int partition = partition(arr, left, right); quickSort(arr,left,partition-1); quickSort(arr,partition+1,right); } return arr; } /** * 设置基准值并排序 * @param nums * @param left * @param right * @return 下一轮基准值 */ public int partition(int[] nums, int left, int right){ //基准值下标 int pivot = left; //开始比较下标 int index = pivot + 1; for (int i = index; i < nums.length; i++) { if(nums[pivot] > nums[i]){ //交换元素 swap(nums,i,index); //原基准值下标 = index - 1 index++; } } swap(nums,pivot,index - 1); return index - 1; } /** * 交换元素 * @param nums 数组 * @param pivot * @param i */ public void swap(int[] nums, int pivot, int i){ int temp = nums[pivot]; nums[pivot] = nums[i]; nums[i] = temp; } \",\"解二：\",\"public stati int[] quickSort1(int[] nums, int start, int end){ int left = start, right = end; int pivot = nums[(start+end)/ 2]; while (left <= right){ while (left<= right && nums[left] < pivot){ left++; } while (left <= right && nums[left] > pivot){ right++; } if(left <= right){ int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; left++; right--; } quickSort(nums,start,right); quickSort(nums,left,end); } return nums; } \"]},{\"header\":\"3、快速排序和归并排序的区别：\",\"slug\":\"_3、快速排序和归并排序的区别\",\"contents\":[\"快速排序是另外一种分治的排序算法，它将一个数组分成两个子数组，将两部分独立的排序。\",\"快速排序和归并排序是互补的：\",\"归并排序将数组分成两个子数组分别排序，并将有序的子数组归并从而将整个数组排序。\",\"而快速排序的方式则是当两个数组都有序时，整个数组自然就有序了。\",\"在归并排序中，一个数组被等分为两半，归并调用发生在处理整个数组之前。\",\"在快速排序中，切分数组的位置取决于数组的内容，递归调用发生在处理整个数组之后。\"]}]},\"/study-tutorial/basic/dataAndAlgorithm/analysis.html\":{\"title\":\"2、算法分析\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"研究算法的最终目的就是如何花更少的时间，如何占用更少的内存去完成相同的需求，并且也通过案例演示了不同算法之间时间耗费和空间耗费上的差异，但我们并不能将时间占用和空间占用量化，因此，接下来我们要学习有关算法时间耗费和算法空间耗费的描述和分析。有关算法时间耗费分析，我们称之为算法的时间复杂度分析，有关算法的空间耗费分析，我们称之为算法的空间复杂度分析。\"]},{\"header\":\"1、时间复杂度\",\"slug\":\"_1、时间复杂度\",\"contents\":[]},{\"header\":\"1、大O记法\",\"slug\":\"_1、大o记法\",\"contents\":[\"在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随着n的变化情况并确定T(n)的量级。算法的时间复杂度，就是算法的时间量度，记作:T(n)=O(f(n))。它表示随着问题规模n的增大，算法执行时间的增长率和f(n)的增长率同，称作算法的渐近时间复杂度，简称时间复杂度，其中f(n)是问题规模n的某个函数。 在这里，我们需要明确一个事情：执行次数=执行时间 用大写O()来体现算法时间复杂度的记法，我们称之为大O记法。一般情况下，随着输入规模n的增大，T(n)增长最慢的算法为最优算法。\",\"案例1：\",\"public static void main(String[] args) { int sum = 0;//执行1次 int n=100;//执行1次 sum = (n+1)*n/2;//执行1次 System.out.println(\\\"sum=\\\"+sum); } \",\"案例2：\",\"public static void main(String[] args) { int sum = 0;//执行1次 int n=100;//执行1次 for (int i = 1; i <= n; i++) { sum += i;//执行了n次 } System.out.println(\\\"sum=\\\" + sum); } \",\"案例3：\",\"public static void main(String[] args) { int sum=0;//执行1次 int n=100;//执行1次 for (int i = 1; i <=n ; i++) { for (int j = 1; j <=n ; j++) { sum+=i;//执行n^2次 } } System.out.println(\\\"sum=\\\"+sum); } \",\"如果忽略判断条件的执行次数和输出语句的执行次数，那么当输入规模为n时，以上算法执行的次数分别为： 案例一：3次 案例二：n+3次 案例三：n^2+2次\",\"基于我们对函数渐近增长的分析，推导大O阶的表示法有以下几个规则可以使用：\",\"用常数1取代运行时间中的所有加法常数；\",\"在修改后的运行次数中，只保留高阶项；\",\"如果最高阶项存在，且常数因子不为1，则去除与这个项相乘的常数；\",\"所以，对应上面的3个案例为：\",\"案例1：O(1)\",\"案例1：O(n)\",\"案例1：O(n^2)\"]},{\"header\":\"2、常见的大O阶\",\"slug\":\"_2、常见的大o阶\",\"contents\":[]},{\"header\":\"1、线性阶\",\"slug\":\"_1、线性阶\",\"contents\":[\"一般含有非嵌套循环涉及线性阶，线性阶就是随着输入规模的扩大，对应计算次数呈直线增长\",\"public static void main(String[] args) { int sum = 0; int n=100; for (int i = 1; i <= n; i++) { sum += i; } System.out.println(\\\"sum=\\\" + sum); } \",\"上面这段代码，它的循环的时间复杂度为O(n),因为循环体中的代码需要执行n次\"]},{\"header\":\"2、平方阶\",\"slug\":\"_2、平方阶\",\"contents\":[\"一般嵌套循环属于这种时间复杂度\",\"public static void main(String[] args) { int sum=0,n=100; for (int i = 1; i <=n ; i++) { for (int j = 1; j <=n ; j++) { sum+=i; } } System.out.println(sum); } \",\"上面这段代码，n=100，也就是说，外层循环每执行一次，内层循环就执行100次，那总共程序想要从这两个循环出来，就需要执行100100次，也就是n的平方次，所以这段代码的时间复杂度是O(n^2).*\"]},{\"header\":\"3、立方阶\",\"slug\":\"_3、立方阶\",\"contents\":[\"一般三层嵌套循环属于这种时间复杂度\",\"public static void main(String[] args) { int x=0,n=100; for (int i = 1; i <=n ; i++) { for (int j = i; j <=n ; j++) { for (int j = i; j <=n ; j++) { x++; } } } System.out.println(x); } \",\"上面这段代码，n=100，也就是说，外层循环每执行一次，中间循环循环就执行100次，中间循环每执行一次，最内层循环需要执行100次，那总共程序想要从这三个循环中出来，就需要执行100100100次，也就是n的立方，所以这段代码的时间复杂度是O(n^3).\"]},{\"header\":\"4、对数阶\",\"slug\":\"_4、对数阶\",\"contents\":[\"对数，属于高中数学的内容，我们分析程序以程序为主，数学为辅\",\"int i=1,n=100; while(i<n){ i = i*2; } \",\"由于每次i2之后，就距离n更近一步，假设有x个2相乘后大于n，则会退出循环。由于是2^x=n,得到x=log(2)n,所以这个循环的时间复杂度为O(logn);*\"]},{\"header\":\"5、常数阶\",\"slug\":\"_5、常数阶\",\"contents\":[\"一般不涉及循环操作的都是常数阶，因为它不会随着n的增长而增加操作次数。\",\"public static void main(String[] args) { int n=100; int i=n+2; System.out.println(i); } \",\"上述代码，不管输入规模n是多少，都执行2次，根据大O推导法则，常数用1来替换，所以上述代码的时间复杂度为O(1)\",\"复杂程度从低到高依次为：O(1)<O(logn)<O(n)<O(nlogn)<O(n2)<O(n3)\"]},{\"header\":\"3、函数调用的时间复杂度分析\",\"slug\":\"_3、函数调用的时间复杂度分析\",\"contents\":[\"案例1：\",\"public static void main(String[] args) { int n=100; for (int i = 0; i < n; i++) { show(i); } } private static void show(int i) { System.out.println(i); } \",\"在main方法中，有一个for循环，循环体调用了show方法，由于show方法内部只执行了一行代码，所以show方法的时间复杂度为O(1),那main方法的时间复杂度就是O(n)\",\"案例2：\",\"public static void main(String[] args) { int n=100; for (int i = 0; i < n; i++) { show(i); } } private static void show(int i) { for (int j = 0; j < i; i++) { System.out.println(i); } } \",\"在main方法中，有一个for循环，循环体调用了show方法，由于show方法内部也有一个for循环，所以show方法的时间复杂度为O(n),那main方法的时间复杂度为O(n^2)\",\"案例3：\",\"public static void main(String[] args) { int n=100; show(n); for (int i = 0; i < n; i++) { show(i); } for (int i = 0; i < n; i++) { for (int j = 0; j < n; j++) { System.out.println(j); } } } private static void show(int i) { for (int j = 0; j < i; i++) { System.out.println(i); } } \",\"在show方法中，有一个for循环，所以show方法的时间复杂度为O(n),在main方法中，show(n)这行代码内部执行的次数为n，第一个for循环内调用了show方法，所以其执行次数为nn,第二个嵌套for循环内只执行了一行代码，所以其执行次数为nn,那么main方法总执行次数为n+n2+n2=2nn+n。根据大O推导规则，去掉n保留最高阶项，并去掉最高阶项的常数因子2，所以最终main方法的时间复杂度为O(n^2)*\"]},{\"header\":\"4、最坏的情况\",\"slug\":\"_4、最坏的情况\",\"contents\":[\"有一个存储了n个随机数字的数组，请从中查找出指定的数字。\",\"public int search(int num){ int[] arr={11,10,8,9,7,22,23,0}; for (int i = 0; i < arr.length; i++) { if (num==arr[i]){ return i; } } return -1; } \",\"最好的情况：查找的第一个数字就是期望的数字，那么算法的时间复杂度为O(1)\",\"最坏的情况：查找的最后一个数字，才是期望的数字，那么算法的时间复杂度为O(n)\",\"平均情况：任何数字查找的平均成本是O(n/2)\"]},{\"header\":\"2、空间复杂度\",\"slug\":\"_2、空间复杂度\",\"contents\":[]},{\"header\":\"1、Java中常见的内存占用\",\"slug\":\"_1、java中常见的内存占用\",\"contents\":[\"数据类型\",\"占用字节数\",\"byte\",\"1\",\"short\",\"2\",\"int\",\"4\",\"long\",\"8\",\"float\",\"4\",\"double\",\"8\",\"boolean\",\"1\",\"char\",\"2\",\"一个引用（机器地址）需要8个字节表示：\",\"例如： Date date = new Date(),则date这个变量需要占用8个字节来表示\",\"创建一个对象，比如new Date()，除了Date对象内部存储的数据(例如年月日等信息)占用的内存，该对象本身也有内存开销，每个对象的自身开销是16个字节，用来保存对象的头信息。\",\"一般内存的使用，如果不够8个字节，都会被自动填充为8字节\",\"java中数组被被限定为对象，他们一般都会因为记录长度而需要额外的内存，一个原始数据类型的数组一般需要24字节的头信息(16个自己的对象开销，4字节用于保存长度以及4个填充字节)再加上保存值所需的内存。\"]},{\"header\":\"2、空间复杂度\",\"slug\":\"_2、空间复杂度-1\",\"contents\":[\"算法的空间复杂度计算公式记作：S(n)=O(f(n))，其中n为输入规模，f(n)为语句关于n所占存储空间的函数。\",\"案例：对指定的数组元素进行反转，并返回反转的内容。\",\"解法1：\",\"public static int[] reverse2(int[] nums){ int n = nums.length;//申请4个字节 int temp;//申请4个字节 for (int start = 0,end = n-1; start < end; start++,end--) { temp = nums[start]; nums[start] = nums[end]; nums[end] = temp; } return nums; } \",\"解法2：\",\"/** * 数组反转 * @param nums * @return */ public static int[] reverse(int[] nums){ int n = nums.length;//申请4个字节 int[] resultArr = new int[n];//申请n*4个字节+数组自身头信息开销24个字节 for (int i = n - 1; i >= 0; i--) { resultArr[n - 1 - i] = nums[i]; } return resultArr; } \",\"解法1：不管传入的数组大小为多少，始终额外申请4+4=8个字节；空间复杂度为O(1)\",\"解法2：4+4n+24=4n+28；空间复杂度为O(n)\",\"从空间占用的角度讲，解法一要优于解法二\"]}]},\"/study-tutorial/basic/dataAndAlgorithm/andlookup.html\":{\"title\":\"11、并查集\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"并查集是一种树型的数据结构 ，并查集可以高效地进行如下操作：\",\"查询元素p和元素q是否属于同一组\",\"合并元素p和元素q所在的组\"]},{\"header\":\"1、并查集的结构\",\"slug\":\"_1、并查集的结构\",\"contents\":[\"并查集也是一种树型结构，但这棵树跟我们之前讲的二叉树、红黑树、B树等都不一样，这种树的要求比较简单：\",\"每个元素都唯一的对应一个结点；\",\"每一组数据中的多个元素都在同一颗树中；\",\"一个组中的数据对应的树和另外一个组中的数据对应的树之间没有任何联系；\",\"元素在树中并没有子父级关系的硬性要求；\"]},{\"header\":\"2、并查集的API设计与实现\",\"slug\":\"_2、并查集的api设计与实现\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"类名\",\"UnionFind\",\"构造方法\",\"UF(int N)：初始化并查集，以整数标识(0,N-1)个结点\",\"成员方法\",\"public int count()：获取当前并查集中的数据有多少个分组public boolean connected(int p,int q):判断并查集中元素p和元素q是否在同一分组中public int find(int p):元素p所在分组的标识符public void union(int p,int q)：把p元素所在分组和q元素所在分组合并\",\"成员变量\",\"private int[] eleAndGroup: 记录结点元素和该元素所在分组的标识private int count：记录并查集中数据的分组个数\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[]},{\"header\":\"1、UF(int N)构造方法实现\",\"slug\":\"_1、uf-int-n-构造方法实现\",\"contents\":[\"初始情况下，每个元素都在一个独立的分组中，所以，初始情况下，并查集中的数据默认分为N个组；\",\"初始化数组eleAndGroup；\",\"把eleAndGroup数组的索引看做是每个结点存储的元素，把eleAndGroup数组每个索引处的值看做是该结点所在的分组，那么初始化情况下，i索引处存储的值就是i\"]},{\"header\":\"2、union(int p,int q)合并方法实现\",\"slug\":\"_2、union-int-p-int-q-合并方法实现\",\"contents\":[\"如果p和q已经在同一个分组中，则无需合并\",\"如果p和q不在同一个分组，则只需要将p元素所在组的所有的元素的组标识符修改为q元素所在组的标识符即可\",\"分组数量-1\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现\",\"contents\":[\"public class UnionFind { /** * 记录结点元素和该元素所在分组的标识 */ private int[] eleAndGroup; /** * 记录并查集中数据的分组个数 */ private int count; public UnionFind(int n) { //初始情况下，每个元素都在一个独立的分组中，所以，初始情况下，并查集中的数据默认分为N个组 this.count = n; //初始化数组 eleAndGroup = new int[n]; //把eleAndGroup数组的索引看做是每个结点存储的元素， // 把eleAndGroup数组每个索引处的值看做是该结点所在的分组， // 那么初始化情况下，i索引处存储的值就是i for (int i = 0; i < n; i++) { eleAndGroup[i] = i; } } /** * 获取当前并查集中的数据有多少个分组 * @return */ public int count(){ return count; } /** * 判断并查集中元素p和元素q是否在同一分组中 * @param p * @param q * @return */ public boolean connected(int p,int q){ return eleAndGroup[p] == eleAndGroup[q]; } /** * 元素p所在分组的标识符 * @param p * @return */ public int find(int p){ return eleAndGroup[p]; } /** * 把p元素所在分组和q元素所在分组合并 * @param p * @param q */ public void union(int p,int q){ //如果q和p已经在同一个分组中，不需要合并 if(connected(p, q)){ return; } //不在一个分组中 int pFind = find(p); int qFind = find(q); for (int i = 0; i < eleAndGroup.length; i++) { if (eleAndGroup[i] == pFind){ eleAndGroup[i] = qFind; } } //数量减1 count--; } } \",\"测试类\",\"public class UnionFindTest { public static void main(String[] args) { UnionFind uf = new UnionFind(5); int count = uf.count(); System.out.println(\\\"总共有\\\"+count+\\\"个分组\\\"); Scanner scanner = new Scanner(System.in); while (true){ System.out.println(\\\"请输入你要合并的第一个点\\\"); int i = scanner.nextInt(); System.out.println(\\\"请输入你要合并的第二个点\\\"); int j = scanner.nextInt(); if(uf.connected(i,j)){ System.out.println(\\\"结点\\\"+ i +\\\"和结点\\\"+ j +\\\"已经在同一个组\\\"); continue; } uf.union(i,j); System.out.println(\\\"总共还有\\\"+uf.count()+\\\"个分组\\\"); } } } \"]},{\"header\":\"3、并查集应用举例\",\"slug\":\"_3、并查集应用举例\",\"contents\":[\"如果我们并查集存储的每一个整数表示的是一个大型计算机网络中的计算机，则我们就可以通过connected(intp,int q)来检测，该网络中的某两台计算机之间是否连通？如果连通，则他们之间可以通信，如果不连通，则不能通信，此时我们又可以调用union(int p,int q)使得p和q之间连通，这样两台计算机之间就可以通信了。\",\"一般像计算机这样网络型的数据，我们要求网络中的每两个数据之间都是相连通的，也就是说，我们需要调用很多次union方法，使得网络中所有数据相连，其实我们很容易可以得出，如果要让网络中的数据都相连，则我们至少要调用N-1次union方法才可以，但由于我们的union方法中使用for循环遍历了所有的元素，所以很明显，我们之前实现的合并算法的时间复杂度是O(N^2)，如果要解决大规模问题，它是不合适的，所以我们需要对算法进行优化。\"]},{\"header\":\"4、算法优化\",\"slug\":\"_4、算法优化\",\"contents\":[\"为了提升union算法的性能，我们需要重新设计find方法和union方法的实现，此时我们先需要对我们的之前数据结构中的eleAndGourp数组的含义进行重新设定：\",\"我们仍然让eleAndGroup数组的索引作为某个结点的元素；\",\"eleAndGroup[i]的值不再是当前结点所在的分组标识，而是该结点的父结点；\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-1\",\"contents\":[\"类名\",\"UF_Tree\",\"构造方法\",\"UF_Tree(int N)：初始化并查集，以整数标识(0,N-1)个结点\",\"成员方法\",\"public int count()：获取当前并查集中的数据有多少个分组public boolean connected(int p,int q):判断并查集中元素p和元素q是否在同一分组中public int find(int p):元素p所在分组的标识符public void union(int p,int q)：把p元素所在分组和q元素所在分组合并\",\"成员变量\",\"private int[] eleAndGroup: 记录结点元素和该元素的父结点private int count：记录并查集中数据的分组个数\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-1\",\"contents\":[]},{\"header\":\"1、find(int p)查询方法实现\",\"slug\":\"_1、find-int-p-查询方法实现\",\"contents\":[\"判断当前元素p的父结点eleAndGroup[p]是不是自己，如果是自己则证明已经是根结点了；\",\"如果当前元素p的父结点不是自己，则让p=eleAndGroup[p]，继续找父结点的父结点,直到找到根结点为止；\"]},{\"header\":\"2、union(int p,int q)合并方法实现\",\"slug\":\"_2、union-int-p-int-q-合并方法实现-1\",\"contents\":[\"找到p元素所在树的根结点\",\"找到q元素所在树的根结点\",\"如果p和q已经在同一个树中，则无需合并；\",\"如果p和q不在同一个分组，则只需要将p元素所在树根结点的父结点设置为q元素的根结点即可；\",\"分组数量-1\"]},{\"header\":\"3、完成代码\",\"slug\":\"_3、完成代码\",\"contents\":[\"public class UF_Tree { /** * 记录结点元素和该元素所在分组的标识 */ private int[] eleAndGroup; /** * 记录并查集中数据的分组个数 */ private int count; public UF_Tree(int n) { //初始情况下，每个元素都在一个独立的分组中，所以，初始情况下，并查集中的数据默认分为N个组 this.count = n; //初始化数组 eleAndGroup = new int[n]; //把eleAndGroup数组的索引看做是每个结点存储的元素， // 把eleAndGroup数组每个索引处的值看做是该结点所在的分组， // 那么初始化情况下，i索引处存储的值就是i for (int i = 0; i < n; i++) { eleAndGroup[i] = i; } } /** * 获取当前并查集中的数据有多少个分组 * @return */ public int count(){ return count; } /** * 判断并查集中元素p和元素q是否在同一分组中 * @param p * @param q * @return */ public boolean connected(int p,int q){ return eleAndGroup[p] == eleAndGroup[q]; } /** * 元素p所在分组的标识符 * @param p * @return */ public int find(int p){ while (true){ //判断当前元素p的父结点eleAndGroup[p]是不是自己，如果是自己则证明已经是根结点了； if(p == eleAndGroup[p]){ return p; } //如果当前元素p的父结点不是自己，则让p=eleAndGroup[p]，继续找父结点的父结点,直到找到根结点为止； p = eleAndGroup[p]; } } /** * 把p元素所在分组和q元素所在分组合并 * @param p * @param q */ public void union(int p,int q){ //不在一个分组中 int pFind = find(p); int qFind = find(q); if (qFind == pFind){ return; } //如果p和q不在同一个分组，则只需要将p元素所在树根结点的父结点设置为q元素的根结点即可； eleAndGroup[pFind] = qFind; //数量减1 count--; } } \",\"测试类\",\"public class UnionFindTreeTest { public static void main(String[] args) { UF_Tree uf = new UF_Tree(5); int count = uf.count(); System.out.println(\\\"总共有\\\"+count+\\\"个分组\\\"); Scanner scanner = new Scanner(System.in); while (true){ System.out.println(\\\"请输入你要合并的第一个点\\\"); int i = scanner.nextInt(); System.out.println(\\\"请输入你要合并的第二个点\\\"); int j = scanner.nextInt(); if(uf.connected(i,j)){ System.out.println(\\\"结点\\\"+ i +\\\"和结点\\\"+ j +\\\"已经在同一个组\\\"); continue; } uf.union(i,j); System.out.println(\\\"总共还有\\\"+uf.count()+\\\"个分组\\\"); } } } \"]},{\"header\":\"5、路径压缩\",\"slug\":\"_5、路径压缩\",\"contents\":[\"UF_Tree中最坏情况下union算法的时间复杂度为O(N^2)，其最主要的问题在于最坏情况下，树的深度和数组的大小一样，如果我们能够通过一些算法让合并时，生成的树的深度尽可能的小，就可以优化find方法。\",\"之前我们在union算法中，合并树的时候将任意的一棵树连接到了另外一棵树，这种合并方法是比较暴力的，如果我们把并查集中每一棵树的大小记录下来，然后在每次合并树的时候，把较小的树连接到较大的树上，就可以减小树的深度。\",\"只要我们保证每次合并，都能把小树合并到大树上，就能够压缩合并后新树的路径，这样就能提高find方法的效率。为了完成这个需求，我们需要另外一个数组来记录存储每个根结点对应的树中元素的个数，并且需要一些代码调整数组中的值。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-2\",\"contents\":[\"类名\",\"UF_Tree_Weighted\",\"构造方法\",\"UF_Tree_Weighted(int N)：初始化并查集，以整数标识(0,N-1)个结点\",\"成员方法\",\"public int count()：获取当前并查集中的数据有多少个分组public boolean connected(int p,int q):判断并查集中元素p和元素q是否在同一分组中public int find(int p):元素p所在分组的标识符public void union(int p,int q)：把p元素所在分组和q元素所在分组合并\",\"成员变量\",\"private int[] eleAndGroup: 记录结点元素和该元素的父结点private int[] sz: 存储每个根结点对应的树中元素的个数private int count：记录并查集中数据的分组个数\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-2\",\"contents\":[\"public class UFTreeWeighted { private int[] eleAndGroup; private int[] rootSize; private int count; public UFTreeWeighted(int count) { this.count = count; //初始化数组 eleAndGroup = new int[count]; rootSize = new int[count]; /** * 把eleAndGroup数组的索引看做是每个结点存储的元素， * 把eleAndGroup数组每个索引处的值看做是该结点所在的分组， * 那么初始化情况下，i索引处存储的值就是i */ for (int i = 0; i < count; i++) { eleAndGroup[i] = i; } //把sz数组中所有的元素初始化为1，默认情况下，每个结点都是一个独立的树，每个树中只有一个元素 for (int i = 0; i < count; i++) { rootSize[i] = 1; } } /** * 获取当前并查集中的数据有多少个分组 * @return count */ public int count(){ return count; } /** * 判断并查集中元素p和元素q是否在同一分组中 * @param p * @param q * @return */ public boolean connected(int p,int q){ return find(p) == find(q); } /** * 元素p所在分组的标识符 * @param p * @return */ public int find(int p){ while (true){ if(p == eleAndGroup[p]){ return p; } p = eleAndGroup[p]; } } /** * ：把p元素所在分组和q元素所在分组合并 * @param p * @param q */ public void union(int p,int q){ //找到p元素的根结点 int pRoot = find(p); //找到q元素的根结点 int qRoot = find(q); //如果已经在一个组中，无需合并 if(pRoot == qRoot){ return; } //不在一个组中，比较q所在树中的元素个数和p所在树中的元素个数，小树向大树合并 if(rootSize[pRoot] < rootSize[qRoot]){ eleAndGroup[pRoot] = qRoot; rootSize[qRoot] += rootSize[pRoot]; }else { eleAndGroup[qRoot] = pRoot; rootSize[pRoot] += rootSize[qRoot]; } //分组数组-1 count--; } } \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/binarytree.html\":{\"title\":\"7、二叉树\",\"contents\":[{\"header\":\"1、树的相关概念\",\"slug\":\"_1、树的相关概念\",\"contents\":[]},{\"header\":\"1、树的基本定义\",\"slug\":\"_1、树的基本定义\",\"contents\":[\"树是我们计算机中非常重要的一种数据结构，同时使用树这种数据结构，可以描述现实生活中的很多事物，例如家谱、单位的组织架构、等等。 树是由n（n>=1）个有限结点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。\",\"树具有以下特点：\",\"每个结点有零个或多个子结点；\",\"没有父结点的结点为根结点；\",\"每一个非根结点只有一个父结点；\",\"每个结点及其后代结点整体上可以看做是一棵树，称为当前结点的父结点的一个子树；\"]},{\"header\":\"2、相关术语\",\"slug\":\"_2、相关术语\",\"contents\":[]},{\"header\":\"1、结点的度\",\"slug\":\"_1、结点的度\",\"contents\":[\"一个结点含有的子树的个数称为该结点的度；\"]},{\"header\":\"2、叶子结点\",\"slug\":\"_2、叶子结点\",\"contents\":[\"度为0的结点称为叶结点，也可以叫做终端结点\"]},{\"header\":\"3、分支结点\",\"slug\":\"_3、分支结点\",\"contents\":[\"度不为0的结点称为分支结点，也可以叫做非终端结点\"]},{\"header\":\"4、结点的层次\",\"slug\":\"_4、结点的层次\",\"contents\":[\"从根结点开始，根结点的层次为1，根的直接后继层次为2，以此类推\"]},{\"header\":\"5、结点的层序编号\",\"slug\":\"_5、结点的层序编号\",\"contents\":[\"将树中的结点，按照从上层到下层，同层从左到右的次序排成一个线性序列，把他们编成连续的自然数。\"]},{\"header\":\"6、树的度\",\"slug\":\"_6、树的度\",\"contents\":[\"树中所有结点的度的最大值\"]},{\"header\":\"7、树的高度(深度)\",\"slug\":\"_7、树的高度-深度\",\"contents\":[\"树中结点的最大层次\"]},{\"header\":\"8、森林\",\"slug\":\"_8、森林\",\"contents\":[\"m（m>=0）个互不相交的树的集合，将一颗非空树的根结点删去，树就变成一个森林；给森林增加一个统一的根结点，森林就变成一棵树\"]},{\"header\":\"9、孩子结点\",\"slug\":\"_9、孩子结点\",\"contents\":[\"一个结点的直接后继结点称为该结点的孩子结点\"]},{\"header\":\"10、双亲结点(父结点)\",\"slug\":\"_10、双亲结点-父结点\",\"contents\":[\"一个结点的直接前驱称为该结点的双亲结点\"]},{\"header\":\"11、兄弟结点\",\"slug\":\"_11、兄弟结点\",\"contents\":[\"同一双亲结点的孩子结点间互称兄弟结点\"]},{\"header\":\"2、二叉树\",\"slug\":\"_2、二叉树\",\"contents\":[\"二叉树就是度不超过2的树(每个结点最多有两个子结点)\"]},{\"header\":\"1、相关二叉树\",\"slug\":\"_1、相关二叉树\",\"contents\":[]},{\"header\":\"1、满二叉树\",\"slug\":\"_1、满二叉树\",\"contents\":[\"一个二叉树，如果每一个层的结点树都达到最大值，则这个二叉树就是满二叉树。\"]},{\"header\":\"2、完全二叉树\",\"slug\":\"_2、完全二叉树\",\"contents\":[\"叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树\"]},{\"header\":\"2、创建二叉查找树\",\"slug\":\"_2、创建二叉查找树\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"结点类\",\"类名\",\"Node<Key,Value>\",\"构造方法\",\"Node(Key key, Value value, Node left, Node right)：创建Node对象\",\"成员变量\",\"1.public Node left:记录左子结点2.public Node right:记录右子结点3.public Key key:存储键4.public Value value:存储值\",\"二叉树\",\"类名\",\"BinaryTree<Key,value>\",\"构造方法\",\"BinaryTree()：创建BinaryTree对象\",\"成员变量\",\"1.private Node root:记录根结点2.private int N:记录树中元素的个数\",\"成员方法\",\"1. public void put(Key key,Value value):向树中插入一个键值对2.private Node put(Node x, Key key, Value val)：给指定树x上，添加键一个键值对，并返回添加后的新树3.public Value get(Key key):根据key，从树中找出对应的值4.private Value get(Node x, Key key):从指定的树x中，找出key对应的值5.public void delete(Key key):根据key，删除树中对应的键值对6.private Node delete(Node x, Key key):删除指定树x上的键为key的键值对，并返回删除后的新树7.public int size():获取树中元素的个数\"]},{\"header\":\"1、put方法实现思路\",\"slug\":\"_1、put方法实现思路\",\"contents\":[\"如果当前树中没有任何一个结点，则直接把新结点当做根结点使用\",\"如果当前树不为空，则从根结点开始：\",\"如果新结点的key小于当前结点的key，则继续找当前结点的左子结点；\",\"如果新结点的key大于当前结点的key，则继续找当前结点的右子结点；\",\"如果新结点的key等于当前结点的key，则树中已经存在这样的结点，替换该结点的value值即可。\",\"/** * 给指定树x上，添加键一个键值对，并返回添加后的新树 * @param x 树节点 * @param key 键 * @param val 值 * @return Node */ public Node<Key, Value> put(Node<Key,Value> x, Key key, Value val){ //当树为空时，该节点为根节点 if(null == x){ size++; return new Node<>(key, val, null, null); } int compare = key.compareTo(x.key); //如果compare > 0 ,则 key > x.key;继续x的右子节点 if(0 < compare){ x.right = put(x.right,key,val); }else if(0 > compare){ //如果compare < 0 ,则 key < x.key;继续x的左子节点 x.left = put(x.left,key,val); }else { //如果compare = 0 ,则 key = x.key;替换x.value的值 x.value = val; } return x; } \"]},{\"header\":\"2、get方法实现思路\",\"slug\":\"_2、get方法实现思路\",\"contents\":[\"从根节点开始：\",\"如果要查询的key小于当前结点的key，则继续找当前结点的左子结点；\",\"如果要查询的key大于当前结点的key，则继续找当前结点的右子结点；\",\"如果要查询的key等于当前结点的key，则树中返回当前结点的value。\",\"/** * 从指定的树x中，找出key对应的值 * @param x 节点 * @param key 键 * @return 节点 */ public Value getNode(Node<Key,Value> x, Key key){ if (null == x){ return null; } int compare = key.compareTo(x.key); if(0 < compare){ return getNode(x.right,key); }else if(0 > compare){ return getNode(x.left, key); }else { return x.value; } } \"]},{\"header\":\"3、delete方法的实现思路\",\"slug\":\"_3、delete方法的实现思路\",\"contents\":[\"找到被删除结点；\",\"找到被删除结点右子树中的最小结点minNode\",\"删除右子树中的最小结点\",\"让被删除结点的左子树称为最小结点minNode的左子树，让被删除结点的右子树称为最小结点minNode的右子树\",\"让被删除结点的父节点指向最小结点minNode\",\"/** * 删除指定树x中的key对应的value，并返回删除后的新树 * @param x * @param key * @return */ public Node<Key,Value> delete(Node<Key, Value> x, Key key){ if (null == x){ return null; } int compare = key.compareTo(x.key); if(0 < compare){ x.right = delete(x.right,key); }else if(0 > compare){ x.left = delete(x.left,key); }else { //个数-1 size--; //新结点的key等于当前结点的key,当前x就是要删除的结点 if(x.right == null){ return x.left; }if(x.left == null){ return x.right; } //左右子结点都存在的情况下，找右子树最小的节点 Node<Key, Value> minRight = x.right; while (null != minRight.left){ minRight = minRight.left; } Node<Key, Value> node = x.right; while (node.left != null) { if (node.left.left == null) { node.left = null; } else { node = node.left; } } //让被删除结点的左子树称为最小结点minNode的左子树，让被删除结点的右子树称为最小结点minNode的右子树 minRight.left = x.left; minRight.right = x.right; //让被删除结点的父节点指向最小结点minNode x = minRight; } return x; } \"]},{\"header\":\"4、完整代码\",\"slug\":\"_4、完整代码\",\"contents\":[\"package com.xiaobear.BinaryTree; /** * @Author xiaobear * @date 2021年07月30日 13:50 * @Description 二叉树 */ public class BinaryTree<Key extends Comparable<Key>,Value> { /** * 根节点 */ private Node<Key,Value> root; /** * 节点数量 */ private int size; public void put(Key key, Value val){ root = put(root,key,val); } /** * 给指定树x上，添加键一个键值对，并返回添加后的新树 * @param x 树节点 * @param key 键 * @param val 值 * @return Node */ public Node<Key, Value> put(Node<Key,Value> x, Key key, Value val){ //当树为空时，该节点为根节点 if(null == x){ size++; return new Node<>(key, val, null, null); } int compare = key.compareTo(x.key); //如果compare > 0 ,则 key > x.key;继续x的右子节点 if(0 < compare){ x.right = put(x.right,key,val); }else if(0 > compare){ //如果compare < 0 ,则 key < x.key;继续x的左子节点 x.left = put(x.left,key,val); }else { //如果compare = 0 ,则 key = x.key;替换x.value的值 x.value = val; } return x; } /** * 根据key，从树中找出对应的值 * @param key 键 */ public Value getNode(Key key){ return getNode(root,key); } /** * 从指定的树x中，找出key对应的值 * @param x 节点 * @param key 键 * @return 节点 */ public Value getNode(Node<Key,Value> x, Key key){ if (null == x){ return null; } int compare = key.compareTo(x.key); if(0 < compare){ return getNode(x.right,key); }else if(0 > compare){ return getNode(x.left, key); }else { return x.value; } } public void delete(Key key){ root = delete(root, key); } /** * 删除指定树x中的key对应的value，并返回删除后的新树 * @param x * @param key * @return */ public Node<Key,Value> delete(Node<Key, Value> x, Key key){ if (null == x){ return null; } int compare = key.compareTo(x.key); if(0 < compare){ x.right = delete(x.right,key); }else if(0 > compare){ x.left = delete(x.left,key); }else { //个数-1 size--; //新结点的key等于当前结点的key,当前x就是要删除的结点 if(x.right == null){ return x.left; }if(x.left == null){ return x.right; } //左右子结点都存在的情况下，找右子树最小的节点 Node<Key, Value> minRight = x.right; while (null != minRight.left){ minRight = minRight.left; } Node<Key, Value> node = x.right; while (node.left != null) { if (node.left.left == null) { node.left = null; } else { node = node.left; } } //让被删除结点的左子树称为最小结点minNode的左子树，让被删除结点的右子树称为最小结点minNode的右子树 minRight.left = x.left; minRight.right = x.right; //让被删除结点的父节点指向最小结点minNode x = minRight; } return x; } public int size(){ return size; } /** * 节点类 * @param <Key> * @param <Value> */ private class Node<Key,Value>{ public Key key; public Value value; public Node<Key,Value> left; public Node<Key, Value> right; public Node(Key key, Value value, Node left, Node<Key, Value> right) { this.key = key; this.value = value; this.left = left; this.right = right; } } } \",\"测试代码\",\"public class BinaryTreeTest { public static void main(String[] args) { BinaryTree<Integer, String> binaryTree = new BinaryTree<>(); binaryTree.put(1,\\\"yhx\\\"); binaryTree.put(2,\\\"love\\\"); binaryTree.put(3,\\\"lwh\\\"); System.out.println(binaryTree.size()); binaryTree.delete(2); String node = binaryTree.getNode(2); System.out.println(node); System.out.println(binaryTree.size()); } } \"]},{\"header\":\"3、查找二叉树中最大/最小的键\",\"slug\":\"_3、查找二叉树中最大-最小的键\",\"contents\":[]},{\"header\":\"1、最小的键\",\"slug\":\"_1、最小的键\",\"contents\":[\"方法\",\"描述\",\"public Key min()\",\"找出树中最小的键\",\"private Node min(Node x)\",\"找出指定树x中，最小键所在的结点\",\"/** * 查找树中最小的键 * @return */ public Key minKey(){ return minKey(root).key; } /** * 根据二叉树的特点，左子树 < 右子树 so最小的键肯定是位于左边 * @param x * @return */ public Node<Key,Value> minKey(Node<Key,Value> x){ if (x.left != null){ return minKey(x.left); }else { return x; } } \"]},{\"header\":\"2、最大的键\",\"slug\":\"_2、最大的键\",\"contents\":[\"方法\",\"描述\",\"public Key max()\",\"找出树中最大的键\",\"public Node max(Node x)\",\"找出指定树x中，最大键所在的结点\",\"/** * 查询树中最大的键 * @return */ public Key maxKey(){ return maxKey(root).key; } /** * 根据二叉树的特点，右子树 > 左子树 so最大的键肯定是位于右边 * @param x * @return */ public Node<Key,Value> maxKey(Node<Key,Value> x){ if(x.right != null){ return maxKey(x.right); }else { return x; } } \"]},{\"header\":\"3、二叉树的遍历\",\"slug\":\"_3、二叉树的遍历\",\"contents\":[\"我们把树简单的画作上图中的样子，由一个根节点、一个左子树、一个右子树组成，那么按照根节点什么时候被访 问，我们可以把二叉树的遍历分为以下三种方式：\",\"前序遍历\",\"先访问根结点，然后再访问左子树，最后访问右子树\",\"中序遍历\",\"先访问左子树，中间访问根节点，最后访问右子树\",\"后序遍历\",\"先访问左子树，再访问右子树，最后访问根节点\"]},{\"header\":\"1、前序遍历\",\"slug\":\"_1、前序遍历\",\"contents\":[\"前序遍历的API\",\"方法\",\"描述\",\"public Queue<Key> preErgodic() \",\"使用前序遍历，获取整个树中的所有键\",\"private void preErgodic(Node x,Queue<Key> keys)\",\"使用前序遍历，把指定树x中的所有键放入到keys队列中\",\"/** * 前序遍历 * @return */ public Queue<Key> preErgodic(){ Queue<Key> queue = new Queue<>(); preErgodic(root,queue); return queue; } /** * 前序遍历操作 先访问根结点，然后再访问左子树，最后访问右子树 * @param x 根节点 * @param queue 队列 */ private void preErgodic(Node<Key,Value> x, Queue<Key> queue){ if (x == null) { return; } //把当前结点的key放入到队列中 queue.enqueue(x.key); //访问左子树 if(x.left != null){ preErgodic(x.left,queue); } //访问右子树 if(x.right != null){ preErgodic(x.right,queue); } } \"]},{\"header\":\"2、中序遍历\",\"slug\":\"_2、中序遍历\",\"contents\":[\"中序遍历的API\",\"方法\",\"描述\",\"public Queue<Key> midErgodic() \",\"使用中序遍历，获取整个树中的所有键\",\"private void midErgodic(Node x,Queue<Key> keys)\",\"使用中序遍历，把指定树x中的所有键放入到keys队列中\",\"/** * 中序遍历 * @return */ public Queue<Key> midErgodic(){ Queue<Key> queue = new Queue<>(); midErgodic(root,queue); return queue; } /** * 先访问左子树，中间访问根节点，最后访问右子树 * @param x * @param queue */ private void midErgodic(Node<Key,Value> x, Queue<Key> queue){ if (x == null) { return; } //访问左子树 if(x.left != null){ preErgodic(x.left,queue); } //把当前结点的key放入到队列中 queue.enqueue(x.key); //访问右子树 if(x.right != null){ preErgodic(x.right,queue); } } \"]},{\"header\":\"3、后序遍历\",\"slug\":\"_3、后序遍历\",\"contents\":[\"后序遍历的API\",\"方法\",\"描述\",\"public Queue<Key> afterErgodic() \",\"使用后序遍历，获取整个树中的所有键\",\"private void afterErgodic(Node x,Queue<Key> keys)\",\"使用后序遍历，把指定树x中的所有键放入到keys队列中\",\" /** * 后序遍历 * @return */ public Queue<Key> afterErgodic(){ Queue<Key> queue = new Queue<>(); afterErgodic(root,queue); return queue; } /** * 先访问左子树，再访问右子树，最后访问根节点 * @param x * @param queue */ private void afterErgodic(Node<Key,Value> x, Queue<Key> queue){ if (x == null) { return; } //访问左子树 if(x.left != null){ preErgodic(x.left,queue); } //访问右子树 if(x.right != null){ preErgodic(x.right,queue); } //把当前结点的key放入到队列中 queue.enqueue(x.key); } \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"public class BinaryTreeErgodicTest { public static void main(String[] args) { BinaryTree<String, String> bt = new BinaryTree<>(); bt.put(\\\"E\\\", \\\"5\\\"); bt.put(\\\"B\\\", \\\"2\\\"); bt.put(\\\"G\\\", \\\"7\\\"); bt.put(\\\"A\\\", \\\"1\\\"); bt.put(\\\"D\\\", \\\"4\\\"); bt.put(\\\"F\\\", \\\"6\\\"); bt.put(\\\"H\\\", \\\"8\\\"); bt.put(\\\"C\\\", \\\"3\\\"); //前序遍历 Queue<String> preErgodic = bt.preErgodic(); //中序遍历 Queue<String> midErgodic = bt.midErgodic(); //后序遍历 Queue<String> afterErgodic = bt.afterErgodic(); for (String key : preErgodic) { System.out.println(key+\\\"=\\\" +bt.getNode(key)); } } } \"]},{\"header\":\"4、层次遍历\",\"slug\":\"_4、层次遍历\",\"contents\":[\"所谓的层序遍历，就是从根节点（第一层）开始，依次向下，获取每一层所有结点的值\",\"层次遍历的结果是：EBGADFHC\",\"方法\",\"描述\",\"public Queue<Key> layerErgodic()：\",\"使用层序遍历，获取整个树中的所有键实\",\"实现步骤：\",\"创建队列，存储每一层的结点；\",\"使用循环从队列中弹出一个结点： \",\"获取当前结点的key\",\"如果当前结点的左子结点不为空，则把左子结点放入到队列中\",\"如果当前结点的右子结点不为空，则把右子结点放入到队列中\",\"/** * 层次遍历 * @return */ public Queue<Key> layerErgodic(){ //存储key Queue<Key> keys = new Queue<>(); //存储node Queue<Node<Key,Value>> nodes = new Queue<>(); //入队头结点 nodes.enqueue(root); while(!nodes.isEmpty()){ //出队当前节点 Node<Key,Value> dequeue = nodes.dequeue(); //当前节点头入队 keys.enqueue(dequeue.key); if(dequeue.left != null){ nodes.enqueue(dequeue.left); } if(dequeue.right != null){ nodes.enqueue(dequeue.right); } } return keys; } \"]},{\"header\":\"5、二叉树的最大深度\",\"slug\":\"_5、二叉树的最大深度\",\"contents\":[\"最大深度（树的根节点到最远叶子结点的最长路径上的结点数）\",\"上面这颗树的最大深度为：E–>B–>D–>C，深度为4\",\"方法\",\"描述\",\"public int maxDepth()\",\"计算整棵树的最大深度\",\"private int maxDepth(Node x)\",\"计算指定树x的最大深度\",\"实现步骤：\",\"如果根结点为空，则最大深度为0；\",\"计算左子树的最大深度；\",\"计算右子树的最大深度；\",\"当前树的最大深度=左子树的最大深度和右子树的最大深度中的较大者+1\",\" /** * 计算整棵树的最大深度 * @return */ public int maxDepth(){ return maxDepth(root); } /** * 计算指定节点的最大深度 * @param x * @return */ private int maxDepth(Node<Key,Value> x){ if (x == null) { return 0; } int maxRight = 0; int maxLeft = 0; int maxDepth; if(x.left != null){ maxLeft = maxDepth(x.left); } if (x.right != null) { maxRight = maxDepth(x.right); } maxDepth = maxLeft > maxRight ? maxLeft + 1 : maxRight + 1; return maxDepth; } \"]},{\"header\":\"6、折纸问题\",\"slug\":\"_6、折纸问题\",\"contents\":[\"请把一段纸条竖着放在桌子上，然后从纸条的下边向上方对折1次，压出折痕后展开。此时 折痕是凹下去的，即折痕突起的方向指向纸条的背面。如果从纸条的下边向上方连续对折2 次，压出折痕后展开，此时有三条折痕，从上到下依次是下折痕、下折痕和上折痕。 给定一 个输入参数N，代表纸条都从下边向上方连续对折N次，请从上到下打印所有折痕的方向 例如：N=1时，打印： down；N=2时，打印： down down up\"]},{\"header\":\"分析\",\"slug\":\"分析\",\"contents\":[\"我们把对折后的纸张翻过来，让粉色朝下，这时把第一次对折产生的折痕看做是根结点，那第二次对折产生的下折痕就是该结点的左子结点，而第二次对折产生的上折痕就是该结点的右子结点，这样我们就可以使用树型数据结构来描述对折后产生的折痕。\",\"这棵树有这样的特点：\",\"根结点为下折痕；\",\"每一个结点的左子结点为下折痕；\",\"每一个结点的右子结点为上折痕；\",\"实现步骤：\",\"构建节点类\",\"构建深度为n的折痕树\",\"使用中序遍历，打印树中所有节点的内容\",\"构建深度为N的折痕树：\",\"第一次对折，只有一条折痕，创建根节点\",\"如果不是第一次对折，则使用队列保存根节点\",\"循环遍历队列 \",\"从队列中拿出一个节点\",\"如果当前节点的左节点不为空，则把这个左节点加入队列中\",\"如果当前节点的右节点不为空，则把这个右节点加入队列中\",\"判断当前结点的左子结点和右子结点都为空，如果是，则需要为当前结点创建一个值为down的左子结点，一个值为up的右子结点。\",\"public class PaperFolding { /** * 创建折痕树 * @param size 深度 */ public static Node createTree(int size){ Node root = null; for (int i = 0; i < size; i++) { //第一次对折，只有一条折痕，创建根节点 if (0 == i){ root = new Node(\\\"down\\\",null,null); }else { //如果不是第一次对折，则使用队列保存根节点 Queue<Node> nodes = new Queue<>(); nodes.enqueue(root); //循环遍历 while(!nodes.isEmpty()){ //从队列中拿出一个节点 Node dequeue = nodes.dequeue(); //如果当前节点的左节点不为空，则把这个左节点加入队列中 if(dequeue.left != null){ nodes.enqueue(dequeue.left); } //如果当前节点的右节点不为空，则把这个右节点加入队列中 if(dequeue.right != null){ nodes.enqueue(dequeue.right); } //判断当前结点的左子结点和右子结点都为空，则需要为当前结点创建一个值为down的左子结点，一个值为up的右子结点。 if(dequeue.left == null && dequeue.right == null){ dequeue.left = new Node(\\\"down\\\",null,null); dequeue.right = new Node(\\\"up\\\",null,null); } } } } return root; } /** * 采用中序遍历 * @param root */ public static void printTree(Node root){ if (root == null) { return; } printTree(root.left); System.out.print(root.item+\\\" \\\"); printTree(root.right); } /** * 节点类 */ private static class Node{ String item; Node left; Node right; public Node(String item, Node left, Node right) { this.item = item; this.left = left; this.right = right; } } public static void main(String[] args) { Node tree = createTree(2); printTree(tree); } } \",\"down down up \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/graph.html\":{\"title\":\"12、图\",\"contents\":[{\"header\":\"1、图的定义及分类\",\"slug\":\"_1、图的定义及分类\",\"contents\":[\"图是由一组顶点和一组能够将两个顶点相连的边组成的\",\"特殊的图：\",\"自环：即一条连接一个顶点和其自身的边；\",\"平行边：连接同一对顶点的两条边；\",\"图的分类\",\"按照连接两个顶点的边的不同，可以把图分为以下两种：\",\"无向图：边仅仅连接两个顶点，没有其他含义；\",\"有向图：边不仅连接两个顶点，并且具有方向；\"]},{\"header\":\"2、图的相关术语\",\"slug\":\"_2、图的相关术语\",\"contents\":[]},{\"header\":\"1、相邻顶点\",\"slug\":\"_1、相邻顶点\",\"contents\":[\"当两个顶点通过一条边相连时，我们称这两个顶点是相邻的，并且称这条边依附于这两个顶点。\"]},{\"header\":\"2、度\",\"slug\":\"_2、度\",\"contents\":[\"某个顶点的度就是依附于该顶点的边的个数\"]},{\"header\":\"3、子图\",\"slug\":\"_3、子图\",\"contents\":[\"是一幅图的所有边的子集(包含这些边依附的顶点)组成的图；\"]},{\"header\":\"4、路径\",\"slug\":\"_4、路径\",\"contents\":[\"是由边顺序连接的一系列的顶点组成\"]},{\"header\":\"5、环\",\"slug\":\"_5、环\",\"contents\":[\"是一条至少含有一条边且终点和起点相同的路径\"]},{\"header\":\"6、连通图\",\"slug\":\"_6、连通图\",\"contents\":[\"如果图中任意一个顶点都存在一条路径到达另外一个顶点，那么这幅图就称之为连通图\"]},{\"header\":\"7、连通子图\",\"slug\":\"_7、连通子图\",\"contents\":[\"一个非连通图由若干连通的部分组成，每一个连通的部分都可以称为该图的连通子图\"]},{\"header\":\"3、图的存储结构\",\"slug\":\"_3、图的存储结构\",\"contents\":[\"要表示一幅图，只需要表示清楚以下两部分内容即可：\",\"图中所有的顶点；\",\"所有连接顶点的边；\"]},{\"header\":\"1、邻接矩阵\",\"slug\":\"_1、邻接矩阵\",\"contents\":[\"使用一个VV的二维数组intV adj,把索引的值看做是顶点；*\",\"如果顶点v和顶点w相连，我们只需要将adjv和adjw的值设置为1,否则设置为0即可。\",\"很明显，邻接矩阵这种存储方式的空间复杂度是V^2的，如果我们处理的问题规模比较大的话，内存空间极有可能不够用。\"]},{\"header\":\"2、邻接表\",\"slug\":\"_2、邻接表\",\"contents\":[\"1.使用一个大小为V的数组 Queue[V] adj，把索引看做是顶点； 2.每个索引处adj[v]存储了一个队列，该队列中存储的是所有与该顶点相邻的其他顶点\",\"很明显，邻接表的空间并不是是线性级别的，所以后面我们一直采用邻接表这种存储形式来表示图。\"]},{\"header\":\"4、图的实现\",\"slug\":\"_4、图的实现\",\"contents\":[]},{\"header\":\"1、图的API设计\",\"slug\":\"_1、图的api设计\",\"contents\":[\"类名\",\"Graph\",\"构造方法\",\"Graph(int V)：创建一个包含V个顶点但不包含边的图\",\"成员方法\",\"public int V():获取图中顶点的数量public int E():获取图中边的数量public void addEdge(int v,int w):向图中添加一条边 v-wpublic Queue adj(int v)：获取和顶点v相邻的所有顶点\",\"成员变量\",\"private final int V: 记录顶点数量private int E: 记录边数量private Queue[] adj: 邻接表\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[\"public class Graph { /** * :记录顶点数量 */ private final int V; /** * :记录边数量 */ private int E; /** * : 邻接表 */ private Queue[] adj; public Graph(int v){ //初始化顶点数量 this.V = v; //初始化边数量 this.E = 0; //初始化邻接表 this.adj = new Queue[v]; for (int i = 0; i < adj.length; i++) { adj[i] = new Queue<Integer>(); } } /** * 获取图中顶点的数量 * @return */ public int V(){ return V; } /** * 获取图中边的数量 * @return */ public int E(){ return E; } /** * 向图中添加一条边 v-w * @param v * @param w */ public void addEdge(int v,int w){ //把w增加到v的链表上，顶点v多了一个相邻点 adj(v).enqueue(w); adj(w).enqueue(v); E++; } /** * 获取和顶点v相邻的所有顶点 * @param v * @return */ public Queue<Integer> adj(int v){ return adj[v]; } } \",\"测试类\",\"public class GraphTest { public static void main(String[] args) { Graph graph = new Graph(20); graph.addEdge(0,5); graph.addEdge(0,1); graph.addEdge(0,2); graph.addEdge(0,6); graph.addEdge(6,4); graph.addEdge(4,3); graph.addEdge(4,5); graph.addEdge(5,3); graph.addEdge(7,8); graph.addEdge(9,10); graph.addEdge(9,11); graph.addEdge(11,12); System.out.println(\\\"图中边的数量为：\\\" + graph.E()); System.out.println(\\\"图中顶点的数量为：\\\" + graph.V()); graph.addEdge(5,7); } } \"]},{\"header\":\"5、图的搜索\",\"slug\":\"_5、图的搜索\",\"contents\":[\"最经典的算法有：\",\"深度优先搜索\",\"广度优先搜索\"]},{\"header\":\"1、深度优先搜索\",\"slug\":\"_1、深度优先搜索\",\"contents\":[\"所谓的深度优先搜索，指的是在搜索时，如果遇到一个结点既有子结点，又有兄弟结点，那么先找子结点，然后找兄弟结点。\",\"很明显，在由于边是没有方向的，所以，如果4和5顶点相连，那么4会出现在5的相邻链表中，5也会出现在4的相邻链表中，那么为了不对顶点进行重复搜索，应该要有相应的标记来表示当前顶点有没有搜索过，可以使用一个布尔类型的数组 boolean[V] marked,索引代表顶点，值代表当前顶点是否已经搜索，如果已经搜索，标记为true，如果没有搜索，标记为false；\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"类名\",\"DepthFirstSearch\",\"构造方法\",\"DepthFirstSearch(Graph G,int s)：构造深度优先搜索对象，使用深度优先搜索找出G图中s顶点的所有相通顶点\",\"成员方法\",\"private void dfs(Graph G, int v)：使用深度优先搜索找出G图中v顶点的所有相通顶点public boolean marked(int w):判断w顶点与s顶点是否相通public int count():获取与顶点s相通的所有顶点的总数\",\"成员变量\",\"private boolean[] marked: 索引代表顶点，值表示当前顶点是否已经被搜索private int count：记录有多少个顶点与s顶点相通\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"public class DepthFirstSearch { /** * 索引代表顶点，值表示当前顶点是否已经被搜索 */ private boolean[] marked; /** * 记录有多少个顶点与s顶点相通 */ private int count; /** * 构造深度优先搜索对象，使用深度优先搜索找出G图中s顶点的所有相通顶点 * @param G * @param s */ public DepthFirstSearch(Graph G,int s){ //创建一个和图的顶点数一样大小的布尔数组 marked = new boolean[G.V()]; //搜索G图中与顶点s相同的所有顶点 dfs(G, s); } /** * 使用深度优先搜索找出G图中v顶点的所有相通顶点 * @param G * @param v */ private void dfs(Graph G, int v){ //把当前顶点标记为已搜索 marked[v] = true; //遍历v顶点的邻接表，得到每一个顶点i for (Integer i : G.adj(v)) { //如果当前顶点i没有被搜索过，则递归搜索与w顶点相通的其他顶点 if(!marked[i]){ dfs(G,i); } } //相通的顶点数量+1 count++; } /** * 判断w顶点与s顶点是否相通 * @param w * @return */ public boolean marked(int w){ return marked[w]; } /** * 获取与顶点s相通的所有顶点的总数 * @return */ public int count(){ return count; } } \",\"测试类\",\"public class DepthFirstSearchTest { public static void main(String[] args) { Graph graph = new Graph(20); graph.addEdge(0,5); graph.addEdge(0,1); graph.addEdge(0,2); graph.addEdge(0,6); graph.addEdge(6,4); graph.addEdge(4,3); graph.addEdge(4,5); graph.addEdge(5,3); graph.addEdge(7,8); graph.addEdge(9,10); graph.addEdge(9,11); graph.addEdge(11,12); DepthFirstSearch search = new DepthFirstSearch(graph, 0); System.out.println(\\\"与0相邻的顶点个数：\\\" + search.count()); System.out.println(\\\"0和5是否相邻：\\\" + search.marked(5)); System.out.println(\\\"0和7是否相邻：\\\" + search.marked(7)); } } \"]},{\"header\":\"2、广度优先搜索\",\"slug\":\"_2、广度优先搜索\",\"contents\":[\"从根结点开始，沿着树的宽度遍历树的结点，如果所有结点被访问，则终止\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-1\",\"contents\":[\"类名\",\"BreadthFirstSearch\",\"构造方法\",\"public BreadthFirstSearch(Graph G,int s)：构造广度优先搜索对象，使用广度优先搜索找到G图中s顶点的所有相邻顶点\",\"成员方法\",\"private void bfs(Graph G, int v)：使用广度优先搜索找出G图中v顶点的所有相邻顶点public boolean marked(int w):判断w顶点与s顶点是否相通public int count():获取与顶点s相通的所有顶点的总数\",\"成员变量\",\"private boolean[] marked: 索引代表顶点，值表示当前顶点是否已经被搜索private int count：记录有多少个顶点与s顶点相通private Queue waitSearch: 用来存储待搜索邻接表的点\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-1\",\"contents\":[\"public class BreadthFirstSearch { /** * :索引代表顶点，值表示当前顶点是否已经被搜索 */ private boolean[] marked; /** * 记录有多少个顶点与s顶点相通 */ private int count; /** * 用来存储待搜索邻接表的点 */ private Queue<Integer> waitSearch; /** * 使用广度优先搜索，找到g图中s顶点的所有相邻顶点 * @param g * @param s */ public BreadthFirstSearch(Graph g, int s){ //创建一个和图一样的boolean数组 marked = new boolean[g.V()]; //初始化待搜索的索引 waitSearch = new Queue<Integer>(); bfs(g, s); } /** * 使用广度优先搜索找出G图中v顶点的所有相邻顶点 * @param G * @param v */ private void bfs(Graph G, int v){ //标记当前v的搜索状态为true marked[v] = true; //将当前顶点放入队列中，等待搜索他的邻接表 waitSearch.enqueue(v); //待搜索的队列不为空 while (!waitSearch.isEmpty()){ Integer dequeue = waitSearch.dequeue(); //遍历wait顶点的邻接表，得到每一个顶点 for (Integer w : G.adj(dequeue)) { if(!marked(w)){ bfs(G,w); } } } count++; } /** * 判断w顶点与s顶点是否相通 * @param w * @return */ public boolean marked(int w){ return marked(w); } /** * 获取与顶点s相通的所有顶点的总数 * @return */ public int count(){ return count; } } \",\"测试类\",\"public class BreadthFirstSearchTest { public static void main(String[] args) { Graph graph = new Graph(20); graph.addEdge(0,5); graph.addEdge(0,1); graph.addEdge(0,2); graph.addEdge(0,6); graph.addEdge(6,4); graph.addEdge(4,3); graph.addEdge(4,5); graph.addEdge(5,3); graph.addEdge(7,8); graph.addEdge(9,10); graph.addEdge(9,11); graph.addEdge(11,12); BreadthFirstSearch search = new BreadthFirstSearch(graph, 0); System.out.println(\\\"与0相邻的顶点个数：\\\" + search.count()); System.out.println(\\\"0和5是否相邻：\\\" + search.marked(5)); System.out.println(\\\"0和7是否相邻：\\\" + search.marked(7)); } } \"]},{\"header\":\"3、路径查找\",\"slug\":\"_3、路径查找\",\"contents\":[\"例如在上图上查找顶点0到顶点4的路径用红色标识出来,那么我们可以把该路径表示为 0-2-3-4。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-2\",\"contents\":[\"类名\",\"DepthFirstPaths\",\"构造方法\",\"DepthFirstPaths(Graph G,int s)：构造深度优先搜索对象，使用深度优先搜索找出G图中起点为s的所有路径\",\"成员方法\",\"private void dfs(Graph G, int v)：使用深度优先搜索找出G图中v顶点的所有相邻顶点public boolean hasPathTo(int v):判断v顶点与s顶点是否存在路径public Stack pathTo(int v):找出从起点s到顶点v的路径(就是该路径经过的顶点)\",\"成员变量\",\"private boolean[] marked: 索引代表顶点，值表示当前顶点是否已经被搜索private int s:起点private int[] edgeTo:索引代表顶点，值代表从起点s到当前顶点路径上的最后一个顶点\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-2\",\"contents\":[\"public class DepthFirstPaths { /** * 索引代表顶点，值表示当前顶点是否已经被搜索 */ private boolean[] marked; /** * 起点 */ private int s; /** * 索引代表顶点，值代表从起点s到当前顶点路径上的最后一个顶点 */ private int[] edgeTo; /** * 深度优先搜索 * @param graph 图 * @param s 起点 */ public DepthFirstPaths(Graph graph, int s){ //初始化数组 marked = new boolean[graph.V()]; edgeTo = new int[graph.V()]; this.s = s; dfs(graph, s); } /** * 使用深度优先搜索找出G图中v顶点的所有相邻顶点 * @param G * @param v */ private void dfs(Graph G, int v){ marked[v] = true; //如果当前顶点w没有被搜索过，则将edgeTo[w]设置为v,表示w的前一个顶点为v，并递归搜索与w顶点相通的其他顶点 for (Integer w : G.adj(v)) { if (!marked[w]){ edgeTo[w] = v; dfs(G, w); } } } /** * 判断v顶点与s顶点是否存在路径 * @param v * @return */ public boolean hasPathTo(int v){ return marked[v]; } /** * 找出从起点s到顶点v的路径(就是该路径经过的顶点) * @param v * @return */ public Stack pathTo(int v){ //当前v顶点与s顶点不连通，所以直接返回null，没有路径 if (!hasPathTo(v)){ return null; } Stack<Integer> stack = new Stack<>(); //第一次把当前顶点存进去，然后将x变换为到达当前顶点的前一个顶点edgeTo[x],在把前一个顶点存进去， // 继续将x变化为到达前一个顶点的前一个顶点，继续存，一直到x的值为s为止，相当于逆推法，最后把s放进去 for (int i = v; i != s ; i = edgeTo[i]) { //把当前顶点放入容器 stack.push(i); } //把起点s放入容器 stack.push(s); return stack; } } \",\"测试类\",\"public class DepthFirstPathsTest { public static void main(String[] args) throws Exception{ //创建一个BufferReader读取流 BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(DepthFirstPathsTest.class.getClassLoader().getResourceAsStream(\\\"depth_first_search.txt\\\"))); //读取第一行数据 int i = Integer.parseInt(bufferedReader.readLine()); //创建图 Graph graph = new Graph(i); //读取城市道路条数（边数） int i1 = Integer.parseInt(bufferedReader.readLine()); for (int j = 0; j < i1; j++) { String s = bufferedReader.readLine(); String[] split = s.split(\\\" \\\"); int i2 = Integer.parseInt(split[0]); int i3 = Integer.parseInt(split[1]); graph.addEdge(i2,i3); } //起点为0 DepthFirstPaths paths = new DepthFirstPaths(graph, 0); //查找0-4的路径 Stack<Integer> stack = paths.pathTo(4); StringBuilder builder = new StringBuilder(); for (Integer o : stack) { builder.append(o + \\\"-\\\"); } //删除最后一个- builder.deleteCharAt(builder.length()-1); System.out.println(builder); } } \",\"输出结果\",\"0-2-3-4 \"]},{\"header\":\"6、有向图\",\"slug\":\"_6、有向图\",\"contents\":[]},{\"header\":\"1、有向图的定义和相关术语\",\"slug\":\"_1、有向图的定义和相关术语\",\"contents\":[]},{\"header\":\"1、定义\",\"slug\":\"_1、定义\",\"contents\":[\"有向图是一副具有方向性的图，是由一组顶点和一组有方向的边组成的，每条方向的边都连着一对有序的顶点。\"]},{\"header\":\"2、出度\",\"slug\":\"_2、出度\",\"contents\":[\"由某个顶点指出的边的个数称为该顶点的出度。\"]},{\"header\":\"3、入度\",\"slug\":\"_3、入度\",\"contents\":[\"指向某个顶点的边的个数称为该顶点的入度。\"]},{\"header\":\"4、有向路径\",\"slug\":\"_4、有向路径\",\"contents\":[\"由一系列顶点组成，对于其中的每个顶点都存在一条有向边，从它指向序列中的下一个顶点。\"]},{\"header\":\"5、有向环\",\"slug\":\"_5、有向环\",\"contents\":[\"一条至少含有一条边，且起点和终点相同的有向路径。\"]},{\"header\":\"6、一副有向图中两个顶点v和w可能存在以下四种关系\",\"slug\":\"_6、一副有向图中两个顶点v和w可能存在以下四种关系\",\"contents\":[\"没有边相连\",\"存在从v到w的边v—>w\",\"存在从w到v的边w—>v\",\"既存在w到v的边，也存在v到w的边，即双向连接；\"]},{\"header\":\"2、API设计\",\"slug\":\"_2、api设计\",\"contents\":[\"类名\",\"Digragh\",\"构造方法\",\"Digraph(int V)：创建一个包含V个顶点但不包含边的有向图\",\"成员方法\",\"public int V():获取图中顶点的数量public int E():获取图中边的数量public void addEdge(int v,int w):向有向图中添加一条边 v->wpublic Queue adj(int v)：获取由v指出的边所连接的所有顶点private Digraph reverse():该图的反向图\",\"成员变量\",\"private final int V: 记录顶点数量private int E: 记录边数量private Queue[] adj: 邻接表\"]},{\"header\":\"3、实现\",\"slug\":\"_3、实现\",\"contents\":[\"public class Digraph { /** * 记录顶点数量 */ private final int V; /** * 记录边数量 */ private int E; /** * 邻接表 */ private Queue[] adj; /** * 创建一个包含V个顶点但不包含边的有向图 * @param v */ public Digraph(int v){ this.V = v; this.E = 0; this.adj = new Queue[v]; for (int i = 0; i < this.adj.length; i++) { adj[i] = new Queue<Integer>(); } } /** * 获取图中顶点的数量 * @return */ public int V(){ return V; } /** * 获取图中边的数量 * @return */ public int E(){ return E; } /** * 向有向图中添加一条边 v->w * @param v * @param w */ public void addEdge(int v,int w){ //由于有向图中边是有向的，v->w 边，只需要让w出现在v的邻接表中，而不需要让v出现在w的邻接表中 adj[v].enqueue(w); E++; } /** * 获取由v指出的边所连接的所有顶点 * @param v * @return */ public Queue<Integer> adj(int v){ return adj[v]; } /** * 该图的反向图 * @return */ private Digraph reverse(){ Digraph digraph = new Digraph(V); for (int i = 0; i < V; i++) { //得到原图中的v顶点对应的邻接表,原图中的边为 v->w,则反向图中边为w->v; for (Integer w : adj(i)) { digraph.addEdge(w,i); } } return digraph; } } \"]},{\"header\":\"7、拓扑排序\",\"slug\":\"_7、拓扑排序\",\"contents\":[\"在现实生活中，我们经常会同一时间接到很多任务去完成，但是这些任务的完成是有先后次序的。以我们学习java学科为例，我们需要学习很多知识，但是这些知识在学习的过程中是需要按照先后次序来完成的。从java基础，到jsp/servlet，到ssm，到springboot等是个循序渐进且有依赖的过程。在学习jsp前要首先掌握java基础和html基础，学习ssm框架前要掌握jsp/servlet之类才行。\",\"为了简化问题，我们使用整数为顶点编号的标准模型来表示这个案例：\",\"拓扑排序\",\"给定一副有向图，将所有的顶点排序，使得所有的有向边均从排在前面的元素指向排在后面的元素，此时就可以明确的表示出每个顶点的优先级。\"]},{\"header\":\"1、检测有向图中的环\",\"slug\":\"_1、检测有向图中的环\",\"contents\":[\"如果学习x课程前必须先学习y课程，学习y课程前必须先学习z课程，学习z课程前必须先学习x课程，那么一定是有问题了，我们就没有办法学习了，因为这三个条件没有办法同时满足。其实这三门课程x、y、z的条件组成了一个环。\",\"因此，如果我们要使用拓扑排序解决优先级问题，首先得保证图中没有环的存在。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-3\",\"contents\":[\"类名\",\"DirectedCycle\",\"构造方法\",\"public DirectedCycle(Digraph G)：创建一个检测环对象，检测图G中是否有环\",\"成员方法\",\"private void dfs(Digraph G,int v)：基于深度优先搜索，检测图G中是否有环public boolean hasCycle():判断图中是否有环\",\"成员变量\",\"private boolean[] marked: 索引代表顶点，值表示当前顶点是否已经被搜索private boolean hasCycle: 记录图中是否有环private boolean[] onStack:索引代表顶点，使用栈的思想，记录当前顶点有没有已经处于正在搜索的有向路径上\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-3\",\"contents\":[\"在API中添加了onStack[] 布尔数组，索引为图的顶点，当我们深度搜索时：\",\"在如果当前顶点正在搜索，则把对应的onStack数组中的值改为true，标识进栈；\",\"如果当前顶点搜索完毕，则把对应的onStack数组中的值改为false，标识出栈；\",\"如果即将要搜索某个顶点，但该顶点已经在栈中，则图中有环；\",\"public class DirectedCycle { /** * 索引代表顶点，值表示当前顶点是否已经被搜索 */ private boolean[] marked; /** * 记录图中是否有环 */ private boolean hasCycle; /** * 索引代表顶点，使用栈的思想，记录当前顶点有没有已经处于正在搜索的有向路径上 */ private boolean[] onStack; /** * 创建一个检测环对象，检测图G中是否有环 * @param G */ public DirectedCycle(Digraph G){ //创建一个和图的顶点数一样大小的marked数组 this.marked = new boolean[G.V()]; //默认不存在环 this.hasCycle = false; //创建一个和图的顶点数一样的onStack数组 this.onStack = new boolean[G.V()]; for (int i = 0; i < G.V(); i++) { //如果当前顶点没有搜索过，则搜索 if (!marked[i]){ dfs(G,i); } } } /** * 基于深度优先搜索，检测图G中是否有环 * @param G * @param v */ private void dfs(Digraph G,int v){ //标记当前顶点已被搜索 marked[v] = true; //让当前顶点进栈 onStack[v] = true; //遍历v顶点的邻接表，得到每一个顶点w for (Integer w : G.adj(v)) { //如果当前顶点未被搜索，则搜索该顶点 if (!marked[w]){ dfs(G,w); } //如果顶点w已经被搜索过，则查看顶点w是否在栈中，如果在，则证明图中有环，修改hasCycle标记，结束循环 if (onStack[w]){ hasCycle = true; return; } } onStack[v] = false; } /** * 判断图中是否有环 * @return */ public boolean hasCycle(){ return hasCycle; } } \"]},{\"header\":\"2、基于深度优先的顶点排序\",\"slug\":\"_2、基于深度优先的顶点排序\",\"contents\":[\"如果要把图中的顶点生成线性序列其实是一件非常简单的事，之前我们学习并使用了多次深度优先搜索，我们会发现其实深度优先搜索有一个特点，那就是在一个连通子图上，每个顶点只会被搜索一次，如果我们能在深度优先搜索的基础上，添加一行代码，只需要将搜索的顶点放入到线性序列的数据结构中，我们就能完成这件事。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-4\",\"contents\":[\"类名\",\"DepthFirstOrder\",\"构造方法\",\"DepthFirstOrder(Digraph G)：创建一个顶点排序对象，生成顶点线性序列；\",\"成员方法\",\"private void dfs(Digraph G,int v)：基于深度优先搜索，生成顶点线性序列public Stack reversePost():获取顶点线性序列\",\"成员变量\",\"private boolean[] marked: 索引代表顶点，值表示当前顶点是否已经被搜索private Stack reversePost: 使用栈，存储顶点序列\"]},{\"header\":\"2、顶点排序实现\",\"slug\":\"_2、顶点排序实现\",\"contents\":[\"在API的设计中，我们添加了一个栈reversePost用来存储顶点，当我们深度搜索图时，每搜索完毕一个顶点，把该顶点放入到reversePost中，这样就可以实现顶点排序。\",\"public class DepthFirstOrder { /** * 索引代表顶点，值表示当前顶点是否已经被搜索 */ private boolean[] marked; /** * 使用栈，存储顶点序列 */ private Stack reversePost; /** * 创建一个顶点排序对象，生成顶点线性序列； * @param G */ public DepthFirstOrder(Digraph G){ //初始化 marked = new boolean[G.V()]; reversePost = new Stack<Integer>(); //遍历图中的每一个顶点 for (int i = 0; i < G.V(); i++) { if(!marked[i]){ dfs(G,i); } } } /** * 基于深度优先搜索，生成顶点线性序列 * @param G * @param v */ private void dfs(Digraph G,int v){ //标记当前顶点已被搜索 marked[v] = true; //循环遍历每一个顶点 for (Integer w : G.adj(v)) { if(!marked[w]){ dfs(G,w); } } //当前顶点已经搜索完毕，让当前顶点入栈 reversePost.push(v); } /** * :获取顶点线性序列 * @return */ public Stack reversePost(){ return reversePost; } } \"]},{\"header\":\"3、拓扑排序实现\",\"slug\":\"_3、拓扑排序实现\",\"contents\":[\"前面已经实现了环的检测以及顶点排序，那么拓扑排序就很简单了，基于一幅图，先检测有没有环，如果没有环，则调用顶点排序即可。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-5\",\"contents\":[\"类名\",\"TopoLogical\",\"构造方法\",\"TopoLogical(Digraph G)：构造拓扑排序对象\",\"成员方法\",\"public boolean isCycle()：判断图G是否有环public Stack order():获取拓扑排序的所有顶点\",\"成员变量\",\"private Stack order: 顶点的拓扑排序\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-1\",\"contents\":[\"public class TopologicalSort { /** * 顶点的拓扑排序 */ private Stack<Integer> order; /** * 构造拓扑排序对象 * @param G */ public TopologicalSort(Digraph G){ //创建检测环对象，检测图G中是否有环 DirectedCycle dCycle = new DirectedCycle(G); if(!dCycle.hasCycle()){ //如果没有环，创建顶点排序对象，进行顶点排序 DepthFirstOrder depthFirstOrder = new DepthFirstOrder(G); order = depthFirstOrder.reversePost(); } } /** * 判断图G是否有环 * @return */ public boolean isCycle(){ return order == null; } /** * 获取拓扑排序的所有顶点 * @return */ public Stack<Integer> order(){ return order; } } \",\"测试类\",\"public class TopologicalSortTest { public static void main(String[] args) { //创建一个有向图 Digraph digraph = new Digraph(6); digraph.addEdge(0,2); digraph.addEdge(0,3); digraph.addEdge(2,4); digraph.addEdge(3,4); digraph.addEdge(4,5); digraph.addEdge(1,3); TopologicalSort topologicalSort = new TopologicalSort(digraph); Stack<Integer> order = topologicalSort.order(); for (Integer o : order) { System.out.print(o+\\\"->\\\"); } } } \",\"1->0->3->2->4->5-> \"]},{\"header\":\"8、加权无向图\",\"slug\":\"_8、加权无向图\",\"contents\":[\"加权无向图是一种为每条边关联一个权重值或是成本的图模型。这种图能够自然地表示许多应用。在一副航空图中，边表示航线，权值则可以表示距离或是费用。在一副电路图中，边表示导线，权值则可能表示导线的长度即成本，或是信号通过这条先所需的时间。此时我们很容易就能想到，最小成本的问题，例如，从西安飞纽约，怎样飞才能使时间成本最低或者是金钱成本最低？ 在下图中，从顶点0到顶点4有三条路径，分别为0-2-3-4,0-2-4,0-5-3-4,那我们如果要通过那条路径到达4顶点最好呢？此时就要考虑，那条路径的成本最低。\"]},{\"header\":\"1、加权无向图边的表示\",\"slug\":\"_1、加权无向图边的表示\",\"contents\":[\"加权无向图中的边我们就不能简单的使用v-w两个顶点表示了，而必须要给边关联一个权重值，因此我们可以使用对象来描述一条边。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-6\",\"contents\":[\"类名\",\"Edge implements Comparable\",\"构造方法\",\"Edge(int v,int w,double weight)：通过顶点v和w，以及权重weight值构造一个边对象\",\"成员方法\",\"public double weight():获取边的权重值public int either():获取边上的一个点public int other(int vertex)):获取边上除了顶点vertex外的另外一个顶点public int compareTo(Edge that)：比较当前边和参数that边的权重，如果当前边权重大，返回1，如果一样大，返回0，如果当前权重小，返回-1\",\"成员变量\",\"private final int v：顶点一private final int w：顶点二private final double weight：当前边的权重\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-4\",\"contents\":[\"public class Edge implements Comparable<Edge>{ /** * 顶点一 */ private final int v; /** * 顶点二 */ private final int w; /** * 当前边的权重 */ private final double weight; public Edge(int v, int w, double weight) { this.v = v; this.w = w; this.weight = weight; } /** * 获取边的权重值 * @return */ public double weight(){ return weight; } /** * 获取边上的一个点 * @return */ public int either(){ return v; } /** * 获取边上除了顶点vertex外的另外一个顶点 * @param vertex * @return */ public int other(int vertex){ if (vertex == v){ return w; }else { return v; } } @Override public int compareTo(Edge o) { int temp; if(this.weight() == o.weight()){ temp = 0; }else { temp = this.weight() > o.weight() ? 1 : -1; } return temp; } } \"]},{\"header\":\"2、加权无向图的实现\",\"slug\":\"_2、加权无向图的实现\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-7\",\"contents\":[\"类名\",\"EdgeWeightedGraph\",\"构造方法\",\"EdgeWeightedGraph(int V)：创建一个含有V个顶点的空加权无向图\",\"成员方法\",\"public int V():获取图中顶点的数量public int E():获取图中边的数量public void addEdge(Edge e):向加权无向图中添加一条边epublic Queue adj(int v)：获取和顶点v关联的所有边public Queue edges()：获取加权无向图的所有边\",\"成员变量\",\"private final int V: 记录顶点数量private int E: 记录边数量private Queue[] adj: 邻接表\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-5\",\"contents\":[\"public class EdgeWeightedGraph { /** * 记录顶点数量 */ private final int V; /** * 记录边数量 */ private int E; /** * : 邻接表 */ private Queue<Edge>[] adj; public EdgeWeightedGraph(int v) { this.V = v; this.E = 0; this.adj = new Queue[v]; for (int i = 0; i < adj.length; i++) { adj[i] = new Queue<Edge>(); } } /** * 获取图中顶点的数量 * @return */ public int V(){ return V; } /** * 获取图中边的数量 * @return */ public int E(){ return E; } /** * 向加权无向图中添加一条边e * @param e */ public void addEdge(Edge e){ //获取边上的一个顶点 int v = e.either(); //获取边上的另一个顶点 int w = e.either(); //因为是无向图，所以边e需要同时出现在两个顶点的邻接表中 adj[v].enqueue(e); adj[w].enqueue(e); //边的数量+1 E++; } /** * 获取和顶点v关联的所有边 * @param v * @return */ public Queue<Edge> adj(int v){ return adj[v]; } /** * 获取加权无向图的所有边 * @return */ public Queue<Edge> edges(){ //创建一个队列，存储所有的边 Queue<Edge> queue = new Queue<>(); //遍历顶点，拿到顶点的邻接表 for (int i = 0; i < this.V; i++) { for (Edge edge : adj(i)) { if(edge.other(i) < V){ queue.enqueue(edge); } } } return queue; } } \"]},{\"header\":\"9、最小生成树\",\"slug\":\"_9、最小生成树\",\"contents\":[\"之前学习的加权图，我们发现它的边关联了一个权重，那么我们就可以根据这个权重解决最小成本问题，但如何才能找到最小成本对应的顶点和边呢？最小生成树相关算法可以解决。\"]},{\"header\":\"1、最小生成树定义及相关约定\",\"slug\":\"_1、最小生成树定义及相关约定\",\"contents\":[\"图的生成树是它的一棵含有其所有顶点的无环连通子图，一副加权无向图的最小生成树它的一棵权值(树中所有边的权重之和)最小的生成树\",\"约定：\",\"只考虑连通图。最小生成树的定义说明它只能存在于连通图中，如果图不是连通的，那么分别计算每个连通图子图的最小生成树，合并到一起称为最小生成森林。\",\"所有边的权重都各不相同。如果不同的边权重可以相同，那么一副图的最小生成树就可能不唯一了，虽然我们的算法可以处理这种情况，但为了好理解，我们约定所有边的权重都各不相同。\"]},{\"header\":\"1、最小生成树的原理\",\"slug\":\"_1、最小生成树的原理\",\"contents\":[\"用一条边接树中的任意两个顶点都会产生一个新的环；\",\"从树中删除任意一条边，将会得到两棵独立的树；\"]},{\"header\":\"2、切分定理\",\"slug\":\"_2、切分定理\",\"contents\":[\"要从一副连通图中找出该图的最小生成树，需要通过切分定理完成。\",\"切分：\",\"将图的所有顶点按照某些规则分为两个非空且没有交集的集合。\",\"横切边：\",\"连接两个属于不同集合的顶点的边称之为横切边。\",\"例如我们将图中的顶点切分为两个集合，灰色顶点属于一个集合，白色顶点属于另外一个集合，那么效果如下：\",\"切分定理：\",\"在一副加权图中，给定任意的切分，它的横切边中的权重最小者必然属于图中的最小生成树。\",\"注意:一次切分产生的多个横切边中，权重最小的边不一定是所有横切边中唯一属于图的最小生成树的边。\"]},{\"header\":\"2、贪心算法\",\"slug\":\"_2、贪心算法\",\"contents\":[\"贪心算法是计算图的最小生成树的基础算法，它的基本原理就是切分定理，使用切分定理找到最小生成树的一条边，不断的重复直到找到最小生成树的所有边。如果图有V个顶点，那么需要找到V-1条边，就可以表示该图的最小生成树。\",\"计算图的最小生成树的算法有很多种，但这些算法都可以看做是贪心算法的一种特殊情况，这些算法的不同之处在于：保存切分和判定权重最小的横切边的方式。\"]},{\"header\":\"3、Prim算法\",\"slug\":\"_3、prim算法\",\"contents\":[\"它的每一步都会为一棵生成中的树添加一条边。一开始这棵树只有一个顶点，然后会向它添加V-1条边，每次总是将下一条连接树中的顶点与不在树中的顶点且权重最小的边加入到树中。\",\"Prim算法的切分规则：\",\"把最小生成树中的顶点看做是一个集合，把不在最小生成树中的顶点看做是另外一个集合。\"]},{\"header\":\"1、API算法\",\"slug\":\"_1、api算法\",\"contents\":[\"类名\",\"PrimMST\",\"构造方法\",\"PrimMST(EdgeWeightedGraph G)：根据一副加权无向图，创建最小生成树计算对象；\",\"成员方法\",\"private void visit(EdgeWeightedGraph G, int v)：将顶点v添加到最小生成树中，并且更新数据public Queue edges():获取最小生成树的所有边\",\"成员变量\",\"private Edge[] edgeTo: 索引代表顶点，值表示当前顶点和最小生成树之间的最短边private double[] distTo: 索引代表顶点，值表示当前顶点和最小生成树之间的最短边的权重private boolean[] marked:索引代表顶点，如果当前顶点已经在树中，则值为true，否则为falseprivate IndexMinPriorityQueue pq:存放树中顶点与非树中顶点之间的有效横切边\"]},{\"header\":\"2、实现原理\",\"slug\":\"_2、实现原理\",\"contents\":[\"Prim算法始终将图中的顶点切分成两个集合，最小生成树顶点和非最小生成树顶点，通过不断的重复做某些操作，可以逐渐将非最小生成树中的顶点加入到最小生成树中，直到所有的顶点都加入到最小生成树中。\",\"我们在设计API的时候，使用最小索引优先队列存放树中顶点与非树中顶点的有效横切边，那么它是如何表示的呢？\",\"我们可以让最小索引优先队列的索引值表示图的顶点，让最小索引优先队列中的值表示从其他某个顶点到当前顶点的边权重。\",\"初始化状态，先默认0是最小生成树中的唯一顶点，其他的顶点都不在最小生成树中，此时横切边就是顶点0的邻接表中0-2,0-4,0-6,0-7这四条边，我们只需要将索引优先队列的2、4、6、7索引处分别存储这些边的权重值就可以表示了。\",\"现在只需要从这四条横切边中找出权重最小的边，然后把对应的顶点加进来即可。所以找到0-7这条横切边的权重最小，因此把0-7这条边添加进来，此时0和7属于最小生成树的顶点，其他的不属于，现在顶点7的邻接表中的边也成为了横切边，这时需要做两个作：\",\"0-7这条边已经不是横切边了，需要让它失效：只需要调用最小索引优先队列的delMin()方法即可完成；\",\"2和4顶点各有两条连接指向最小生成树，需要只保留一条：\",\"4-7的权重小于0-4的权重，所以保留4-7，调用索引优先队列的change(4,0.37)即可，\",\"0-2的权重小于2-7的权重，所以保留0-2，不需要做额外操作。\",\"我们不断重复上面的动作，就可以把所有的顶点添加到最小生成树中。\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现\",\"contents\":[\"public class PrimMST { /** * 索引代表顶点，值表示当前顶点和最小生成树之间的最短边 */ private Edge[] edgeTo; /** * 索引代表顶点，值表示当前顶点和最小生成树之间的最短边的权重 */ private double[] distTo; /** * 索引代表顶点，如果当前顶点已经在树中，则值为true，否则为false */ private boolean[] marked; /** * 存放树中顶点与非树中顶点之间的有效横切边 */ private IndexMinPriorityQueue pq; /** * 根据一副加权无向图，创建最小生成树计算对象； * @param G */ public PrimMST(EdgeWeightedGraph G){ //创建一个和图的顶点数一样大小的Edge数组，表示边 this.edgeTo = new Edge[G.V()]; //创建一个和图的顶点数一样大小的double数组，表示权重，并且初始化数组中的内容为无穷大，无穷大即表示不存在这样的边 this.distTo = new double[G.V()]; Arrays.fill(distTo, Double.POSITIVE_INFINITY); //创建一个和图的顶点数一样大小的boolean数组，表示当前顶点是否已经在树中 this.marked = new boolean[G.V()]; //创建一个和图的顶点数一样大小的索引优先队列，存储有效横切边 this.pq = new IndexMinPriorityQueue<>(G.V()); //默认让顶点0进入树中，0没有与其他顶点连接，初始化distTo[0]=0.0 distTo[0] = 0.0; //初始化pq pq.insert(0,0.0); //遍历有效边队列 while (!pq.isEmpty()){ //找到权重最小的横切边对应的顶点，加入到最小生成树中 visit(G, pq.delMin()); } } /** * 将顶点v添加到最小生成树中，并且更新数据 * @param G * @param v */ private void visit(EdgeWeightedGraph G, int v){ //把顶点v增加在树中 marked[v] = true; //遍历顶点v的邻接表，得到每一条边Edge e for (Edge edge : G.adj(v)) { //找到另一个顶点w int w = edge.other(v); if (marked[w]){ return; } //如果v-w边e的权重比目前distTo[w]权重小，则需要修正数据 if (edge.weight() < distTo[w]){ //把顶点w距离最小生成树的边修改为e edgeTo[w] = edge; //把顶点w距离最小生成树的边的权重修改为e.weight() distTo[w] = edge.weight(); //如果pq中存储的有效横切边已经包含了w顶点，则需要修正最小索引优先队列w索引关联的权重值 if (pq.contains(w)) { pq.changeItem(w, edge.weight()); } else { //如果pq中存储的有效横切边不包含w顶点，则需要向最小索引优先队列中添加v-w和其权重值 pq.insert(w, edge.weight()); } } } } /** * 获取最小生成树的所有边 * @return */ public Queue<Edge> edges(){ Queue<Edge> edges = new Queue<>(); for (int i = 0; i < marked.length; i++) { if (edgeTo[i] != null){ edges.enqueue(edgeTo[i]); } } return edges; } } \",\"测试数据\",\"8 16 4 5 0.35 4 7 0.37 5 7 0.28 0 7 0.16 1 5 0.32 0 4 0.38 2 3 0.17 1 7 0.19 0 2 0.26 1 2 0.36 1 3 0.29 2 7 0.34 6 2 0.40 3 6 0.52 6 0 0.58 6 4 0.93 \",\"测试类\",\"public class PrimTest { public static void main(String[] args) throws Exception{ BufferedReader br = new BufferedReader(new InputStreamReader(Objects.requireNonNull(PrimTest.class.getClassLoader().getResourceAsStream(\\\"primTestData.txt\\\")))); //读取顶点数目，初始化EdgeWeightedGraph图 int number = Integer.parseInt(br.readLine()); EdgeWeightedGraph edgeWeightedGraph = new EdgeWeightedGraph(number); //读取边的数目 int edgeNumber = Integer.parseInt(br.readLine()); //循环读取每一条边，并调用addEdge方法 for (int i = 0; i < edgeNumber; i++) { String line = br.readLine(); int v = Integer.parseInt(line.split(\\\" \\\")[0]); int w = Integer.parseInt(line.split(\\\" \\\")[1]); double weight = Double.parseDouble(line.split(\\\" \\\")[2]); edgeWeightedGraph.addEdge(new Edge(v, w, weight)); } //构建PrimMST对象 PrimMST mst = new PrimMST(edgeWeightedGraph); //获取最小生成树的边 Queue<Edge> edges = mst.edges(); //打印输出 for (Edge edge : edges) { if (edge!=null){ System.out.println(edge.either() + \\\"-\\\" + edge.other(edge.either()) + \\\"::\\\" + edge.weight()); } } } } \"]},{\"header\":\"4、kruskal算法\",\"slug\":\"_4、kruskal算法\",\"contents\":[\"kruskal算法是计算一副加权无向图的最小生成树的另外一种算法，它的主要思想是按照边的权重(从小到大)处理它们，将边加入最小生成树中，加入的边不会与已经加入最小生成树的边构成环，直到树中含有V-1条边为止。\",\"kruskal算法和prim算法的区别：\",\"Prim算法是一条边一条边的构造最小生成树，每一步都为一棵树添加一条边。\",\"kruskal算法构造最小生成树的时候也是一条边一条边地构造，但它的切分规则是不一样的。它每一次寻找的边会连接一片森林中的两棵树。如果一副加权无向图由V个顶点组成，初始化情况下每个顶点都构成一棵独立的树，则V个顶点对应V棵树，组成一片森林，kruskal算法每一次处理都会将两棵树合并为一棵树，直到整个森林中只剩一棵树为止。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-8\",\"contents\":[\"类名\",\"KruskalMST\",\"构造函数\",\"KruskalMST(EdgeWeightedGraph G)：根据一副加权无向图，创建最小生成树计算对象；\",\"成员方法\",\"public Queue edges():获取最小生成树的所有边\",\"成员变量\",\"private Queue mst：保存最小生成树的所有边private UF_Tree_Weighted uf: 索引代表顶点，使用uf.connect(v,w)可以判断顶点v和顶点w是否在同一颗树中，使用uf.union(v,w)可以把顶点v所在的树和顶点w所在的树合并private MinPriorityQueue pq: 存储图中所有的边，使用最小优先队列，对边按照权重进行排序\"]},{\"header\":\"2实现原理\",\"slug\":\"_2实现原理\",\"contents\":[\"在设计API的时候，使用了一个MinPriorityQueue pq存储图中所有的边，每次使用pq.delMin()取出权重最小的边，并得到该边关联的两个顶点v和w，通过uf.connect(v,w)判断v和w是否已经连通，如果连通，则证明这两个顶点在同一棵树中，那么就不能再把这条边添加到最小生成树中，因为在一棵树的任意两个顶点上添加一条边，都会形成环，而最小生成树不能有环的存在，如果不连通，则通过uf.connect(v,w)把顶点v所在的树和顶点w所在的树合并成一棵树，并把这条边加入到mst队列中，这样如果把所有的边处理完，最终mst中存储的就是最小生树的所有边。\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现-1\",\"contents\":[\"public class KruskalMST { /** * 保存最小生成树的所有边 */ private Queue<Edge> mst; /** * :索引代表顶点，使用uf.connect(v,w)可以判断顶点v和顶点w是否在同一颗树中 * 使用uf.union(v,w)可以把顶点v所在的树和顶点w所在的树合并 */ private UFTreeWeighted uf; /** * 存储图中所有的边，使用最小优先队列，对边按照权重进行排序 */ private MinPriorityQueue<Edge> pq; public KruskalMST(EdgeWeightedGraph G){ //初始化mst队列 this.mst = new Queue<Edge>(); //初始化并查集对象uf,容量和图的顶点数相同 this.uf = new UFTreeWeighted(G.V()); //初始化最小优先队列pq，容量比图的边的数量大1，并把图中所有的边放入pq中 this.pq = new MinPriorityQueue<>(G.E()+1); for (Edge edge : G.edges()) { pq.insert(edge); } //如果优先队列pq不为空，也就是还有边未处理，并且mst中的边还不到V-1条，继续遍历 while (!pq.isEmpty() && mst.size() < G.V() - 1) { //取出pq中权重最小的边e Edge e = pq.delMin(); //获取边e的两个顶点v和w int v = e.either(); int w = e.other(v); /* 通过uf.connect(v,w)判断v和w是否已经连通， 如果连通: 则证明这两个顶点在同一棵树中，那么就不能再把这条边添加到最小生成树中，因为在一棵 树的任意两个顶点上添加一条边，都会形成环，而最小生成树不能有环的存在; 如果不连通: 则通过uf.connect(v,w)把顶点v所在的树和顶点w所在的树合并成一棵树,并把这条边加入到mst队列中 */ if (uf.connected(v,w)){ continue; } uf.union(v,w); mst.enqueue(e); } } /** * 获取最小生成树的所有边 * @return */ public Queue<Edge> edges() { return mst; } } \",\"测试类\",\"public class KruskalTest { public static void main(String[] args) throws Exception{ BufferedReader br = new BufferedReader(new InputStreamReader(Objects.requireNonNull(KruskalTest.class.getClassLoader().getResourceAsStream(\\\"primTestData.txt\\\")))); //读取顶点数目，初始化EdgeWeightedGraph图 int number = Integer.parseInt(br.readLine()); EdgeWeightedGraph edgeWeightedGraph = new EdgeWeightedGraph(number); //读取边的数目 int edgeNumber = Integer.parseInt(br.readLine()); //循环读取每一条边，并调用addEdge方法 for (int i = 0; i < edgeNumber; i++) { String line = br.readLine(); int v = Integer.parseInt(line.split(\\\" \\\")[0]); int w = Integer.parseInt(line.split(\\\" \\\")[1]); double weight = Double.parseDouble(line.split(\\\" \\\")[2]); edgeWeightedGraph.addEdge(new Edge(v, w, weight)); } //构建PrimMST对象 KruskalMST mst = new KruskalMST(edgeWeightedGraph); //获取最小生成树的边 Queue<Edge> edges = mst.edges(); //打印输出 for (Edge edge : edges) { if (edge!=null){ System.out.println(edge.either() + \\\"-\\\" + edge.other(edge.either()) + \\\"::\\\" + edge.weight()); } } } } \"]},{\"header\":\"10、加权有向图\",\"slug\":\"_10、加权有向图\",\"contents\":[]},{\"header\":\"1、加权有向图的表示\",\"slug\":\"_1、加权有向图的表示\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-9\",\"contents\":[\"类名\",\"DirectedEdge\",\"构造方法\",\"DirectedEdge(int v,int w,double weight)：通过顶点v和w，以及权重weight值构造一个边对象\",\"成员方法\",\"public double weight():获取边的权重值public int from():获取有向边的起点public int to():获取有向边的终点\",\"成员变量\",\"private final int v：起点private final int w：终点private final double weight：当前边的权重\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-6\",\"contents\":[\"public class DirectedEdge { /** * 起点 */ private final int v; /** * 终点 */ private final int w; /** * 当前边的权重 */ private final double weight; public DirectedEdge(int v, int w, double weight) { this.v = v; this.w = w; this.weight = weight; } /** * 获取边的权重值 * @return */ public double weight(){ return weight; } /** * 获取有向边的起点 * @return */ public int from(){ return v; } /** * :获取有向边的终点 * @return */ public int to(){ return w; } } \"]},{\"header\":\"2、加权有向图的实现\",\"slug\":\"_2、加权有向图的实现\",\"contents\":[\"之前我们已经完成了有向图，在有向图的基础上，我们只需要把边的表示切换成DirectedEdge对象即可。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-10\",\"contents\":[\"类名\",\"EdgeWeightedDigraph\",\"构造方法\",\"EdgeWeightedDigraph(int V)：创建一个含有V个顶点的空加权有向图\",\"成员方法\",\"public int V():获取图中顶点的数量public int E():获取图中边的数量public void addEdge(DirectedEdge e):向加权有向图中添加一条边epublic Queue adj(int v)：获取由顶点v指出的所有的边public Queue edges()：获取加权有向图的所有边\",\"成员变量\",\"private final int V: 记录顶点数量private int E: 记录边数量private Queue[] adj: 邻接表\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-7\",\"contents\":[\"public class EdgeWeightedDigraph { /** * : 记录顶点数量 */ private final int V; /** * : 记录边数量 */ private int E; /** * : 邻接表 */ private Queue<DirectedEdge>[] adj; public EdgeWeightedDigraph(int v) { //初始化顶点数 this.V = v; //初始化边数量 this.E = 0; this.adj = new Queue[v]; //初始化邻接表的空队列 for (int i = 0; i < this.adj.length; i++) { adj[i] = new Queue<DirectedEdge>(); } } /** * 获取图中顶点的数量 * @return */ public int V(){ return V; } /** * 获取边的数量 * @return */ public int E(){ return E; } /** * 向加权有向图添加一条边 * @param e */ public void addEdge(DirectedEdge e){ //获取有向边的起点 int from = e.from(); //因为是有向图，所以边e只需要出现在起点v的邻接表中 adj[from].enqueue(e); //边的数量+1 E++; } /** * 获取由顶点v指出的所有的边 * @param v * @return */ public Queue<DirectedEdge> adj(int v){ return adj[v]; } /** * 获取加权有向图的所有边 * @return */ public Queue<DirectedEdge> edges(){ Queue<DirectedEdge> queue = new Queue<>(); //遍历顶点，拿到每个顶点的邻接表 for (int i = 0; i < this.V; i++) { //遍历邻接表，拿到邻接表中的每条边存储到队列中 for (DirectedEdge e : adj(i)) { queue.enqueue(e); } } return queue; } } \"]},{\"header\":\"11、最短路径\",\"slug\":\"_11、最短路径\",\"contents\":[]},{\"header\":\"1、最短路径定义及性质\",\"slug\":\"_1、最短路径定义及性质\",\"contents\":[]},{\"header\":\"1、定义\",\"slug\":\"_1、定义-1\",\"contents\":[\"在一副加权有向图中，从顶点s到顶点t的最短路径是所有从顶点s到顶点t的路径中总权重最小的那条路径。\"]},{\"header\":\"2、性质\",\"slug\":\"_2、性质\",\"contents\":[\"路径具有方向性；\",\"权重不一定等价于距离。权重可以是距离、时间、花费等内容，权重最小指的是成本最低\",\"只考虑连通图。一副图中并不是所有的顶点都是可达的，如果s和t不可达，那么它们之间也就不存在最短路径，为了简化问题，这里只考虑连通图。\",\"最短路径不一定是唯一的。从一个顶点到达另外一个顶点的权重最小的路径可能会有很多条，这里只需要找出一条即可。\",\"最短路径树：\",\"给定一副加权有向图和一个顶点s，以s为起点的一棵最短路径树是图的一副子图，它包含顶点s以及从s可达的所有顶点。这棵有向树的根结点为s，树的每条路径都是有向图中的一条最短路径。\"]},{\"header\":\"2、最短路径树的API设计\",\"slug\":\"_2、最短路径树的api设计\",\"contents\":[\"计算最短路径树的经典算法是dijstra算法\",\"类名\",\"DijkstraSP\",\"构造方法\",\"public DijkstraSP(EdgeWeightedDigraph G, int s)：根据一副加权有向图G和顶点s，创建一个计算顶点为s的最短路径树对象\",\"成员方法\",\"private void relax(EdgeWeightedDigraph G, int v)：松弛图G中的顶点vpublic double distTo(int v):获取从顶点s到顶点v的最短路径的总权重public boolean hasPathTo(int v):判断从顶点s到顶点v是否可达public Queue pathTo(int v):查询从起点s到顶点v的最短路径中所有的边\",\"成员变量\",\"private DirectedEdge[] edgeTo: 索引代表顶点，值表示从顶点s到当前顶点的最短路径上的最后一条边private double[] distTo: 索引代表顶点，值从顶点s到当前顶点的最短路径的总权重private IndexMinPriorityQueue pq:存放树中顶点与非树中顶点之间的有效横切边\"]},{\"header\":\"3、松弛技术\",\"slug\":\"_3、松弛技术\",\"contents\":[\"松弛这个词来源于生活：一条橡皮筋沿着两个顶点的某条路径紧紧展开，如果这两个顶点之间的路径不止一条，还有存在更短的路径，那么把皮筋转移到更短的路径上，皮筋就可以放松了。\",\"松弛这种简单的原理刚好可以用来计算最短路径树。\",\"在我们的API中，需要用到两个成员变量edgeTo和distTo，分别存储边和权重。一开始给定一幅图G和顶点s，我们只知道图的边以及这些边的权重，其他的一无所知，此时初始化顶点s到顶点s的最短路径的总权重disto[s]=0；顶点s到其他顶点的总权重默认为无穷大，随着算法的执行，不断的使用松弛技术处理图的边和顶点，并按一定的条件更新edgeTo和distTo中的数据，最终就可以得到最短路劲树。\"]},{\"header\":\"1、边的松弛\",\"slug\":\"_1、边的松弛\",\"contents\":[\"放松边v->w意味着检查从s到w的最短路径是否先从s到v，然后再从v到w？\",\"如果是，则v-w这条边需要加入到最短路径树中，更新edgeTo和distTo中的内容：edgeTo[w]=表示v->w这条边的DirectedEdge对象，distTo[w]=distTo[v]+v->w这条边的权重；\",\"如果不是，则忽略v->w这条边。\"]},{\"header\":\"2、顶点的松弛\",\"slug\":\"_2、顶点的松弛\",\"contents\":[\"顶点的松弛是基于边的松弛完成的，只需要把某个顶点指出的所有边松弛，那么该顶点就松弛完毕。例如要松弛顶点v，只需要遍历v的邻接表，把每一条边都松弛，那么顶点v就松弛了。\",\"如果把起点设置为顶点0，那么找出起点0到顶点6的最短路径0->2->7>3->6的过程如下:\"]},{\"header\":\"4、 Dijstra算法实现\",\"slug\":\"_4、-dijstra算法实现\",\"contents\":[\"Disjstra算法的实现和Prim算法很类似，构造最短路径树的每一步都是向这棵树中添加一条新的边，而这条新的边是有效横切边pq队列中的权重最小的边。\",\"public class DijkstraSP { /** * 索引代表顶点，值表示从顶点s到当前顶点的最短路径上的最后一条边 */ private DirectedEdge[] edgeTo; /** * 索引代表顶点，值从顶点s到当前顶点的最短路径的总权重 */ private double[] distTo; /** * 存放树中顶点与非树中顶点之间的有效横切边 */ private IndexMinPriorityQueue<Double> pq; /** * 根据一副加权有向图G和顶点s，创建一个计算顶点为s的最短路径树对象 */ public DijkstraSP(EdgeWeightedDigraph G, int s){ //创建一个和图的顶点数一样的数组 this.edgeTo = new DirectedEdge[G.V()]; this.distTo = new double[G.V()]; for (int i = 0; i < distTo.length; i++) { distTo[i] = Double.POSITIVE_INFINITY; } //创建一个和图的顶点数一样大小的索引优先队列，存储有效横切边 this.pq = new IndexMinPriorityQueue<>(G.V()); //默认让顶点s进入树中，但s顶点目前没有与树中其他的顶点相连接，因此初始化distTo[s]=0.0 distTo[s] = 0.0; //使用顶点s和权重0.0初始化pq pq.insert(s, 0.0); //遍历有效边队列 while (!pq.isEmpty()) { //松弛图G中的顶点 relax(G, pq.delMin()); } } /** * ：松弛图G中的顶点v * @param G * @param v */ private void relax(EdgeWeightedDigraph G, int v){ //松弛顶点v就是松弛顶点v邻接表中的每一条边，遍历邻接表 for (DirectedEdge e : G.adj(v)) { //获取边e的终点 int w = e.to(); //起点s到顶点w的权重是否大于起点s到顶点v的权重+边e的权重,如果大于， // 则修改s->w的路径：edgeTo[w]=e,并修改distTo[v] = distTo[v]+e.weitht(),如果不大于，则忽略 if (distTo(w)>distTo(v)+e.weight()){ distTo[w]=distTo[v]+e.weight(); edgeTo[w]=e; //如果顶点w已经存在于优先队列pq中，则重置顶点w的权重 if (pq.contains(w)){ pq.changeItem(w,distTo(w)); }else{ //如果顶点w没有出现在优先队列pq中，则把顶点w及其权重加入到pq中 pq.insert(w,distTo(w)); } } } } /** * :获取从顶点s到顶点v的最短路径的总权重 * @param v * @return */ public double distTo(int v){ return distTo[v]; } /** * :判断从顶点s到顶点v是否可达 * @param v * @return */ public boolean hasPathTo(int v){ return distTo[v]<Double.POSITIVE_INFINITY; } /** * :查询从起点s到顶点v的最短路径中所有的边 * @param v * @return */ public Queue pathTo(int v){ //如果顶点s到v不可达，则返回null if (!hasPathTo(v)){ return null; } //创建队列Queue保存最短路径的边 Queue<DirectedEdge> edges = new Queue<>(); //从顶点v开始，逆向寻找，一直找到顶点s为止，而起点s为最短路劲树的根结点，所以edgeTo[s]=null; DirectedEdge e; while(true){ e = edgeTo[v]; if (e==null){ break; } edges.enqueue(e); v = e.from(); } return edges; } } \",\"测试类\",\"public class DijkstraSPTest { public static void main(String[] args) throws Exception{ BufferedReader br = new BufferedReader(new InputStreamReader(Objects.requireNonNull(DijkstraSPTest.class.getClassLoader().getResourceAsStream(\\\"primTestData.txt\\\")))); //读取顶点数目，初始化EdgeWeightedGraph图 int number = Integer.parseInt(br.readLine()); EdgeWeightedDigraph edgeWeightedGraph = new EdgeWeightedDigraph(number); //读取边的数目 int edgeNumber = Integer.parseInt(br.readLine()); //循环读取每一条边，并调用addEdge方法 for (int i = 0; i < edgeNumber; i++) { String line = br.readLine(); int v = Integer.parseInt(line.split(\\\" \\\")[0]); int w = Integer.parseInt(line.split(\\\" \\\")[1]); double weight = Double.parseDouble(line.split(\\\" \\\")[2]); edgeWeightedGraph.addEdge(new DirectedEdge(v, w, weight)); } //构建PrimMST对象 DijkstraSP mst = new DijkstraSP(edgeWeightedGraph,0); //获取起点0到顶点6的最短路径 Queue<DirectedEdge> edges = mst.pathTo(6); //打印输出 for (DirectedEdge edge : edges) { System.out.println(edge.from() + \\\"->\\\" + edge.to() + \\\"::\\\" + edge.weight()); } } } \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/heap.html\":{\"title\":\"8、堆\",\"contents\":[{\"header\":\"1、堆的定义\",\"slug\":\"_1、堆的定义\",\"contents\":[\"堆是计算机科学中一类特殊的数据结构的统称，堆通常可以被看做是一棵完全二叉树的数组对象。\"]},{\"header\":\"1、堆的特性\",\"slug\":\"_1、堆的特性\",\"contents\":[\"它是完全二叉树，除了树的最后一层结点不需要是满的，其它的每一层从左到右都是满的，如果最后一层结点不是满的，那么要求左满右不满。\",\"它是由数组实现的。就是将二叉树的结点按层级顺序放入数组中，根节点在位置1，它的子节点在2和3，以此类推；\",\"如果一个节点的位置为k，则它的父节点的位置为k/2，而它的两个子节点的位置分别为2k和2k+1\",\"我们可以通过计算数组的索引在树中上下移动，从a[k]向上一层，就令 k = k/2，向下一层就令k = 2k/ k = 2k+1\",\"每个节点都大于等于它的两个子节点\",\"ps：这里要注意堆中仅仅规定了每个结点大于等于它的两个子结点，但这两个子结点的顺序并没有做规定，跟我们之前学习的二叉查找树是有区别的。\"]},{\"header\":\"2、堆的设计\",\"slug\":\"_2、堆的设计\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"类名\",\"Heap<>\",\"构造方法\",\"Heap(int capacity)：创建容量为capacity的Heap对象\",\"成员方法\",\"private boolean less(int i,int j)：判断堆中索引i处的元素是否小于索引j处的元素private void exch(int i,int j):交换堆中i索引和j索引处的值public T delMax():删除堆中最大的元素,并返回这个最大元素public void insert(T t)：往堆中插入一个元素private void swim(int k):使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置private void sink(int k):使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置\",\"成员变量\",\"private T[] items : 用来存储元素的数组private int N：记录堆中元素的个数\"]},{\"header\":\"2、插入方法实现\",\"slug\":\"_2、插入方法实现\",\"contents\":[\"堆是用数组完成数据的存储的，由于数组的底层是一串连续的内存地址，所以我们要往堆中插入数据，我们只能往数组中从索引0处开始，依次往后存放数据，但是堆中对元素的顺序是有要求的，每一个结点的数据要大于等于它的两个子结点的数据，所以每次插入一个元素，都会使得堆中的数据顺序变乱，这个时候我们就需要通过一些方法让刚才插入的这个数据放入到合适的位置。\",\"总结：如果往堆中新插入元素，我们只需要不断的比较新结点a[k]和它的父结点a[k/2]的大小，然后根据结果完成数据元素的交换，就可以完成堆的有序调整。\"]},{\"header\":\"3、删除方法实现\",\"slug\":\"_3、删除方法实现\",\"contents\":[\"由堆的特性我们可以知道，索引1处的元素，也就是根结点就是最大的元素，当我们把根结点的元素删除后，需要有一个新的根结点出现，这时我们可以暂时把堆中最后一个元素放到索引1处，充当根结点，但是它有可能不满足堆的有序性需求，这个时候我们就需要通过一些方法，让这个新的根结点放入到合适的位置。\",\"总结：当删除掉最大元素后，只需要将最后一个元素放到索引1处，并不断的拿着当前结点a[k]与它的子结点a[2k]和a[2k+1]中的较大者交换位置，即可完成堆的有序调整。\"]},{\"header\":\"4、完整代码\",\"slug\":\"_4、完整代码\",\"contents\":[\"public class Heap<T extends Comparable<T>> { /** * 存储元素 */ private T[] items; /** * 堆元素个数 */ private int size; public Heap(int capacity){ items = (T[]) new Comparable[capacity]; size = 0; } /** * 判断堆中索引i的值是否小于索引j处的值 * @param i * @param j * @return */ private boolean less(int i, int j){ return items[i].compareTo(items[j]) < 0; } /** * 交换索引i和索引j的值 * @param i * @param j */ private void each(int i, int j){ T temp = items[i]; items[i] = items[j]; items[j] = temp; } /** * 插入一个元素 * @param t */ public void insert(T t){ items[++size] = t; //上浮，重新排序 swim(size); } /** * 上浮算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ public void swim(int k){ //从叶子节点往上面循环，到根节点就结束循环 while (1 < k){ //比较当前节点与父节点 if(less(k/2, 2)){ //父结点小于当前结点，需要交换 each(k/2,k); } k = k/2; } } /** * 删除堆中最大的元素 * @return max */ public T delMax(){ //获取最大元素 T max = items[1]; //交换索引1处和索引size处的值 each(1,size); //交换后置null items[size] = null; size--; //将索引1的值下沉 sink(1); return max; } /** * 下沉算法，使索引k的元素位于堆中一个正确的位置 * @param k */ private void sink(int k){ //如果到底层就结束循环 2*k > size while(2*k <= size){ int max; //是否存在右子节点 if(2*k + 1 <= size){ // if (less(2*k,2*k+1)){ max = 2*k + 1; }else { max = 2*k; } }else { //不存在右节点 max = 2*k; } //比较当前结点和子结点中的较大者，如果当前结点不小，则结束循环 if(!less(k,max)){ break; } each(k, max); k = max; } } } \",\"测试类\",\"public class HeapTest { public static void main(String[] args){ Heap<String> heap = new Heap<String>(20); heap.insert(\\\"S\\\"); heap.insert(\\\"G\\\"); heap.insert(\\\"I\\\"); heap.insert(\\\"E\\\"); heap.insert(\\\"N\\\"); heap.insert(\\\"H\\\"); heap.insert(\\\"O\\\"); heap.insert(\\\"A\\\"); heap.insert(\\\"T\\\"); heap.insert(\\\"P\\\"); heap.insert(\\\"R\\\"); String del; while ((del= heap.delMax()) != null){ System.out.print(del + \\\" \\\"); } } } \"]},{\"header\":\"3、堆排序\",\"slug\":\"_3、堆排序\",\"contents\":[\"堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。\",\"String[] arr = {\\\"S\\\",\\\"O\\\",\\\"R\\\",\\\"T\\\",\\\"E\\\",\\\"X\\\",\\\"A\\\",\\\"M\\\",\\\"P\\\",\\\"L\\\",\\\"E\\\"}; \",\"实现步骤\",\"构造堆\",\"获取堆顶元素，这就是最大值\",\"交换堆顶元素和数组中的最后一个元素，此时所有元素中的最大元素已经放到合适位置\",\"对堆进行调整，重新让除了最后一个元素的剩余元素中的最大值放到堆顶\",\"重复2-4步骤，直到堆中剩一个元素为止\",\"API设计\",\"类名\",\"HeapSort\",\"成员方法\",\"public static void sort(Comparable[] source)：对source数组中的数据从小到大排序private static void createHeap(Comparable[] source, Comparable[] heap):根据原数组source，构造出堆heapprivate static boolean less(Comparable[] heap, int i, int j)：判断heap堆中索引i处的元素是否小于索引j处的元素private static void exch(Comparable[] heap, int i, int j):交换heap堆中i索引和j索引处的值private static void sink(Comparable[] heap, int target, int range):在heap堆中，对target处的元素做下沉，范围是0~range。\"]},{\"header\":\"1、堆构造过程\",\"slug\":\"_1、堆构造过程\",\"contents\":[\"堆的构造，最直观的想法就是另外再创建一个和新数组数组，然后从左往右遍历原数组，每得到一个元素后，添加 到新数组中，并通过上浮，对堆进行调整，最后新的数组就是一个堆。 上述的方式虽然很直观，也很简单，但是我们可以用更聪明一点的办法完成它。创建一个新数组，把原数组0到length-1的数据拷贝到新数组的1~length处，再从新数组长度的一半处开始往1索引处扫描（从右往左），然后对扫描到的每一个元素做下沉调整即可。\"]},{\"header\":\"2、堆排序过程\",\"slug\":\"_2、堆排序过程\",\"contents\":[\"对构造好的堆，我们只需要做类似于堆的删除操作，就可以完成排序。\",\"将堆顶元素和堆中最后一个元素交换位置；\",\"通过对堆顶元素下沉调整堆，把最大的元素放到堆顶(此时最后一个元素不参与堆的调整，因为最大的数据已经到了数组的最右边)\",\"重复1~2步骤，直到堆中剩最后一个元素。\"]},{\"header\":\"3、完整代码\",\"slug\":\"_3、完整代码\",\"contents\":[\"public class HeapSort { /** * 判断堆中索引i处的值是否小于索引j的值 * @param heap 堆 * @param i 索引i * @param j 索引j * @return true or false */ private static boolean less(Comparable[] heap, int i, int j){ return heap[i].compareTo(heap[j]) < 0; } /** * 交换索引i和索引j处的值 * @param heap * @param i * @param j */ private static void swap(Comparable[] heap, int i, int j){ Comparable temp = heap[i]; heap[i] = heap[j]; heap[j] = temp; } /** * 根据原数组source，构造出堆heap * @param source 原数组 * @param heap 目标数组 */ private static void createHeap(Comparable[] source, Comparable[] heap){ //把source中的数据拷贝到heap中，从heap的1索引处开始填充 System.arraycopy(source,0,heap,1,source.length); //从heap索引的一半处开始倒叙遍历，对得到的每一个元素做下沉操作 for (int i = (heap.length - 1) / 2; i > 0; i--) { sink(heap,i, heap.length - 1); } } /** * 对source数组中从小到大排序 * @param source */ public static void sort(Comparable[] source){ //创建一个比原数组大1的数组 Comparable[] heap = new Comparable[source.length + 1]; //创造堆 createHeap(source,heap); int n = heap.length - 1; while (1 != n){ //交换heap中索引1处的元素和N处的元素 swap(heap, 1, n); n--; //对索引1处的元素在0~N范围内做下沉操作 sink(heap, 1, n); } //heap中的数据已经有序，拷贝到source中 System.arraycopy(heap,1,source,0,source.length); } /** * 在heap堆中，对target处的元素做下沉，范围是0~range。 * @param heap 堆 * @param target 索引 * @param range 范围 */ private static void sink(Comparable[] heap, int target, int range){ //没有子节点 退出循环 while (2 * target <= range){ //找出target结点的两个子结点中的较大值 int max = 2*target; if(2 * target + 1 <= range){ if (less(heap,2*target,2*target+1)){ max = 2 * target + 1; } } //如果当前结点的值小于子结点中的较大值，则交换 if(less(heap,target,max)){ swap(heap, target, max); } //更新target的值 target = max; } } public static void main(String[] args) { String[] arr = {\\\"S\\\", \\\"O\\\", \\\"R\\\", \\\"T\\\", \\\"E\\\", \\\"X\\\", \\\"A\\\", \\\"M\\\", \\\"P\\\", \\\"L\\\", \\\"E\\\"}; HeapSort.sort(arr); System.out.println(Arrays.toString(arr)); } } \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/lineartable.html\":{\"title\":\"5、线性表\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"线性表是最基本、最简单、也是最常用的一种数据结构。一个线性表是n个具有相同特性的数据元素的有限序列。\",\"前驱元素：若A元素在B元素的前面，则称A为B的前驱元素\",\"后继元素：若B元素在A元素的后面，则称B为A的后继元素\",\"线性表的特征：数据元素之间具有一种“一对一”的逻辑关系。\",\"第一个数据元素没有前驱，这个数据元素被称为头结点；\",\"最后一个数据元素没有后继，这个数据元素被称为尾结点；\",\"除了第一个和最后一个数据元素外，其他数据元素有且仅有一个前驱和一个后继。\",\"如果把线性表用数学语言来定义，则可以表示为(a1,...ai-1,ai,ai+1,...an)，ai-1领先于ai,ai领先于ai+1，称ai-1是ai的前驱元素，ai+1是ai的后继元素\",\"线性表的分类：\",\"存储结构 \",\"顺序存储\",\"链式存储\",\"存储方式 \",\"顺序表\",\"链表\",\"常见的线性结构：数组、队列、链表和栈\"]},{\"header\":\"1、顺序表\",\"slug\":\"_1、顺序表\",\"contents\":[\"顺序表是在计算机内存中以数组的形式保存的线性表，线性表的顺序存储是指用一组地址连续的存储单元，依次存储线性表中的各个元素、使得线性表中再逻辑结构上响铃的数据元素存储在相邻的物理存储单元中，即通过数据元素物理存储的相邻关系来反映数据元素之间逻辑上的相邻关系。\"]},{\"header\":\"1、顺序表的实现\",\"slug\":\"_1、顺序表的实现\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"类名\",\"SequenceCase\",\"构造方法\",\"SequenceCase(int capacity)：创建容量为capacity的SequenceList对象\",\"成员方法\",\"public void clear()：空置线性表public boolean isEmpty()：判断线性表是否为空，是返回true，否返回falsepublic int length():获取线性表中元素的个数public T get(int i):读取并返回线性表中的第i个元素的值public void insert(int i,T t)：在线性表的第i个元素之前插入一个值为t的数据元素。public void insert(T t):向线性表中添加一个元素tpublic T remove(int i):删除并返回线性表中第i个数据元素。public int indexOf(T t):返回线性表中首次出现的指定的数据元素的位序号，若不存在，则返回-1。\",\"成员变量\",\"private T[] arr：存储元素的数组private int N:当前线性表的长度\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"public class SequenceCase<T> { /** * 存储线性表 */ public T[] arr; /** * 线性表元素个数 */ public int sequenceLength; public SequenceCase(int capacity) { arr = (T[]) new Object[capacity]; sequenceLength = 0; } /** * 插入元素 * @param name */ public void insert(T name){ if(sequenceLength == arr.length){ throw new RuntimeException(\\\"线性表已满\\\"); } arr[sequenceLength++] = name; } /** * 指定索引插入元素 * @param i * @param name */ public void insert(int i, T name){ if (i == arr.length){ throw new RuntimeException(\\\"当前表已满\\\"); } if (i < 0 || i > sequenceLength){ throw new RuntimeException(\\\"插入的位置不合法\\\"); } for (int j = sequenceLength; j > i; j--) { arr[j] = arr[j-1]; } arr[i] = name; sequenceLength++; } /** * 根据索引删除元素 * @param i */ public T remove(int i){ if (i < 0 || i > sequenceLength-1){ throw new RuntimeException(\\\"当前要删除的元素不存在\\\"); } T t = arr[i]; for (int j = i; j < sequenceLength; j++) { arr[j] = arr[j+1]; } sequenceLength--; return t; } /** * 清空整个线性表 */ public void clear(){ sequenceLength = 0; } /** * 线性表是否为空 * @return */ public boolean isEmpty(){ return 0 == sequenceLength; } /** * 获取线性表的长度 * @return */ public int getSequenceLength(){ return sequenceLength; } /** * 根据索引查询线性表 * @param i * @return */ public T getNameByIndex(int i){ if(0 > i || i > sequenceLength){ throw new RuntimeException(\\\"未存在该元素\\\"); } return arr[i]; } /** * 获取线性表首次出现的序列号 * @param t * @return */ public int indexOf(T t){ for (int i = 0; i < sequenceLength; i++) { if (t.equals(arr[i])){ return i; } } return -1; } } \",\"public class SequenceCaseTest { public static void main(String[] args) { SequenceCase<String> aCase = new SequenceCase<>(10); aCase.insert(\\\"yhx\\\"); aCase.insert(\\\"xiaobear\\\"); aCase.insert(\\\"lwh\\\"); aCase.insert(\\\"xiaohuahua\\\"); aCase.insert(2,\\\"love\\\"); String nameByIndex = aCase.getNameByIndex(2); System.out.println(\\\"索引2的名字为：\\\" + nameByIndex); String remove = aCase.remove(2); System.out.println(\\\"删除索引2的元素名称为：\\\" + remove); //清空操作 aCase.clear(); System.out.println(\\\"线性表长度为：\\\" + aCase.getSequenceLength()); } } \"]},{\"header\":\"2、顺序表的遍历\",\"slug\":\"_2、顺序表的遍历\",\"contents\":[\"在java中，遍历集合的方式一般都是用的是foreach循环，如果想让我们的SequenceList也能支持foreach循环，则需要做如下操作：\",\"让SequenceList实现Iterable接口，重写iterator方法；\",\"在SequenceList内部提供一个内部类SIterator,实现Iterator接口，重写hasNext方法和next方法；\",\"public class SequenceCase<T> implements Iterable<T>{ private class SequenceIterator implements Iterator{ private int temp; //遍历从0开始 public SequenceIterator() { this.temp = 0; } @Override public boolean hasNext() { return temp < sequenceLength; } @Override public Object next() { return arr[temp++]; } } @Override public Iterator<T> iterator() { return new SequenceIterator(); } } \"]},{\"header\":\"3、顺序表的扩容\",\"slug\":\"_3、顺序表的扩容\",\"contents\":[\"在之前的实现中，当我们使用SequenceCase时，先new SequenceCase(5)创建一个对象，创建对象时就需要指定容器的大小，初始化指定大小的数组来存储元素，当我们插入元素时，如果已经插入了5个元素，还要继续插入数据，则会报错，就不能插入了。这种设计不符合容器的设计理念，因此我们在设计顺序表时，应该考虑它的容量的伸缩性。\"]},{\"header\":\"1、添加元素\",\"slug\":\"_1、添加元素\",\"contents\":[\"添加元素时，应该检查当前数组的大小是否能容纳新的元素，如果不能容纳，则需要创建新的容量更大的数组，我们这里创建一个是原数组两倍容量的新数组存储元素。\"]},{\"header\":\"2、删除元素\",\"slug\":\"_2、删除元素\",\"contents\":[\"移除元素时，应该检查当前数组的大小是否太大，比如正在用100个容量的数组存储10个元素，这样就会造成内存空间的浪费，应该创建一个容量更小的数组存储元素。如果我们发现数据元素的数量不足数组容量的1/4，则创建一个是原数组容量的1/2的新数组存储元素。\",\"/** * 重置线性表大小 * @param newSize */ public void resize(int newSize){ T[] temp = arr; arr = (T[]) new Object[newSize]; for (int i = 0; i < temp.length; i++) { arr[i] = temp[i]; } } \",\"添加元素时：\",\"/** * 指定索引插入元素 * @param i * @param name */ public void insert(int i, T name){ if (i == arr.length){ resize(arr.length*2); } if (i < 0 || i > sequenceLength){ throw new RuntimeException(\\\"插入的位置不合法\\\"); } for (int j = sequenceLength; j > i; j--) { arr[j] = arr[j-1]; } arr[i] = name; sequenceLength++; } \",\"删除元素时：\",\"/** * 根据索引删除元素 * @param i */ public T remove(int i){ if (i < 0 || i > sequenceLength-1){ throw new RuntimeException(\\\"当前要删除的元素不存在\\\"); } T t = arr[i]; for (int j = i; j < sequenceLength; j++) { arr[j] = arr[j+1]; } sequenceLength--; //当线性表元素不足数组的1/4时，重置数组元素大小 if(0 < sequenceLength && sequenceLength < arr.length/4){ resize(arr.length/2); } return t; } \"]},{\"header\":\"4、顺序表的时间复杂度\",\"slug\":\"_4、顺序表的时间复杂度\",\"contents\":[\"get(i):不难看出，不论数据元素量N有多大，只需要一次eles[i]就可以获取到对应的元素，所以时间复杂度为O(1);\",\"insert(int i,T t):每一次插入，都需要把i位置后面的元素移动一次，随着元素数量N的增大，移动的元素也越多，时间复杂为O(n);\",\"remove(int i):每一次删除，都需要把i位置后面的元素移动一次，随着数据量N的增大,移动的元素也越多，时间复杂度为O(n);\",\"由于顺序表的底层由数组实现，数组的长度是固定的，所以在操作的过程中涉及到了容器扩容操作。这样会导致顺序表在使用过程中的时间复杂度不是线性的，在某些需要扩容的结点处，耗时会突增，尤其是元素越多，这个问题越明显\"]},{\"header\":\"2、链表\",\"slug\":\"_2、链表\",\"contents\":[\"链表是一种线性数据结构，其中的每个元素实际上是一个单独的对象，而所有对象都通过每个元素中的引用字段链接在一起。\",\"链表是一种物理存储单元上非连续、非顺序的存储结构，其物理结构不能只管的表示数据元素的逻辑顺序，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列的结点（链表中的每一个元素称为结点）组成，结点可以在运行时动态生成。\",\"public class LinkedList<T>{ /** * 存储元素 */ private T item; /** * 指向下一节点 */ private LinkedList next; public LinkedList(T item, LinkedList next) { this.item = item; this.next = next; } } \"]},{\"header\":\"1、单链表\",\"slug\":\"_1、单链表\",\"contents\":[\"单链表中的每个结点不仅包含值，还包含链接到下一个结点的引用字段。通过这种方式，单链表将所有结点按顺序组织起来。\",\"单向链表是链表的一种，它由多个结点组成，每个结点都由一个数据域和一个指针域组成，数据域用来存储数据，指针域用来指向其后继结点。链表的头结点的数据域不存储数据，指针域指向第一个真正存储数据的结点。\"]},{\"header\":\"1、添加元素\",\"slug\":\"_1、添加元素-1\",\"contents\":[\"如果我们想在给定的结点 prev 之后添加新值，我们应该：\",\"使用给定值初始化新结点 cur；\",\"将 cur 的 next 字段链接到 prev 的下一个结点 next ；\",\"将 prev 中的 next 字段链接到 cur 。\",\"与数组不同，我们不需要将所有元素移动到插入元素之后。因此，我们可以在 O(1) 时间复杂度中将新结点插入到链表中，这非常高效。\",\"在开头添加节点\",\"我们使用头结点开始遍历整个链表，头结点对我们来说是非常重要的。\",\"步骤：\",\"初始化一个新节点cur\",\"将新节点链接到我们的原始节点head\",\"将cur指定为head\"]},{\"header\":\"2、删除元素\",\"slug\":\"_2、删除元素-1\",\"contents\":[\"让我们尝试把结点 6从上面的单链表中删除。\",\"从头遍历链表，直到我们找到前一个结点 prev，即结点 23\",\"将 prev（结点 23）与 next（结点 15）链接\",\"结点 6 现在不在我们的单链表中。\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现\",\"contents\":[\"public class SinglyListNode<T> implements Iterable<T> { /** * 记录头结点 */ private LinkedList head; /** * 链表长度 */ private int n; public SinglyListNode() { head = new LinkedList(null,null); n = 0; } /** * 清空链表 * 头结点.next 指向下一节点为空，头结点元素为空，链表长度为空 */ public void clear(){ head.next = null; head.item = null; n = 0; } /** * 链表长度 * @return n */ public int length(){ return n; } /** * 判断链表是否为空 * @return */ public boolean isEmpty(){ return n==0; } /** * 链表增加元素，最后节点 * @param t 元素 */ public void insertNode(T t){ //获取头结点 LinkedList linkedList = head; //寻找最后一个节点 while(null != linkedList.next){ linkedList = linkedList.next; } //初始化插入的节点,下一节点为空 LinkedList linkedList1 = new LinkedList(t, null); //最后节点的next指向下一节点 linkedList.next = linkedList1; //链表长度+1 n++; } /** * 指定位置插入元素 * @param i 插入位置 * @param t 元素 */ public void insertNode(int i,T t){ //获取头结点 LinkedList pre = head; //获取插入元素的前一节点 for (int j = 0; j < i; j++) { pre = pre.next; } //获取下一节点（位于i的节点） LinkedList next = pre.next; //初始化插入节点，且next指向插入元素的下一节点（位于i的节点） LinkedList tLinkedList = new LinkedList(t, next); //插入元素的前一节点指向插入元素 pre.next = tLinkedList; //链表长度+1 n++; } /** * 删除指定位置的节点 * @param i */ public T delete(int i){ //获取头结点 LinkedList pre = head; for (int j = 0; j < i; j++) { pre = pre.next; } //当前i位置的结点 LinkedList next = pre.next; //前一个结点指向下一个结点，删除当前结点 pre.next = next.next; //长度-1 n--; return next.item; } public T getIndexOf(int i){ LinkedList pre = head; for (int j = 0; j < i; j++) { pre = pre.next; } LinkedList next = pre.next; return next.item; } private class LinkedList{ /** * 存储元素 */ T item; /** * 指向下一节点 */ LinkedList next; public LinkedList(T item,LinkedList next) { this.item = item; this.next = next; } } @Override public Iterator<T> iterator() { return new LIterator(); } private class LIterator implements Iterator<T>{ private LinkedList n; public LIterator() { this.n = head; } @Override public boolean hasNext() { return n.next!=null; } @Override public T next() { n = n.next; return (T) n.item; } } } \",\"测试类\",\"public class SinglyListNodeTest{ public static void main(String[] args) { SinglyListNode<String> head = new SinglyListNode<>(); head.insertNode(0,\\\"张胜男\\\"); head.insertNode(1,\\\"李四\\\"); head.insertNode(2,\\\"王五\\\"); head.insertNode(3,\\\"6666\\\"); head.insertNode(4,\\\"7777\\\"); for (String s : head) { System.out.println(s); } head.delete(0); String indexOf = head.getIndexOf(3); System.out.println(indexOf); for (String s : head) { System.out.println(s); } } } \"]},{\"header\":\"2、双链表\",\"slug\":\"_2、双链表\",\"contents\":[\"双向链表也叫双向表，是链表的一种，它由多个结点组成，每个结点都由一个数据域和两个指针域组成，数据域用 来存储数据，其中一个指针域用来指向其后继结点，另一个指针域用来指向前驱结点。链表的头结点的数据域不存 储数据，指向前驱结点的指针域值为null，指向后继结点的指针域指向第一个真正存储数据的结点。\",\"我们可以与单链表相同的方式访问数据：\",\"我们不能在常量级的时间内访问随机位置。\",\"我们必须从头部遍历才能得到我们想要的第一个结点。\",\"在最坏的情况下，时间复杂度将是 O(N)，其中 N 是链表的长度\"]},{\"header\":\"1、添加元素\",\"slug\":\"_1、添加元素-2\",\"contents\":[\"如果我们想在现有的结点 prev 之后插入一个新的结点 cur，我们可以将此过程分为两个步骤：\",\"链接 cur 与 prev 和 next，其中 next 是 prev 原始的下一个节点；\",\"用 cur 重新链接 prev 和 next。\"]},{\"header\":\"2、删除链表\",\"slug\":\"_2、删除链表\",\"contents\":[\"如果我们想从双链表中删除一个现有的结点 cur，我们可以简单地将它的前一个结点 prev 与下一个结点 next 链接起来。\",\"与单链表不同，使用“prev”字段可以很容易地在常量时间内获得前一个结点。\",\"因为我们不再需要遍历链表来获取前一个结点，所以时间和空间复杂度都是O(1)。\",\"我们的目标是从双链表中删除结点 6。\",\"因此，我们将它的前一个结点 23 和下一个结点 15 链接起来：\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现-1\",\"contents\":[\"package com.xiaobear.LinkedList; import java.util.Iterator; /** * @Author xiaobear * @date 2021年07月27日 14:11 * @Description 双向链表 */ public class DoublyLinkedList<T> implements Iterable<T>{ /** * 链表长度 */ private int n; /** * 头结点 */ private Node head; /** * 尾结点 */ private Node last; /** * 初始化链表 */ public DoublyLinkedList() { head = new Node(null,null,null); last = null; n = 0; } /** * 清空链表 */ public void clear(){ head = null; last = null; n = 0; } /** * 链表是否为空 * @return */ public boolean isEmpty(){ return n==0; } /** * 数组长度 * @return */ public int length(){ return n; } /** * 插入元素t * @param t */ public void insert(T t){ if (last==null){ last = new Node(t,head,null); head.next = last; }else{ Node oldLast = last; Node node = new Node(t, oldLast, null); oldLast.next = node; last = node; } //长度+1 n++; } /** * 指定位置插入元素 * @param i * @param t */ public void insert(int i,T t){ if (i < 0 || i >= n){ throw new RuntimeException(\\\"位置不合法\\\"); } Node pre = head; for (int j = 0; j < i; j++) { pre = pre.next; } //插入位置的下一节点 Node nextNode = pre.next; //初始化节点 Node newNode = new Node(t, pre, nextNode); //插入节点的前一节点指向head nextNode.pre = newNode; pre.next = newNode; n++; } /** * 尾部插入元素 * @param t */ public void insertLast(T t){ insert(n,t); } /** * 获取第一个元素 * @return */ public T getFirst(){ if(isEmpty()){ return null; } return head.next.item; } /** * 获取最后一个元素 * @return */ public T getLast(){ if(isEmpty()){ return null; } return last.item; } /** * 获取第i个元素的值 * @param i * @return */ public T getItem(int i){ if (isEmpty()){ return null; } Node firstNode = head.next; for (int j = 0; j < i; j++) { firstNode = firstNode.next; } return firstNode.item; } /** * 删除第I个元素 * @param i * @return */ public T remove(int i){ Node pre = head; for (int j = 0; j < i; j++) { pre = pre.next; } //获取当前元素 Node cur = pre.next; Node curNext = cur.next; pre.next = curNext; curNext.pre = pre; n--; return cur.item; } @Override public Iterator<T> iterator() { return new DIterator(); } private class DIterator implements Iterator{ private Node n = head; @Override public boolean hasNext() { return n.next != null; } @Override public Object next() { n = n.next; return n.item; } } /** * 节点类 */ private class Node{ //存储元素 public T item; //前一节点 public Node pre; //后一个节点 public Node next; public Node(T item, Node pre, Node next) { this.item = item; this.pre = pre; this.next = next; } } } \",\"public class ListNode { int val; ListNode next; ListNode prev; ListNode(int x) { val = x; } } class MyLinkedList { int size; // sentinel nodes as pseudo-head and pseudo-tail ListNode head, tail; public MyLinkedList() { size = 0; head = new ListNode(0); tail = new ListNode(0); head.next = tail; tail.prev = head; } /** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */ public int get(int index) { // if index is invalid if (index < 0 || index >= size) return -1; // choose the fastest way: to move from the head // or to move from the tail ListNode curr = head; if (index + 1 < size - index) for(int i = 0; i < index + 1; ++i) curr = curr.next; else { curr = tail; for(int i = 0; i < size - index; ++i) curr = curr.prev; } return curr.val; } /** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */ public void addAtHead(int val) { ListNode pred = head, succ = head.next; ++size; ListNode toAdd = new ListNode(val); toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } /** Append a node of value val to the last element of the linked list. */ public void addAtTail(int val) { ListNode succ = tail, pred = tail.prev; ++size; ListNode toAdd = new ListNode(val); toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } /** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */ public void addAtIndex(int index, int val) { // If index is greater than the length, // the node will not be inserted. if (index > size) return; // [so weird] If index is negative, // the node will be inserted at the head of the list. if (index < 0) index = 0; // find predecessor and successor of the node to be added ListNode pred, succ; if (index < size - index) { pred = head; for(int i = 0; i < index; ++i) pred = pred.next; succ = pred.next; } else { succ = tail; for (int i = 0; i < size - index; ++i) succ = succ.prev; pred = succ.prev; } // insertion itself ++size; ListNode toAdd = new ListNode(val); toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } /** Delete the index-th node in the linked list, if the index is valid. */ public void deleteAtIndex(int index) { // if the index is invalid, do nothing if (index < 0 || index >= size) return; // find predecessor and successor of the node to be deleted ListNode pred, succ; if (index < size - index) { pred = head; for(int i = 0; i < index; ++i) pred = pred.next; succ = pred.next.next; } else { succ = tail; for (int i = 0; i < size - index - 1; ++i) succ = succ.prev; pred = succ.prev.prev; } // delete pred.next --size; pred.next = succ; succ.prev = pred; } } \"]},{\"header\":\"3、链表反转\",\"slug\":\"_3、链表反转\",\"contents\":[\"定义一个函数， 输入一个链表的头节点， 反转该链表并输出反转后链表的头节点。\",\"原链表中数据为：1->2->3>4\",\"反转后链表中数据为：4->3->2->1\"]},{\"header\":\"1、递归\",\"slug\":\"_1、递归\",\"contents\":[\"回顾递归的模板\",\"public ListNode reverseList(ListNode head){ if(终止条件){ return; } //逻辑处理 //递归调用 ListNode temp = reverseList(参数); //逻辑处理 } \",\"终止条件：链表为空或者链表没有尾节点的时候\",\"if(head == null || head.next == null){ return head; } \",\"怎样递归调用：从当前节点的下一节点开始递归\",\"逻辑处理：把当前节点挂在递归之后的链表的末尾\",\" /** * 链表反转 */ public void reverse(){ if(null == head){ return; } //从下一节点开始 reverseList(head.next); } /** * 链表反转 * @param curr * @return */ public LinkedList reverseList(LinkedList curr){ //如果链表的下一节点为空 if(null == curr.next){ head.next = curr; return curr; } //当前节点的上一个节点 LinkedList pre = reverseList(curr.next); pre.next = curr; //当前节点的下一节点指向null curr.next = null; //返回当前节点 return curr; } \",\"第二种：\",\"public ListNode reverseList(ListNode head){ if(null == head || null ==head.next){ return head; } //保存当前节点的下一节点 ListNode temp = head.next; //开始递归调用,reverse就是反转之后的链表，不包括头结点 ListNode reverse = reverseList(temp); //将头结点挂在当前节点的后面 temp.next = head; //反转之后头节点变成尾节点了，下一节点为空 head.next = null; return reverse; } \",\"因为递归调用之后h e a d . n e x t 节点就会成为r e v e r s e 节点的尾结点， 我们可以直接让h e a d . n e x t . n e x t = h e a d ; ， 这样代码会更简洁一些。\",\"public ListNode reverse(ListNode head){ if(null == head || null ==head.next){ return head; } ListNode reverse = reverse(head.next); head.next.next = head; head.next = null; return reverse; } \",\"这种递归往下传递的时候基本上没有逻辑处理， 当往回反弹的时候才开始处理， 也就是从链表的尾端往前开始处理的。我们还可以再来改一下， 在链表递归的时候从前往后处理， 处理完之后直接返回递归的结果， 这就是所谓的尾递归， 这种运行效率要比上一种好很多。\",\"public ListNode reverseList(ListNode head){ return reverseListEnd(head, null); } public ListNode reverseListEnd(ListNode head, ListNode newNode){ if(head == null){ return newNode; } //当前节点的下一节点 ListNode next = head.next; head.next = newNode; ListNode node = reverseListEnd(next,head); return node; } \"]},{\"header\":\"2、双链表\",\"slug\":\"_2、双链表-1\",\"contents\":[\"双链表求解是把原链表的结点一个个摘掉， 每次摘掉的链表都让它成为新的链表的头结点， 然后更新新链表。\",\"public ListNode reverseList(ListNode head){ //初始化新链表 ListNode result = null; while(head != null){ //保存当前节点的下一节点 ListNode temp = head.next; //每次访问的原链表节点都会成为新链表的头结点 head.next = result; //更新新链表 result = head; //重新赋值 head = temp; } return result; } \"]},{\"header\":\"3、栈\",\"slug\":\"_3、栈\",\"contents\":[\"栈的原理：先进后出\",\"实现原理就是把链表节点一个个入栈， 当全部入栈完之后再一个个出栈， 出栈的时候在把出栈的结点串成一个新的链表。\",\"public LinkedList reverseListByStack(ListNode head){ Stack<LinkedList> stack = new Stack<>(); while (head != null){ //链表入栈 stack.push(head); head = head.next; } if(stack.isEmpty()){ return null; } //栈中元素出栈 LinkedList pop = stack.pop(); LinkedList result = pop; while (!stack.isEmpty()){ LinkedList pop1 = stack.pop(); pop.next = pop1; pop = pop.next; } //最后一个结点就是反转前的头结点，一定要让他的next等于空，否则会构成环 pop.next =null; return result; } \"]},{\"header\":\"4、快慢指针\",\"slug\":\"_4、快慢指针\",\"contents\":[\"快慢指针指的是定义两个指针，这两个指针的移动速度一块一慢，以此来制造出自己想要的差值，这个差值可以然我们找到链表上相应的结点。一般情况下，快指针的移动步长为慢指针的两倍\"]},{\"header\":\"1、中间值问题\",\"slug\":\"_1、中间值问题\",\"contents\":[\"找到链表的中间元素并返回\",\"利用快慢指针，我们把一个链表看成一个跑道，假设a的速度是b的两倍，那么当a跑完全程后，b刚好跑一半，以此来达到找到中间节点的目的。\",\"public class FastAndSlowPointer { public static void main(String[] args) { Node<String> aa = new Node<>(\\\"aa\\\", null); Node<String> bb = new Node<>(\\\"bb\\\", null); Node<String> cc = new Node<>(\\\"cc\\\", null); Node<String> dd = new Node<>(\\\"dd\\\", null); Node<String> ee = new Node<>(\\\"ee\\\", null); Node<String> ff = new Node<>(\\\"ff\\\", null); Node<String> gg = new Node<>(\\\"gg\\\", null); aa.next = bb; bb.next = cc; cc.next = dd; dd.next = ee; ee.next = ff; ff.next = gg; System.out.println(\\\"链表的中间元素为：\\\"+getMiddle(aa)); } /** * 寻找链表的中间元素 * @param first 链表的头结点 * @return 链表中间节点的值 */ public static String getMiddle(Node<String> first){ //定义两个指针分别等于first Node<String> fast = first; Node<String> slow = first; while(null != fast && null != fast.next){ fast = fast.next.next; slow = slow.next; } return slow.item; } public static class Node<T>{ T item; Node next; public Node(T item, Node next) { this.item = item; this.next = next; } } } \",\"方法代码：\",\"/** * 寻找链表的中间元素 * @param first 链表的头结点 * @return 链表中间节点的值 */ public String getMiddle(Node<String> first){ //定义两个指针分别等于first Node<String> fast = first; Node<String> slow = first; while(null != fast && null != fast.next){ fast = fast.next.next; slow = slow.next; } return slow.item; } \"]},{\"header\":\"2、单向链表是否有环\",\"slug\":\"_2、单向链表是否有环\",\"contents\":[\"使用快慢指针的思想，还是把链表比作一条跑道，链表中有环，那么这条跑道就是一条圆环跑道，在一条圆环跑道中，两个人有速度差，那么迟早两个人会相遇，只要相遇那么就说明有环。\",\"使用哈希表，将链表的值存入Set中，如果有存在相同元素的，就说明有环\",\"快慢指针\",\"/** * 判断链表是否有环--快慢指针 * @param first 链表的头结点 * @return */ public static boolean isCircle(Node<String> first){ //定义两个指针分别等于first Node<String> fast = first; Node<String> slow = first; while(null != first && null != first.next){ fast = fast.next.next; slow = slow.next; if(fast.equals(slow)){ return true; } } return false; } \",\"哈希表\",\"/** * * @param first * @return */ public static boolean isCircleBySet(Node<String> first){ Set<String> strings = new HashSet<>(); while(null !=first){ //将值存入set集合中 if(!strings.add(first.item)){ return true; } first = first.next; } return false; } \"]},{\"header\":\"3、有环链表入口问题\",\"slug\":\"_3、有环链表入口问题\",\"contents\":[\"当快慢指针相遇时，我们可以判断到链表中有环，这时重新设定一个新指针指向链表的起点，且步长与慢指针一样为1，则慢指针与“新”指针相遇的地方就是环的入口\",\"/** * 寻找有环链表的入口 * @param first * @return */ public String getEntrance(Node<String> first){ //定义两个指针分别等于first Node<String> fast = first; Node<String> slow = first; //新节点 Node<String> temp = null; while(null != first && null != first.next){ fast = fast.next.next; slow = slow.next; //快慢指针相遇，初始化新节点，接着遍历 if(fast.equals(slow)){ temp = first; continue; } if(null != temp){ temp = temp.next; if(temp.equals(slow)){ return temp.item; } } } return null; } \"]},{\"header\":\"5、循环链表\",\"slug\":\"_5、循环链表\",\"contents\":[\"循环链表，顾名思义，链表整体要形成一个圆环状。在单向链表中，最后一个节点的指针为null，不指向任何结点，因为没有下一个元素了。要实现循环链表，我们只需要让单向链表的最后一个节点的指针指向头结点即可。\"]},{\"header\":\"6、约瑟夫问题\",\"slug\":\"_6、约瑟夫问题\",\"contents\":[\"传说有这样一个故事，在罗马人占领乔塔帕特后，39 个犹太人与约瑟夫及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人抓到，于是决定了一个自杀方式，41个人排成一个圆圈，第一个人从1开始报数，依次往后，如果有人报数到3，那么这个人就必须自杀，然后再由他的下一个人重新从1开始报数，直到所有人都自杀身亡为止。然而约瑟夫和他的朋友并不想遵从。于是，约瑟夫要他的朋友先假装遵从，他将朋友与自己安排在第16个与第31个位置，从而逃过了这场死亡游戏 。\",\"问题转换：\",\"41个人坐一圈，第一个人编号为1，第二个人编号为2，第n个人编号为n。 1.编号为1的人开始从1报数，依次向后，报数为3的那个人退出圈； 2.自退出那个人开始的下一个人再次从1开始报数，以此类推； 3.求出最后退出的那个人的编号。\",\"解题思路：\",\"构建含有41个结点的单向循环链表，分别存储1~41的值，分别代表这41个人；\",\"使用计数器count，记录当前报数的值；\",\"遍历链表，每循环一次，count++；\",\"判断count的值，如果是3，则从链表中删除这个结点并打印结点的值，把count重置为0；\",\"public class JosephQuestion { public static void main(String[] args) { //先构建循环链表 Node<Integer> first = null; //记录当前节点的值 Node<Integer> pre = null; for (int i = 1; i <= 41; i++) { //如果是第一个节点 if(1 == i){ first = new Node<>(i,null); pre = first; continue; } Node node = new Node(i, null); pre.next = node; //重置pre pre = node; //构建循环链表，让最后一个结点指向第一个结点 if (41 == i){ pre.next = first; } } //记录当前的报数值 int count = 0; Node<Integer> head = first; //记录当前节点 Node<Integer> temp = null; while (head != head.next){ count++; if(3 == count){ //判断count的值，如果是3，则从链表中删除这个结点并打印结点的值，把count重置为0； temp.next = head.next; System.out.print(head.item+\\\",\\\"); count = 0; }else { temp = head; } head = head.next; } System.out.println(head.item); } public static class Node<T>{ T item; Node next; public Node(T item, Node next) { this.item = item; this.next = next; } } } \"]},{\"header\":\"3、栈\",\"slug\":\"_3、栈-1\",\"contents\":[\"栈是一种基于先进后出(FILO)的数据结构，是一种只能在一端进行插入和删除操作的特殊线性表。它按照先进后出的原则存储数据，先进入的数据被压入栈底，最后的数据在栈顶，需要读数据的时候从栈顶开始弹出数据（最后一个数据被第一个读出来）。\",\"我们称数据进入到栈的动作为压栈，数据从栈中出去的动作为弹栈。\"]},{\"header\":\"1、栈的API设计\",\"slug\":\"_1、栈的api设计\",\"contents\":[\"类名\",\"Stack\",\"构造方法\",\"Stack：创建Stack对象\",\"成员方法\",\"1.public boolean isEmpty()：判断栈是否为空，是返回true，否返回false2.public int size():获取栈中元素的个数3.public T pop():弹出栈顶元素4.public void push(T t)：向栈中压入元素t\",\"成员变量\",\"private Node head:记录首结点private int N:当前栈的元素个数\",\"public class Stack<T> implements Iterable<T>{ /** * 栈长度 */ private int size; /** * 头结点 */ private Node head; public Stack() { head = new Node(null,null); size = 0; } public boolean isEmpty(){ return size == 0; } public int size(){ return size; } /** * 数据入栈 * @param t */ public void push(T t){ Node next = head.next; //新节点 Node newNode = new Node(t, next); //重新赋值 head.next = newNode; size++; } /** * 数据出栈 * @return */ public T pop(){ Node next = head.next; if(null == next){ return null; } head.next = head.next.next; size--; return next.item; } @Override public Iterator<T> iterator() { return new SIterator(); } private class SIterator implements Iterator<T>{ private Node n = head; @Override public boolean hasNext() { return n.next!=null; } @Override public T next() { Node node = n.next; n = n.next; return node.item; } } private class Node{ public T item; public Node next; public Node(T item, Node next) { this.item = item; this.next = next; } } } \"]},{\"header\":\"2、括号匹配问题\",\"slug\":\"_2、括号匹配问题\",\"contents\":[\"给定一个字符串，里边可能包含\\\"()\\\"小括号和其他字符，请编写程序检查该字符串的中的小括号是否成对出现。 例如： \\\"(上海)(长安)\\\"：正确匹配 \\\"上海((长安))\\\"：正确匹配 \\\"上海(长安(北京)(深圳)南京)\\\":正确匹配 \\\"上海(长安))\\\"：错误匹配 \\\"((上海)长安\\\"：错误匹配\",\"步骤：\",\"创建一个栈用来存储左括号\",\"从左往右遍历字符串，拿到每一个字符\",\"判断该字符是不是左括号，如果是，放入栈中存储\",\"判断该字符是不是右括号，如果不是，继续下一次循环\",\"如果该字符是右括号，则从栈中弹出一个元素t；\",\"判断元素t是否为null，如果不是，则证明有对应的左括号，如果不是，则证明没有对应的左括号\",\"循环结束后，判断栈中还有没有剩余的左括号，如果有，则不匹配，如果没有，则匹配\",\"/** * 左括号入栈，右括号出栈 * @param str * @return */ public boolean isMatch(String str){ //创建一个栈存储左括号 Stack<String> stack = new Stack<>(); for (int i = 0; i < str.length(); i++) { if(s.equals(\\\"(\\\")){ stack.push(s); }else if(s.equals(\\\")\\\")){ String pop = stack.pop(); if (pop == null) { return false; } } } //栈中元素是否为空 if(stack.size == 0){ return true; } return false; } \"]},{\"header\":\"3、逆波兰表达式\",\"slug\":\"_3、逆波兰表达式\",\"contents\":[\"中缀表达式就是我们平常生活中使用的表达式，例如：1+32,2-(1+3)等等*\",\"中缀表达式的特点是：二元运算符总是置于两个操作数中间\",\"逆波兰表达式(后缀表达式)：\",\"逆波兰表达式是波兰逻辑学家J・卢卡西维兹(J・ Lukasewicz)于19 29年首先提出的一种表达式的表示方法，后缀表达式的特点：运算符总是放在跟它相关的操作数之后。\",\"给定一个只包含加减乘除四种运算的逆波兰表达式的数组表示方式，求出该逆波兰表达式的结果\",\"/** * 求逆波兰表达式 * @param number * @return */ public int caculate(String[] number){ Stack<Integer> stack = new Stack<>(); for (int i = 0; i < number.length; i++) { String s = number[i]; Integer pop; Integer pop1; Integer temp; if(\\\"+\\\".equals(s)){ pop = stack.pop(); pop1 = stack.pop(); temp = pop + pop1; stack.push(temp); }else if(\\\"-\\\".equals(s)){ pop = stack.pop(); pop1 = stack.pop(); temp = pop1 - pop; stack.push(temp); }else if(\\\"*\\\".equals(s)){ pop = stack.pop(); pop1 = stack.pop(); temp = pop1 * pop; stack.push(temp); }else if(\\\"/\\\".equals(s)){ pop = stack.pop(); pop1 = stack.pop(); temp = pop1 / pop; stack.push(temp); }else { stack.push(Integer.parseInt(s)); } } return stack.pop(); } \"]},{\"header\":\"4、队列\",\"slug\":\"_4、队列\",\"contents\":[\"队列是一种基于先进先出(FIFO)的数据结构，是一种只能在一端进行插入,在另一端进行删除操作的特殊线性表，它按照先进先出的原则存储数据，先进入的数据，在读取数据时先读被读出来。\"]},{\"header\":\"1、队列的API设计\",\"slug\":\"_1、队列的api设计\",\"contents\":[\"类名\",\"Queue\",\"构造方法\",\"Queue()：创建Queue对象\",\"成员方法\",\"1.public boolean isEmpty()：判断队列是否为空，是返回true，否返回false2.public int size():获取队列中元素的个数3.public T dequeue():从队列中拿出一个元素4.public void enqueue(T t)：往队列中插入一个元素\",\"成员变量\",\"1.private Node head:记录首结点2.private int N:当前栈的元素个数3.private Node last:记录最后一个结点\",\"public class Queue<T> implements Iterable<T>{ private int size; private Node head; private Node last; public Queue() { size = 0; head = new Node(null,null); last = null; } public boolean isEmpty(){ return size == 0; } public int size(){ return size; } /** * 插入元素到队列 * @param t */ public void enqueue(T t){ //如果链尾都为空 if(null == last){ last = new Node(t,null); head.next = last; }else { //如果链尾不为空 Node pre = last; last = new Node(t, null); pre.next = last; } size++; } /** * 取出元素 * @return */ public T dequeue(){ if (isEmpty()){ return null; } Node oldHead = head.next; head.next = oldHead.next; size--; if (isEmpty()){ last = null; } return oldHead.item; } /** * 节点类 */ private class Node{ public T item; public Node next; public Node(T item, Node next) { this.item = item; this.next = next; } } @Override public Iterator<T> iterator() { return new QIterator(); } public class QIterator implements Iterator<T>{ private Node n = head; @Override public boolean hasNext() { return n.next != null; } @Override public T next() { Node node = n.next; n = n.next; return node.item; } } } \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/overview.html\":{\"title\":\"1、概述\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[]},{\"header\":\"1、什么是数据结构\",\"slug\":\"_1、什么是数据结构\",\"contents\":[\"数据结构是一门研究非数值计算的程序设计问题中的操作对象，以及他们之间的关系和操作等相关问题的学科。\",\"简单的说：数据结构就是把数据元素按照一定的关系组织起来的集合，用来组织和存储数据\"]},{\"header\":\"2、数据结构分类\",\"slug\":\"_2、数据结构分类\",\"contents\":[\"传统上，我们可以把数据结构分为逻辑结构和物理结构两大类。\"]},{\"header\":\"1、逻辑结构\",\"slug\":\"_1、逻辑结构\",\"contents\":[\"逻辑结构是从具体问题中抽象出来的模型，是抽象意义上的结构，按照对象中数据元素之间的相互关系分类。\",\"集合结构：集合结构中数据元素除了属于同一个集合外，他们之间没有任何其他的关系。\",\"线性结构：线性结构中的数据元素之间存在一对一的关系\",\"树形结构：树形结构中的数据元素之间存在一对多的层次关系\",\"图形结构：图形结构的数据元素是多对多的关系\"]},{\"header\":\"2、物理结构\",\"slug\":\"_2、物理结构\",\"contents\":[\"逻辑结构在计算机中真正的表示方式（又称为映像）称为物理结构，也可以叫做存储结构。常见的物理结构有顺序存储结构、链式存储结构。\",\"顺序存储结构：把数据元素放到地址连续的存储单元里面，其数据间的逻辑关系和物理关系是一致的 ，比如我们常用的数组就是顺序存储结构。\",\"顺序存储结构存在一定的弊端，就像生活中排时也会有人插队也可能有人有特殊情况突然离开，这时候整个结构都处于变化中，此时就需要链式存储结构。\",\"链式存储结构：是把数据元素存放在任意的存储单元里面，这组存储单元可以是连续的也可以是不连续的。此时，数据元素之间并不能反映元素间的逻辑关系，因此在链式存储结构中引进了一个指针存放数据元素的地址，这样通过地址就可以找到相关联数据元素的位置\"]},{\"header\":\"3、什么是算法\",\"slug\":\"_3、什么是算法\",\"contents\":[\"算法是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法解决问题的策略 机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。\",\"简单的说：根据一定的条件，对一些数据进行计算，得到需要的结果。\"]},{\"header\":\"4、算法初体验\",\"slug\":\"_4、算法初体验\",\"contents\":[]},{\"header\":\"1、案例1\",\"slug\":\"_1、案例1\",\"contents\":[\"计算1到100的和\",\"解法1：\",\"/** * 解法1：利用for循环相加 * @param n * @return */ public int getSum(int n){ int sum = 0; for (int i = 1; i <= n; i++) { sum += i; } return sum; } \",\"解法2：\",\"/** * 利用高斯定理 * @param n * @return */ public int getSum2(int n){ int sum = (n + 1)*n/2; return sum; } \",\"第一种解法要完成需求，要完成以下几个动作： 1.定义两个整型变量； 2.执行100次加法运算； 3.打印结果到控制台；\",\"第二种解法要完成需求，要完成以下几个动作： 1.定义两个整型变量； 2.执行1次加法运算，1次乘法运算，一次除法运算，总共3次运算； 3.打印结果到控制台；\",\"很明显，第二种算法完成需求，花费的时间更少一些。\"]},{\"header\":\"2、案例2\",\"slug\":\"_2、案例2\",\"contents\":[\"计算10的阶乘\",\"解法1：\",\"/** * 利用递归计算 * @param n * @return */ public int getFactorial(int n){ if(1 == n){ return 1; } return n * getFactorial(n - 1); } \",\"解法2：\",\" public int getFactorial2(int n){ int sum = 1; for (int i = 1; i <= n; i++) { sum *= i; } return sum; } \",\"第一种解法，使用递归完成需求，fun1方法会执行10次，并且第一次执行未完毕，调用第二次执行，第二次执行未完毕，调用第三次执行...最终，最多的时候，需要在栈内存同时开辟10块内存分别执行10个fun1方法。\",\"第二种解法，使用for循环完成需求，fun2方法只会执行一次，最终，只需要在栈内存开辟一块内存执行fun2方法 即可。\",\"很明显，第二种算法完成需求，占用的内存空间更小。\"]}]},\"/study-tutorial/basic/dataAndAlgorithm/priorityqueue.html\":{\"title\":\"9、优先队列\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"普通的队列是一种先进先出的数据结构，元素在队列尾追加，而从队列头删除。在某些情况下，我们可能需要找出队列中的最大值或者最小值，例如使用一个队列保存计算机的任务，一般情况下计算机的任务都是有优先级的，我们需要在这些计算机的任务中找出优先级最高的任务先执行，执行完毕后就需要把这个任务从队列中移除。普通的队列要完成这样的功能，需要每次遍历队列中的所有元素，比较并找出最大值，效率不是很高，这个时候，我们就可以使用一种特殊的队列来完成这种需求，优先队列。\",\"优先队列按照其作用不同，可以分为以下两种：\",\"最大优先队列：可以获取并删除队列中最大的值\",\"最小优先队列：可以获取并删除队列中最小的值\"]},{\"header\":\"1、最大优先队列\",\"slug\":\"_1、最大优先队列\",\"contents\":[]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计\",\"contents\":[\"类名\",\"MaxPriorityQueue\",\"构造方法\",\"MaxPriorityQueue(int capacity)：创建容量为capacity的MaxPriorityQueue对象\",\"成员方法\",\"private boolean less(int i,int j)：判断堆中索引i处的元素是否小于索引j处的元素private void each(int i,int j):交换堆中i索引和j索引处的值public T delMax():删除队列中最大的元素,并返回这个最大元素public void insert(T t)：往队列中插入一个元素private void swim(int k):使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置private void sink(int k):使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置public int size():获取队列中元素的个数public boolean isEmpty():判断队列是否为空\",\"成员变量\",\"private T items：用来存储元素的数组private int N：记录堆中元素的个数\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"public class MaxPriorityQueue<T extends Comparable<T>> { private T[] items; private int size; public MaxPriorityQueue(int capacity){ items = (T[]) new Comparable[capacity + 1]; size = 0; } /** * 判断堆中索引i处的元素是否小于索引j处的元素 * @param i * @param j * @return true */ private boolean less(int i,int j){ return items[i].compareTo(items[j]) < 0; } /** * 交换堆中i索引和j索引处的值 * @param i * @param j */ private void each(int i,int j){ T temp = items[i]; items[i] = items[j]; items[j] = temp; } /** * 删除队列中最大的元素,并返回这个最大元素 * @return */ public T delMax(){ T max = items[1]; each(1,size); items[size] = null; size--; sink(1); return max; } /** * 往队列中插入一个元素 * @param t */ public void insert(T t){ items[++size] = t; swim(size); } /** * 使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ private void swim(int k){ while (1 < k){ //比较k是否小于k/2，如果小于则交换元素 if(less(k/2,k)){ each(k/2,k); } k = k/2; } } /** * 使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ private void sink(int k){ while (2 * k <= size){ int max = 2 * k; //如果存在右子结点 if(2 * k + 1 <= size){ if(less(2*k,2*k+1)){ max = 2 * k + 1; } } //比较当前结点和子结点中的较大者，如果当前结点不小，则结束循环 if(!less(k,max)){ break; } each(k,max); k = max; } } /** * 获取队列中元素的个数 * @return */ public int size(){ return size; } /** * 判断队列是否为空 * @return */ public boolean isEmpty(){ return size == 0; } } \",\"测试类\",\"public class MaxPriorityQueueTest { public static void main(String[] args) { String[] arr = {\\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\", \\\"E\\\", \\\"F\\\", \\\"G\\\"}; MaxPriorityQueue<String> queue = new MaxPriorityQueue(10); for (String s : arr) { queue.insert(s); } System.out.println(queue.size()); String max; while (!queue.isEmpty()){ max = queue.delMax(); System.out.print(max+ \\\" \\\"); } } } \",\"输出结果\",\"7 G F E D C B A \"]},{\"header\":\"2、最小优先队列\",\"slug\":\"_2、最小优先队列\",\"contents\":[\"最小的元素放在数组的索引1处。\",\"每个结点的数据总是小于等于它的两个子结点的数据。\"]},{\"header\":\"1、API设计\",\"slug\":\"_1、api设计-1\",\"contents\":[\"类名\",\"MinPriorityQueue\",\"构造方法\",\"MinPriorityQueue(int capacity)：创建容量为capacity的MinPriorityQueue对象\",\"成员方法\",\"private boolean less(int i,int j)：判断堆中索引i处的元素是否小于索引j处的元素private void exch(int i,int j):交换堆中i索引和j索引处的值public T delMin():删除队列中最小的元素,并返回这个最小元素public void insert(T t)：往队列中插入一个元素private void swim(int k):使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置private void sink(int k):使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置public int size():获取队列中元素的个数public boolean isEmpty():判断队列是否为空\",\"成员变量\",\"private T[] imtes : 用来存储元素的数组private int N：记录堆中元素的个数\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-1\",\"contents\":[\"public class MinPriorityQueue<T extends Comparable<T>> { private T[] items; private int size; public MinPriorityQueue(int capacity){ items = (T[]) new Comparable[capacity+1]; size = 0; } /** * 判断堆中索引i处的元素是否小于索引j处的元素 * @param i * @param j * @return */ private boolean less(int i,int j){ return items[i].compareTo(items[j]) < 0; } /** * 交换堆中i索引和j索引处的值 * @param i * @param j */ private void each(int i,int j){ T temp = items[i]; items[i] = items[j]; items[j] = temp; } /** * 删除队列中最小的元素,并返回这个最小元素 * @return */ public T delMin(){ T min = items[1]; each(1,size); items[size] = null; size--; sink(1); return min; } /** * 往队列中插入一个元素<br/> * @param t */ public void insert(T t){ items[++size] = t; swim(size); } /** * :使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置<br/> * @param k */ private void swim(int k){ while (1 < k){ if(less(k,k/2)){ each(k,k/2); } k = k/2; } } /** * 使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ private void sink(int k){ while (2*k <= size){ int min = 2*k; //如果存在右子结点 if(2*k + 1 <= size){ if(less(2*k + 1,2*k)){ min = 2*k + 1; } } if(less(k,min)){ break; } each(min,k); k = min; } } /** * 获取队列中元素的个数 * @return */ public int size(){ return size; } /** * 判断队列是否为空 * @return */ public boolean isEmpty(){ return size == 0; } } \",\"测试类\",\"public class MinPriorityQueueTest { public static void main(String[] args) { String[] arr = {\\\"G\\\", \\\"F\\\", \\\"E\\\", \\\"D\\\", \\\"C\\\", \\\"B\\\", \\\"A\\\"}; MinPriorityQueue<String> queue = new MinPriorityQueue(10); for (String s : arr) { queue.insert(s); } System.out.println(queue.size()); String del; while (!queue.isEmpty()){ del = queue.delMin(); System.out.print(del+\\\" \\\"); } } } \",\"输出结果\",\"7 A B C D E F G \"]},{\"header\":\"3、索引优先队列\",\"slug\":\"_3、索引优先队列\",\"contents\":[]},{\"header\":\"1、实现思路\",\"slug\":\"_1、实现思路\",\"contents\":[\"存储数据时，给每一个数据元素关联一个整数，例如insert(int k,T t),我们可以看做k是t关联的整数，那么我们的实现需要通过k这个值，快速获取到队列中t这个元素，此时有个k这个值需要具有唯一性。\",\"最直观的想法就是我们可以用一个T[] items数组来保存数据元素，在insert(int k,T t)完成插入时，可以把k看做是items数组的索引，把t元素放到items数组的索引k处，这样我们再根据k获取元素t时就很方便了，直接就可以拿到items[k]即可。\",\"第一步完成后的结果，虽然我们给每个元素关联了一个整数，并且可以使用这个整数快速的获取到该元素，但是，items数组中的元素顺序是随机的，并不是堆有序的，所以，为了完成这个需求，我们可以增加一个数组int[]pq,来保存每个元素在items数组中的索引，pq数组需要堆有序，也就是说，pq[1]对应的数据元素items[pq[1]]要小于等于pq[2]和pq[3]对应的数据元素items[pq[2]]和items[pq[3]]。\",\"通过第二步的分析，我们可以发现，其实我们通过上浮和下沉做堆调整的时候，其实调整的是pq数组。如果需要对items中的元素进行修改，比如让items[0]=“H”,那么很显然，我们需要对pq中的数据做堆调整，而且是调整pq[9]中元素的位置。但现在就会遇到一个问题，我们修改的是items数组中0索引处的值，如何才能快速的知道需要挑中pq[9]中元素的位置呢？\",\"最直观的想法就是遍历pq数组，拿出每一个元素和0做比较，如果当前元素是0，那么调整该索引处的元素即可，但是效率很低。 我们可以另外增加一个数组，int[] qp,用来存储pq的逆序。例如： 在pq数组中：pq[1]=6; 那么在qp数组中，把6作为索引，1作为值，结果是：qp[6]=1;\",\"当有了pq数组后，如果我们修改items[0]=\\\"H\\\"，那么就可以先通过索引0，在qp数组中找到qp的索引：qp[0]=9,那么直接调整pq[9]即可。\"]},{\"header\":\"2、API设计\",\"slug\":\"_2、api设计\",\"contents\":[\"类名\",\"IndexMinPriorityQueue\",\"构造方法\",\"IndexMinPriorityQueue(int capacity)：创建容量为capacity的IndexMinPriorityQueue对象\",\"成员方法\",\"private boolean less(int i,int j)：判断堆中索引i处的元素是否小于索引j处的元素private void exch(int i,int j):交换堆中i索引和j索引处的值public int delMin():删除队列中最小的元素,并返回该元素关联的索引public void insert(int i,T t)：往队列中插入一个元素,并关联索引iprivate void swim(int k):使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置private void sink(int k):使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置public int size():获取队列中元素的个数public boolean isEmpty():判断队列是否为空public boolean contains(int k):判断k对应的元素是否存在public void changeItem(int i, T t):把与索引i关联的元素修改为为tpublic int minIndex():最小元素关联的索引public void delete(int i):删除索引i关联的元素\",\"成员变量\",\"private T[] imtes : 用来存储元素的数组private int[] pq:保存每个元素在items数组中的索引，pq数组需要堆有序private int [] qp:保存qp的逆序，pq的值作为索引，pq的索引作为值private int N：记录堆中元素的个数\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现\",\"contents\":[\"public class IndexMinPriorityQueue<T extends Comparable<T>> { /** * 堆中元素 */ private T[] items; /** * 元素个数 */ private int size; /** * 保存每个元素在items数组中的索引，pq数组需要堆有序 */ private int[] pq; /** * 保存qp的逆序，pq的值作为索引，pq的索引作为值 */ private int[] qp; public IndexMinPriorityQueue(int capacity){ items = (T[]) new Comparable[capacity+1]; size = 0; qp = new int[capacity+1]; pq = new int[capacity+1]; Arrays.fill(qp, -1); } /** * 判断堆中索引i处的元素是否小于索引j处的元素 * @param i * @param j * @return */ private boolean less(int i,int j){ return items[pq[i]].compareTo(items[pq[j]]) < 0; } /** * 换堆中i索引和j索引处的值 * @param i * @param j */ private void swap(int i,int j){ //先交换pq数组中的值 int temp = pq[i]; pq[i] = pq[j]; pq[j] = temp; //更新qp数组中的值 qp[pq[i]] = i; qp[pq[j]] = j; } /** * 删除队列中最小的元素,并返回该元素关联的索引 * @return */ public int delMin(){ //找到items中最小元素的索引 int minIndex = pq[1]; //交换pq中索引1处的值和N处的值 swap(1, size); //删除qp中索引pq[N]处的值 qp[pq[size]] = -1; //删除pq中索引N处的值 pq[size] = -1; //删除items中的最小元素 items[minIndex] = null; //元素数量-1 size--; //对pq[1]做下沉，让堆有序 sink(1); return minIndex; } /** * 往队列中插入一个元素,并关联索引i * @param i * @param t */ public void insert(int i,T t){ if(contains(i)){ throw new RuntimeException(\\\"该索引已存在！\\\"); } size++; //把元素存放到items数组中 items[i] = t; //使用pq存放i这个索引 pq[size] = i; //在qp的i索引处存放N qp[i] = size; //上浮items[pq[N]],让pq堆有序 swim(size); } /** * 使用上浮算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ private void swim(int k){ while (1 < k){ //比较当前结点和父结点，如果当前结点比父结点小，则交换位置 if(less(k,k/2)){ swap(k,k/2); } k = k/2; } } /** * :使用下沉算法，使索引k处的元素能在堆中处于一个正确的位置 * @param k */ private void sink(int k){ //如果当前结点已经没有子结点了，则结束下沉 while (2*k <= size){ int min = 2*k; if(2*k+1 <= size && less(2*k+1,2*k)){ min = 2*k + 1; } if(less(k,min)){ break; } swap(k,min); k = min; } } /** * :获取队列中元素的个数 * @return */ public int size(){ return size; } /** * :判断队列是否为空 * @return */ public boolean isEmpty(){ return size == 0; } /** * :判断k对应的元素是否存在 * @param k * @return */ public boolean contains(int k){ //默认情况下，qp的所有元素都为-1，如果某个位置插入了数据，则不为-1 return qp[k] != -1; } /** * :把与索引i关联的元素修改为为t * @param i * @param t */ public void changeItem(int i, T t){ //修改item数组中索引i的值为t items[i] = t; //找到i在pq中的位置 int i1 = pq[i]; //对pq[i1]做下沉，让堆有序 sink(i1); //对pq[k]做上浮，让堆有序 swim(i1); } /** * :最小元素关联的索引 * @return */ public int minIndex(){ //pq的索引1处，存放的是最小元素在items中的索引 return pq[1]; } /** * :删除索引i关联的元素 * @param i */ public void delete(int i){ //找出i在pq中的索引 int k = pq[i]; //把pq中索引k处的值和索引N处的值交换 swap(k,size); //删除qp中索引pq[N]处的值 qp[pq[size]] = -1; //删除pq中索引N处的值 pq[size] = -1; //删除items中索引i处的值 items[i] = null; //元素数量-1 size--; //对pq[k]做下沉，让堆有序 sink(k); //对pq[k]做上浮，让堆有序 swim(k); } } \",\"测试类\",\"public class IndexMinPriorityQueueTest { public static void main(String[] args) { String[] arr = {\\\"S\\\", \\\"O\\\", \\\"R\\\", \\\"T\\\", \\\"E\\\", \\\"X\\\", \\\"A\\\", \\\"M\\\", \\\"P\\\", \\\"L\\\", \\\"E\\\"}; IndexMinPriorityQueue<String> indexMinPQ = new IndexMinPriorityQueue<>(20); //插入 for (int i = 0; i < arr.length; i++) { indexMinPQ.insert(i,arr[i]); } System.out.println(indexMinPQ.size()); //获取最小值的索引 System.out.println(indexMinPQ.minIndex()); //测试修改 indexMinPQ.changeItem(0,\\\"Z\\\"); int minIndex=-1; while(!indexMinPQ.isEmpty()){ minIndex = indexMinPQ.delMin(); System.out.print(minIndex+\\\",\\\"); } } } \",\"输出\",\"11 6 10,4,9,7,1,8,2,3,5,0,0, \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/simplesort.html\":{\"title\":\"3、简单排序\",\"contents\":[{\"header\":\"1、Comparable接口介绍\",\"slug\":\"_1、comparable接口介绍\",\"contents\":[\"Comparable接口用于元素之间的比较\",\"回顾：\"]},{\"header\":\"1、需求\",\"slug\":\"_1、需求\",\"contents\":[\"定义一个学生类Student，具有年龄age和姓名username两个属性，并通过Comparable接口提供比较规则；\",\"定义测试类Test，在测试类Test中定义测试方法Comparable getMax(Comparable c1,Comparable c2)完成测试\",\"Student类\",\"public class Student implements Comparable<Student>{ public String name; public int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public String toString() { final StringBuilder sb = new StringBuilder(\\\"Student{\\\"); sb.append(\\\"name='\\\").append(name).append('\\\\''); sb.append(\\\", age=\\\").append(age); sb.append('}'); return sb.toString(); } @Override public int compareTo(Student s){ return this.getAge() - s.getAge(); } } \",\"测试类\",\"public class TestCompare { public static void main(String[] args) { Student s1 = new Student(); s1.setName(\\\"lwh\\\"); s1.setAge(18); Student s2 = new Student(); s1.setName(\\\"yhx\\\"); s1.setAge(19); Comparable comparable = compareMax(s1, s2); System.out.println(comparable); } /** * 比较大小 * @param c1 * @param c2 * @return */ public static Comparable compareMax(Comparable c1,Comparable c2){ int i = c1.compareTo(c2); /** * i > 0,c1 > c2 * i == 0,c1 == c2 * i < 0,c1 < c2 */ if(i >= 0){ return c1; }else { return c2; } } } \"]},{\"header\":\"2、冒泡排序\",\"slug\":\"_2、冒泡排序\",\"contents\":[\"冒泡排序（Bubble Sort）也是一种简单直观的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢\\\"浮\\\"到数列的顶端。\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤\",\"contents\":[\"比较相邻的元素。如果第一个比第二个大，就交换他们两个。\",\"对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。\",\"针对所有的元素重复以上的步骤，除了最后一个。\",\"持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现\",\"contents\":[\"/** * 冒泡排序算法 比较并交换 * @param nums * @return */ public int[] bubbleSort(int[] nums){ int n = nums.length; for (int i = n - 1; i >= 0; i--) { for (int j = 0; j < i; j++) { //交换两个元素 if(nums[j] > nums[j+1]){ int temp = nums[j]; nums[j] = nums[j+1]; nums[j+1] = temp; } } } return nums; } \"]},{\"header\":\"3、时间复杂度分析\",\"slug\":\"_3、时间复杂度分析\",\"contents\":[\"冒泡排序使用了双层for循环，其中内层循环的循环体是真正完成排序的代码。\",\"元素的比较次数：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)(N-1)/2 = N2/2-N/2；*\",\"元素的交换次数：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)(N-1)/2 = N2/2-N/2;*\",\"总的执行次数：N^N - N\",\"按照大O推导法则，保留函数中的最高阶项那么最终冒泡排序的时间复杂度为O(N2).\"]},{\"header\":\"2、选择排序\",\"slug\":\"_2、选择排序\",\"contents\":[\"选择排序是一种简单直观的排序算法，无论什么数据进去都是 O(n²) 的时间复杂度\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤-1\",\"contents\":[\"首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。\",\"再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。\",\"重复第二步，直到所有元素均排序完毕。\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-1\",\"contents\":[\"public int[] selectSort(int[] nums){ int n = nums.length; //这里n-2是因为最后一次是跟自己比较 for (int i = 0; i <= n-2; i++) { int index = i; //每一次内层循环找到最小元素的下标，如果不与i相等，则交换元素 for (int j = i + 1; j < n; j++) { if(nums[index] > nums[j]){ index = j; } } //交换元素 if(i != index){ int temp = nums[i]; nums[i] = nums[index]; nums[index] = temp; } } return nums; } \"]},{\"header\":\"3、时间复杂度分析\",\"slug\":\"_3、时间复杂度分析-1\",\"contents\":[\"选择排序使用了双层for循环，其中外层循环完成了数据交换，内层循环完成了数据比较\",\"元素比较次数：（n-1）+(n-2)+……+1 =((n-1)+1)(n-1)/2 = n2/2 - n/2*\",\"元素交换次数：（n-1）\",\"总的执行次数： n2/2 - n/2 + （n-1） = n2/2 + n/2 -1\",\"根据大O推导法则，保留最高阶项，去除常数因子，时间复杂度为O(n2)\"]},{\"header\":\"3、插入排序\",\"slug\":\"_3、插入排序\",\"contents\":[\"插入排序：是一种最简单直观的排序算法，它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。\",\"插入排序和冒泡排序一样，也有一种优化算法，叫做拆半插入。\",\"插入排序的代码实现虽然没有冒泡排序和选择排序那么简单粗暴，但它的原理应该是最容易理解的了，因为只要打过扑克牌的人都应该能够秒懂。\"]},{\"header\":\"1、算法步骤\",\"slug\":\"_1、算法步骤-2\",\"contents\":[\"把所有的元素分为两组，已经排序的和未排序的；\",\"找到未排序的组中的第一个元素，向已经排序的组中进行插入；\",\"倒叙遍历已经排序的元素，依次和待插入的元素进行比较，直到找到一个元素小于等于待插入元素，那么就把待插入元素放到这个位置，其他的元素向后移动一位；\"]},{\"header\":\"2、代码实现\",\"slug\":\"_2、代码实现-2\",\"contents\":[\"/** * 插入排序 * @param nums * @return */ public int[] insertSort(int[] nums){ int n = nums.length; // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的 for(int i=1;i < n;i++){ //当前元素为a[i],依次和i前面的元素比较，找到一个小于等于a[i]的元素 for (int j = i; j > 0; j--) { //交换元素 if(nums[j] < nums[j-1]){ int temp = nums[j]; nums[j] = nums[ j-1]; nums[j-1] = temp; }else { break; } } } return nums; } \"]},{\"header\":\"3、时间复杂度分析\",\"slug\":\"_3、时间复杂度分析-2\",\"contents\":[\"插入排序使用了双层for循环，其中内层循环的循环体是真正完成排序的代码\",\"最坏的情况：就是原数组倒叙排列\",\"比较的次数：（n-1）+(n-2)+……+1 =((n-1)+1)(n-1)/2 = n2/2 - n/2*\",\"交换的次数：（n-1）+(n-2)+……+1 =((n-1)+1)(n-1)/2 = n2/2 - n/2*\",\"总执行次数：n2 - n\",\"按照大O推导法则，保留函数中的最高阶项那么最终插入排序的时间复杂度为O(n2)\"]}]},\"/study-tutorial/basic/dataAndAlgorithm/symboltable.html\":{\"title\":\"6、符号表\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"符号表最主要的目的就是将一个键和一个值联系起来，符号表能够将存储的数据元素是一个键和一个值共同组成的键值对数据，我们可以根据键来查找对应的值。\",\"符号表中，键具有唯一性。\",\"符号表的应用：\",\"应用\",\"查找目的\",\"键\",\"值\",\"字典\",\"找出单词的释义\",\"单词\",\"释义\",\"图书索引\",\"找出某个术语相关的页码\",\"术语\",\"一串页码\",\"网络搜索\",\"找出某个关键字对应的网页\",\"关键字\",\"网页名称\"]},{\"header\":\"1、符号表的API设计\",\"slug\":\"_1、符号表的api设计\",\"contents\":[\"节点类\",\"类名\",\"Node<Key,Value>\",\"构造方法\",\"Node(Key key,Value value,Node next)：创建Node对象\",\"成员变量\",\"1.public Key key:存储键2.public Value value:存储值3.public Node next:存储下一个结点\",\"符号表\",\"类名\",\"SymbolTable<Key,Value>\",\"构造方法\",\"SymbolTable()：创建SymbolTable对象\",\"成员方法\",\"1.public Value get(Key key)：根据键key，找对应的值2.public void put(Key key,Value val):向符号表中插入一个键值对3.public void delete(Key key):删除键为key的键值对4.public int size()：获取符号表的大小\",\"成员变量\",\"1.private Node head:记录首结点2.private int N:记录符号表中键值对的个数\",\"public class SymbolTable<Key,Value> { //头结点 private Node head; //长度 private int size; public SymbolTable() { head = new Node(null,null,null); size = 0; } /** * ：根据键key，找对应的值 * @param key * @return */ public Value get(Key key){ Node<Key,Value> n = head; while(n.next != null){ n = n.next; if(n.key.equals(key)){ return n.value; } } return null; } /** * 向符号表中插入一个键值对 * @param key * @param val */ public void put(Key key,Value val){ Node<Key,Value> pre = head; while(pre.next != null){ pre = pre.next; if(pre.key.equals(key)){ pre.value = val; return; } } Node oldFirst = head.next; Node<Key, Value> node = new Node<>(key, val, oldFirst); head.next = node; size++; } /** * 删除键为key的键值对<br/> * @param key */ public void delete(Key key){ Node n = head; while(null != n.next){ if(n.next.key.equals(key)){ n.next = n.next.next; size--; return; } n = n.next; } } /** * 获取符号表的大小 * @return */ public int size(){ return size; } private class Node<Key,Value>{ Key key; Value value; Node next; public Node(Key key, Value value, Node next) { this.key = key; this.value = value; this.next = next; } } } \"]},{\"header\":\"2、有序符号表\",\"slug\":\"_2、有序符号表\",\"contents\":[\"刚才实现的符号表，我们可以称之为无序符号表，因为在插入的时候，并没有考虑键值对的顺序，而在实际生活中，有时候我们需要根据键的大小进行排序，插入数据时要考虑顺序，那么接下来我们就实现一下有序符号表。\",\"package com.xiaobear.SymbolTable; /** * @Author xiaobear * @date 2021年07月30日 10:00 * @Description 符号表 */ public class OrderSymbolTable<Key extends Comparable<Key>,Value> { //头结点 private Node head; //长度 private int size; public OrderSymbolTable() { head = new Node(null,null,null); size = 0; } /** * ：根据键key，找对应的值 * @param key * @return */ public Value get(Key key){ Node<Key,Value> n = head; while(n.next != null){ n = n.next; if(n.key.equals(key)){ return n.value; } } return null; } /** * 向符号表中插入一个键值对 * @param key * @param val */ public void put(Key key,Value val){ Node<Key,Value> curr = head.next; //记录上一个节点 Node pre = head; //1.如果key大于当前结点的key，则一直寻找下一个结点 while(curr!=null && key.compareTo(curr.key)>0){ pre = curr; curr = curr.next; } //2.如果当前结点curr的key和将要插入的key一样，则替换 if (curr!=null && curr.key.compareTo(key)==0){ curr.value = val; return; } //3.没有找到相同的key，把新结点插入到curr之前 Node newNode = new Node(key, val, curr); pre.next = newNode; size++; } /** * 删除键为key的键值对<br/> * @param key */ public void delete(Key key){ Node n = head; while(null != n.next){ if(n.next.key.equals(key)){ n.next = n.next.next; size--; return; } n = n.next; } } /** * 获取符号表的大小 * @return */ public int size(){ return size; } private class Node<Key,Value>{ Key key; Value value; Node next; public Node(Key key, Value value, Node next) { this.key = key; this.value = value; this.next = next; } } } \"]}]},\"/study-tutorial/basic/dataAndAlgorithm/Treeadvancement.html\":{\"title\":\"10、树的进阶\",\"contents\":[{\"header\":\"1、平衡树--查找树\",\"slug\":\"_1、平衡树-查找树\",\"contents\":[]},{\"header\":\"1、查找树的定义\",\"slug\":\"_1、查找树的定义\",\"contents\":[\"一棵2-3查找树要么为空，要么满足满足下面两个要求：\",\"2-结点\",\"含有一个键(及其对应的值)和两条链，左链接指向2-3树中的键都小于该结点，右链接指向的2-3树中的键都大于该结点。\",\"3-结点\",\"含有两个键(及其对应的值)和三条链，左链接指向的2-3树中的键都小于该结点，中链接指向的2-3树中的键都位于该结点的两个键之间，右链接指向的2-3树中的键都大于该结点。\"]},{\"header\":\"2、查找树的操作\",\"slug\":\"_2、查找树的操作\",\"contents\":[]},{\"header\":\"1、查找\",\"slug\":\"_1、查找\",\"contents\":[\"将二叉查找树的查找算法一般化我们就能够直接得到2-3树的查找算法。要判断一个键是否在树中，我们先将它和根结点中的键比较。如果它和其中任意一个相等，查找命中；否则我们就根据比较的结果找到指向相应区间的连接，并在其指向的子树中递归地继续查找。如果这个是空链接，查找未命中。\"]},{\"header\":\"2、插入\",\"slug\":\"_2、插入\",\"contents\":[]},{\"header\":\"1、向2-结点中插入新键\",\"slug\":\"_1、向2-结点中插入新键\",\"contents\":[\"往2-3树中插入元素和往二叉查找树中插入元素一样，首先要进行查找，然后将节点挂到未找到的节点上。2-3树之所以能够保证在最差的情况下的效率的原因在于其插入之后仍然能够保持平衡状态。如果查找后未找到的节点是一个2-结点，那么很容易，我们只需要将新的元素放到这个2-结点里面使其变成一个3-结点即可。但是如果查找的节点结束于一个3-结点，那么可能有点烦。\"]},{\"header\":\"2、向一棵只含有一个3-结点的树中插入新键\",\"slug\":\"_2、向一棵只含有一个3-结点的树中插入新键\",\"contents\":[\"假设2-3树只包含一个3-结点，这个结点有两个键，没有空间来插入第三个键了，最自然的方式是我们假设这个结点能存放三个元素，暂时使其变成一个4-结点，同时他包含四条链接。然后，我们将这个4-结点的中间元素提升，左边的键作为其左子结点，右边的键作为其右子结点。插入完成，变为平衡2-3查找树，树的高度从0变为1。\"]},{\"header\":\"3、向一个父结点为2-结点的3-结点中插入新键\",\"slug\":\"_3、向一个父结点为2-结点的3-结点中插入新键\",\"contents\":[\"和上面的情况一样一样，我们也可以将新的元素插入到3-结点中，使其成为一个临时的4-结点，然后，将该结点中的中间元素提升到父结点即2-结点中，使其父结点成为一个3-结点，然后将左右结点分别挂在这个3-结点的恰当位置。\"]},{\"header\":\"4、向一个父结点为3-结点的3-结点中插入新键\",\"slug\":\"_4、向一个父结点为3-结点的3-结点中插入新键\",\"contents\":[\"当我们插入的结点是3-结点的时候，我们将该结点拆分，中间元素提升至父结点，但是此时父结点是一个3-结点，插入之后，父结点变成了4-结点，然后继续将中间元素提升至其父结点，直至遇到一个父结点是2-结点，然后将其变为3-结点，不需要继续进行拆分。\"]},{\"header\":\"5、分解根结点\",\"slug\":\"_5、分解根结点\",\"contents\":[\"当插入结点到根结点的路径上全部是3-结点的时候，最终我们的根结点会编程一个临时的4-结点，此时，就需要将根结点拆分为两个2-结点，树的高度加1。\"]},{\"header\":\"3、树的性质\",\"slug\":\"_3、树的性质\",\"contents\":[]},{\"header\":\"1、2-3树的性质\",\"slug\":\"_1、2-3树的性质\",\"contents\":[\"通过对2-3树插入操作的分析，我们发现在插入的时候，2-3树需要做一些局部的变换来保持2-3树的平衡。\",\"一棵完全平衡的2-3树具有以下性质：\",\"任意空链接到根结点的路径长度都是相等的。\",\"4-结点变换为3-结点时，树的高度不会发生变化，只有当根结点是临时的4-结点，分解根结点时，树高+1。\",\"2-3树与普通二叉查找树最大的区别在于，普通的二叉查找树是自顶向下生长，而2-3树是自底向上生长。\"]},{\"header\":\"4、树的实现\",\"slug\":\"_4、树的实现\",\"contents\":[]},{\"header\":\"1、2-3的实现\",\"slug\":\"_1、2-3的实现\",\"contents\":[\"直接实现2-3树比较复杂，因为：\",\"需要处理不同的结点类型，非常繁琐；\",\"需要多次比较操作来将结点下移；\",\"需要上移来拆分4-结点；\",\"拆分4-结点的情况有很多种；\",\"2-3查找树实现起来比较复杂，在某些情况插入后的平衡操作可能会使得效率降低。但是2-3查找树作为一种比较重要的概念和思路对于我们后面要讲到的红黑树、B树和B+树非常重要。\"]},{\"header\":\"2、平衡树--红黑树\",\"slug\":\"_2、平衡树-红黑树\",\"contents\":[\"我们前面介绍了2-3树，可以看到2-3树能保证在插入元素之后，树依然保持平衡状态，它的最坏情况下所有子结点都是2-结点，树的高度为lgN,相比于我们普通的二叉查找树，最坏情况下树的高度为N，确实保证了最坏情况下的时间复杂度，但是2-3树实现起来过于复杂，所以我们介绍一种2-3树思想的简单实现：红黑树。\",\"红黑树主要是对2-3树进行编码，红黑树背后的基本思想是用标准的二叉查找树(完全由2-结点构成)和一些额外的信 息(替换3-结点)来表示2-3树。我们将树中的链接分为两种类型：\",\"红链接：将两个2-结点连接起来构成一个3-结点\",\"黑链接：2-3树中的普通链接\",\"我们将3-结点表示为由由一条左斜的红色链接(两个2-结点其中之一是另一个的左子结点)相连的两个2-结点。这种表示法的一个优点是，我们无需修改就可以直接使用标准的二叉查找树的get方法。\"]},{\"header\":\"1、红黑树的定义\",\"slug\":\"_1、红黑树的定义\",\"contents\":[\"红黑树是含有红黑链接并满足下列条件的二叉查找树：\",\"红链接均为左连接\",\"没有任何一个节点同时和两条红链接相连\",\"该树是完美黑色平衡的，即是任意空链接到根节点的路径上的黑链接数量相同\",\"红黑树与2-3树的对应关系：\"]},{\"header\":\"2、红黑树的API\",\"slug\":\"_2、红黑树的api\",\"contents\":[\"因为每个结点都只会有一条指向自己的链接（从它的父结点指向它），我们可以在之前的Node结点中添加一个布尔类型的变量color来表示链接的颜色。如果指向它的链接是红色的，那么该变量的值为true，如果链接是黑色的，那么该变量的值为false。\",\"API设计\",\"类名\",\"Node<Key,Value>\",\"构造方法\",\"Node(Key key, Value value, Node left, Node right，boolean color)：创建Node对象\",\"成员变量\",\"public Node left:记录左子结点public Node right:记录右子结点public Key key:存储键public Value value:存储值public boolean color:由其父结点指向它的链接的颜色\"]},{\"header\":\"3、平衡化\",\"slug\":\"_3、平衡化\",\"contents\":[\"在对红黑树进行一些增删改查的操作后，很有可能会出现红色的右链接或者两条连续红色的链接，而这些都不满足红黑树的定义，所以我们需要对这些情况通过旋转进行修复，让红黑树保持平衡。\"]},{\"header\":\"1、左旋\",\"slug\":\"_1、左旋\",\"contents\":[\"当某个结点的左子结点为黑色，右子结点为红色，此时需要左旋。\",\"前提：当前结点为h，它的右子结点为x；\",\"左旋过程：\",\"让x的左子结点变为h的右子结点：\",\"h.right=x.left; \",\"让h成为x的左子结点：\",\"x.left=h; \",\"让h的color属性变为x的color属性值：\",\"x.color=h.color; \",\"让h的color属性变为RED：\",\"h.color=true; \"]},{\"header\":\"2、右旋\",\"slug\":\"_2、右旋\",\"contents\":[\"当某个结点的左子结点是红色，且左子结点的左子结点也是红色，需要右旋\",\"前提：当前结点为h，它的左子结点为x；\",\"右旋过程：\",\"让x的右子结点成为h的左子结点：\",\"h.left = x.right; \",\"让h成为x的右子结点：\",\"x.right=h; \",\"让x的color变为h的color属性值：\",\"x.color = h.color; \",\"让h的color为RED；\"]},{\"header\":\"4、操作\",\"slug\":\"_4、操作\",\"contents\":[]},{\"header\":\"1、向单个2-结点中插入新键\",\"slug\":\"_1、向单个2-结点中插入新键\",\"contents\":[\"一棵只含有一个键的红黑树只含有一个2-结点。插入另一个键后，我们马上就需要将他们旋转。\",\"如果新键小于当前结点的键，我们只需要新增一个红色结点即可，新的红黑树和单个3-结点完全等价。\",\"如果新键大于当前结点的键，那么新增的红色结点将会产生一条红色的右链接，此时我们需要通过左旋，把红色右链接变成左链接，插入操作才算完成。形成的新的红黑树依然和3-结点等价，其中含有两个键，一条红色链接。\"]},{\"header\":\"2、向底部的2-结点插入新键\",\"slug\":\"_2、向底部的2-结点插入新键\",\"contents\":[\"用和二叉查找树相同的方式向一棵红黑树中插入一个新键，会在树的底部新增一个结点（可以保证有序性），唯一区别的地方是我们会用红链接将新结点和它的父结点相连。如果它的父结点是一个2-结点，那么刚才讨论的两种方式仍然适用。\"]},{\"header\":\"3、颜色反转\",\"slug\":\"_3、颜色反转\",\"contents\":[\"当一个结点的左子结点和右子结点的color都为RED时，也就是出现了临时的4-结点，此时只需要把左子结点和右子结点的颜色变为BLACK，同时让当前结点的颜色变为RED即可。\"]},{\"header\":\"4、向一棵双键树(即一个3-结点)中插入新键\",\"slug\":\"_4、向一棵双键树-即一个3-结点-中插入新键\",\"contents\":[\"这种情况分为三种情况：\",\"新键大于原树中的两个键\",\"新键小于原树中的两个键\",\"新键介于原树中两个键之间\"]},{\"header\":\"5、根节点的颜色总是黑色\",\"slug\":\"_5、根节点的颜色总是黑色\",\"contents\":[\"由于根结点不存在父结点，所以每次插入操作后，我们都需要把根结点的颜色设置为黑色。\"]},{\"header\":\"6、向树底部的3-结点插入新键\",\"slug\":\"_6、向树底部的3-结点插入新键\",\"contents\":[\"假设在树的底部的一个3-结点下加入一个新的结点。前面我们所讲的3种情况都会出现。指向新结点的链接可能是：\",\"3-结点的右链接（此时我们只需要转换颜色即可）\",\"或是左链接(此时我们需要进行右旋转然后再转换)\",\"或是中链接(此时需要先左旋转然后再右旋转，最后转换颜色)。\",\"颜色转换会使中间结点的颜色变红，相当于将它送入了父结点。这意味着父结点中继续插入一个新键，我们只需要使用相同的方法解决即可，直到遇到一个2-结点或者根结点为止。\"]},{\"header\":\"5、API的设计\",\"slug\":\"_5、api的设计\",\"contents\":[\"类名\",\"RedBlackTree<Key,Value>\",\"构造方法\",\"RedBlackTree()：创建RedBlackTree对象\",\"成员方法\",\"1.private boolean isRed(Node x)：判断当前结点的父指向链接是否为红色2.private Node rotateLeft(Node h):左旋调整3.private Node rotateRight(Node h):右旋调整4.private void flipColors(Node h)：颜色反转,相当于完成拆分4-结点5.public void put(Key key, Value val):在整个树上完成插入操作6.private Node put(Node h, Key key, Value val):在指定树中，完成插入操作,并返回添加元素后新的树7.public Value get(Key key):根据key，从树中找出对应的值8.private Value get(Node x, Key key):从指定的树x中，找出key对应的值9.public int size():获取树中元素的个数\",\"成员变量\",\"1.private Node root : 记录根结点2.private int N:记录树中元素的个数3.private static final boolean RED：红色链接标识4.private static final boolean BLACK:黑色链接标识\"]},{\"header\":\"6、代码实现\",\"slug\":\"_6、代码实现\",\"contents\":[\"public class RedBlackTree<Key extends Comparable<Key>,Value> { /** * 根结点 */ private Node root; /** * 元素个数 */ private int size; /** * 红链接 */ private static final boolean RED = true; /** * 黑链接 */ private static final boolean BLACK = false; /** * 判断当前结点的父指向链接是否为红色 * @param x * @return */ private boolean isRed(Node x){ if (x == null) { return false; } return x.color == RED; } /** * 左旋调整 * @param h * @return */ private Node rotateLeft(Node h){ //找到当前节点的右子结点 Node rightNode = h.right; //找到右子结点的左子结点 Node left = rightNode.left; //让当前结点h的右子结点的左子结点成为当前结点的右子结点 h.right = left; //让当前节点h变为rightNode的左子节点 rightNode.left = h; //让当前结点h的color变成右子结点的color rightNode.color = h.color; //让当前结点的color变成RED h.color = RED; return rightNode; } /** * 右旋调整 * @param h * @return */ private Node rotateRight(Node h){ //找到当前结点的左子结点 Node leftNode = h.left; //找到leftNode的右子结点 Node right = leftNode.right; //让right变成当前结点的左子结点 h.left = right; //让当前节点变为leftNode的右子结点 leftNode.right = h; //让当前结点h的color值变为左子结点的color值 leftNode.color = h.color; //让当前结点的color变为RED h.color = RED; return leftNode; } /** * 颜色反转,相当于完成拆分4-结点 * @param h */ private void flipColors(Node h){ //当前结点变为RED h.color = RED; //左右子结点的color变为BLACK h.left.color = BLACK; h.right.color = BLACK; } /** * 在整个树上完成插入操作 * @param key * @param val */ public void put(Key key, Value val){ root = put(root,key,val); //让根节点颜色变为BLACK root.color = BLACK; } /** * 在指定树中，完成插入操作,并返回添加元素后新的树 * @param h * @param key * @param val * @return */ private Node put(Node h, Key key, Value val){ //根结点 if (h == null) { size++; return new Node(null,null,key,val,RED); } //比较当前节点的键与key int compare = key.compareTo(h.key); if(compare < 0){ //往左子结点插入 h.left = put(h.left,key,val); }else if(compare > 0){ //往右子结点插入 h.right = put(h.right,key,val); }else { //更新值 h.value = val; } //如果当前结点的右链接是红色，左链接为黑色，需要左旋 if(isRed(h.right) && !isRed(h.left)){ h = rotateLeft(h); } //如果当前节点的左子结点以及左子结点的左子结点都是红链接，需要右旋 if(isRed(h.right) && isRed(h.right.right)){ h = rotateRight(h); } //颜色变换：当前节点的左右子结点都是红链接 if(isRed(h.right) && isRed(h.left)){ flipColors(h); } return h; } /** * 根据key，从树中找出对应的值 * @param key * @return */ public Value get(Key key){ return get(root,key); } /** * 从指定的树x中，找出key对应的值 * @param x * @param key * @return */ private Value get(Node x, Key key){ if (x == null) { return null; } int compare = key.compareTo(x.key); if(0 > compare){ return get(x.left,key); }else if(0 < compare){ return get(x.right,key); }else { return x.value; } } /** * 获取树中元素的个数 * @return */ public int size(){ return size; } /** * 结点类 */ private class Node{ public Node left; public Node right; public Key key; public Value value; public boolean color; public Node(Node left, Node right, Key key, Value value, boolean color) { this.left = left; this.right = right; this.key = key; this.value = value; this.color = color; } } } \",\"测试类\",\"public class RedBlackTreeTest { public static void main(String[] args) { RedBlackTree<Integer, String> tree = new RedBlackTree<>(); tree.put(1,\\\"1111\\\"); tree.put(2,\\\"2222\\\"); tree.put(3,\\\"3333\\\"); tree.put(4,\\\"4444\\\"); System.out.println(tree.size()); tree.put(2,\\\"666666\\\"); System.out.println(tree.get(2)); } } \"]},{\"header\":\"3、B-树\",\"slug\":\"_3、b-树\",\"contents\":[\"前面我们已经学习了二叉查找树、2-3树以及它的实现红黑树。2-3树中，一个结点做多能有两个key，它的实现红黑树中使用对链接染色的方式去表达这两个key。接下来我们学习另外一种树型结构B树，这种数据结构中，一个结点允许多于两个key的存在。\",\"B树是一种树状数据结构，它能够存储数据、对其进行排序并允许以O(logn)的时间复杂度进行查找、顺序读取、插入和删除等操作。\"]},{\"header\":\"1、B-树的特性\",\"slug\":\"_1、b-树的特性\",\"contents\":[\"B树中允许一个结点中包含多个key，可以是3个、4个、5个甚至更多，并不确定，需要看具体的实现。现在我们选择一个参数M，来构造一个B树，我们可以把它称作是M阶的B树，那么该树会具有如下特点：\",\"每个结点最多有M-1个Key，并且升序排列\",\"每个结点最多有M个子结点\",\"根节点至少有两个子结点\",\"在实际应用中B树的阶数一般都比较大（通常大于100），所以，即使存储大量的数据，B树的高度仍然比较小，这样在某些应用场景下，就可以体现出它的优势。\"]},{\"header\":\"2、B-树存储数据\",\"slug\":\"_2、b-树存储数据\",\"contents\":[\"若参数M选择为5，那么每个结点最多包含4个键值对，我们以5阶B树为例，看看B树的数据存储。\"]},{\"header\":\"3、B树在磁盘文件中的应用\",\"slug\":\"_3、b树在磁盘文件中的应用\",\"contents\":[\"在我们的程序中，不可避免的需要通过IO操作文件，而我们的文件是存储在磁盘上的。计算机操作磁盘上的文件是通过文件系统进行操作的，在文件系统中就使用到了B树这种数据结构。\"]},{\"header\":\"1、磁盘\",\"slug\":\"_1、磁盘\",\"contents\":[\"磁盘能够保存大量的数据，从GB一直到TB级，但是 他的读取速度比较慢，因为涉及到机器操作，读取速度为毫秒级 。\",\"磁盘由盘片构成,每个盘片有两面，又称为盘面 。盘片中央有一个可以旋转的主轴，他使得盘片以固定的旋转速率旋转，通常是5400rpm或者是7200rpm,一个磁盘中包含了多个这样的盘片并封装在一个密封的容器内 。盘片的每个表面是由一组称为磁道同心圆组成的 ，每个磁道被划分为了一组扇区 ，每个扇区包含相等数量的数据位，通常是512个子节，扇区之间由一些间隙隔开,这些间隙中不存储数据 。\"]},{\"header\":\"2、磁盘IO\",\"slug\":\"_2、磁盘io\",\"contents\":[\"磁盘用磁头来读写存储在盘片表面的位，而磁头连接到一个移动臂上，移动臂沿着盘片半径前后移动，可以将磁头定位到任何磁道上，这称之为寻道操作。一旦定位到磁道后，盘片转动，磁道上的每个位经过磁头时，读写磁头就可以感知到该位的值，也可以修改值。对磁盘的访问时间分为 寻道时间，旋转时间，以及传送时间。\",\"由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，因此为了提高效率，要尽量减少磁盘I/O，减少读写操作。 为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此预读可以提高I/O效率。\",\"页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（1024个字节或其整数倍），预读的长度一般为页的整倍数。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。\",\"文件系统的设计者利用了磁盘预读原理，将一个结点的大小设为等于一个页（1024个字节或其整数倍），这样每个结点只需要一次I/O就可以完全载入。那么3层的B树可以容纳102410241024差不多10亿个数据，如果换成二叉查找树，则需要30层！假定操作系统一次读取一个节点，并且根节点保留在内存中，那么B树在10亿个数据中查找目标值，只需要小于3次硬盘读取就可以找到目标值，但红黑树需要小于30次，因此B树大大提高了IO的操作效率。\"]},{\"header\":\"4、B+树\",\"slug\":\"_4、b-树\",\"contents\":[\"B+树是对B树的一种变形树，它与B树的差异在于：\",\"非叶结点仅具有索引作用，也就是说，非叶子结点只存储key，不存储value；\",\"树的所有叶结点构成一个有序链表，可以按照key排序的次序遍历全部数据。\"]},{\"header\":\"1、B+树存储数据\",\"slug\":\"_1、b-树存储数据\",\"contents\":[\"若参数M选择为5，那么每个结点最多包含4个键值对，我们以5阶B+树为例，看看B+树的数据存储。\"]},{\"header\":\"2、B+树和B树的对比\",\"slug\":\"_2、b-树和b树的对比\",\"contents\":[\"B+ 树的优点\",\"由于B+树在非叶子结点上不包含真正的数据，只当做索引使用，因此在内存相同的情况下，能够存放更多的key。 2.B+树的叶子结点都是相连的，因此对整棵树的遍历只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。\",\"B树的优点\",\"由于B树的每一个节点都包含key和value，因此我们根据key查找value时，只需要找到key所在的位置，就能找到value，但B+树只有叶子结点存储数据，索引每一次查找，都必须一次一次，一直找到树的最大深度处，也就是叶子结点的深度，才能找到value。\"]},{\"header\":\"3、B+树在数据库中的应用\",\"slug\":\"_3、b-树在数据库中的应用\",\"contents\":[\"在数据库的操作中，查询操作可以说是最频繁的一种操作，因此在设计数据库时，必须要考虑到查询的效率问题，在很多数据库中，都是用到了B+树来提高查询的效率； 在操作数据库时，我们为了提高查询效率，可以基于某张表的某个字段建立索引，就可以提高查询效率，那其实这个索引就是B+树这种数据结构实现的。\"]},{\"header\":\"1、未建立主键索引查询\",\"slug\":\"_1、未建立主键索引查询\",\"contents\":[\"执行\",\"select * from user where id=18 \",\"需要从第一条数据开始，一直查询到第6条，发现id=18，此时才能查询出目标结果，共需要比较6次；\"]},{\"header\":\"2、建立主键索引查询\",\"slug\":\"_2、建立主键索引查询\",\"contents\":[]},{\"header\":\"3、区间查询\",\"slug\":\"_3、区间查询\",\"contents\":[\"select * from user where id>=12 and id<=18 \",\"如果有了索引，由于B+树的叶子结点形成了一个有序链表，所以我们只需要找到id为12的叶子结点，按照遍历链表的方式顺序往后查即可，效率非常高。\"]}]},\"/study-tutorial/database/mysql/aggregate-function.html\":{\"title\":\"8、聚合函数\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"官网：[http://www.atguigu.com](http://www.atguigu.com我们上一章讲到了 SQL 单行函数。实际上 SQL 函数还有一类，叫做聚合（或聚集、分组）函数，它是对一组数据进行汇总的函数，输入的是一组数据的集合，输出的是单个值。\"]},{\"header\":\"1. 聚合函数介绍\",\"slug\":\"_1-聚合函数介绍\",\"contents\":[\"什么是聚合函数\",\"聚合函数作用于一组数据，并对一组数据返回一个值。\",\"聚合函数类型\",\"AVG()\",\"SUM()\",\"MAX()\",\"MIN()\",\"**COUNT() **\",\"聚合函数语法\",\"聚合函数不能嵌套调用。比如不能出现类似“AVG(SUM(字段名称))”形式的调用。\"]},{\"header\":\"1.1 AVG和SUM函数\",\"slug\":\"_1-1-avg和sum函数\",\"contents\":[\"可以对数值型数据使用AVG 和 SUM 函数。\",\"SELECT AVG(salary), MAX(salary),MIN(salary), SUM(salary) FROM employees WHERE job_id LIKE '%REP%'; \"]},{\"header\":\"1.2 MIN和MAX函数\",\"slug\":\"_1-2-min和max函数\",\"contents\":[\"可以对任意数据类型的数据使用 MIN 和 MAX 函数。\",\"SELECT MIN(hire_date), MAX(hire_date) FROM employees; \"]},{\"header\":\"1.3 COUNT函数\",\"slug\":\"_1-3-count函数\",\"contents\":[\"COUNT(*)返回表中记录总数，适用于任意数据类型。\",\"SELECT COUNT(*) FROM employees WHERE department_id = 50; \",\"COUNT(expr) 返回expr不为空的记录总数。\",\"SELECT COUNT(commission_pct) FROM employees WHERE department_id = 50; \",\"问题：用count(*)，count(1)，count(列名)谁好呢?\",\"其实，对于MyISAM引擎的表是没有区别的。这种引擎内部有一计数器在维护着行数。\",\"Innodb引擎的表用count(*),count(1)直接读行数，复杂度是O(n)，因为innodb真的要去数一遍。但好于具体的count(列名)。\",\"问题：能不能使用count(列名)替换count(*)?\",\"不要使用 count(列名)来替代 count(*)，count(*)是 SQL92 定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。\",\"说明：count(*)会统计值为 NULL 的行，而 count(列名)不会统计此列为 NULL 值的行。\"]},{\"header\":\"2. GROUP BY\",\"slug\":\"_2-group-by\",\"contents\":[]},{\"header\":\"2.1 基本使用\",\"slug\":\"_2-1-基本使用\",\"contents\":[\"可以使用GROUP BY子句将表中的数据分成若干组\",\"SELECT column, group_function(column) FROM table [WHERE condition] [GROUP BY group_by_expression] [ORDER BY column]; \",\"明确：WHERE一定放在FROM后面\",\"在SELECT列表中所有未包含在组函数中的列都应该包含在 GROUP BY子句中\",\"SELECT department_id, AVG(salary) FROM employees GROUP BY department_id ; \",\"包含在 GROUP BY 子句中的列不必包含在SELECT 列表中\",\"SELECT AVG(salary) FROM employees GROUP BY department_id ; \"]},{\"header\":\"2.2 使用多个列分组\",\"slug\":\"_2-2-使用多个列分组\",\"contents\":[\"SELECT department_id dept_id, job_id, SUM(salary) FROM employees GROUP BY department_id, job_id ; \"]},{\"header\":\"2.3 GROUP BY中使用WITH ROLLUP\",\"slug\":\"_2-3-group-by中使用with-rollup\",\"contents\":[\"使用WITH ROLLUP关键字之后，在所有查询出的分组记录之后增加一条记录，该记录计算查询出的所有记录的总和，即统计记录数量。\",\"SELECT department_id,AVG(salary) FROM employees WHERE department_id > 80 GROUP BY department_id WITH ROLLUP; \",\"注意：\",\"当使用ROLLUP时，不能同时使用ORDER BY子句进行结果排序，即ROLLUP和ORDER BY是互相排斥的。\"]},{\"header\":\"3. HAVING\",\"slug\":\"_3-having\",\"contents\":[]},{\"header\":\"3.1 基本使用\",\"slug\":\"_3-1-基本使用\",\"contents\":[\"过滤分组：HAVING子句\",\"行已经被分组。\",\"使用了聚合函数。\",\"满足HAVING 子句中条件的分组将被显示。\",\"HAVING 不能单独使用，必须要跟 GROUP BY 一起使用。\",\"SELECT department_id, MAX(salary) FROM employees GROUP BY department_id HAVING MAX(salary)>10000 ; \",\"**非法使用聚合函数 ： 不能在 WHERE 子句中使用聚合函数。**如下：\",\"SELECT department_id, AVG(salary) FROM employees WHERE AVG(salary) > 8000 GROUP BY department_id; \"]},{\"header\":\"3.2 WHERE和HAVING的对比\",\"slug\":\"_3-2-where和having的对比\",\"contents\":[\"区别1：WHERE 可以直接使用表中的字段作为筛选条件，但不能使用分组中的计算函数作为筛选条件；HAVING 必须要与 GROUP BY 配合使用，可以把分组计算的函数和分组字段作为筛选条件。\",\"这决定了，在需要对数据进行分组统计的时候，HAVING 可以完成 WHERE 不能完成的任务。这是因为，在查询语法结构中，WHERE 在 GROUP BY 之前，所以无法对分组结果进行筛选。HAVING 在 GROUP BY 之后，可以使用分组字段和分组中的计算函数，对分组的结果集进行筛选，这个功能是 WHERE 无法完成的。另外，WHERE排除的记录不再包括在分组中。\",\"区别2：如果需要通过连接从关联表中获取需要的数据，WHERE 是先筛选后连接，而 HAVING 是先连接后筛选。 这一点，就决定了在关联查询中，WHERE 比 HAVING 更高效。因为 WHERE 可以先筛选，用一个筛选后的较小数据集和关联表进行连接，这样占用的资源比较少，执行效率也比较高。HAVING 则需要先把结果集准备好，也就是用未被筛选的数据集进行关联，然后对这个大的数据集进行筛选，这样占用的资源就比较多，执行效率也较低。\",\"小结如下：\",\"优点\",\"缺点\",\"WHERE\",\"先筛选数据再关联，执行效率高\",\"不能使用分组中的计算函数进行筛选\",\"HAVING\",\"可以使用分组中的计算函数\",\"在最后的结果集中进行筛选，执行效率较低\",\"开发中的选择：\",\"WHERE 和 HAVING 也不是互相排斥的，我们可以在一个查询里面同时使用 WHERE 和 HAVING。包含分组统计函数的条件用 HAVING，普通条件用 WHERE。这样，我们就既利用了 WHERE 条件的高效快速，又发挥了 HAVING 可以使用包含分组统计函数的查询条件的优点。当数据量特别大的时候，运行效率会有很大的差别。\"]},{\"header\":\"4. SELECT的执行过程\",\"slug\":\"_4-select的执行过程\",\"contents\":[]},{\"header\":\"4.1 查询的结构\",\"slug\":\"_4-1-查询的结构\",\"contents\":[\"#方式1： SELECT ...,....,... FROM ...,...,.... WHERE 多表的连接条件 AND 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #方式2： SELECT ...,....,... FROM ... JOIN ... ON 多表的连接条件 JOIN ... ON ... WHERE 不包含组函数的过滤条件 AND/OR 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #其中： #（1）from：从哪些表中筛选 #（2）on：关联多表查询时，去除笛卡尔积 #（3）where：从表中筛选的条件 #（4）group by：分组依据 #（5）having：在统计结果中再次筛选 #（6）order by：排序 #（7）limit：分页 \"]},{\"header\":\"4.2 SELECT执行顺序\",\"slug\":\"_4-2-select执行顺序\",\"contents\":[\"你需要记住 SELECT 查询时的两个顺序：\",\"1. 关键字的顺序是不能颠倒的：\",\"SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT... \",\"2.SELECT 语句的执行顺序（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）：\",\"FROM -> WHERE -> GROUP BY -> HAVING -> SELECT 的字段 -> DISTINCT -> ORDER BY -> LIMIT \",\"比如你写了一个 SQL 语句，那么它的关键字顺序和执行顺序是下面这样的：\",\"SELECT DISTINCT player_id, player_name, count(*) as num ## 顺序 5 FROM player JOIN team ON player.team_id = team.team_id ## 顺序 1 WHERE height > 1.80 ## 顺序 2 GROUP BY player.team_id ## 顺序 3 HAVING num > 2 ## 顺序 4 ORDER BY num DESC ## 顺序 6 LIMIT 2 ## 顺序 7 \",\"在 SELECT 语句执行这些步骤的时候，每个步骤都会产生一个虚拟表，然后将这个虚拟表传入下一个步骤中作为输入。需要注意的是，这些步骤隐含在 SQL 的执行过程中，对于我们来说是不可见的。\"]},{\"header\":\"4.3 SQL 的执行原理\",\"slug\":\"_4-3-sql-的执行原理\",\"contents\":[\"SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤：\",\"首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1；\",\"通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2；\",\"添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。\",\"当然如果我们操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得到是我们的原始数据。\",\"当我们拿到了查询数据表的原始数据，也就是最终的虚拟表 vt1，就可以在此基础上再进行 WHERE 阶段。在这个阶段中，会根据 vt1 表的结果进行筛选过滤，得到虚拟表 vt2。\",\"然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段。在这个阶段中，实际上是在虚拟表 vt2 的基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4。\",\"当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段。\",\"首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2。\",\"当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段，得到虚拟表 vt6。\",\"最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段，得到最终的结果，对应的是虚拟表 vt7。\",\"当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。\",\"同时因为 SQL 是一门类似英语的结构化查询语言，所以我们在写 SELECT 语句的时候，还要注意相应的关键字顺序，所谓底层运行的原理，就是我们刚才讲到的执行顺序。\"]}]},\"/study-tutorial/database/mysql/basic-select.html\":{\"title\":\"3、基本的SELECT语句\",\"contents\":[{\"header\":\"1. SQL概述\",\"slug\":\"_1-sql概述\",\"contents\":[]},{\"header\":\"1.1 SQL背景知识\",\"slug\":\"_1-1-sql背景知识\",\"contents\":[\"1946 年，世界上第一台电脑诞生，如今，借由这台电脑发展起来的互联网已经自成江湖。在这几十年里，无数的技术、产业在这片江湖里沉浮，有的方兴未艾，有的已经几幕兴衰。但在这片浩荡的波动里，有一门技术从未消失，甚至“老当益壮”，那就是 SQL。\",\"45 年前，也就是 1974 年，IBM 研究员发布了一篇揭开数据库技术的论文《SEQUEL：一门结构化的英语查询语言》，直到今天这门结构化的查询语言并没有太大的变化，相比于其他语言，SQL 的半衰期可以说是非常长了。\",\"不论是前端工程师，还是后端算法工程师，都一定会和数据打交道，都需要了解如何又快又准确地提取自己想要的数据。更别提数据分析师了，他们的工作就是和数据打交道，整理不同的报告，以便指导业务决策。\",\"SQL（Structured Query Language，结构化查询语言）是使用关系模型的数据库应用语言，与数据直接打交道，由IBM上世纪70年代开发出来。后由美国国家标准局（ANSI）开始着手制定SQL标准，先后有SQL-86，SQL-89，SQL-92，SQL-99等标准。\",\"SQL 有两个重要的标准，分别是 SQL92 和 SQL99，它们分别代表了 92 年和 99 年颁布的 SQL 标准，我们今天使用的 SQL 语言依然遵循这些标准。\",\"不同的数据库生产厂商都支持SQL语句，但都有特有内容。\"]},{\"header\":\"1.2 SQL语言排行榜\",\"slug\":\"_1-2-sql语言排行榜\",\"contents\":[\"自从 SQL 加入了 TIOBE 编程语言排行榜，就一直保持在 Top 10。\"]},{\"header\":\"1.3 SQL 分类\",\"slug\":\"_1-3-sql-分类\",\"contents\":[\"SQL语言在功能上主要分为如下3大类：\",\"DDL（Data Definition Languages、数据定义语言），这些语句定义了不同的数据库、表、视图、索引等数据库对象，还可以用来创建、删除、修改数据库和数据表的结构。\",\"主要的语句关键字包括CREATE、DROP、ALTER等。\",\"DML（Data Manipulation Language、数据操作语言），用于添加、删除、更新和查询数据库记录，并检查数据完整性。\",\"主要的语句关键字包括INSERT、DELETE、UPDATE、SELECT等。\",\"SELECT是SQL语言的基础，最为重要。\",\"DCL（Data Control Language、数据控制语言），用于定义数据库、表、字段、用户的访问权限和安全级别。\",\"主要的语句关键字包括GRANT、REVOKE、COMMIT、ROLLBACK、SAVEPOINT等。\",\"因为查询语句使用的非常的频繁，所以很多人把查询语句单拎出来一类：DQL（数据查询语言）。\",\"还有单独将COMMIT、ROLLBACK 取出来称为TCL （Transaction Control Language，事务控制语言）。\"]},{\"header\":\"2. SQL语言的规则与规范\",\"slug\":\"_2-sql语言的规则与规范\",\"contents\":[]},{\"header\":\"2.1 基本规则\",\"slug\":\"_2-1-基本规则\",\"contents\":[\"SQL 可以写在一行或者多行。为了提高可读性，各子句分行写，必要时使用缩进\",\"每条命令以 ; 或 \\\\g 或 \\\\G 结束\",\"关键字不能被缩写也不能分行\",\"关于标点符号 \",\"必须保证所有的()、单引号、双引号是成对结束的\",\"必须使用英文状态下的半角输入方式\",\"字符串型和日期时间类型的数据可以使用单引号（' '）表示\",\"列的别名，尽量使用双引号（\\\" \\\"），而且不建议省略as\"]},{\"header\":\"2.2 SQL大小写规范 （建议遵守）\",\"slug\":\"_2-2-sql大小写规范-建议遵守\",\"contents\":[\"MySQL 在 Windows 环境下是大小写不敏感的\",\"MySQL 在 Linux 环境下是大小写敏感的\",\"数据库名、表名、表的别名、变量名是严格区分大小写的\",\"关键字、函数名、列名(或字段名)、列的别名(字段的别名) 是忽略大小写的。\",\"推荐采用统一的书写规范：\",\"数据库名、表名、表别名、字段名、字段别名等都小写\",\"SQL 关键字、函数名、绑定变量等都大写\"]},{\"header\":\"2.3 注 释\",\"slug\":\"_2-3-注-释\",\"contents\":[\"可以使用如下格式的注释结构\",\"单行注释：#注释文字(MySQL特有的方式) 单行注释：-- 注释文字(--后面必须包含一个空格。) 多行注释：/* 注释文字 */ \"]},{\"header\":\"2.4 命名规则（暂时了解）\",\"slug\":\"_2-4-命名规则-暂时了解\",\"contents\":[\"数据库、表名不得超过30个字符，变量名限制为29个\",\"必须只能包含 A–Z, a–z, 0–9, _共63个字符\",\"数据库名、表名、字段名等对象名中间不要包含空格\",\"同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名\",\"必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使用`（着重号）引起来\",\"保持字段名和类型的一致性，在命名字段并为其指定数据类型的时候一定要保证一致性。假如数据类型在一个表里是整数，那在另一个表里可就别变成字符型了\",\"举例：\",\"#以下两句是一样的，不区分大小写 show databases; SHOW DATABASES; #创建表格 #create table student info(...); #表名错误，因为表名有空格 create table student_info(...); #其中order使用``飘号，因为order和系统关键字或系统函数名等预定义标识符重名了 CREATE TABLE `order`( id INT, lname VARCHAR(20) ); select id as \\\"编号\\\", `name` as \\\"姓名\\\" from t_stu; #起别名时，as都可以省略 select id as 编号, `name` as 姓名 from t_stu; #如果字段别名中没有空格，那么可以省略\\\"\\\" select id as 编 号, `name` as 姓 名 from t_stu; #错误，如果字段别名中有空格，那么不能省略\\\"\\\" \"]},{\"header\":\"2.5 数据导入指令\",\"slug\":\"_2-5-数据导入指令\",\"contents\":[\"在命令行客户端登录mysql，使用source指令导入\",\"mysql> source d:\\\\mysqldb.sql \",\"mysql> desc employees; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | employee_id | int(6) | NO | PRI | 0 | | | first_name | varchar(20) | YES | | NULL | | | last_name | varchar(25) | NO | | NULL | | | email | varchar(25) | NO | UNI | NULL | | | phone_number | varchar(20) | YES | | NULL | | | hire_date | date | NO | | NULL | | | job_id | varchar(10) | NO | MUL | NULL | | | salary | double(8,2) | YES | | NULL | | | commission_pct | double(2,2) | YES | | NULL | | | manager_id | int(6) | YES | MUL | NULL | | | department_id | int(4) | YES | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 11 rows in set (0.00 sec) \"]},{\"header\":\"3. 基本的SELECT语句\",\"slug\":\"_3-基本的select语句\",\"contents\":[]},{\"header\":\"3.0 SELECT...\",\"slug\":\"_3-0-select\",\"contents\":[\"SELECT 1; #没有任何子句 SELECT 9/2; #没有任何子句 \"]},{\"header\":\"3.1 SELECT ... FROM\",\"slug\":\"_3-1-select-from\",\"contents\":[\"语法：\",\"SELECT 标识选择哪些列 FROM 标识从哪个表中选择 \",\"选择全部列：\",\"SELECT * FROM departments; \",\"一般情况下，除非需要使用表中所有的字段数据，最好不要使用通配符‘*’。使用通配符虽然可以节省输入查询语句的时间，但是获取不需要的列数据通常会降低查询和所使用的应用程序的效率。通配符的优势是，当不知道所需要的列的名称时，可以通过它获取它们。\",\"在生产环境下，不推荐你直接使用SELECT *进行查询。\",\"选择特定的列：\",\"SELECT department_id, location_id FROM departments; \",\"MySQL中的SQL语句是不区分大小写的，因此SELECT和select的作用是相同的，但是，许多开发人员习惯将关键字大写、数据列和表名小写，读者也应该养成一个良好的编程习惯，这样写出来的代码更容易阅读和维护。\"]},{\"header\":\"3.2 列的别名\",\"slug\":\"_3-2-列的别名\",\"contents\":[\"重命名一个列\",\"便于计算\",\"紧跟列名，也可以在列名和别名之间加入关键字AS，别名使用双引号，以便在别名中包含空格或特殊的字符并区分大小写。\",\"AS 可以省略\",\"建议别名简短，见名知意\",\"举例\",\"SELECT last_name AS name, commission_pct comm FROM employees; \",\"SELECT last_name \\\"Name\\\", salary*12 \\\"Annual Salary\\\" FROM employees; \"]},{\"header\":\"3.3 去除重复行\",\"slug\":\"_3-3-去除重复行\",\"contents\":[\"默认情况下，查询会返回全部行，包括重复行。\",\"SELECT department_id FROM employees; \",\"在SELECT语句中使用关键字DISTINCT去除重复行\",\"SELECT DISTINCT department_id FROM employees; \",\"针对于：\",\"SELECT DISTINCT department_id,salary FROM employees; \",\"这里有两点需要注意：\",\"DISTINCT 需要放到所有列名的前面，如果写成SELECT salary, DISTINCT department_id FROM employees会报错。\",\"DISTINCT 其实是对后面所有列名的组合进行去重，你能看到最后的结果是 74 条，因为这 74 个部门id不同，都有 salary 这个属性值。如果你想要看都有哪些不同的部门（department_id），只需要写DISTINCT department_id即可，后面不需要再加其他的列名了。\"]},{\"header\":\"3.4 空值参与运算\",\"slug\":\"_3-4-空值参与运算\",\"contents\":[\"所有运算符或列值遇到null值，运算的结果都为null\",\"SELECT employee_id,salary,commission_pct, 12 * salary * (1 + commission_pct) \\\"annual_sal\\\" FROM employees; \",\"这里你一定要注意，在 MySQL 里面， 空值不等于空字符串。一个空字符串的长度是 0，而一个空值的长度是空。而且，在 MySQL 里面，空值是占用空间的。\"]},{\"header\":\"3.5 着重号\",\"slug\":\"_3-5-着重号\",\"contents\":[\"错误的\",\"mysql> SELECT * FROM ORDER; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ORDER' at line 1 \",\"正确的\",\"mysql> SELECT * FROM `ORDER`; +----------+------------+ | order_id | order_name | +----------+------------+ | 1 | shkstart | | 2 | tomcat | | 3 | dubbo | +----------+------------+ 3 rows in set (0.00 sec) mysql> SELECT * FROM `order`; +----------+------------+ | order_id | order_name | +----------+------------+ | 1 | shkstart | | 2 | tomcat | | 3 | dubbo | +----------+------------+ 3 rows in set (0.00 sec) \",\"结论\",\"我们需要保证表中的字段、表名等没有和保留字、数据库系统或常用方法冲突。如果真的相同，请在SQL语句中使用一对``（着重号）引起来。\"]},{\"header\":\"3.6 5、查询常数\",\"slug\":\"_3-6-5、查询常数\",\"contents\":[\"SELECT 查询还可以对常数进行查询。对的，就是在 SELECT 查询结果中增加一列固定的常数列。这列的取值是我们指定的，而不是从数据表中动态取出的。\",\"你可能会问为什么我们还要对常数进行查询呢？\",\"SQL 中的 SELECT 语法的确提供了这个功能，一般来说我们只从一个表中查询数据，通常不需要增加一个固定的常数列，但如果我们想整合不同的数据源，用常数列作为这个表的标记，就需要查询常数。\",\"比如说，我们想对 employees 数据表中的员工姓名进行查询，同时增加一列字段corporation，这个字段固定值为“尚硅谷”，可以这样写：\",\"SELECT '尚硅谷' as corporation, last_name FROM employees; \"]},{\"header\":\"4. 显示表结构\",\"slug\":\"_4-显示表结构\",\"contents\":[\"使用DESCRIBE 或 DESC 命令，表示表结构。\",\"DESCRIBE employees; 或 DESC employees; \",\"mysql> desc employees; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | employee_id | int(6) | NO | PRI | 0 | | | first_name | varchar(20) | YES | | NULL | | | last_name | varchar(25) | NO | | NULL | | | email | varchar(25) | NO | UNI | NULL | | | phone_number | varchar(20) | YES | | NULL | | | hire_date | date | NO | | NULL | | | job_id | varchar(10) | NO | MUL | NULL | | | salary | double(8,2) | YES | | NULL | | | commission_pct | double(2,2) | YES | | NULL | | | manager_id | int(6) | YES | MUL | NULL | | | department_id | int(4) | YES | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 11 rows in set (0.00 sec) \",\"其中，各个字段的含义分别解释如下：\",\"Field：表示字段名称。\",\"Type：表示字段类型，这里 barcode、goodsname 是文本型的，price 是整数类型的。\",\"Null：表示该列是否可以存储NULL值。\",\"Key：表示该列是否已编制索引。PRI表示该列是表主键的一部分；UNI表示该列是UNIQUE索引的一部分；MUL表示在列中某个给定值允许出现多次。\",\"Default：表示该列是否有默认值，如果有，那么值是多少。\",\"Extra：表示可以获取的与给定列有关的附加信息，例如AUTO_INCREMENT等。\"]},{\"header\":\"5. 过滤数据\",\"slug\":\"_5-过滤数据\",\"contents\":[\"背景：\",\"语法：\",\"SELECT 字段1,字段2 FROM 表名 WHERE 过滤条件 \",\"使用WHERE 子句，将不满足条件的行过滤掉\",\"WHERE子句紧随 FROM子句\",\"举例\",\"SELECT employee_id, last_name, job_id, department_id FROM employees WHERE department_id = 90 ; \"]}]},\"/study-tutorial/database/mysql/constraint.html\":{\"title\":\"13、约束\",\"contents\":[{\"header\":\"1.约束(constraint)概述\",\"slug\":\"_1-约束-constraint-概述\",\"contents\":[]},{\"header\":\"1.1 为什么需要约束\",\"slug\":\"_1-1-为什么需要约束\",\"contents\":[\"数据完整性（Data Integrity）是指数据的精确性（Accuracy）和可靠性（Reliability）。它是防止数据库中存在不符合语义规定的数据和防止因错误信息的输入输出造成无效操作或错误信息而提出的。\",\"为了保证数据的完整性，SQL规范以约束的方式对表数据进行额外的条件限制。从以下四个方面考虑：\",\"实体完整性（Entity Integrity）：例如，同一个表中，不能存在两条完全相同无法区分的记录\",\"域完整性（Domain Integrity）：例如：年龄范围0-120，性别范围“男/女”\",\"引用完整性（Referential Integrity）：例如：员工所在部门，在部门表中要能找到这个部门\",\"用户自定义完整性（User-defined Integrity）：例如：用户名唯一、密码不能为空等，本部门经理的工资不得高于本部门职工的平均工资的5倍。\"]},{\"header\":\"1.2 什么是约束\",\"slug\":\"_1-2-什么是约束\",\"contents\":[\"约束是表级的强制规定。\",\"可以在创建表时规定约束（通过 CREATE TABLE 语句），或者在表创建之后通过 ALTER TABLE 语句规定约束。\"]},{\"header\":\"1.3 约束的分类\",\"slug\":\"_1-3-约束的分类\",\"contents\":[\"**根据约束数据列的限制，**约束可分为： \",\"单列约束：每个约束只约束一列\",\"多列约束：每个约束可约束多列数据\",\"根据约束的作用范围，约束可分为： \",\"列级约束：只能作用在一个列上，跟在列的定义后面\",\"表级约束：可以作用在多个列上，不与列一起，而是单独定义\",\" 位置 支持的约束类型 是否可以起约束名 列级约束： 列的后面 语法都支持，但外键没有效果 不可以 表级约束： 所有列的下面 默认和非空不支持，其他支持 可以（主键没有效果） \",\"根据约束起的作用，约束可分为： \",\"NOT NULL非空约束，规定某个字段不能为空\",\"UNIQUE唯一约束，规定某个字段在整个表中是唯一的\",\"PRIMARY KEY 主键(非空且唯一)约束\",\"FOREIGN KEY外键约束\",\"CHECK检查约束\",\"DEFAULT默认值约束\",\"注意： MySQL不支持check约束，但可以使用check约束，而没有任何效果\",\"查看某个表已有的约束\",\"#information_schema数据库名（系统库） #table_constraints表名称（专门存储各个表的约束） SELECT * FROM information_schema.table_constraints WHERE table_name = '表名称'; \"]},{\"header\":\"2. 非空约束\",\"slug\":\"_2-非空约束\",\"contents\":[]},{\"header\":\"2.1 作用\",\"slug\":\"_2-1-作用\",\"contents\":[\"限定某个字段/某列的值不允许为空\"]},{\"header\":\"2.2 关键字\",\"slug\":\"_2-2-关键字\",\"contents\":[\"NOT NULL\"]},{\"header\":\"2.3 特点\",\"slug\":\"_2-3-特点\",\"contents\":[\"默认，所有的类型的值都可以是NULL，包括INT、FLOAT等数据类型\",\"非空约束只能出现在表对象的列上，只能某个列单独限定非空，不能组合非空\",\"一个表可以有很多列都分别限定了非空\",\"空字符串''不等于NULL，0也不等于NULL\"]},{\"header\":\"2.4 添加非空约束\",\"slug\":\"_2-4-添加非空约束\",\"contents\":[\"（1）建表时\",\"CREATE TABLE 表名称( 字段名 数据类型, 字段名 数据类型 NOT NULL, 字段名 数据类型 NOT NULL ); \",\"举例：\",\"CREATE TABLE emp( id INT(10) NOT NULL, NAME VARCHAR(20) NOT NULL, sex CHAR NULL ); \",\"CREATE TABLE student( sid int, sname varchar(20) not null, tel char(11) , cardid char(18) not null ); \",\"insert into student values(1,'张三','13710011002','110222198912032545'); #成功 insert into student values(2,'李四','13710011002',null);#身份证号为空 ERROR 1048 (23000): Column 'cardid' cannot be null insert into student values(2,'李四',null,'110222198912032546');#成功，tel允许为空 insert into student values(3,null,null,'110222198912032547');#失败 ERROR 1048 (23000): Column 'sname' cannot be null \",\"（2）建表后\",\"alter table 表名称 modify 字段名 数据类型 not null; \",\"举例：\",\"ALTER TABLE emp MODIFY sex VARCHAR(30) NOT NULL; \",\"alter table student modify sname varchar(20) not null; \"]},{\"header\":\"2.5 删除非空约束\",\"slug\":\"_2-5-删除非空约束\",\"contents\":[\"alter table 表名称 modify 字段名 数据类型 NULL;#去掉not null，相当于修改某个非注解字段，该字段允许为空 或 alter table 表名称 modify 字段名 数据类型;#去掉not null，相当于修改某个非注解字段，该字段允许为空 \",\"举例：\",\"ALTER TABLE emp MODIFY sex VARCHAR(30) NULL; \",\"ALTER TABLE emp MODIFY NAME VARCHAR(15) DEFAULT 'abc' NULL; \"]},{\"header\":\"3. 唯一性约束\",\"slug\":\"_3-唯一性约束\",\"contents\":[]},{\"header\":\"3.1 作用\",\"slug\":\"_3-1-作用\",\"contents\":[\"用来限制某个字段/某列的值不能重复。\"]},{\"header\":\"3.2 关键字\",\"slug\":\"_3-2-关键字\",\"contents\":[\"UNIQUE\"]},{\"header\":\"3.3 特点\",\"slug\":\"_3-3-特点\",\"contents\":[\"同一个表可以有多个唯一约束。\",\"唯一约束可以是某一个列的值唯一，也可以多个列组合的值唯一。\",\"唯一性约束允许列值为空。\",\"在创建唯一约束的时候，如果不给唯一约束命名，就默认和列名相同。\",\"MySQL会给唯一约束的列上默认创建一个唯一索引。\"]},{\"header\":\"3.4 添加唯一约束\",\"slug\":\"_3-4-添加唯一约束\",\"contents\":[\"（1）建表时\",\"create table 表名称( 字段名 数据类型, 字段名 数据类型 unique, 字段名 数据类型 unique key, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] unique key(字段名) ); \",\"举例：\",\"create table student( sid int, sname varchar(20), tel char(11) unique, cardid char(18) unique key ); \",\"CREATE TABLE t_course( cid INT UNIQUE, cname VARCHAR(100) UNIQUE, description VARCHAR(200) ); \",\"CREATE TABLE USER( id INT NOT NULL, NAME VARCHAR(25), PASSWORD VARCHAR(16), -- 使用表级约束语法 CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD) ); \",\"表示用户名和密码组合不能重复\",\"insert into student values(1,'张三','13710011002','101223199012015623'); insert into student values(2,'李四','13710011003','101223199012015624'); \",\"mysql> select * from student; +-----+-------+-------------+--------------------+ | sid | sname | tel | cardid | +-----+-------+-------------+--------------------+ | 1 | 张三 | 13710011002 | 101223199012015623 | | 2 | 李四 | 13710011003 | 101223199012015624 | +-----+-------+-------------+--------------------+ 2 rows in set (0.00 sec) \",\"insert into student values(3,'王五','13710011004','101223199012015624'); #身份证号重复 ERROR 1062 (23000): Duplicate entry '101223199012015624' for key 'cardid' insert into student values(3,'王五','13710011003','101223199012015625'); ERROR 1062 (23000): Duplicate entry '13710011003' for key 'tel' \",\"（2）建表后指定唯一键约束\",\"#字段列表中如果是一个字段，表示该列的值唯一。如果是两个或更多个字段，那么复合唯一，即多个字段的组合是唯一的 #方式1： alter table 表名称 add unique key(字段列表); \",\"#方式2： alter table 表名称 modify 字段名 字段类型 unique; \",\"举例：\",\"ALTER TABLE USER ADD UNIQUE(NAME,PASSWORD); \",\"ALTER TABLE USER ADD CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD); \",\"ALTER TABLE USER MODIFY NAME VARCHAR(20) UNIQUE; \",\"举例：\",\"create table student( sid int primary key, sname varchar(20), tel char(11) , cardid char(18) ); \",\"alter table student add unique key(tel); alter table student add unique key(cardid); \"]},{\"header\":\"3.5 关于复合唯一约束\",\"slug\":\"_3-5-关于复合唯一约束\",\"contents\":[\"create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, unique key(字段列表) #字段列表中写的是多个字段名，多个字段名用逗号分隔，表示那么是复合唯一，即多个字段的组合是唯一的 ); \",\"#学生表 create table student( sid int, #学号 sname varchar(20), #姓名 tel char(11) unique key, #电话 cardid char(18) unique key #身份证号 ); #课程表 create table course( cid int, #课程编号 cname varchar(20) #课程名称 ); #选课表 create table student_course( id int, sid int, cid int, score int, unique key(sid,cid) #复合唯一 ); \",\"insert into student values(1,'张三','13710011002','101223199012015623');#成功 insert into student values(2,'李四','13710011003','101223199012015624');#成功 insert into course values(1001,'Java'),(1002,'MySQL');#成功 \",\"mysql> select * from student; +-----+-------+-------------+--------------------+ | sid | sname | tel | cardid | +-----+-------+-------------+--------------------+ | 1 | 张三 | 13710011002 | 101223199012015623 | | 2 | 李四 | 13710011003 | 101223199012015624 | +-----+-------+-------------+--------------------+ 2 rows in set (0.00 sec) mysql> select * from course; +------+-------+ | cid | cname | +------+-------+ | 1001 | Java | | 1002 | MySQL | +------+-------+ 2 rows in set (0.00 sec) \",\"insert into student_course values (1, 1, 1001, 89), (2, 1, 1002, 90), (3, 2, 1001, 88), (4, 2, 1002, 56);#成功 \",\"mysql> select * from student_course; +----+------+------+-------+ | id | sid | cid | score | +----+------+------+-------+ | 1 | 1 | 1001 | 89 | | 2 | 1 | 1002 | 90 | | 3 | 2 | 1001 | 88 | | 4 | 2 | 1002 | 56 | +----+------+------+-------+ 4 rows in set (0.00 sec) \",\"insert into student_course values (5, 1, 1001, 88);#失败 #ERROR 1062 (23000): Duplicate entry '1-1001' for key 'sid' 违反sid-cid的复合唯一 \"]},{\"header\":\"3.5 删除唯一约束\",\"slug\":\"_3-5-删除唯一约束\",\"contents\":[\"添加唯一性约束的列上也会自动创建唯一索引。\",\"删除唯一约束只能通过删除唯一索引的方式删除。\",\"删除时需要指定唯一索引名，唯一索引名就和唯一约束名一样。\",\"如果创建唯一约束时未指定名称，如果是单列，就默认和列名相同；如果是组合列，那么默认和()中排在第一个的列名相同。也可以自定义唯一性约束名。\",\"SELECT * FROM information_schema.table_constraints WHERE table_name = '表名'; #查看都有哪些约束 \",\"ALTER TABLE USER DROP INDEX uk_name_pwd; \",\"注意：可以通过 show index from 表名称; 查看表的索引\"]},{\"header\":\"4. PRIMARY KEY 约束\",\"slug\":\"_4-primary-key-约束\",\"contents\":[]},{\"header\":\"4.1 作用\",\"slug\":\"_4-1-作用\",\"contents\":[\"用来唯一标识表中的一行记录。\"]},{\"header\":\"4.2 关键字\",\"slug\":\"_4-2-关键字\",\"contents\":[\"primary key\"]},{\"header\":\"4.3 特点\",\"slug\":\"_4-3-特点\",\"contents\":[\"主键约束相当于唯一约束+非空约束的组合，主键约束列不允许重复，也不允许出现空值。\",\"一个表最多只能有一个主键约束，建立主键约束可以在列级别创建，也可以在表级别上创建。\",\"主键约束对应着表中的一列或者多列（复合主键）\",\"如果是多列组合的复合主键约束，那么这些列都不允许为空值，并且组合的值不允许重复。\",\"MySQL的主键名总是PRIMARY，就算自己命名了主键约束名也没用。\",\"当创建主键约束时，系统默认会在所在的列或列组合上建立对应的主键索引（能够根据主键查询的，就根据主键查询，效率更高）。如果删除主键约束了，主键约束对应的索引就自动删除了。\",\"需要注意的一点是，不要修改主键字段的值。因为主键是数据记录的唯一标识，如果修改了主键的值，就有可能会破坏数据的完整性。\"]},{\"header\":\"4.4 添加主键约束\",\"slug\":\"_4-4-添加主键约束\",\"contents\":[\"（1）建表时指定主键约束\",\"create table 表名称( 字段名 数据类型 primary key, #列级模式 字段名 数据类型, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] primary key(字段名) #表级模式 ); \",\"举例：\",\"create table temp( id int primary key, name varchar(20) ); \",\"mysql> desc temp; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | name | varchar(20) | YES | | NULL | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) \",\"insert into temp values(1,'张三');#成功 insert into temp values(2,'李四');#成功 \",\"mysql> select * from temp; +----+------+ | id | name | +----+------+ | 1 | 张三 | | 2 | 李四 | +----+------+ 2 rows in set (0.00 sec) \",\"insert into temp values(1,'张三');#失败 ERROR 1062 (23000): Duplicate（重复） entry（键入，输入） '1' for key 'PRIMARY' insert into temp values(1,'王五');#失败 ERROR 1062 (23000): Duplicate entry '1' for key 'PRIMARY' insert into temp values(3,'张三');#成功 \",\"mysql> select * from temp; +----+------+ | id | name | +----+------+ | 1 | 张三 | | 2 | 李四 | | 3 | 张三 | +----+------+ 3 rows in set (0.00 sec) \",\"insert into temp values(4,null);#成功 insert into temp values(null,'李琦');#失败 ERROR 1048 (23000): Column 'id' cannot be null \",\"mysql> select * from temp; +----+------+ | id | name | +----+------+ | 1 | 张三 | | 2 | 李四 | | 3 | 张三 | | 4 | NULL | +----+------+ 4 rows in set (0.00 sec) \",\"#演示一个表建立两个主键约束 create table temp( id int primary key, name varchar(20) primary key ); ERROR 1068 (42000): Multiple（多重的） primary key defined（定义） \",\"再举例：\",\"列级约束\",\"CREATE TABLE emp4( id INT PRIMARY KEY AUTO_INCREMENT , NAME VARCHAR(20) ); \",\"表级约束\",\"CREATE TABLE emp5( id INT NOT NULL AUTO_INCREMENT, NAME VARCHAR(20), pwd VARCHAR(15), CONSTRAINT emp5_id_pk PRIMARY KEY(id) ); \",\"（2）建表后增加主键约束\",\"ALTER TABLE 表名称 ADD PRIMARY KEY(字段列表); #字段列表可以是一个字段，也可以是多个字段，如果是多个字段的话，是复合主键 \",\"ALTER TABLE student ADD PRIMARY KEY (sid); \",\"ALTER TABLE emp5 ADD PRIMARY KEY(NAME,pwd); \"]},{\"header\":\"4.5 关于复合主键\",\"slug\":\"_4-5-关于复合主键\",\"contents\":[\"create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, primary key(字段名1,字段名2) #表示字段1和字段2的组合是唯一的，也可以有更多个字段 ); \",\"#学生表 create table student( sid int primary key, #学号 sname varchar(20) #学生姓名 ); #课程表 create table course( cid int primary key, #课程编号 cname varchar(20) #课程名称 ); #选课表 create table student_course( sid int, cid int, score int, primary key(sid,cid) #复合主键 ); \",\"insert into student values(1,'张三'),(2,'李四'); insert into course values(1001,'Java'),(1002,'MySQL'); \",\"mysql> select * from student; +-----+-------+ | sid | sname | +-----+-------+ | 1 | 张三 | | 2 | 李四 | +-----+-------+ 2 rows in set (0.00 sec) mysql> select * from course; +------+-------+ | cid | cname | +------+-------+ | 1001 | Java | | 1002 | MySQL | +------+-------+ 2 rows in set (0.00 sec) \",\"insert into student_course values(1, 1001, 89),(1,1002,90),(2,1001,88),(2,1002,56); \",\"mysql> select * from student_course; +-----+------+-------+ | sid | cid | score | +-----+------+-------+ | 1 | 1001 | 89 | | 1 | 1002 | 90 | | 2 | 1001 | 88 | | 2 | 1002 | 56 | +-----+------+-------+ 4 rows in set (0.00 sec) \",\"insert into student_course values(1, 1001, 100); ERROR 1062 (23000): Duplicate entry '1-1001' for key 'PRIMARY' \",\"mysql> desc student_course; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | sid | int(11) | NO | PRI | NULL | | | cid | int(11) | NO | PRI | NULL | | | score | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 3 rows in set (0.00 sec) \",\"再举例\",\"CREATE TABLE emp6( id INT NOT NULL, NAME VARCHAR(20), pwd VARCHAR(15), CONSTRAINT emp7_pk PRIMARY KEY(NAME,pwd) ); \"]},{\"header\":\"4.6 删除主键约束\",\"slug\":\"_4-6-删除主键约束\",\"contents\":[\"alter table 表名称 drop primary key; \",\"举例：\",\"ALTER TABLE student DROP PRIMARY KEY; \",\"ALTER TABLE emp5 DROP PRIMARY KEY; \",\"说明：删除主键约束，不需要指定主键名，因为一个表只有一个主键，删除主键约束后，非空还存在。\"]},{\"header\":\"5. 自增列：AUTO_INCREMENT\",\"slug\":\"_5-自增列-auto-increment\",\"contents\":[]},{\"header\":\"5.1 作用\",\"slug\":\"_5-1-作用\",\"contents\":[\"某个字段的值自增\"]},{\"header\":\"5.2 关键字\",\"slug\":\"_5-2-关键字\",\"contents\":[\"auto_increment\"]},{\"header\":\"5.3 特点和要求\",\"slug\":\"_5-3-特点和要求\",\"contents\":[\"（1）一个表最多只能有一个自增长列\",\"（2）当需要产生唯一标识符或顺序值时，可设置自增长\",\"（3）自增长列约束的列必须是键列（主键列，唯一键列）\",\"（4）自增约束的列的数据类型必须是整数类型\",\"（5）如果自增列指定了 0 和 null，会在当前最大值的基础上自增；如果自增列手动指定了具体值，直接赋值为具体值。\",\"错误演示：\",\"create table employee( eid int auto_increment, ename varchar(20) ); ## ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key \",\"create table employee( eid int primary key, ename varchar(20) unique key auto_increment ); ## ERROR 1063 (42000): Incorrect column specifier for column 'ename' 因为ename不是整数类型 \"]},{\"header\":\"5.4 如何指定自增约束\",\"slug\":\"_5-4-如何指定自增约束\",\"contents\":[\"（1）建表时\",\"create table 表名称( 字段名 数据类型 primary key auto_increment, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); create table 表名称( 字段名 数据类型 default 默认值 , 字段名 数据类型 unique key auto_increment, 字段名 数据类型 not null default 默认值,, primary key(字段名) ); \",\"create table employee( eid int primary key auto_increment, ename varchar(20) ); \",\"mysql> desc employee; +-------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+----------------+ | eid | int(11) | NO | PRI | NULL | auto_increment | | ename | varchar(20) | YES | | NULL | | +-------+-------------+------+-----+---------+----------------+ 2 rows in set (0.00 sec) \",\"（2）建表后\",\"alter table 表名称 modify 字段名 数据类型 auto_increment; \",\"例如：\",\"create table employee( eid int primary key , ename varchar(20) ); \",\"alter table employee modify eid int auto_increment; \",\"mysql> desc employee; +-------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+----------------+ | eid | int(11) | NO | PRI | NULL | auto_increment | | ename | varchar(20) | YES | | NULL | | +-------+-------------+------+-----+---------+----------------+ 2 rows in set (0.00 sec) \"]},{\"header\":\"5.5 如何删除自增约束\",\"slug\":\"_5-5-如何删除自增约束\",\"contents\":[\"#alter table 表名称 modify 字段名 数据类型 auto_increment;#给这个字段增加自增约束 alter table 表名称 modify 字段名 数据类型; #去掉auto_increment相当于删除 \",\"alter table employee modify eid int; \",\"mysql> desc employee; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | YES | | NULL | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) \"]},{\"header\":\"5.6 MySQL 8.0新特性—自增变量的持久化\",\"slug\":\"_5-6-mysql-8-0新特性—自增变量的持久化\",\"contents\":[\"在MySQL 8.0之前，自增主键AUTO_INCREMENT的值如果大于max(primary key)+1，在MySQL重启后，会重置AUTO_INCREMENT=max(primary key)+1，这种现象在某些情况下会导致业务主键冲突或者其他难以发现的问题。 下面通过案例来对比不同的版本中自增变量是否持久化。 在MySQL 5.7版本中，测试步骤如下： 创建的数据表中包含自增主键的id字段，语句如下：\",\"CREATE TABLE test1( id INT PRIMARY KEY AUTO_INCREMENT ); \",\"插入4个空值，执行如下：\",\"INSERT INTO test1 VALUES(0),(0),(0),(0); \",\"查询数据表test1中的数据，结果如下：\",\"mysql> SELECT * FROM test1; +----+ | id | +----+ | 1 | | 2 | | 3 | | 4 | +----+ 4 rows in set (0.00 sec) \",\"删除id为4的记录，语句如下：\",\"DELETE FROM test1 WHERE id = 4; \",\"再次插入一个空值，语句如下：\",\"INSERT INTO test1 VALUES(0); \",\"查询此时数据表test1中的数据，结果如下：\",\"mysql> SELECT * FROM test1; +----+ | id | +----+ | 1 | | 2 | | 3 | | 5 | +----+ 4 rows in set (0.00 sec) \",\"从结果可以看出，虽然删除了id为4的记录，但是再次插入空值时，并没有重用被删除的4，而是分配了5。 删除id为5的记录，结果如下：\",\"DELETE FROM test1 where id=5; \",\"重启数据库，重新插入一个空值。\",\"INSERT INTO test1 values(0); \",\"再次查询数据表test1中的数据，结果如下：\",\"mysql> SELECT * FROM test1; +----+ | id | +----+ | 1 | | 2 | | 3 | | 4 | +----+ 4 rows in set (0.00 sec) \",\"从结果可以看出，新插入的0值分配的是4，按照重启前的操作逻辑，此处应该分配6。出现上述结果的主要原因是自增主键没有持久化。 在MySQL 5.7系统中，对于自增主键的分配规则，是由InnoDB数据字典内部一个计数器来决定的，而该计数器只在内存中维护，并不会持久化到磁盘中。当数据库重启时，该计数器会被初始化。\",\"在MySQL 8.0版本中，上述测试步骤最后一步的结果如下：\",\"mysql> SELECT * FROM test1; +----+ | id | +----+ | 1 | | 2 | | 3 | | 6 | +----+ 4 rows in set (0.00 sec) \",\"从结果可以看出，自增变量已经持久化了。\",\"MySQL 8.0将自增主键的计数器持久化到重做日志中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB会根据重做日志中的信息来初始化计数器的内存值。\"]},{\"header\":\"6. FOREIGN KEY 约束\",\"slug\":\"_6-foreign-key-约束\",\"contents\":[]},{\"header\":\"6.1 作用\",\"slug\":\"_6-1-作用\",\"contents\":[\"限定某个表的某个字段的引用完整性。\",\"比如：员工表的员工所在部门的选择，必须在部门表能找到对应的部分。\"]},{\"header\":\"6.2 关键字\",\"slug\":\"_6-2-关键字\",\"contents\":[\"FOREIGN KEY\"]},{\"header\":\"6.3 主表和从表/父表和子表\",\"slug\":\"_6-3-主表和从表-父表和子表\",\"contents\":[\"主表（父表）：被引用的表，被参考的表\",\"从表（子表）：引用别人的表，参考别人的表\",\"例如：员工表的员工所在部门这个字段的值要参考部门表：部门表是主表，员工表是从表。\",\"例如：学生表、课程表、选课表：选课表的学生和课程要分别参考学生表和课程表，学生表和课程表是主表，选课表是从表。\"]},{\"header\":\"6.4 特点\",\"slug\":\"_6-4-特点\",\"contents\":[\"（1）从表的外键列，必须引用/参考主表的主键或唯一约束的列\",\"​ 为什么？因为被依赖/被参考的值必须是唯一的\",\"（2）在创建外键约束时，如果不给外键约束命名，默认名不是列名，而是自动产生一个外键名（例如 student_ibfk_1;），也可以指定外键约束名。\",\"（3）创建(CREATE)表时就指定外键约束的话，先创建主表，再创建从表\",\"（4）删表时，先删从表（或先删除外键约束），再删除主表\",\"（5）当主表的记录被从表参照时，主表的记录将不允许删除，如果要删除数据，需要先删除从表中依赖该记录的数据，然后才可以删除主表的数据\",\"（6）在“从表”中指定外键约束，并且一个表可以建立多个外键约束\",\"（7）从表的外键列与主表被参照的列名字可以不相同，但是数据类型必须一样，逻辑意义一致。如果类型不一样，创建子表时，就会出现错误“ERROR 1005 (HY000): Can't create table'database.tablename'(errno: 150)”。\",\"​ 例如：都是表示部门编号，都是int类型。\",\"（8）当创建外键约束时，系统默认会在所在的列上建立对应的普通索引。但是索引名是外键的约束名。（根据外键查询效率很高）\",\"（9）删除外键约束后，必须手动删除对应的索引\"]},{\"header\":\"6.5 添加外键约束\",\"slug\":\"_6-5-添加外键约束\",\"contents\":[\"（1）建表时\",\"create table 主表名称( 字段1 数据类型 primary key, 字段2 数据类型 ); create table 从表名称( 字段1 数据类型 primary key, 字段2 数据类型, [CONSTRAINT <外键约束名称>] FOREIGN KEY（从表的某个字段) references 主表名(被参考字段) ); #(从表的某个字段)的数据类型必须与主表名(被参考字段)的数据类型一致，逻辑意义也一样 #(从表的某个字段)的字段名可以与主表名(被参考字段)的字段名一样，也可以不一样 -- FOREIGN KEY: 在表级指定子表中的列 -- REFERENCES: 标示在父表中的列 \",\"create table dept( #主表 did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp(#从表 eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) #在从表中指定外键约束 #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号 ); 说明： （1）主表dept必须先创建成功，然后才能创建emp表，指定外键成功。 （2）删除表时，先删除从表emp，再删除主表dept \",\"（2）建表后\",\"一般情况下，表与表的关联都是提前设计好了的，因此，会在创建表的时候就把外键约束定义好。不过，如果需要修改表的设计（比如添加新的字段，增加新的关联关系），但没有预先定义外键约束，那么，就要用修改表的方式来补充定义。\",\"格式：\",\"ALTER TABLE 从表名 ADD [CONSTRAINT 约束名] FOREIGN KEY (从表的字段) REFERENCES 主表名(被引用字段) [on update xx][on delete xx]; \",\"举例：\",\"ALTER TABLE emp1 ADD [CONSTRAINT emp_dept_id_fk] FOREIGN KEY(dept_id) REFERENCES dept(dept_id); \",\"举例：\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int #员工所在的部门 ); #这两个表创建时，没有指定外键的话，那么创建顺序是随意 \",\"alter table emp add foreign key (deptid) references dept(did); \"]},{\"header\":\"6.6 演示问题\",\"slug\":\"_6-6-演示问题\",\"contents\":[\"（1）失败：不是键列\",\"create table dept( did int , #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) ); #ERROR 1215 (HY000): Cannot add foreign key constraint 原因是dept的did不是键列 \",\"（2）失败：数据类型不一致\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid char, #员工所在的部门 foreign key (deptid) references dept(did) ); #ERROR 1215 (HY000): Cannot add foreign key constraint 原因是从表的deptid字段和主表的did字段的数据类型不一致，并且要它俩的逻辑意义一致 \",\"（3）成功，两个表字段名一样\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 did int, #员工所在的部门 foreign key (did) references dept(did) #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号 #是否重名没问题，因为两个did在不同的表中 ); \",\"（4）添加、删除、修改问题\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号 ); \",\"insert into dept values(1001,'教学部'); insert into dept values(1003, '财务部'); insert into emp values(1,'张三',1001); #添加从表记录成功，在添加这条记录时，要求部门表有1001部门 insert into emp values(2,'李四',1005);#添加从表记录失败 ERROR 1452 (23000): Cannot add（添加） or update（修改） a child row: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) 从表emp添加记录失败，因为主表dept没有1005部门 \",\"mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1003 | 财务部 | +------+--------+ 2 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | +-----+-------+--------+ 1 row in set (0.00 sec) \",\"update emp set deptid = 1002 where eid = 1;#修改从表失败 ERROR 1452 (23000): Cannot add（添加） or update（修改） a child row（子表的记录）: a foreign key constraint fails（外键约束失败） (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) #部门表did字段现在没有1002的值，所以员工表中不能修改员工所在部门deptid为1002 update dept set did = 1002 where did = 1001;#修改主表失败 ERROR 1451 (23000): Cannot delete（删除） or update（修改） a parent row（父表的记录）: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) #部门表did的1001字段已经被emp引用了，所以部门表的1001字段就不能修改了。 update dept set did = 1002 where did = 1003;#修改主表成功 因为部门表的1003部门没有被emp表引用，所以可以修改 \",\"delete from dept where did=1001; #删除主表失败 ERROR 1451 (23000): Cannot delete（删除） or update（修改） a parent row（父表记录）: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) #因为部门表did的1001字段已经被emp引用了，所以部门表的1001字段对应的记录就不能被删除 \",\"总结：约束关系是针对双方的\",\"添加了外键约束后，主表的修改和删除数据受约束\",\"添加了外键约束后，从表的添加和修改数据受约束\",\"在从表上建立外键，要求主表必须存在\",\"删除主表时，要求从表从表先删除，或将从表中外键引用该主表的关系先删除\"]},{\"header\":\"6.7 约束等级\",\"slug\":\"_6-7-约束等级\",\"contents\":[\"Cascade方式：在父表上update/delete记录时，同步update/delete掉子表的匹配记录\",\"Set null方式：在父表上update/delete记录时，将子表上匹配记录的列设为null，但是要注意子表的外键列不能为not null\",\"No action方式：如果子表中有匹配的记录，则不允许对父表对应候选键进行update/delete操作\",\"Restrict方式：同no action， 都是立即检查外键约束\",\"Set default方式（在可视化工具SQLyog中可能显示空白）：父表有变更时，子表将外键列设置成一个默认的值，但Innodb不能识别\",\"如果没有指定等级，就相当于Restrict方式。\",\"对于外键约束，最好是采用: ON UPDATE CASCADE ON DELETE RESTRICT 的方式。\",\"（1）演示1：on update cascade on delete set null\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) on update cascade on delete set null #把修改操作设置为级联修改等级，把删除操作设置为set null等级 ); \",\"insert into dept values(1001,'教学部'); insert into dept values(1002, '财务部'); insert into dept values(1003, '咨询部'); insert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门 insert into emp values(2,'李四',1001); insert into emp values(3,'王五',1002); \",\"mysql> select * from dept; mysql> select * from emp; \",\"#修改主表成功，从表也跟着修改，修改了主表被引用的字段1002为1004，从表的引用字段就跟着修改为1004了 mysql> update dept set did = 1004 where did = 1002; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1003 | 咨询部 | | 1004 | 财务部 | #原来是1002，修改为1004 +------+--------+ 3 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | | 2 | 李四 | 1001 | | 3 | 王五 | 1004 | #原来是1002，跟着修改为1004 +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"#删除主表的记录成功，从表对应的字段的值被修改为null mysql> delete from dept where did = 1001; Query OK, 1 row affected (0.01 sec) mysql> select * from dept; +------+--------+ | did | dname | #记录1001部门被删除了 +------+--------+ | 1003 | 咨询部 | | 1004 | 财务部 | +------+--------+ 2 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | NULL | #原来引用1001部门的员工，deptid字段变为null | 2 | 李四 | NULL | | 3 | 王五 | 1004 | +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"（2）演示2：on update set null on delete cascade\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) on update set null on delete cascade #把修改操作设置为set null等级，把删除操作设置为级联删除等级 ); \",\"insert into dept values(1001,'教学部'); insert into dept values(1002, '财务部'); insert into dept values(1003, '咨询部'); insert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门 insert into emp values(2,'李四',1001); insert into emp values(3,'王五',1002); \",\"mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1002 | 财务部 | | 1003 | 咨询部 | +------+--------+ 3 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | | 2 | 李四 | 1001 | | 3 | 王五 | 1002 | +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"#修改主表，从表对应的字段设置为null mysql> update dept set did = 1004 where did = 1002; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1003 | 咨询部 | | 1004 | 财务部 | #原来did是1002 +------+--------+ 3 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | | 2 | 李四 | 1001 | | 3 | 王五 | NULL | #原来deptid是1002，因为部门表1002被修改了，1002没有对应的了，就设置为null +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"#删除主表的记录成功，主表的1001行被删除了，从表相应的记录也被删除了 mysql> delete from dept where did=1001; Query OK, 1 row affected (0.00 sec) mysql> select * from dept; +------+--------+ | did | dname | #部门表中1001部门被删除 +------+--------+ | 1003 | 咨询部 | | 1004 | 财务部 | +------+--------+ 2 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid |#原来1001部门的员工也被删除了 +-----+-------+--------+ | 3 | 王五 | NULL | +-----+-------+--------+ 1 row in set (0.00 sec) \",\"（3）演示：on update cascade on delete cascade\",\"create table dept( did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp( eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) on update cascade on delete cascade #把修改操作设置为级联修改等级，把删除操作也设置为级联删除等级 ); \",\"insert into dept values(1001,'教学部'); insert into dept values(1002, '财务部'); insert into dept values(1003, '咨询部'); insert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门 insert into emp values(2,'李四',1001); insert into emp values(3,'王五',1002); \",\"mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1002 | 财务部 | | 1003 | 咨询部 | +------+--------+ 3 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | | 2 | 李四 | 1001 | | 3 | 王五 | 1002 | +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"#修改主表，从表对应的字段自动修改 mysql> update dept set did = 1004 where did = 1002; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql> select * from dept; +------+--------+ | did | dname | +------+--------+ | 1001 | 教学部 | | 1003 | 咨询部 | | 1004 | 财务部 | #部门1002修改为1004 +------+--------+ 3 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | +-----+-------+--------+ | 1 | 张三 | 1001 | | 2 | 李四 | 1001 | | 3 | 王五 | 1004 | #级联修改 +-----+-------+--------+ 3 rows in set (0.00 sec) \",\"#删除主表的记录成功，主表的1001行被删除了，从表相应的记录也被删除了 mysql> delete from dept where did=1001; Query OK, 1 row affected (0.00 sec) mysql> select * from dept; +------+--------+ | did | dname | #1001部门被删除了 +------+--------+ | 1003 | 咨询部 | | 1004 | 财务部 | +------+--------+ 2 rows in set (0.00 sec) mysql> select * from emp; +-----+-------+--------+ | eid | ename | deptid | #1001部门的员工也被删除了 +-----+-------+--------+ | 3 | 王五 | 1004 | +-----+-------+--------+ 1 row in set (0.00 sec) \"]},{\"header\":\"6.8 删除外键约束\",\"slug\":\"_6-8-删除外键约束\",\"contents\":[\"流程如下：\",\"(1)第一步先查看约束名和删除外键约束 SELECT * FROM information_schema.table_constraints WHERE table_name = '表名称';#查看某个表的约束名 ALTER TABLE 从表名 DROP FOREIGN KEY 外键约束名; （2）第二步查看索引名和删除索引。（注意，只能手动删除） SHOW INDEX FROM 表名称; #查看某个表的索引名 ALTER TABLE 从表名 DROP INDEX 索引名; \",\"举例：\",\"mysql> SELECT * FROM information_schema.table_constraints WHERE table_name = 'emp'; mysql> alter table emp drop foreign key emp_ibfk_1; Query OK, 0 rows affected (0.02 sec) Records: 0 Duplicates: 0 Warnings: 0 \",\"mysql> show index from emp; mysql> alter table emp drop index deptid; Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> show index from emp; \"]},{\"header\":\"6.9 开发场景\",\"slug\":\"_6-9-开发场景\",\"contents\":[\"问题1：如果两个表之间有关系（一对一、一对多），比如：员工表和部门表（一对多），它们之间是否一定要建外键约束？\",\"答：不是的\",\"问题2：建和不建外键约束有什么区别？\",\"答：建外键约束，你的操作（创建表、删除表、添加、修改、删除）会受到限制，从语法层面受到限制。例如：在员工表中不可能添加一个员工信息，它的部门的值在部门表中找不到。\",\"不建外键约束，你的操作（创建表、删除表、添加、修改、删除）不受限制，要保证数据的引用完整性，只能依靠程序员的自觉，或者是在Java程序中进行限定。例如：在员工表中，可以添加一个员工的信息，它的部门指定为一个完全不存在的部门。\",\"问题3：那么建和不建外键约束和查询有没有关系？\",\"答：没有\",\"在 MySQL 里，外键约束是有成本的，需要消耗系统资源。对于大并发的 SQL 操作，有可能会不适合。比如大型网站的中央数据库，可能会因为外键约束的系统开销而变得非常慢。所以， MySQL 允许你不使用系统自带的外键约束，在应用层面完成检查数据一致性的逻辑。也就是说，即使你不用外键约束，也要想办法通过应用层面的附加逻辑，来实现外键约束的功能，确保数据的一致性。\"]},{\"header\":\"6.10 阿里开发规范\",\"slug\":\"_6-10-阿里开发规范\",\"contents\":[\"【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\",\"说明：（概念解释）学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度。\"]},{\"header\":\"7. CHECK 约束\",\"slug\":\"_7-check-约束\",\"contents\":[]},{\"header\":\"7.1 作用\",\"slug\":\"_7-1-作用\",\"contents\":[\"检查某个字段的值是否符号xx要求，一般指的是值的范围\"]},{\"header\":\"2、关键字\",\"slug\":\"_2、关键字\",\"contents\":[\"CHECK\"]},{\"header\":\"3、说明：MySQL 5.7 不支持\",\"slug\":\"_3、说明-mysql-5-7-不支持\",\"contents\":[\"MySQL5.7 可以使用check约束，但check约束对数据验证没有任何作用。添加数据时，没有任何错误或警告\",\"但是MySQL 8.0中可以使用check约束了。\",\"create table employee( eid int primary key, ename varchar(5), gender char check ('男' or '女') ); \",\"insert into employee values(1,'张三','妖'); \",\"mysql> select * from employee; +-----+-------+--------+ | eid | ename | gender | +-----+-------+--------+ | 1 | 张三 | 妖 | +-----+-------+--------+ 1 row in set (0.00 sec) \",\"再举例\",\"CREATE TABLE temp( id INT AUTO_INCREMENT, NAME VARCHAR(20), age INT CHECK(age > 20), PRIMARY KEY(id) ); \",\"再举例\",\"age tinyint check(age >20) 或 sex char(2) check(sex in(‘男’,’女’)) \",\"再举例\",\"CHECK(height>=0 AND height<3) \"]},{\"header\":\"8. DEFAULT约束\",\"slug\":\"_8-default约束\",\"contents\":[]},{\"header\":\"8.1 作用\",\"slug\":\"_8-1-作用\",\"contents\":[\"给某个字段/某列指定默认值，一旦设置默认值，在插入数据时，如果此字段没有显式赋值，则赋值为默认值。\"]},{\"header\":\"8.2 关键字\",\"slug\":\"_8-2-关键字\",\"contents\":[\"DEFAULT\"]},{\"header\":\"8.3 如何给字段加默认值\",\"slug\":\"_8-3-如何给字段加默认值\",\"contents\":[\"（1）建表时\",\"create table 表名称( 字段名 数据类型 primary key, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); create table 表名称( 字段名 数据类型 default 默认值 , 字段名 数据类型 not null default 默认值, 字段名 数据类型 not null default 默认值, primary key(字段名), unique key(字段名) ); 说明：默认值约束一般不在唯一键和主键列上加 \",\"create table employee( eid int primary key, ename varchar(20) not null, gender char default '男', tel char(11) not null default '' #默认是空字符串 ); \",\"mysql> desc employee; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | NO | | NULL | | | gender | char(1) | YES | | 男 | | | tel | char(11) | NO | | | | +--------+-------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) \",\"insert into employee values(1,'汪飞','男','13700102535'); #成功 \",\"mysql> select * from employee; +-----+-------+--------+-------------+ | eid | ename | gender | tel | +-----+-------+--------+-------------+ | 1 | 汪飞 | 男 | 13700102535 | +-----+-------+--------+-------------+ 1 row in set (0.00 sec) \",\"insert into employee(eid,ename) values(2,'天琪'); #成功 \",\"mysql> select * from employee; +-----+-------+--------+-------------+ | eid | ename | gender | tel | +-----+-------+--------+-------------+ | 1 | 汪飞 | 男 | 13700102535 | | 2 | 天琪 | 男 | | +-----+-------+--------+-------------+ 2 rows in set (0.00 sec) \",\"insert into employee(eid,ename) values(3,'二虎'); #ERROR 1062 (23000): Duplicate entry '' for key 'tel' #如果tel有唯一性约束的话会报错，如果tel没有唯一性约束，可以添加成功 \",\"再举例：\",\"CREATE TABLE myemp( id INT AUTO_INCREMENT PRIMARY KEY, NAME VARCHAR(15), salary DOUBLE(10,2) DEFAULT 2000 ); \",\"（2）建表后\",\"alter table 表名称 modify 字段名 数据类型 default 默认值; #如果这个字段原来有非空约束，你还保留非空约束，那么在加默认值约束时，还得保留非空约束，否则非空约束就被删除了 #同理，在给某个字段加非空约束也一样，如果这个字段原来有默认值约束，你想保留，也要在modify语句中保留默认值约束，否则就删除了 alter table 表名称 modify 字段名 数据类型 default 默认值 not null; \",\"create table employee( eid int primary key, ename varchar(20), gender char, tel char(11) not null ); \",\"mysql> desc employee; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | YES | | NULL | | | gender | char(1) | YES | | NULL | | | tel | char(11) | NO | | NULL | | +--------+-------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) \",\"alter table employee modify gender char default '男'; #给gender字段增加默认值约束 alter table employee modify tel char(11) default ''; #给tel字段增加默认值约束 \",\"mysql> desc employee; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | YES | | NULL | | | gender | char(1) | YES | | 男 | | | tel | char(11) | YES | | | | +--------+-------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) \",\"alter table employee modify tel char(11) default '' not null;#给tel字段增加默认值约束，并保留非空约束 \",\"mysql> desc employee; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | YES | | NULL | | | gender | char(1) | YES | | 男 | | | tel | char(11) | NO | | | | +--------+-------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) \"]},{\"header\":\"8.4 如何删除默认值约束\",\"slug\":\"_8-4-如何删除默认值约束\",\"contents\":[\"alter table 表名称 modify 字段名 数据类型 ;#删除默认值约束，也不保留非空约束 alter table 表名称 modify 字段名 数据类型 not null; #删除默认值约束，保留非空约束 \",\"alter table employee modify gender char; #删除gender字段默认值约束，如果有非空约束，也一并删除 alter table employee modify tel char(11) not null;#删除tel字段默认值约束，保留非空约束 \",\"mysql> desc employee; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | eid | int(11) | NO | PRI | NULL | | | ename | varchar(20) | YES | | NULL | | | gender | char(1) | YES | | NULL | | | tel | char(11) | NO | | NULL | | +--------+-------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) \"]},{\"header\":\"9. 面试\",\"slug\":\"_9-面试\",\"contents\":[\"面试1、为什么建表时，加 not null default '' 或 default 0\",\"答：不想让表中出现null值。\",\"面试2、为什么不想要 null 的值\",\"答:（1）不好比较。null是一种特殊值，比较时只能用专门的is null 和 is not null来比较。碰到运算符，通常返回null。\",\"​ （2）效率不高。影响提高索引效果。因此，我们往往在建表时 not null default '' 或 default 0\",\"面试3、带AUTO_INCREMENT约束的字段值是从1开始的吗？ 在MySQL中，默认AUTO_INCREMENT的初始值是1，每新增一条记录，字段值自动加1。设置自增属性（AUTO_INCREMENT）的时候，还可以指定第一条插入记录的自增字段的值，这样新插入的记录的自增字段值从初始值开始递增，如在表中插入第一条记录，同时指定id值为5，则以后插入的记录的id值就会从6开始往上增加。添加主键约束时，往往需要设置字段自动增加属性。\",\"面试4、并不是每个表都可以任意选择存储引擎？ 外键约束（FOREIGN KEY）不能跨引擎使用。\",\"MySQL支持多种存储引擎，每一个表都可以指定一个不同的存储引擎，需要注意的是：外键约束是用来保证数据的参照完整性的，如果表之间需要关联外键，却指定了不同的存储引擎，那么这些表之间是不能创建外键约束的。所以说，存储引擎的选择也不完全是随意的。\"]}]},\"/study-tutorial/database/mysql/crud.html\":{\"title\":\"11、数据处理之增删改\",\"contents\":[{\"header\":\"1. 插入数据\",\"slug\":\"_1-插入数据\",\"contents\":[]},{\"header\":\"1.1 实际问题\",\"slug\":\"_1-1-实际问题\",\"contents\":[\"解决方式：使用 INSERT 语句向表中插入数据。\"]},{\"header\":\"1.2 方式1：VALUES的方式添加\",\"slug\":\"_1-2-方式1-values的方式添加\",\"contents\":[\"使用这种语法一次只能向表中插入一条数据。\",\"情况1：为表的所有字段按默认顺序插入数据\",\"INSERT INTO 表名 VALUES (value1,value2,....); \",\"值列表中需要为表的每一个字段指定值，并且值的顺序必须和数据表中字段定义时的顺序相同。\",\"举例：\",\"INSERT INTO departments VALUES (70, 'Pub', 100, 1700); \",\"INSERT INTO departments VALUES (100, 'Finance', NULL, NULL); \",\"情况2：为表的指定字段插入数据\",\"INSERT INTO 表名(column1 [, column2, …, columnn]) VALUES (value1 [,value2, …, valuen]); \",\"为表的指定字段插入数据，就是在INSERT语句中只向部分字段中插入值，而其他字段的值为表定义时的默认值。\",\"在 INSERT 子句中随意列出列名，但是一旦列出，VALUES中要插入的value1,....valuen需要与column1,...columnn列一一对应。如果类型不同，将无法插入，并且MySQL会产生错误。\",\"举例：\",\"INSERT INTO departments(department_id, department_name) VALUES (80, 'IT'); \",\"情况3：同时插入多条记录\",\"INSERT语句可以同时向数据表中插入多条记录，插入时指定多个值列表，每个值列表之间用逗号分隔开，基本语法格式如下：\",\"INSERT INTO table_name VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); \",\"或者\",\"INSERT INTO table_name(column1 [, column2, …, columnn]) VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); \",\"举例：\",\"mysql> INSERT INTO emp(emp_id,emp_name) -> VALUES (1001,'shkstart'), -> (1002,'atguigu'), -> (1003,'Tom'); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 \",\"使用INSERT同时插入多条记录时，MySQL会返回一些在执行单行插入时没有的额外信息，这些信息的含义如下： ● Records：表明插入的记录条数。 ● Duplicates：表明插入时被忽略的记录，原因可能是这些记录包含了重复的主键值。 ● Warnings：表明有问题的数据值，例如发生数据类型转换。\",\"一个同时插入多行记录的INSERT语句等同于多个单行插入的INSERT语句，但是多行的INSERT语句在处理过程中效率更高。因为MySQL执行单条INSERT语句插入多行数据比使用多条INSERT语句快，所以在插入多条记录时最好选择使用单条INSERT语句的方式插入。\",\"小结：\",\"VALUES也可以写成VALUE，但是VALUES是标准写法。\",\"字符和日期型数据应包含在单引号中。\"]},{\"header\":\"1.3 方式2：将查询结果插入到表中\",\"slug\":\"_1-3-方式2-将查询结果插入到表中\",\"contents\":[\"INSERT还可以将SELECT语句查询的结果插入到表中，此时不需要把每一条记录的值一个一个输入，只需要使用一条INSERT语句和一条SELECT语句组成的组合语句即可快速地从一个或多个表中向一个表中插入多行。\",\"基本语法格式如下：\",\"INSERT INTO 目标表名 (tar_column1 [, tar_column2, …, tar_columnn]) SELECT (src_column1 [, src_column2, …, src_columnn]) FROM 源表名 [WHERE condition] \",\"在 INSERT 语句中加入子查询。\",\"不必书写VALUES子句。\",\"子查询中的值列表应与 INSERT 子句中的列名对应。\",\"举例：\",\"INSERT INTO emp2 SELECT * FROM employees WHERE department_id = 90; \",\"INSERT INTO sales_reps(id, name, salary, commission_pct) SELECT employee_id, last_name, salary, commission_pct FROM employees WHERE job_id LIKE '%REP%'; \"]},{\"header\":\"2. 更新数据\",\"slug\":\"_2-更新数据\",\"contents\":[\"使用 UPDATE 语句更新数据。语法如下：\",\"UPDATE table_name SET column1=value1, column2=value2, … , column=valuen [WHERE condition] \",\"可以一次更新多条数据。\",\"如果需要回滚数据，需要保证在DML前，进行设置：SET AUTOCOMMIT = FALSE;\",\"使用 WHERE 子句指定需要更新的数据。\",\"UPDATE employees SET department_id = 70 WHERE employee_id = 113; \",\"如果省略 WHERE 子句，则表中的所有数据都将被更新。\",\"UPDATE copy_emp SET department_id = 110; \",\"更新中的数据完整性错误\",\"UPDATE employees SET department_id = 55 WHERE department_id = 110; \",\"说明：不存在 55 号部门\"]},{\"header\":\"3. 删除数据\",\"slug\":\"_3-删除数据\",\"contents\":[\"使用 DELETE 语句从表中删除数据\",\"DELETE FROM table_name [WHERE <condition>]; \",\"table_name指定要执行删除操作的表；[WHERE <condition>]为可选参数，指定删除条件，如果没有WHERE子句，DELETE语句将删除表中的所有记录。\",\"使用 WHERE 子句删除指定的记录。\",\"DELETE FROM departments WHERE department_name = 'Finance'; \",\"如果省略 WHERE 子句，则表中的全部数据将被删除\",\"DELETE FROM copy_emp; \",\"删除中的数据完整性错误\",\"DELETE FROM departments WHERE department_id = 60; \",\"说明：You cannot delete a row that contains a primary key that is used as a foreign key in another table.\"]},{\"header\":\"4. MySQL8新特性：计算列\",\"slug\":\"_4-mysql8新特性-计算列\",\"contents\":[\"什么叫计算列呢？简单来说就是某一列的值是通过别的列计算得来的。例如，a列值为1、b列值为2，c列不需要手动插入，定义a+b的结果为c的值，那么c就是计算列，是通过别的列计算得来的。\",\"在MySQL 8.0中，CREATE TABLE 和 ALTER TABLE 中都支持增加计算列。下面以CREATE TABLE为例进行讲解。\",\"举例：定义数据表tb1，然后定义字段id、字段a、字段b和字段c，其中字段c为计算列，用于计算a+b的值。 首先创建测试表tb1，语句如下：\",\"CREATE TABLE tb1( id INT, a INT, b INT, c INT GENERATED ALWAYS AS (a + b) VIRTUAL ); \",\"插入演示数据，语句如下：\",\"INSERT INTO tb1(a,b) VALUES (100,200); \",\"查询数据表tb1中的数据，结果如下：\",\"mysql> SELECT * FROM tb1; +------+------+------+------+ | id | a | b | c | +------+------+------+------+ | NULL | 100 | 200 | 300 | +------+------+------+------+ 1 row in set (0.00 sec) \",\"更新数据中的数据，语句如下：\",\"mysql> UPDATE tb1 SET a = 500; Query OK, 0 rows affected (0.00 sec) Rows matched: 1 Changed: 0 Warnings: 0 \"]},{\"header\":\"5. 综合案例\",\"slug\":\"_5-综合案例\",\"contents\":[\"## 1、创建数据库test01_library ## 2、创建表 books，表结构如下： \",\"字段名\",\"字段说明\",\"数据类型\",\"id\",\"书编号\",\"INT\",\"name\",\"书名\",\"VARCHAR(50)\",\"authors\",\"作者\",\"VARCHAR(100)\",\"price\",\"价格\",\"FLOAT\",\"pubdate\",\"出版日期\",\"YEAR\",\"note\",\"说明\",\"VARCHAR(100)\",\"num\",\"库存\",\"INT\",\"## 3、向books表中插入记录 ## 1）不指定字段名称，插入第一条记录 ## 2）指定所有字段名称，插入第二记录 ## 3）同时插入多条记录（剩下的所有记录） \",\"id\",\"name\",\"authors\",\"price\",\"pubdate\",\"note\",\"num\",\"1\",\"Tal of AAA\",\"Dickes\",\"23\",\"1995\",\"novel\",\"11\",\"2\",\"EmmaT\",\"Jane lura\",\"35\",\"1993\",\"joke\",\"22\",\"3\",\"Story of Jane\",\"Jane Tim\",\"40\",\"2001\",\"novel\",\"0\",\"4\",\"Lovey Day\",\"George Byron\",\"20\",\"2005\",\"novel\",\"30\",\"5\",\"Old land\",\"Honore Blade\",\"30\",\"2010\",\"law\",\"0\",\"6\",\"The Battle\",\"Upton Sara\",\"30\",\"1999\",\"medicine\",\"40\",\"7\",\"Rose Hood\",\"Richard haggard\",\"28\",\"2008\",\"cartoon\",\"28\",\"## 4、将小说类型(novel)的书的价格都增加5。 ## 5、将名称为EmmaT的书的价格改为40，并将说明改为drama。 ## 6、删除库存为0的记录。 \",\"## 7、统计书名中包含a字母的书 ## 8、统计书名中包含a字母的书的数量和库存总量 ## 9、找出“novel”类型的书，按照价格降序排列 ## 10、查询图书信息，按照库存量降序排列，如果库存量相同的按照note升序排列 ## 11、按照note分类统计书的数量 ## 12、按照note分类统计书的库存量，显示库存量超过30本的 ## 13、查询所有图书，每页显示5本，显示第二页 ## 14、按照note分类统计书的库存量，显示库存量最多的 ## 15、查询书名达到10个字符的书，不包括里面的空格 ## 16、查询书名和类型，其中note值为novel显示小说，law显示法律，medicine显示医药，cartoon显示卡通，joke显示笑话 ## 17、查询书名、库存，其中num值超过30本的，显示滞销，大于0并低于10的，显示畅销，为0的显示需要无货 ## 18、统计每一种note的库存量，并合计总量 ## 19、统计每一种note的数量，并合计总量 ## 20、统计库存量前三名的图书 ## 21、找出最早出版的一本书 ## 22、找出novel中价格最高的一本书 ## 23、找出书名中字数最多的一本书，不含空格 \",\"答案：\",\"#1、创建数据库test01_library CREATE DATABASE IF NOT EXISTS test01_library CHARACTER SET 'utf8'; #指定使用哪个数据库 USE test01_library; #2、创建表 books CREATE TABLE books( id INT, name VARCHAR(50), `authors` VARCHAR(100) , price FLOAT, pubdate YEAR , note VARCHAR(100), num INT ); #3、向books表中插入记录 ## 1）不指定字段名称，插入第一条记录 INSERT INTO books VALUES(1,'Tal of AAA','Dickes',23,1995,'novel',11); ## 2）指定所有字段名称，插入第二记录 INSERT INTO books (id,name,`authors`,price,pubdate,note,num) VALUES(2,'EmmaT','Jane lura',35,1993,'Joke',22); ## 3）同时插入多条记录（剩下的所有记录） INSERT INTO books (id,name,`authors`,price,pubdate,note,num) VALUES (3,'Story of Jane','Jane Tim',40,2001,'novel',0), (4,'Lovey Day','George Byron',20,2005,'novel',30), (5,'Old land','Honore Blade',30,2010,'Law',0), (6,'The Battle','Upton Sara',30,1999,'medicine',40), (7,'Rose Hood','Richard haggard',28,2008,'cartoon',28); ## 4、将小说类型(novel)的书的价格都增加5。 UPDATE books SET price=price+5 WHERE note = 'novel'; ## 5、将名称为EmmaT的书的价格改为40，并将说明改为drama。 UPDATE books SET price=40,note='drama' WHERE name='EmmaT'; ## 6、删除库存为0的记录。 DELETE FROM books WHERE num=0; \",\"## 7、统计书名中包含a字母的书 SELECT * FROM books WHERE name LIKE '%a%'; ## 8、统计书名中包含a字母的书的数量和库存总量 SELECT COUNT(*),SUM(num) FROM books WHERE name LIKE '%a%'; ## 9、找出“novel”类型的书，按照价格降序排列 SELECT * FROM books WHERE note = 'novel' ORDER BY price DESC; ## 10、查询图书信息，按照库存量降序排列，如果库存量相同的按照note升序排列 SELECT * FROM books ORDER BY num DESC,note ASC; ## 11、按照note分类统计书的数量 SELECT note,COUNT(*) FROM books GROUP BY note; ## 12、按照note分类统计书的库存量，显示库存量超过30本的 SELECT note,SUM(num) FROM books GROUP BY note HAVING SUM(num)>30; ## 13、查询所有图书，每页显示5本，显示第二页 SELECT * FROM books LIMIT 5,5; ## 14、按照note分类统计书的库存量，显示库存量最多的 SELECT note,SUM(num) sum_num FROM books GROUP BY note ORDER BY sum_num DESC LIMIT 0,1; ## 15、查询书名达到10个字符的书，不包括里面的空格 SELECT * FROM books WHERE CHAR_LENGTH(REPLACE(name,' ',''))>=10; /* 16、查询书名和类型， 其中note值为 novel显示小说，law显示法律，medicine显示医药，cartoon显示卡通，joke显示笑话 */ SELECT name AS \\\"书名\\\" ,note, CASE note WHEN 'novel' THEN '小说' WHEN 'law' THEN '法律' WHEN 'medicine' THEN '医药' WHEN 'cartoon' THEN '卡通' WHEN 'joke' THEN '笑话' END AS \\\"类型\\\" FROM books; ## 17、查询书名、库存，其中num值超过30本的，显示滞销，大于0并低于10的，显示畅销，为0的显示需要无货 SELECT name,num,CASE WHEN num>30 THEN '滞销' WHEN num>0 AND num<10 THEN '畅销' WHEN num=0 THEN '无货' ELSE '正常' END AS \\\"库存状态\\\" FROM books; ## 18、统计每一种note的库存量，并合计总量 SELECT IFNULL(note,'合计总库存量') AS note,SUM(num) FROM books GROUP BY note WITH ROLLUP; ## 19、统计每一种note的数量，并合计总量 SELECT IFNULL(note,'合计总数') AS note,COUNT(*) FROM books GROUP BY note WITH ROLLUP; ## 20、统计库存量前三名的图书 SELECT * FROM books ORDER BY num DESC LIMIT 0,3; ## 21、找出最早出版的一本书 SELECT * FROM books ORDER BY pubdate ASC LIMIT 0,1; ## 22、找出novel中价格最高的一本书 SELECT * FROM books WHERE note = 'novel' ORDER BY price DESC LIMIT 0,1; ## 23、找出书名中字数最多的一本书，不含空格 SELECT * FROM books ORDER BY CHAR_LENGTH(REPLACE(name,' ','')) DESC LIMIT 0,1; \"]}]},\"/study-tutorial/database/mysql/environment-build.html\":{\"title\":\"2、MySQL环境搭建\",\"contents\":[{\"header\":\"1. MySQL的卸载\",\"slug\":\"_1-mysql的卸载\",\"contents\":[]},{\"header\":\"步骤1：停止MySQL服务\",\"slug\":\"步骤1-停止mysql服务\",\"contents\":[\"在卸载之前，先停止MySQL8.0的服务。按键盘上的“Ctrl + Alt + Delete”组合键，打开“任务管理器”对话框，可以在“服务”列表找到“MySQL8.0”的服务，如果现在“正在运行”状态，可以右键单击服务，选择“停止”选项停止MySQL8.0的服务，如图所示。\"]},{\"header\":\"步骤2：软件的卸载\",\"slug\":\"步骤2-软件的卸载\",\"contents\":[\"方式1：通过控制面板方式\",\"卸载MySQL8.0的程序可以和其他桌面应用程序一样直接在“控制面板”选择“卸载程序”，并在程序列表中找到MySQL8.0服务器程序，直接双击卸载即可，如图所示。这种方式删除，数据目录下的数据不会跟着删除。\",\"方式2：通过360或电脑管家等软件卸载\",\"略\",\"方式3：通过安装包提供的卸载功能卸载\",\"你也可以通过安装向导程序进行MySQL8.0服务器程序的卸载。\",\"① 再次双击下载的mysql-installer-community-8.0.26.0.msi文件，打开安装向导。安装向导会自动检测已安装的MySQL服务器程序。\",\"② 选择要卸载的MySQL服务器程序，单击“Remove”（移除），即可进行卸载。\",\"③ 单击“Next”（下一步）按钮，确认卸载。\",\"④ 弹出是否同时移除数据目录选择窗口。如果想要同时删除MySQL服务器中的数据，则勾选“Remove the data directory”，如图所示。\",\"⑤ 执行卸载。单击“Execute”（执行）按钮进行卸载。\",\"⑥ 完成卸载。单击“Finish”（完成）按钮即可。如果想要同时卸载MySQL8.0的安装向导程序，勾选“Yes，Uninstall MySQL Installer”即可，如图所示。\"]},{\"header\":\"步骤3：残余文件的清理\",\"slug\":\"步骤3-残余文件的清理\",\"contents\":[\"如果再次安装不成功，可以卸载后对残余文件进行清理后再安装。\",\"（1）服务目录：mysql服务的安装目录\",\"（2）数据目录：默认在C:\\\\ProgramData\\\\MySQL\",\"如果自己单独指定过数据目录，就找到自己的数据目录进行删除即可。\",\"注意：请在卸载前做好数据备份\",\"在操作完以后，需要重启计算机，然后进行安装即可。如果仍然安装失败，需要继续操作如下步骤4。\"]},{\"header\":\"步骤4：清理注册表（选做）\",\"slug\":\"步骤4-清理注册表-选做\",\"contents\":[\"如果前几步做了，再次安装还是失败，那么可以清理注册表。\",\"如何打开注册表编辑器：在系统的搜索框中输入regedit\",\"HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\ControlSet001\\\\Services\\\\Eventlog\\\\Application\\\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\ControlSet001\\\\Services\\\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\ControlSet002\\\\Services\\\\Eventlog\\\\Application\\\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\ControlSet002\\\\Services\\\\MySQL服务 目录删除 HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Services\\\\Eventlog\\\\Application\\\\MySQL服务目录删除 HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Services\\\\MySQL服务删除 \",\"注册表中的ControlSet001,ControlSet002,不一定是001和002,可能是ControlSet005、006之类\"]},{\"header\":\"步骤5：删除环境变量配置\",\"slug\":\"步骤5-删除环境变量配置\",\"contents\":[\"找到path环境变量，将其中关于mysql的环境变量删除，切记不要全部删除。\",\"例如：删除 D:\\\\develop_tools\\\\mysql\\\\MySQLServer8.0.26\\\\bin; 这个部分\"]},{\"header\":\"2. MySQL的下载、安装、配置\",\"slug\":\"_2-mysql的下载、安装、配置\",\"contents\":[]},{\"header\":\"2.1 MySQL的4大版本\",\"slug\":\"_2-1-mysql的4大版本\",\"contents\":[\"MySQL Community Server 社区版本，开源免费，自由下载，但不提供官方技术支持，适用于大多数普通用户。\",\"MySQL Enterprise Edition 企业版本，需付费，不能在线下载，可以试用30天。提供了更多的功能和更完备的技术支持，更适合于对数据库的功能和可靠性要求较高的企业客户。\",\"MySQL Cluster 集群版，开源免费。用于架设集群服务器，可将几个MySQL Server封装成一个Server。需要在社区版或企业版的基础上使用。\",\"MySQL Cluster CGE 高级集群版，需付费。\",\"目前最新版本为8.0.27，发布时间2021年10月。此前，8.0.0 在 2016.9.12日就发布了。\",\"本课程中使用8.0.26版本。\",\"此外，官方还提供了MySQL Workbench（GUITOOL）一款专为MySQL设计的图形界面管理工具。MySQLWorkbench又分为两个版本，分别是社区版（MySQL Workbench OSS）、商用版（MySQL WorkbenchSE）。\"]},{\"header\":\"2.2 软件的下载\",\"slug\":\"_2-2-软件的下载\",\"contents\":[\"1. 下载地址\",\"官网：https://www.mysql.com\",\"2. 打开官网，点击DOWNLOADS\",\"然后，点击MySQL Community(GPL) Downloads\",\"3. 点击 MySQL Community Server\",\"4. 在General Availability(GA) Releases中选择适合的版本\",\"Windows平台下提供两种安装文件：MySQL二进制分发版（.msi安装文件）和免安装版（.zip压缩文件）。一般来讲，应当使用二进制分发版，因为该版本提供了图形化的安装向导过程，比其他的分发版使用起来要简单，不再需要其他工具启动就可以运行MySQL。\",\"这里在Windows 系统下推荐下载MSI安装程序；点击Go to Download Page进行下载即可\",\"Windows下的MySQL8.0安装有两种安装程序 \",\"mysql-installer-web-community-8.0.26.0.msi 下载程序大小：2.4M；安装时需要联网安装组件。\",\"mysql-installer-community-8.0.26.0.msi 下载程序大小：450.7M；安装时离线安装即可。推荐。\",\"如果安装MySQL5.7版本的话，选择Archives，接着选择MySQL5.7的相应版本即可。这里下载最近期的MySQL5.7.34版本。\"]},{\"header\":\"2.3 MySQL8.0 版本的安装\",\"slug\":\"_2-3-mysql8-0-版本的安装\",\"contents\":[\"MySQL下载完成后，找到下载文件，双击进行安装，具体操作步骤如下。\",\"步骤1：双击下载的mysql-installer-community-8.0.26.0.msi文件，打开安装向导。\",\"步骤2：打开“Choosing a Setup Type”（选择安装类型）窗口，在其中列出了5种安装类型，分别是Developer Default（默认安装类型）、Server only（仅作为服务器）、Client only（仅作为客户端）、Full（完全安装）、Custom（自定义安装）。这里选择“Custom（自定义安装）”类型按钮，单击“Next(下一步)”按钮。\",\"步骤3：打开“Select Products” （选择产品）窗口，可以定制需要安装的产品清单。例如，选择“MySQL Server 8.0.26-X64”后，单击“→”添加按钮，即可选择安装MySQL服务器，如图所示。采用通用的方法，可以添加其他你需要安装的产品。\",\"此时如果直接“Next”（下一步），则产品的安装路径是默认的。如果想要自定义安装目录，则可以选中对应的产品，然后在下面会出现“Advanced Options”（高级选项）的超链接。\",\"单击“Advanced Options”（高级选项）则会弹出安装目录的选择窗口，如图所示，此时你可以分别设置MySQL的服务程序安装目录和数据存储目录。如果不设置，默认分别在C盘的Program Files目录和ProgramData目录（这是一个隐藏目录）。如果自定义安装目录，请避免“中文”目录。另外，建议服务目录和数据目录分开存放。\",\"步骤4：在上一步选择好要安装的产品之后，单击“Next”（下一步）进入确认窗口，如图所示。单击“Execute”（执行）按钮开始安装。\",\"步骤5：安装完成后在“Status”（状态）列表下将显示“Complete”（安装完成），如图所示。\"]},{\"header\":\"2.4 配置MySQL8.0\",\"slug\":\"_2-4-配置mysql8-0\",\"contents\":[\"MySQL安装之后，需要对服务器进行配置。具体的配置步骤如下。\",\"步骤1：在上一个小节的最后一步，单击“Next”（下一步）按钮，就可以进入产品配置窗口。\",\"步骤2：单击“Next”（下一步）按钮，进入MySQL服务器类型配置窗口，如图所示。端口号一般选择默认端口号3306。\",\"其中，“Config Type”选项用于设置服务器的类型。单击该选项右侧的下三角按钮，即可查看3个选项，如图所示。\",\"Development Machine（开发机器）：该选项代表典型个人用桌面工作站。此时机器上需要运行多个应用程序，那么MySQL服务器将占用最少的系统资源。\",\"Server Machine（服务器）：该选项代表服务器，MySQL服务器可以同其他服务器应用程序一起运行，例如Web服务器等。MySQL服务器配置成适当比例的系统资源。\",\"Dedicated Machine（专用服务器）：该选项代表只运行MySQL服务的服务器。MySQL服务器配置成使用所有可用系统资源。\",\"步骤3：单击“Next”（下一步）按钮，打开设置授权方式窗口。其中，上面的选项是MySQL8.0提供的新的授权方式，采用SHA256基础的密码加密方法；下面的选项是传统授权方法（保留5.x版本兼容性）。\",\"步骤4：单击“Next”（下一步）按钮，打开设置服务器root超级管理员的密码窗口，如图所示，需要输入两次同样的登录密码。也可以通过“Add User”添加其他用户，添加其他用户时，需要指定用户名、允许该用户名在哪台/哪些主机上登录，还可以指定用户角色等。此处暂不添加用户，用户管理在MySQL高级特性篇中讲解。\",\"步骤5：单击“Next”（下一步）按钮，打开设置服务器名称窗口，如图所示。该服务名会出现在Windows服务列表中，也可以在命令行窗口中使用该服务名进行启动和停止服务。本书将服务名设置为“MySQL80”。如果希望开机自启动服务，也可以勾选“Start the MySQL Server at System Startup”选项（推荐）。\",\"下面是选择以什么方式运行服务？可以选择“Standard System Account”(标准系统用户)或者“Custom User”(自定义用户)中的一个。这里推荐前者。\",\"步骤6：单击“Next”（下一步）按钮，打开确认设置服务器窗口，单击“Execute”（执行）按钮。\",\"步骤7：完成配置，如图所示。单击“Finish”（完成）按钮，即可完成服务器的配置。\",\"步骤8：如果还有其他产品需要配置，可以选择其他产品，然后继续配置。如果没有，直接选择“Next”（下一步），直接完成整个安装和配置过程。\",\"步骤9：结束安装和配置。\"]},{\"header\":\"2.5 配置MySQL8.0 环境变量\",\"slug\":\"_2-5-配置mysql8-0-环境变量\",\"contents\":[\"如果不配置MySQL环境变量，就不能在命令行直接输入MySQL登录命令。下面说如何配置MySQL的环境变量：\",\"步骤1：在桌面上右击【此电脑】图标，在弹出的快捷菜单中选择【属性】菜单命令。 步骤2：打开【系统】窗口，单击【高级系统设置】链接。 步骤3：打开【系统属性】对话框，选择【高级】选项卡，然后单击【环境变量】按钮。 步骤4：打开【环境变量】对话框，在系统变量列表中选择path变量。 步骤5：单击【编辑】按钮，在【编辑环境变量】对话框中，将MySQL应用程序的bin目录（C:\\\\Program Files\\\\MySQL\\\\MySQL Server 8.0\\\\bin）添加到变量值中，用分号将其与其他路径分隔开。 步骤6：添加完成之后，单击【确定】按钮，这样就完成了配置path变量的操作，然后就可以直接输入MySQL命令来登录数据库了。\"]},{\"header\":\"2.6 MySQL5.7 版本的安装、配置\",\"slug\":\"_2-6-mysql5-7-版本的安装、配置\",\"contents\":[\"安装\",\"此版本的安装过程与上述过程除了版本号不同之外，其它环节都是相同的。所以这里省略了MySQL5.7.34版本的安装截图。\",\"配置\",\"配置环节与MySQL8.0版本确有细微不同。大部分情况下直接选择“Next”即可，不影响整理使用。\",\"这里配置MySQL5.7时，重点强调：与前面安装好的MySQL8.0不能使用相同的端口号。\"]},{\"header\":\"2.7 安装失败问题\",\"slug\":\"_2-7-安装失败问题\",\"contents\":[\"MySQL的安装和配置是一件非常简单的事，但是在操作过程中也可能出现问题，特别是初学者。\",\"问题1：无法打开MySQL8.0软件安装包或者安装过程中失败，如何解决？\",\"在运行MySQL8.0软件安装包之前，用户需要确保系统中已经安装了.Net Framework相关软件，如果缺少此软件，将不能正常地安装MySQL8.0软件。\",\"解决方案：到这个地址https://www.microsoft.com/en-us/download/details.aspx?id=42642下载Microsoft .NET Framework 4.5并安装后，再去安装MySQL。\",\"另外，还要确保Windows Installer正常安装。windows上安装mysql8.0需要操作系统提前已安装好Microsoft Visual C++ 2015-2019。\",\"解决方案同样是，提前到微软官网https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0，下载相应的环境。\",\"问题2：卸载重装MySQL失败？\",\"该问题通常是因为MySQL卸载时，没有完全清除相关信息导致的。\",\"解决办法是，把以前的安装目录删除。如果之前安装并未单独指定过服务安装目录，则默认安装目录是“C:\\\\Program Files\\\\MySQL”，彻底删除该目录。同时删除MySQL的Data目录，如果之前安装并未单独指定过数据目录，则默认安装目录是“C:\\\\ProgramData\\\\MySQL”，该目录一般为隐藏目录。删除后，重新安装即可。\",\"问题3：如何在Windows系统删除之前的未卸载干净的MySQL服务列表？\",\"操作方法如下，在系统“搜索框”中输入“cmd”，按“Enter”（回车）键确认，弹出命令提示符界面。然后输入“sc delete MySQL服务名”,按“Enter”（回车）键，就能彻底删除残余的MySQL服务了。\"]},{\"header\":\"3. MySQL的登录\",\"slug\":\"_3-mysql的登录\",\"contents\":[]},{\"header\":\"3.1 服务的启动与停止\",\"slug\":\"_3-1-服务的启动与停止\",\"contents\":[\"MySQL安装完毕之后，需要启动服务器进程，不然客户端无法连接数据库。\",\"在前面的配置过程中，已经将MySQL安装为Windows服务，并且勾选当Windows启动、停止时，MySQL也自动启动、停止。\"]},{\"header\":\"方式1：使用图形界面工具\",\"slug\":\"方式1-使用图形界面工具\",\"contents\":[\"步骤1：打开windows服务\",\"方式1：计算机（点击鼠标右键）→ 管理（点击）→ 服务和应用程序（点击）→ 服务（点击）\",\"方式2：控制面板（点击）→ 系统和安全（点击）→ 管理工具（点击）→ 服务（点击）\",\"方式3：任务栏（点击鼠标右键）→ 启动任务管理器（点击）→ 服务（点击）\",\"方式4：单击【开始】菜单，在搜索框中输入“services.msc”，按Enter键确认\",\"步骤2：找到MySQL80（点击鼠标右键）→ 启动或停止（点击）\"]},{\"header\":\"方式2：使用命令行工具\",\"slug\":\"方式2-使用命令行工具\",\"contents\":[\"## 启动 MySQL 服务命令： net start MySQL服务名 ## 停止 MySQL 服务命令： net stop MySQL服务名 \",\"说明：\",\"start和stop后面的服务名应与之前配置时指定的服务名一致。\",\"如果当你输入命令后，提示“拒绝服务”，请以系统管理员身份打开命令提示符界面重新尝试。\"]},{\"header\":\"3.2 自带客户端的登录与退出\",\"slug\":\"_3-2-自带客户端的登录与退出\",\"contents\":[\"当MySQL服务启动完成后，便可以通过客户端来登录MySQL数据库。注意：确认服务是开启的。\"]},{\"header\":\"登录方式1：MySQL自带客户端\",\"slug\":\"登录方式1-mysql自带客户端\",\"contents\":[\"开始菜单 → 所有程序 → MySQL → MySQL 8.0 Command Line Client\",\"说明：仅限于root用户\"]},{\"header\":\"登录方式2：windows命令行\",\"slug\":\"登录方式2-windows命令行\",\"contents\":[\"格式：\",\"mysql -h 主机名 -P 端口号 -u 用户名 -p密码 \",\"举例：\",\"mysql -h localhost -P 3306 -u root -pabc123 ## 这里我设置的root用户的密码是abc123 \",\"注意：\",\"（1）-p与密码之间不能有空格，其他参数名与参数值之间可以有空格也可以没有空格。如：\",\"mysql -hlocalhost -P3306 -uroot -pabc123 \",\"（2）密码建议在下一行输入，保证安全\",\"mysql -h localhost -P 3306 -u root -p Enter password:**** \",\"（3）客户端和服务器在同一台机器上，所以输入localhost或者IP地址127.0.0.1。同时，因为是连接本机： -hlocalhost就可以省略，如果端口号没有修改：-P3306也可以省略\",\"简写成：\",\"mysql -u root -p Enter password:**** \",\"连接成功后，有关于MySQL Server服务版本的信息，还有第几次连接的id标识。\",\"也可以在命令行通过以下方式获取MySQL Server服务版本的信息：\",\"c:\\\\> mysql -V \",\"c:\\\\> mysql --version \",\"或登录后，通过以下方式查看当前版本信息：\",\"mysql> select version(); \"]},{\"header\":\"退出登录\",\"slug\":\"退出登录\",\"contents\":[\"exit 或 quit \"]},{\"header\":\"4. MySQL演示使用\",\"slug\":\"_4-mysql演示使用\",\"contents\":[]},{\"header\":\"4.1 MySQL的使用演示\",\"slug\":\"_4-1-mysql的使用演示\",\"contents\":[\"1、查看所有的数据库\",\"show databases; \",\"“information_schema”是 MySQL 系统自带的数据库，主要保存 MySQL 数据库服务器的系统信息，比如数据库的名称、数据表的名称、字段名称、存取权限、数据文件 所在的文件夹和系统使用的文件夹，等等\",\"“performance_schema”是 MySQL 系统自带的数据库，可以用来监控 MySQL 的各类性能指标。\",\"“sys”数据库是 MySQL 系统自带的数据库，主要作用是以一种更容易被理解的方式展示 MySQL 数据库服务器的各类性能指标，帮助系统管理员和开发人员监控 MySQL 的技术性能。\",\"“mysql”数据库保存了 MySQL 数据库服务器运行时需要的系统信息，比如数据文件夹、当前使用的字符集、约束检查信息，等等\",\"为什么 Workbench 里面我们只能看到“demo”和“sys”这 2 个数据库呢？\",\"这是因为，Workbench 是图形化的管理工具，主要面向开发人 员，“demo”和“sys”这 2 个数据库已经够用了。如果有特殊需求，比如，需要监控 MySQL 数据库各项性能指标、直接操作 MySQL 数据库系统文件等，可以由 DBA 通过 SQL 语句，查看其它的系统数据库。\",\"2、创建自己的数据库\",\"create database 数据库名; #创建atguigudb数据库，该名称不能与已经存在的数据库重名。 create database atguigudb; \",\"3、使用自己的数据库\",\"use 数据库名; #使用atguigudb数据库 use atguigudb; \",\"说明：如果没有使用use语句，后面针对数据库的操作也没有加“数据名”的限定，那么会报“ERROR 1046 (3D000): No database selected”（没有选择数据库）\",\"使用完use语句之后，如果接下来的SQL都是针对一个数据库操作的，那就不用重复use了，如果要针对另一个数据库操作，那么要重新use。\",\"4、查看某个库的所有表格\",\"show tables; #要求前面有use语句 show tables from 数据库名; \",\"5、创建新的表格\",\"create table 表名称( 字段名 数据类型, 字段名 数据类型 ); \",\"说明：如果是最后一个字段，后面就用加逗号，因为逗号的作用是分割每个字段。\",\"#创建学生表 create table student( id int, name varchar(20) #说名字最长不超过20个字符 ); \",\"6、查看一个表的数据\",\"select * from 数据库表名称; \",\"#查看学生表的数据 select * from student; \",\"7、添加一条记录\",\"insert into 表名称 values(值列表); #添加两条记录到student表中 insert into student values(1,'张三'); insert into student values(2,'李四'); \",\"报错：\",\"mysql> insert into student values(1,'张三'); ERROR 1366 (HY000): Incorrect string value: '\\\\xD5\\\\xC5\\\\xC8\\\\xFD' for column 'name' at row 1 mysql> insert into student values(2,'李四'); ERROR 1366 (HY000): Incorrect string value: '\\\\xC0\\\\xEE\\\\xCB\\\\xC4' for column 'name' at row 1 mysql> show create table student; \",\"字符集的问题。\",\"8、查看表的创建信息\",\"show create table 表名称\\\\G #查看student表的详细创建信息 show create table student\\\\G \",\"#结果如下 *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec) \",\"上面的结果显示student的表格的默认字符集是“latin1”不支持中文。\",\"9、查看数据库的创建信息\",\"show create database 数据库名\\\\G #查看atguigudb数据库的详细创建信息 show create database atguigudb\\\\G \",\"#结果如下 *************************** 1. row *************************** Database: atguigudb Create Database: CREATE DATABASE `atguigudb` /*!40100 DEFAULT CHARACTER SET latin1 */ 1 row in set (0.00 sec) \",\"上面的结果显示atguigudb数据库也不支持中文，字符集默认是latin1。\",\"10、删除表格\",\"drop table 表名称; \",\"#删除学生表 drop table student; \",\"11、删除数据库\",\"drop database 数据库名; \",\"#删除atguigudb数据库 drop database atguigudb; \"]},{\"header\":\"4.2 MySQL的编码设置\",\"slug\":\"_4-2-mysql的编码设置\",\"contents\":[]},{\"header\":\"MySQL5.7中\",\"slug\":\"mysql5-7中\",\"contents\":[\"问题再现：命令行操作sql乱码问题\",\"mysql> INSERT INTO t_stu VALUES(1,'张三','男'); ERROR 1366 (HY000): Incorrect string value: '\\\\xD5\\\\xC5\\\\xC8\\\\xFD' for column 'sname' at row 1 \",\"问题解决\",\"步骤1：查看编码命令\",\"show variables like 'character_%'; show variables like 'collation_%'; \",\"步骤2：修改mysql的数据目录下的my.ini配置文件\",\"[mysql] #大概在63行左右，在其下添加 ... default-character-set=utf8 #默认字符集 [mysqld] ## 大概在76行左右，在其下添加 ... character-set-server=utf8 collation-server=utf8_general_ci \",\"注意：建议修改配置文件使用notepad++等高级文本编辑器，使用记事本等软件打开修改后可能会导致文件编码修改为“含BOM头”的编码，从而服务重启失败。\",\"步骤3：重启服务\",\"步骤4：查看编码命令\",\"show variables like 'character_%'; show variables like 'collation_%'; \",\"如果是以上配置就说明对了。接着我们就可以新创建数据库、新创建数据表，接着添加包含中文的数据了。\"]},{\"header\":\"MySQL8.0中\",\"slug\":\"mysql8-0中\",\"contents\":[\"在MySQL 8.0版本之前，默认字符集为latin1，utf8字符集指向的是utf8mb3。网站开发人员在数据库设计的时候往往会将编码修改为utf8字符集。如果遗忘修改默认的编码，就会出现乱码的问题。从MySQL 8.0开始，数据库的默认编码改为utf8mb4，从而避免了上述的乱码问题。\"]},{\"header\":\"5. MySQL图形化管理工具\",\"slug\":\"_5-mysql图形化管理工具\",\"contents\":[\"MySQL图形化管理工具极大地方便了数据库的操作与管理，常用的图形化管理工具有：MySQL Workbench、phpMyAdmin、Navicat Preminum、MySQLDumper、SQLyog、dbeaver、MySQL ODBC Connector。\"]},{\"header\":\"工具1. MySQL Workbench\",\"slug\":\"工具1-mysql-workbench\",\"contents\":[\"MySQL官方提供的图形化管理工具MySQL Workbench完全支持MySQL 5.0以上的版本。MySQL Workbench分为社区版和商业版，社区版完全免费，而商业版则是按年收费。\",\"MySQL Workbench 为数据库管理员、程序开发者和系统规划师提供可视化设计、模型建立、以及数据库管理功能。它包含了用于创建复杂的数据建模ER模型，正向和逆向数据库工程，也可以用于执行通常需要花费大量时间的、难以变更和管理的文档任务。\",\"下载地址：http://dev.mysql.com/downloads/workbench/。\",\"使用：\",\"首先，我们点击 Windows 左下角的“开始”按钮，如果你是 Win10 系统，可以直接看到所有程序。接着，找到“MySQL”，点开，找到“MySQL Workbench 8.0 CE”。点击打开 Workbench，如下图所示：\",\"左下角有个本地连接，点击，录入 Root 的密码，登录本地 MySQL 数据库服务器，如下图所示：\",\"这是一个图形化的界面，我来给你介绍下这个界面。\",\"上方是菜单。左上方是导航栏，这里我们可以看到 MySQL 数据库服务器里面的数据 库，包括数据表、视图、存储过程和函数；左下方是信息栏，可以显示上方选中的数据 库、数据表等对象的信息。\",\"中间上方是工作区，你可以在这里写 SQL 语句，点击上方菜单栏左边的第三个运行按 钮，就可以执行工作区的 SQL 语句了。\",\"中间下方是输出区，用来显示 SQL 语句的运行情况，包括什么时间开始运行的、运行的 内容、运行的输出，以及所花费的时长等信息。\",\"好了，下面我们就用 Workbench 实际创建一个数据库，并且导入一个 Excel 数据文件， 来生成一个数据表。数据表是存储数据的载体，有了数据表以后，我们就能对数据进行操作了。\"]},{\"header\":\"工具2. Navicat\",\"slug\":\"工具2-navicat\",\"contents\":[\"Navicat MySQL是一个强大的MySQL数据库服务器管理和开发工具。它可以与任何3.21或以上版本的MySQL一起工作，支持触发器、存储过程、函数、事件、视图、管理用户等，对于新手来说易学易用。其精心设计的图形用户界面（GUI）可以让用户用一种安全简便的方式来快速方便地创建、组织、访问和共享信息。Navicat支持中文，有免费版本提供。 下载地址：http://www.navicat.com/。\"]},{\"header\":\"工具3. SQLyog\",\"slug\":\"工具3-sqlyog\",\"contents\":[\"SQLyog 是业界著名的 Webyog 公司出品的一款简洁高效、功能强大的图形化 MySQL 数据库管理工具。这款工具是使用C++语言开发的。该工具可以方便地创建数据库、表、视图和索引等，还可以方便地进行插入、更新和删除等操作，同时可以方便地进行数据库、数据表的备份和还原。该工具不仅可以通过SQL文件进行大量文件的导入和导出，还可以导入和导出XML、HTML和CSV等多种格式的数据。 下载地址：http://www.webyog.com/，读者也可以搜索中文版的下载地址。\"]},{\"header\":\"工具4：dbeaver\",\"slug\":\"工具4-dbeaver\",\"contents\":[\"DBeaver是一个通用的数据库管理工具和 SQL 客户端，支持所有流行的数据库：MySQL、PostgreSQL、SQLite、Oracle、DB2、SQL Server、 Sybase、MS Access、Teradata、 Firebird、Apache Hive、Phoenix、Presto等。DBeaver比大多数的SQL管理工具要轻量，而且支持中文界面。DBeaver社区版作为一个免费开源的产品，和其他类似的软件相比，在功能和易用性上都毫不逊色。\",\"唯一需要注意是 DBeaver 是用Java编程语言开发的，所以需要拥有 JDK（Java Development ToolKit）环境。如果电脑上没有JDK，在选择安装DBeaver组件时，勾选“Include Java”即可。\",\"下载地址：https://dbeaver.io/download/\"]},{\"header\":\"可能出现连接问题：\",\"slug\":\"可能出现连接问题\",\"contents\":[\"有些图形界面工具，特别是旧版本的图形界面工具，在连接MySQL8时出现“Authentication plugin 'caching_sha2_password' cannot be loaded”错误。\",\"出现这个原因是MySQL8之前的版本中加密规则是mysql_native_password，而在MySQL8之后，加密规则是caching_sha2_password。解决问题方法有两种，第一种是升级图形界面工具版本，第二种是把MySQL8用户登录密码加密规则还原成mysql_native_password。\",\"第二种解决方案如下，用命令行登录MySQL数据库之后，执行如下命令修改用户密码加密规则并更新用户密码，这里修改用户名为“root@localhost”的用户密码规则为“mysql_native_password”，密码值为“123456”，如图所示。\",\"#使用mysql数据库 USE mysql; #修改'root'@'localhost'用户的密码规则和密码 ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'abc123'; #刷新权限 FLUSH PRIVILEGES; \"]},{\"header\":\"6. MySQL目录结构与源码\",\"slug\":\"_6-mysql目录结构与源码\",\"contents\":[]},{\"header\":\"6.1 主要目录结构\",\"slug\":\"_6-1-主要目录结构\",\"contents\":[\"MySQL的目录结构\",\"说明\",\"bin目录\",\"所有MySQL的可执行文件。如：mysql.exe\",\"MySQLInstanceConfig.exe\",\"数据库的配置向导，在安装时出现的内容\",\"data目录\",\"系统数据库所在的目录\",\"my.ini文件\",\"MySQL的主要配置文件\",\"c:\\\\ProgramData\\\\MySQL\\\\MySQL Server 8.0\\\\data\\\\\",\"用户创建的数据库所在的目录\"]},{\"header\":\"6.2 MySQL 源代码获取\",\"slug\":\"_6-2-mysql-源代码获取\",\"contents\":[\"首先，你要进入 MySQL下载界面。 这里你不要选择用默认的“Microsoft Windows”，而是要通过下拉栏，找到“Source Code”，在下面的操作系统版本里面， 选择 Windows（Architecture Independent），然后点击下载。\",\"接下来，把下载下来的压缩文件解压，我们就得到了 MySQL 的源代码。\",\"MySQL 是用 C++ 开发而成的，我简单介绍一下源代码的组成。\",\"mysql-8.0.22 目录下的各个子目录，包含了 MySQL 各部分组件的源代码：\",\"sql 子目录是 MySQL 核心代码；\",\"libmysql 子目录是客户端程序 API；\",\"mysql-test 子目录是测试工具；\",\"mysys 子目录是操作系统相关函数和辅助函数；\",\"源代码可以用记事本打开查看，如果你有 C++ 的开发环境，也可以在开发环境中打开查看。\",\"如上图所示，源代码并不神秘，就是普通的 C++ 代码，跟你熟悉的一样，而且有很多注释，可以帮助你理解。阅读源代码就像在跟 MySQL 的开发人员对话一样，十分有趣。\"]},{\"header\":\"7. 常见问题的解决(课外内容)\",\"slug\":\"_7-常见问题的解决-课外内容\",\"contents\":[]},{\"header\":\"问题1：root用户密码忘记，重置的操作\",\"slug\":\"问题1-root用户密码忘记-重置的操作\",\"contents\":[\"1: 通过任务管理器或者服务管理，关掉mysqld(服务进程) 2: 通过命令行+特殊参数开启mysqld mysqld --defaults-file=\\\"D:\\\\ProgramFiles\\\\mysql\\\\MySQLServer5.7Data\\\\my.ini\\\" --skip-grant-tables\",\"3: 此时，mysqld服务进程已经打开。并且不需要权限检查 4: mysql -uroot 无密码登陆服务器。另启动一个客户端进行 5: 修改权限表 （1） use mysql; （2）update user set authentication_string=password('新密码') where user='root' and Host='localhost'; （3）flush privileges; 6: 通过任务管理器，关掉mysqld服务进程。 7: 再次通过服务管理，打开mysql服务。 8: 即可用修改后的新密码登陆。\"]},{\"header\":\"问题2：mysql命令报“不是内部或外部命令”\",\"slug\":\"问题2-mysql命令报-不是内部或外部命令\",\"contents\":[\"如果输入mysql命令报“不是内部或外部命令”，把mysql安装目录的bin目录配置到环境变量path中。如下：\"]},{\"header\":\"问题3：错误ERROR ：没有选择数据库就操作表格和数据\",\"slug\":\"问题3-错误error-没有选择数据库就操作表格和数据\",\"contents\":[\"ERROR 1046 (3D000): No database selected\",\"解决方案一：就是使用“USE 数据库名;”语句，这样接下来的语句就默认针对这个数据库进行操作\",\"解决方案二：就是所有的表对象前面都加上“数据库.”\"]},{\"header\":\"问题4：命令行客户端的字符集问题\",\"slug\":\"问题4-命令行客户端的字符集问题\",\"contents\":[\"mysql> INSERT INTO t_stu VALUES(1,'张三','男'); ERROR 1366 (HY000): Incorrect string value: '\\\\xD5\\\\xC5\\\\xC8\\\\xFD' for column 'sname' at row 1 \",\"原因：服务器端认为你的客户端的字符集是utf-8，而实际上你的客户端的字符集是GBK。\",\"查看所有字符集：SHOW VARIABLES LIKE 'character_set_%';\",\"解决方案，设置当前连接的客户端字符集 “SET NAMES GBK;”\"]},{\"header\":\"问题5：修改数据库和表的字符编码\",\"slug\":\"问题5-修改数据库和表的字符编码\",\"contents\":[\"修改编码：\",\"（1)先停止服务，（2）修改my.ini文件（3）重新启动服务\",\"说明：\",\"如果是在修改my.ini之前建的库和表，那么库和表的编码还是原来的Latin1，要么删了重建，要么使用alter语句修改编码。\",\"mysql> create database 0728db charset Latin1; Query OK, 1 row affected (0.00 sec) \",\"mysql> use 0728db; Database changed \",\"mysql> create table student (id int , name varchar(20)) charset Latin1; Query OK, 0 rows affected (0.02 sec) mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec) \",\"mysql> alter table student charset utf8; #修改表字符编码为UTF8 Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) CHARACTER SET latin1 DEFAULT NULL, #字段仍然是latin1编码 PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) mysql> alter table student modify name varchar(20) charset utf8; #修改字段字符编码为UTF8 Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> show create table student\\\\G *************************** 1. row *************************** Table: student Create Table: CREATE TABLE `student` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) \",\"mysql> show create database 0728db;; +--------+-----------------------------------------------------------------+ |Database| Create Database | +------+-------------------------------------------------------------------+ |0728db| CREATE DATABASE `0728db` /*!40100 DEFAULT CHARACTER SET latin1 */ | +------+-------------------------------------------------------------------+ 1 row in set (0.00 sec) mysql> alter database 0728db charset utf8; #修改数据库的字符编码为utf8 Query OK, 1 row affected (0.00 sec) mysql> show create database 0728db; +--------+-----------------------------------------------------------------+ |Database| Create Database | +--------+-----------------------------------------------------------------+ | 0728db | CREATE DATABASE `0728db` /*!40100 DEFAULT CHARACTER SET utf8 */ | +--------+-----------------------------------------------------------------+ 1 row in set (0.00 sec) \"]}]},\"/study-tutorial/database/mysql/management-table.html\":{\"title\":\"10、创建和管理表\",\"contents\":[{\"header\":\"1. 基础知识\",\"slug\":\"_1-基础知识\",\"contents\":[]},{\"header\":\"1.1 一条数据存储的过程\",\"slug\":\"_1-1-一条数据存储的过程\",\"contents\":[\"存储数据是处理数据的第一步。只有正确地把数据存储起来，我们才能进行有效的处理和分析。否则，只能是一团乱麻，无从下手。\",\"那么，怎样才能把用户各种经营相关的、纷繁复杂的数据，有序、高效地存储起来呢？ 在 MySQL 中，一个完整的数据存储过程总共有 4 步，分别是创建数据库、确认字段、创建数据表、插入数据。\",\"我们要先创建一个数据库，而不是直接创建数据表呢？\",\"因为从系统架构的层次上看，MySQL 数据库系统从大到小依次是数据库服务器、数据库、数据表、数据表的行与列。\",\"MySQL 数据库服务器之前已经安装。所以，我们就从创建数据库开始。\"]},{\"header\":\"1.2 标识符命名规则\",\"slug\":\"_1-2-标识符命名规则\",\"contents\":[\"数据库名、表名不得超过30个字符，变量名限制为29个\",\"必须只能包含 A–Z, a–z, 0–9, _共63个字符\",\"数据库名、表名、字段名等对象名中间不要包含空格\",\"同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名\",\"必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使用`（着重号）引起来\",\"保持字段名和类型的一致性：在命名字段并为其指定数据类型的时候一定要保证一致性，假如数据类型在一个表里是整数，那在另一个表里可就别变成字符型了\"]},{\"header\":\"1.3 MySQL中的数据类型\",\"slug\":\"_1-3-mysql中的数据类型\",\"contents\":[\"类型\",\"类型举例\",\"整数类型\",\"TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT\",\"浮点类型\",\"FLOAT、DOUBLE\",\"定点数类型\",\"DECIMAL\",\"位类型\",\"BIT\",\"日期时间类型\",\"YEAR、TIME、DATE、DATETIME、TIMESTAMP\",\"文本字符串类型\",\"CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT\",\"枚举类型\",\"ENUM\",\"集合类型\",\"SET\",\"二进制字符串类型\",\"BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB\",\"JSON类型\",\"JSON对象、JSON数组\",\"空间数据类型\",\"单值：GEOMETRY、POINT、LINESTRING、POLYGON；集合：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION\",\"其中，常用的几类类型介绍如下：\",\"数据类型\",\"描述\",\"INT\",\"从-231到231-1的整型数据。存储大小为 4个字节\",\"CHAR(size)\",\"定长字符数据。若未指定，默认为1个字符，最大长度255\",\"VARCHAR(size)\",\"可变长字符数据，根据字符串实际长度保存，必须指定长度\",\"FLOAT(M,D)\",\"单精度，占用4个字节，M=整数位+小数位，D=小数位。 D<=M<=255,0<=D<=30，默认M+D<=6\",\"DOUBLE(M,D)\",\"双精度，占用8个字节，D<=M<=255,0<=D<=30，默认M+D<=15\",\"DECIMAL(M,D)\",\"高精度小数，占用M+2个字节，D<=M<=65，0<=D<=30，最大取值范围与DOUBLE相同。\",\"DATE\",\"日期型数据，格式'YYYY-MM-DD'\",\"BLOB\",\"二进制形式的长文本数据，最大可达4G\",\"TEXT\",\"长文本数据，最大可达4G\"]},{\"header\":\"2. 创建和管理数据库\",\"slug\":\"_2-创建和管理数据库\",\"contents\":[]},{\"header\":\"2.1 创建数据库\",\"slug\":\"_2-1-创建数据库\",\"contents\":[\"方式1：创建数据库\",\"CREATE DATABASE 数据库名; \",\"方式2：创建数据库并指定字符集\",\"CREATE DATABASE 数据库名 CHARACTER SET 字符集; \",\"方式3：判断数据库是否已经存在，不存在则创建数据库（推荐）\",\"CREATE DATABASE IF NOT EXISTS 数据库名; \",\"如果MySQL中已经存在相关的数据库，则忽略创建语句，不再创建数据库。\",\"注意：DATABASE 不能改名。一些可视化工具可以改名，它是建新库，把所有表复制到新库，再删旧库完成的。\"]},{\"header\":\"2.2 使用数据库\",\"slug\":\"_2-2-使用数据库\",\"contents\":[\"查看当前所有的数据库\",\"SHOW DATABASES; #有一个S，代表多个数据库 \",\"查看当前正在使用的数据库\",\"SELECT DATABASE(); #使用的一个 mysql 中的全局函数 \",\"查看指定库下所有的表\",\"SHOW TABLES FROM 数据库名; \",\"查看数据库的创建信息\",\"SHOW CREATE DATABASE 数据库名; 或者： SHOW CREATE DATABASE 数据库名\\\\G \",\"使用/切换数据库\",\"USE 数据库名; \",\"注意：要操作表格和数据之前必须先说明是对哪个数据库进行操作，否则就要对所有对象加上“数据库名.”。\"]},{\"header\":\"2.3 修改数据库\",\"slug\":\"_2-3-修改数据库\",\"contents\":[\"更改数据库字符集\",\"ALTER DATABASE 数据库名 CHARACTER SET 字符集; #比如：gbk、utf8等 \"]},{\"header\":\"2.4 删除数据库\",\"slug\":\"_2-4-删除数据库\",\"contents\":[\"方式1：删除指定的数据库\",\"DROP DATABASE 数据库名; \",\"方式2：删除指定的数据库（推荐）\",\"DROP DATABASE IF EXISTS 数据库名; \"]},{\"header\":\"3. 创建表\",\"slug\":\"_3-创建表\",\"contents\":[]},{\"header\":\"3.1 创建方式1\",\"slug\":\"_3-1-创建方式1\",\"contents\":[\"必须具备：\",\"CREATE TABLE权限\",\"存储空间\",\"语法格式：\",\"CREATE TABLE [IF NOT EXISTS] 表名( 字段1, 数据类型 [约束条件] [默认值], 字段2, 数据类型 [约束条件] [默认值], 字段3, 数据类型 [约束条件] [默认值], …… [表约束条件] ); \",\"加上了IF NOT EXISTS关键字，则表示：如果当前数据库中不存在要创建的数据表，则创建数据表；如果当前数据库中已经存在要创建的数据表，则忽略建表语句，不再创建数据表。\",\"必须指定：\",\"表名\",\"列名(或字段名)，数据类型，长度\",\"可选指定：\",\"约束条件\",\"默认值\",\"创建表举例1：\",\"-- 创建表 CREATE TABLE emp ( -- int类型 emp_id INT, -- 最多保存20个中英文字符 emp_name VARCHAR(20), -- 总位数不超过15位 salary DOUBLE, -- 日期类型 birthday DATE ); \",\"DESC emp; \",\"MySQL在执行建表语句时，将id字段的类型设置为int(11)，这里的11实际上是int类型指定的显示宽度，默认的显示宽度为11。也可以在创建数据表的时候指定数据的显示宽度。\",\"创建表举例2：\",\"CREATE TABLE dept( -- int类型，自增 deptno INT(2) AUTO_INCREMENT, dname VARCHAR(14), loc VARCHAR(13), -- 主键 PRIMARY KEY (deptno) ); \",\"DESCRIBE dept; \",\"在MySQL 8.x版本中，不再推荐为INT类型指定显示长度，并在未来的版本中可能去掉这样的语法。\"]},{\"header\":\"3.2 创建方式2\",\"slug\":\"_3-2-创建方式2\",\"contents\":[\"使用 AS subquery 选项，将创建表和插入数据结合起来\",\"指定的列和子查询中的列要一一对应\",\"通过列名和默认值定义列\",\"CREATE TABLE emp1 AS SELECT * FROM employees; CREATE TABLE emp2 AS SELECT * FROM employees WHERE 1=2; -- 创建的emp2是空表 \",\"CREATE TABLE dept80 AS SELECT employee_id, last_name, salary*12 ANNSAL, hire_date FROM employees WHERE department_id = 80; \",\"DESCRIBE dept80; \"]},{\"header\":\"3.3 查看数据表结构\",\"slug\":\"_3-3-查看数据表结构\",\"contents\":[\"在MySQL中创建好数据表之后，可以查看数据表的结构。MySQL支持使用DESCRIBE/DESC语句查看数据表结构，也支持使用SHOW CREATE TABLE语句查看数据表结构。\",\"语法格式如下：\",\"SHOW CREATE TABLE 表名\\\\G \",\"使用SHOW CREATE TABLE语句不仅可以查看表创建时的详细语句，还可以查看存储引擎和字符编码。\"]},{\"header\":\"4. 修改表\",\"slug\":\"_4-修改表\",\"contents\":[\"修改表指的是修改数据库中已经存在的数据表的结构。\",\"使用 ALTER TABLE 语句可以实现：\",\"向已有的表中添加列\",\"修改现有表中的列\",\"删除现有表中的列\",\"重命名现有表中的列\"]},{\"header\":\"4.1 追加一个列\",\"slug\":\"_4-1-追加一个列\",\"contents\":[\"语法格式如下：\",\"ALTER TABLE 表名 ADD 【COLUMN】 字段名 字段类型 【FIRST|AFTER 字段名】; \",\"举例：\",\"ALTER TABLE dept80 ADD job_id varchar(15); \"]},{\"header\":\"4.2 修改一个列\",\"slug\":\"_4-2-修改一个列\",\"contents\":[\"可以修改列的数据类型，长度、默认值和位置\",\"修改字段数据类型、长度、默认值、位置的语法格式如下：\",\"ALTER TABLE 表名 MODIFY 【COLUMN】 字段名1 字段类型 【DEFAULT 默认值】【FIRST|AFTER 字段名2】; \",\"举例：\",\"ALTER TABLE dept80 MODIFY last_name VARCHAR(30); \",\"ALTER TABLE dept80 MODIFY salary double(9,2) default 1000; \",\"对默认值的修改只影响今后对表的修改\",\"此外，还可以通过此种方式修改列的约束。这里暂先不讲。\"]},{\"header\":\"4.3 重命名一个列\",\"slug\":\"_4-3-重命名一个列\",\"contents\":[\"使用 CHANGE old_column new_column dataType子句重命名列。语法格式如下：\",\"ALTER TABLE 表名 CHANGE 【column】 列名 新列名 新数据类型; \",\"举例：\",\"ALTER TABLE dept80 CHANGE department_name dept_name varchar(15); \"]},{\"header\":\"4.4 删除一个列\",\"slug\":\"_4-4-删除一个列\",\"contents\":[\"删除表中某个字段的语法格式如下：\",\"ALTER TABLE 表名 DROP 【COLUMN】字段名 \",\"举例：\",\"ALTER TABLE dept80 DROP COLUMN job_id; \"]},{\"header\":\"5. 重命名表\",\"slug\":\"_5-重命名表\",\"contents\":[\"方式一：使用RENAME\",\"RENAME TABLE emp TO myemp; \",\"方式二：\",\"ALTER table dept RENAME [TO] detail_dept; -- [TO]可以省略 \",\"必须是对象的拥有者\"]},{\"header\":\"6. 删除表\",\"slug\":\"_6-删除表\",\"contents\":[\"在MySQL中，当一张数据表没有与其他任何数据表形成关联关系时，可以将当前数据表直接删除。\",\"数据和结构都被删除\",\"所有正在运行的相关事务被提交\",\"所有相关索引被删除\",\"语法格式：\",\"DROP TABLE [IF EXISTS] 数据表1 [, 数据表2, …, 数据表n]; \",\"IF EXISTS的含义为：如果当前数据库中存在相应的数据表，则删除数据表；如果当前数据库中不存在相应的数据表，则忽略删除语句，不再执行删除数据表的操作。\",\"举例：\",\"DROP TABLE dept80; \",\"DROP TABLE 语句不能回滚\"]},{\"header\":\"7. 清空表\",\"slug\":\"_7-清空表\",\"contents\":[\"TRUNCATE TABLE语句：\",\"删除表中所有的数据\",\"释放表的存储空间\",\"举例：\",\"TRUNCATE TABLE detail_dept; \",\"TRUNCATE语句不能回滚，而使用 DELETE 语句删除数据，可以回滚\",\"对比：\",\"SET autocommit = FALSE; DELETE FROM emp2; #TRUNCATE TABLE emp2; SELECT * FROM emp2; ROLLBACK; SELECT * FROM emp2; \",\"阿里开发规范：\",\"【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但 TRUNCATE 无事务且不触发 TRIGGER，有可能造成事故，故不建议在开发代码中使用此语句。\",\"说明：TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同。\"]},{\"header\":\"8. 内容拓展\",\"slug\":\"_8-内容拓展\",\"contents\":[]},{\"header\":\"拓展1：阿里巴巴《Java开发手册》之MySQL字段命名\",\"slug\":\"拓展1-阿里巴巴《java开发手册》之mysql字段命名\",\"contents\":[\"【强制】表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\",\"正例：aliyun_admin，rdc_config，level3_name\",\"反例：AliyunAdmin，rdcConfig，level_3_name\",\"【强制】禁用保留字，如 desc、range、match、delayed 等，请参考 MySQL 官方保留字。\",\"【强制】表必备三字段：id, gmt_create, gmt_modified。\",\"说明：其中 id 必为主键，类型为BIGINT UNSIGNED、单表时自增、步长为 1。gmt_create, gmt_modified 的类型均为 DATETIME 类型，前者现在时表示主动式创建，后者过去分词表示被动式更新\",\"【推荐】表的命名最好是遵循 “业务名称_表的作用”。\",\"正例：alipay_task 、 force_project、 trade_config\",\"【推荐】库名与应用名称尽量一致。\",\"【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。\",\"正例：无符号值可以避免误存负数，且扩大了表示范围。\"]},{\"header\":\"拓展2：如何理解清空表、删除表等操作需谨慎？！\",\"slug\":\"拓展2-如何理解清空表、删除表等操作需谨慎\",\"contents\":[\"表删除操作将把表的定义和表中的数据一起删除，并且MySQL在执行删除操作时，不会有任何的确认信息提示，因此执行删除操时应当慎重。在删除表前，最好对表中的数据进行备份，这样当操作失误时可以对数据进行恢复，以免造成无法挽回的后果。\",\"同样的，在使用 ALTER TABLE 进行表的基本修改操作时，在执行操作过程之前，也应该确保对数据进行完整的备份，因为数据库的改变是无法撤销的，如果添加了一个不需要的字段，可以将其删除；相同的，如果删除了一个需要的列，该列下面的所有数据都将会丢失。\"]},{\"header\":\"拓展3：MySQL8新特性—DDL的原子化\",\"slug\":\"拓展3-mysql8新特性—ddl的原子化\",\"contents\":[\"在MySQL 8.0版本中，InnoDB表的DDL支持事务完整性，即DDL操作要么成功要么回滚。DDL操作回滚日志写入到data dictionary数据字典表mysql.innodb_ddl_log（该表是隐藏的表，通过show tables无法看到）中，用于回滚操作。通过设置参数，可将DDL操作日志打印输出到MySQL错误日志中。\",\"分别在MySQL 5.7版本和MySQL 8.0版本中创建数据库和数据表，结果如下：\",\"CREATE DATABASE mytest; USE mytest; CREATE TABLE book1( book_id INT , book_name VARCHAR(255) ); SHOW TABLES; \",\"（1）在MySQL 5.7版本中，测试步骤如下： 删除数据表book1和数据表book2，结果如下：\",\"mysql> DROP TABLE book1,book2; ERROR 1051 (42S02): Unknown table 'mytest.book2' \",\"再次查询数据库中的数据表名称，结果如下：\",\"mysql> SHOW TABLES; Empty set (0.00 sec) \",\"从结果可以看出，虽然删除操作时报错了，但是仍然删除了数据表book1。\",\"（2）在MySQL 8.0版本中，测试步骤如下： 删除数据表book1和数据表book2，结果如下：\",\"mysql> DROP TABLE book1,book2; ERROR 1051 (42S02): Unknown table 'mytest.book2' \",\"再次查询数据库中的数据表名称，结果如下：\",\"mysql> show tables; +------------------+ | Tables_in_mytest | +------------------+ | book1 | +------------------+ 1 row in set (0.00 sec) \",\"从结果可以看出，数据表book1并没有被删除。\"]}]},\"/study-tutorial/database/mysql/multi-table-query.html\":{\"title\":\"6、多表查询\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"多表查询，也称为关联查询，指两个或更多个表一起完成查询操作。\",\"前提条件：这些一起查询的表之间是有关系的（一对一、一对多），它们之间一定是有关联字段，这个关联字段可能建立了外键，也可能没有建立外键。比如：员工表和部门表，这两个表依靠“部门编号”进行关联。\"]},{\"header\":\"1. 一个案例引发的多表连接\",\"slug\":\"_1-一个案例引发的多表连接\",\"contents\":[]},{\"header\":\"1.1 案例说明\",\"slug\":\"_1-1-案例说明\",\"contents\":[\"从多个表中获取数据：\",\"#案例：查询员工的姓名及其部门名称 SELECT last_name, department_name FROM employees, departments; \",\"查询结果：\",\"+-----------+----------------------+ | last_name | department_name | +-----------+----------------------+ | King | Administration | | King | Marketing | | King | Purchasing | | King | Human Resources | | King | Shipping | | King | IT | | King | Public Relations | | King | Sales | | King | Executive | | King | Finance | | King | Accounting | | King | Treasury | ... | Gietz | IT Support | | Gietz | NOC | | Gietz | IT Helpdesk | | Gietz | Government Sales | | Gietz | Retail Sales | | Gietz | Recruiting | | Gietz | Payroll | +-----------+----------------------+ 2889 rows in set (0.01 sec) \",\"分析错误情况：\",\"SELECT COUNT(employee_id) FROM employees; #输出107行 SELECT COUNT(department_id)FROM departments; #输出27行 SELECT 107*27 FROM dual; \",\"我们把上述多表查询中出现的问题称为：笛卡尔积的错误。\"]},{\"header\":\"1.2 笛卡尔积（或交叉连接）的理解\",\"slug\":\"_1-2-笛卡尔积-或交叉连接-的理解\",\"contents\":[\"笛卡尔乘积是一个数学运算。假设我有两个集合 X 和 Y，那么 X 和 Y 的笛卡尔积就是 X 和 Y 的所有可能组合，也就是第一个对象来自于 X，第二个对象来自于 Y 的所有可能。组合的个数即为两个集合中元素个数的乘积数。\",\"SQL92中，笛卡尔积也称为交叉连接，英文是 CROSS JOIN。在 SQL99 中也是使用 CROSS JOIN表示交叉连接。它的作用就是可以把任意表进行连接，即使这两张表不相关。在MySQL中如下情况会出现笛卡尔积：\",\"#查询员工姓名和所在部门名称 SELECT last_name,department_name FROM employees,departments; SELECT last_name,department_name FROM employees CROSS JOIN departments; SELECT last_name,department_name FROM employees INNER JOIN departments; SELECT last_name,department_name FROM employees JOIN departments; \"]},{\"header\":\"1.3 案例分析与问题解决\",\"slug\":\"_1-3-案例分析与问题解决\",\"contents\":[\"笛卡尔积的错误会在下面条件下产生：\",\"省略多个表的连接条件（或关联条件）\",\"连接条件（或关联条件）无效\",\"所有表中的所有行互相连接\",\"为了避免笛卡尔积， 可以在 WHERE 加入有效的连接条件。\",\"加入连接条件后，查询语法：\",\"SELECT table1.column, table2.column FROM table1, table2 WHERE table1.column1 = table2.column2; #连接条件 \",\"在 WHERE子句中写入连接条件。\",\"正确写法：\",\"#案例：查询员工的姓名及其部门名称 SELECT last_name, department_name FROM employees, departments WHERE employees.department_id = departments.department_id; \",\"在表中有相同列时，在列名之前加上表名前缀。\"]},{\"header\":\"2. 多表查询分类讲解\",\"slug\":\"_2-多表查询分类讲解\",\"contents\":[]},{\"header\":\"分类1：等值连接 vs 非等值连接\",\"slug\":\"分类1-等值连接-vs-非等值连接\",\"contents\":[]},{\"header\":\"等值连接\",\"slug\":\"等值连接\",\"contents\":[\"SELECT employees.employee_id, employees.last_name, employees.department_id, departments.department_id, departments.location_id FROM employees, departments WHERE employees.department_id = departments.department_id; \",\"拓展1：多个连接条件与 AND 操作符\",\"拓展2：区分重复的列名\",\"多个表中有相同列时，必须在列名之前加上表名前缀。\",\"在不同表中具有相同列名的列可以用表名加以区分。\",\"SELECT employees.last_name, departments.department_name,employees.department_id FROM employees, departments WHERE employees.department_id = departments.department_id; \",\"拓展3：表的别名\",\"使用别名可以简化查询。\",\"列名前使用表名前缀可以提高查询效率。\",\"SELECT e.employee_id, e.last_name, e.department_id, d.department_id, d.location_id FROM employees e , departments d WHERE e.department_id = d.department_id; \",\"需要注意的是，如果我们使用了表的别名，在查询字段中、过滤条件中就只能使用别名进行代替，不能使用原有的表名，否则就会报错。\",\"阿里开发规范：\",\"【强制】对于数据库中表记录的查询和变更，只要涉及多个表，都需要在列名前加表的别名（或 表名）进行限定。\",\"说明：对多表进行查询记录、更新记录、删除记录时，如果对操作列没有限定表的别名（或表名），并且操作列在多个表中存在时，就会抛异常。\",\"正例：select t1.name from table_first as t1 , table_second as t2 where t1.id=t2.id;\",\"反例：在某业务中，由于多表关联查询语句没有加表的别名（或表名）的限制，正常运行两年后，最近在 某个表中增加一个同名字段，在预发布环境做数据库变更后，线上查询语句出现出 1052 异常：Column 'name' in field list is ambiguous。\",\"拓展4：连接多个表\",\"**总结：连接 n个表,至少需要n-1个连接条件。**比如，连接三个表，至少需要两个连接条件。\",\"练习：查询出公司员工的 last_name,department_name, city\"]},{\"header\":\"非等值连接\",\"slug\":\"非等值连接\",\"contents\":[\"SELECT e.last_name, e.salary, j.grade_level FROM employees e, job_grades j WHERE e.salary BETWEEN j.lowest_sal AND j.highest_sal; \"]},{\"header\":\"分类2：自连接 vs 非自连接\",\"slug\":\"分类2-自连接-vs-非自连接\",\"contents\":[\"当table1和table2本质上是同一张表，只是用取别名的方式虚拟成两张表以代表不同的意义。然后两个表再进行内连接，外连接等查询。\",\"题目：查询employees表，返回“Xxx works for Xxx”\",\"SELECT CONCAT(worker.last_name ,' works for ' , manager.last_name) FROM employees worker, employees manager WHERE worker.manager_id = manager.employee_id ; \",\"练习：查询出last_name为 ‘Chen’ 的员工的 manager 的信息。\"]},{\"header\":\"分类3：内连接 vs 外连接\",\"slug\":\"分类3-内连接-vs-外连接\",\"contents\":[\"除了查询满足条件的记录以外，外连接还可以查询某一方不满足条件的记录。\",\"内连接: 合并具有同一列的两个以上的表的行, 结果集中不包含一个表与另一个表不匹配的行\",\"外连接: 两个表在连接过程中除了返回满足连接条件的行以外还返回左（或右）表中不满足条件的行，这种连接称为左（或右） 外连接。没有匹配的行时, 结果表中相应的列为空(NULL)。\",\"如果是左外连接，则连接条件中左边的表也称为主表，右边的表称为从表。\",\"如果是右外连接，则连接条件中右边的表也称为主表，左边的表称为从表。\"]},{\"header\":\"SQL92：使用(+)创建连接\",\"slug\":\"sql92-使用-创建连接\",\"contents\":[\"在 SQL92 中采用（+）代表从表所在的位置。即左或右外连接中，(+) 表示哪个是从表。\",\"Oracle 对 SQL92 支持较好，而 MySQL 则不支持 SQL92 的外连接。\",\"#左外连接 SELECT last_name,department_name FROM employees ,departments WHERE employees.department_id = departments.department_id(+); #右外连接 SELECT last_name,department_name FROM employees ,departments WHERE employees.department_id(+) = departments.department_id; \",\"而且在 SQL92 中，只有左外连接和右外连接，没有满（或全）外连接。\"]},{\"header\":\"3. SQL99语法实现多表查询\",\"slug\":\"_3-sql99语法实现多表查询\",\"contents\":[]},{\"header\":\"3.1 基本语法\",\"slug\":\"_3-1-基本语法\",\"contents\":[\"使用JOIN...ON子句创建连接的语法结构：\",\"SELECT table1.column, table2.column,table3.column FROM table1 JOIN table2 ON table1 和 table2 的连接条件 JOIN table3 ON table2 和 table3 的连接条件 \",\"它的嵌套逻辑类似我们使用的 FOR 循环：\",\"for t1 in table1: for t2 in table2: if condition1: for t3 in table3: if condition2: output t1 + t2 + t3 \",\"SQL99 采用的这种嵌套结构非常清爽、层次性更强、可读性更强，即使再多的表进行连接也都清晰可见。如果你采用 SQL92，可读性就会大打折扣。\",\"语法说明：\",\"可以使用ON子句指定额外的连接条件。\",\"这个连接条件是与其它条件分开的。\",\"ON子句使语句具有更高的易读性。\",\"关键字 JOIN、INNER JOIN、CROSS JOIN 的含义是一样的，都表示内连接\"]},{\"header\":\"3.2 内连接(INNER JOIN)的实现\",\"slug\":\"_3-2-内连接-inner-join-的实现\",\"contents\":[\"语法：\",\"SELECT 字段列表 FROM A表 INNER JOIN B表 ON 关联条件 WHERE 等其他子句; \",\"题目1：\",\"SELECT e.employee_id, e.last_name, e.department_id, d.department_id, d.location_id FROM employees e JOIN departments d ON (e.department_id = d.department_id); \",\"题目2：\",\"SELECT employee_id, city, department_name FROM employees e JOIN departments d ON d.department_id = e.department_id JOIN locations l ON d.location_id = l.location_id; \"]},{\"header\":\"3.3 外连接(OUTER JOIN)的实现\",\"slug\":\"_3-3-外连接-outer-join-的实现\",\"contents\":[]},{\"header\":\"3.3.1 左外连接(LEFT OUTER JOIN)\",\"slug\":\"_3-3-1-左外连接-left-outer-join\",\"contents\":[\"语法：\",\"#实现查询结果是A SELECT 字段列表 FROM A表 LEFT JOIN B表 ON 关联条件 WHERE 等其他子句; \",\"举例：\",\"SELECT e.last_name, e.department_id, d.department_name FROM employees e LEFT OUTER JOIN departments d ON (e.department_id = d.department_id) ; \"]},{\"header\":\"3.3.2 右外连接(RIGHT OUTER JOIN)\",\"slug\":\"_3-3-2-右外连接-right-outer-join\",\"contents\":[\"语法：\",\"#实现查询结果是B SELECT 字段列表 FROM A表 RIGHT JOIN B表 ON 关联条件 WHERE 等其他子句; \",\"举例：\",\"SELECT e.last_name, e.department_id, d.department_name FROM employees e RIGHT OUTER JOIN departments d ON (e.department_id = d.department_id) ; \",\"需要注意的是，LEFT JOIN 和 RIGHT JOIN 只存在于 SQL99 及以后的标准中，在 SQL92 中不存在，只能用 (+) 表示。\"]},{\"header\":\"3.3.3 满外连接(FULL OUTER JOIN)\",\"slug\":\"_3-3-3-满外连接-full-outer-join\",\"contents\":[\"满外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。\",\"SQL99是支持满外连接的。使用FULL JOIN 或 FULL OUTER JOIN来实现。\",\"需要注意的是，MySQL不支持FULL JOIN，但是可以用 LEFT JOIN UNION RIGHT join代替。\"]},{\"header\":\"4. UNION的使用\",\"slug\":\"_4-union的使用\",\"contents\":[\"合并查询结果 利用UNION关键字，可以给出多条SELECT语句，并将它们的结果组合成单个结果集。合并时，两个表对应的列数和数据类型必须相同，并且相互对应。各个SELECT语句之间使用UNION或UNION ALL关键字分隔。\",\"语法格式：\",\"SELECT column,... FROM table1 UNION [ALL] SELECT column,... FROM table2 \",\"UNION操作符\",\"UNION 操作符返回两个查询的结果集的并集，去除重复记录。\",\"UNION ALL操作符\",\"UNION ALL操作符返回两个查询的结果集的并集。对于两个结果集的重复部分，不去重。\",\"注意：执行UNION ALL语句时所需要的资源比UNION语句少。如果明确知道合并数据后的结果数据不存在重复数据，或者不需要去除重复的数据，则尽量使用UNION ALL语句，以提高数据查询的效率。\",\"举例：查询部门编号>90或邮箱包含a的员工信息\",\"#方式1 SELECT * FROM employees WHERE email LIKE '%a%' OR department_id>90; \",\"#方式2 SELECT * FROM employees WHERE email LIKE '%a%' UNION SELECT * FROM employees WHERE department_id>90; \",\"举例：查询中国用户中男性的信息以及美国用户中年男性的用户信息\",\"SELECT id,cname FROM t_chinamale WHERE csex='男' UNION ALL SELECT id,tname FROM t_usmale WHERE tGender='male'; \"]},{\"header\":\"5. 7种SQL JOINS的实现\",\"slug\":\"_5-7种sql-joins的实现\",\"contents\":[]},{\"header\":\"5.7.1 代码实现\",\"slug\":\"_5-7-1-代码实现\",\"contents\":[\"#中图：内连接 A∩B SELECT employee_id,last_name,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id`; \",\"#左上图：左外连接 SELECT employee_id,last_name,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id`; \",\"#右上图：右外连接 SELECT employee_id,last_name,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; \",\"#左中图：A - A∩B SELECT employee_id,last_name,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL \",\"#右中图：B-A∩B SELECT employee_id,last_name,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL \",\"#左下图：满外连接 ## 左中图 + 右上图 A∪B SELECT employee_id,last_name,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL #没有去重操作，效率高 SELECT employee_id,last_name,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; \",\"#右下图 #左中图 + 右中图 A ∪B- A∩B 或者 (A - A∩B) ∪ （B - A∩B） SELECT employee_id,last_name,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL SELECT employee_id,last_name,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL \"]},{\"header\":\"5.7.2 语法格式小结\",\"slug\":\"_5-7-2-语法格式小结\",\"contents\":[\"左中图\",\"#实现A - A∩B select 字段列表 from A表 left join B表 on 关联条件 where 从表关联字段 is null and 等其他子句; \",\"右中图\",\"#实现B - A∩B select 字段列表 from A表 right join B表 on 关联条件 where 从表关联字段 is null and 等其他子句; \",\"左下图\",\"#实现查询结果是A∪B #用左外的A，union 右外的B select 字段列表 from A表 left join B表 on 关联条件 where 等其他子句 union select 字段列表 from A表 right join B表 on 关联条件 where 等其他子句; \",\"右下图\",\"#实现A∪B - A∩B 或 (A - A∩B) ∪ （B - A∩B） #使用左外的 (A - A∩B) union 右外的（B - A∩B） select 字段列表 from A表 left join B表 on 关联条件 where 从表关联字段 is null and 等其他子句 union select 字段列表 from A表 right join B表 on 关联条件 where 从表关联字段 is null and 等其他子句 \"]},{\"header\":\"6. SQL99语法新特性\",\"slug\":\"_6-sql99语法新特性\",\"contents\":[]},{\"header\":\"6.1 自然连接\",\"slug\":\"_6-1-自然连接\",\"contents\":[\"SQL99 在 SQL92 的基础上提供了一些特殊语法，比如 NATURAL JOIN 用来表示自然连接。我们可以把自然连接理解为 SQL92 中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。\",\"在SQL92标准中：\",\"SELECT employee_id,last_name,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id` AND e.`manager_id` = d.`manager_id`; \",\"在 SQL99 中你可以写成：\",\"SELECT employee_id,last_name,department_name FROM employees e NATURAL JOIN departments d; \"]},{\"header\":\"6.2 USING连接\",\"slug\":\"_6-2-using连接\",\"contents\":[\"当我们进行连接的时候，SQL99还支持使用 USING 指定数据表里的同名字段进行等值连接。但是只能配合JOIN一起使用。比如：\",\"SELECT employee_id,last_name,department_name FROM employees e JOIN departments d USING (department_id); \",\"你能看出与自然连接 NATURAL JOIN 不同的是，USING 指定了具体的相同的字段名称，你需要在 USING 的括号 () 中填入要指定的同名字段。同时使用 JOIN...USING 可以简化 JOIN ON 的等值连接。它与下面的 SQL 查询结果是相同的：\",\"SELECT employee_id,last_name,department_name FROM employees e ,departments d WHERE e.department_id = d.department_id; \"]},{\"header\":\"7. 章节小结\",\"slug\":\"_7-章节小结\",\"contents\":[\"表连接的约束条件可以有三种方式：WHERE, ON, USING\",\"WHERE：适用于所有关联查询\",\"ON：只能和JOIN一起使用，只能写关联条件。虽然关联条件可以并到WHERE中和其他条件一起写，但分开写可读性更好。\",\"USING：只能和JOIN一起使用，而且要求两个关联字段在关联表中名称一致，而且只能表示关联字段值相等\",\"#关联条件 #把关联条件写在where后面 SELECT last_name,department_name FROM employees,departments WHERE employees.department_id = departments.department_id; #把关联条件写在on后面，只能和JOIN一起使用 SELECT last_name,department_name FROM employees INNER JOIN departments ON employees.department_id = departments.department_id; SELECT last_name,department_name FROM employees CROSS JOIN departments ON employees.department_id = departments.department_id; SELECT last_name,department_name FROM employees JOIN departments ON employees.department_id = departments.department_id; #把关联字段写在using()中，只能和JOIN一起使用 #而且两个表中的关联字段必须名称相同，而且只能表示= #查询员工姓名与基本工资 SELECT last_name,job_title FROM employees INNER JOIN jobs USING(job_id); #n张表关联，需要n-1个关联条件 #查询员工姓名，基本工资，部门名称 SELECT last_name,job_title,department_name FROM employees,departments,jobs WHERE employees.department_id = departments.department_id AND employees.job_id = jobs.job_id; SELECT last_name,job_title,department_name FROM employees INNER JOIN departments INNER JOIN jobs ON employees.department_id = departments.department_id AND employees.job_id = jobs.job_id; \",\"注意：\",\"我们要控制连接表的数量。多表连接就相当于嵌套 for 循环一样，非常消耗资源，会让 SQL 查询性能下降得很严重，因此不要连接不必要的表。在许多 DBMS 中，也都会有最大连接表的限制。\",\"【强制】超过三个表禁止 join。需要 join 的字段，数据类型保持绝对一致；多表关联查询时， 保证被关联的字段需要有索引。\",\"说明：即使双表 join 也要注意表索引、SQL 性能。\",\"来源：阿里巴巴《Java开发手册》\"]},{\"header\":\"附录：常用的 SQL 标准有哪些\",\"slug\":\"附录-常用的-sql-标准有哪些\",\"contents\":[\"在正式开始讲连接表的种类时，我们首先需要知道 SQL 存在不同版本的标准规范，因为不同规范下的表连接操作是有区别的。\",\"SQL 有两个主要的标准，分别是 SQL92 和 SQL99。92 和 99 代表了标准提出的时间，SQL92 就是 92 年提出的标准规范。当然除了 SQL92 和 SQL99 以外，还存在 SQL-86、SQL-89、SQL:2003、SQL:2008、SQL:2011 和 SQL:2016 等其他的标准。\",\"这么多标准，到底该学习哪个呢？实际上最重要的 SQL 标准就是 SQL92 和 SQL99。一般来说 SQL92 的形式更简单，但是写的 SQL 语句会比较长，可读性较差。而 SQL99 相比于 SQL92 来说，语法更加复杂，但可读性更强。我们从这两个标准发布的页数也能看出，SQL92 的标准有 500 页，而 SQL99 标准超过了 1000 页。实际上从 SQL99 之后，很少有人能掌握所有内容，因为确实太多了。就好比我们使用 Windows、Linux 和 Office 的时候，很少有人能掌握全部内容一样。我们只需要掌握一些核心的功能，满足日常工作的需求即可。\",\"**SQL92 和 SQL99 是经典的 SQL 标准，也分别叫做 SQL-2 和 SQL-3 标准。**也正是在这两个标准发布之后，SQL 影响力越来越大，甚至超越了数据库领域。现如今 SQL 已经不仅仅是数据库领域的主流语言，还是信息领域中信息处理的主流语言。在图形检索、图像检索以及语音检索中都能看到 SQL 语言的使用。\"]}]},\"/study-tutorial/database/mysql/one-line-function.html\":{\"title\":\"7、单行函数\",\"contents\":[{\"header\":\"1. 函数的理解\",\"slug\":\"_1-函数的理解\",\"contents\":[]},{\"header\":\"1.1 什么是函数\",\"slug\":\"_1-1-什么是函数\",\"contents\":[\"函数在计算机语言的使用中贯穿始终，函数的作用是什么呢？它可以把我们经常使用的代码封装起来，需要的时候直接调用即可。这样既提高了代码效率，又提高了可维护性。在 SQL 中我们也可以使用函数对检索出来的数据进行函数操作。使用这些函数，可以极大地提高用户对数据库的管理效率。\",\"从函数定义的角度出发，我们可以将函数分成内置函数和自定义函数。在 SQL 语言中，同样也包括了内置函数和自定义函数。内置函数是系统内置的通用函数，而自定义函数是我们根据自己的需要编写的，本章及下一章讲解的是 SQL 的内置函数。\"]},{\"header\":\"1.2 不同DBMS函数的差异\",\"slug\":\"_1-2-不同dbms函数的差异\",\"contents\":[\"我们在使用 SQL 语言的时候，不是直接和这门语言打交道，而是通过它使用不同的数据库软件，即 DBMS。DBMS 之间的差异性很大，远大于同一个语言不同版本之间的差异。实际上，只有很少的函数是被 DBMS 同时支持的。比如，大多数 DBMS 使用（||）或者（+）来做拼接符，而在 MySQL 中的字符串拼接函数为concat()。大部分 DBMS 会有自己特定的函数，这就意味着采用 SQL 函数的代码可移植性是很差的，因此在使用函数的时候需要特别注意。\"]},{\"header\":\"1.3 MySQL的内置函数及分类\",\"slug\":\"_1-3-mysql的内置函数及分类\",\"contents\":[\"MySQL提供了丰富的内置函数，这些函数使得数据的维护与管理更加方便，能够更好地提供数据的分析与统计功能，在一定程度上提高了开发人员进行数据分析与统计的效率。\",\"MySQL提供的内置函数从实现的功能角度可以分为数值函数、字符串函数、日期和时间函数、流程控制函数、加密与解密函数、获取MySQL信息函数、聚合函数等。这里，我将这些丰富的内置函数再分为两类：单行函数、聚合函数（或分组函数）。\",\"两种SQL函数\",\"单行函数\",\"操作数据对象\",\"接受参数返回一个结果\",\"只对一行进行变换\",\"每行返回一个结果\",\"可以嵌套\",\"参数可以是一列或一个值\"]},{\"header\":\"2. 数值函数\",\"slug\":\"_2-数值函数\",\"contents\":[]},{\"header\":\"2.1 基本函数\",\"slug\":\"_2-1-基本函数\",\"contents\":[\"函数\",\"用法\",\"ABS(x)\",\"返回x的绝对值\",\"SIGN(X)\",\"返回X的符号。正数返回1，负数返回-1，0返回0\",\"PI()\",\"返回圆周率的值\",\"CEIL(x)，CEILING(x)\",\"返回大于或等于某个值的最小整数\",\"FLOOR(x)\",\"返回小于或等于某个值的最大整数\",\"LEAST(e1,e2,e3…)\",\"返回列表中的最小值\",\"GREATEST(e1,e2,e3…)\",\"返回列表中的最大值\",\"MOD(x,y)\",\"返回X除以Y后的余数\",\"RAND()\",\"返回0~1的随机值\",\"RAND(x)\",\"返回0~1的随机值，其中x的值用作种子值，相同的X值会产生相同的随机数\",\"ROUND(x)\",\"返回一个对x的值进行四舍五入后，最接近于X的整数\",\"ROUND(x,y)\",\"返回一个对x的值进行四舍五入后最接近X的值，并保留到小数点后面Y位\",\"TRUNCATE(x,y)\",\"返回数字x截断为y位小数的结果\",\"SQRT(x)\",\"返回x的平方根。当X的值为负数时，返回NULL\",\"举例：\",\"SELECT ABS(-123),ABS(32),SIGN(-23),SIGN(43),PI(),CEIL(32.32),CEILING(-43.23),FLOOR(32.32), FLOOR(-43.23),MOD(12,5) FROM DUAL; \",\"SELECT RAND(),RAND(),RAND(10),RAND(10),RAND(-1),RAND(-1) FROM DUAL; \",\"SELECT ROUND(12.33),ROUND(12.343,2),ROUND(12.324,-1),TRUNCATE(12.66,1),TRUNCATE(12.66,-1) FROM DUAL; \"]},{\"header\":\"2.2 角度与弧度互换函数\",\"slug\":\"_2-2-角度与弧度互换函数\",\"contents\":[\"函数\",\"用法\",\"RADIANS(x)\",\"将角度转化为弧度，其中，参数x为角度值\",\"DEGREES(x)\",\"将弧度转化为角度，其中，参数x为弧度值\",\"SELECT RADIANS(30),RADIANS(60),RADIANS(90),DEGREES(2*PI()),DEGREES(RADIANS(90)) FROM DUAL; \"]},{\"header\":\"2.3 三角函数\",\"slug\":\"_2-3-三角函数\",\"contents\":[\"函数\",\"用法\",\"SIN(x)\",\"返回x的正弦值，其中，参数x为弧度值\",\"ASIN(x)\",\"返回x的反正弦值，即获取正弦为x的值。如果x的值不在-1到1之间，则返回NULL\",\"COS(x)\",\"返回x的余弦值，其中，参数x为弧度值\",\"ACOS(x)\",\"返回x的反余弦值，即获取余弦为x的值。如果x的值不在-1到1之间，则返回NULL\",\"TAN(x)\",\"返回x的正切值，其中，参数x为弧度值\",\"ATAN(x)\",\"返回x的反正切值，即返回正切值为x的值\",\"ATAN2(m,n)\",\"返回两个参数的反正切值\",\"COT(x)\",\"返回x的余切值，其中，X为弧度值\",\"举例：\",\"ATAN2(M,N)函数返回两个参数的反正切值。 与ATAN(X)函数相比，ATAN2(M,N)需要两个参数，例如有两个点point(x1,y1)和point(x2,y2)，使用ATAN(X)函数计算反正切值为ATAN((y2-y1)/(x2-x1))，使用ATAN2(M,N)计算反正切值则为ATAN2(y2-y1,x2-x1)。由使用方式可以看出，当x2-x1等于0时，ATAN(X)函数会报错，而ATAN2(M,N)函数则仍然可以计算。\",\"ATAN2(M,N)函数的使用示例如下：\",\"SELECT SIN(RADIANS(30)),DEGREES(ASIN(1)),TAN(RADIANS(45)),DEGREES(ATAN(1)),DEGREES(ATAN2(1,1)) FROM DUAL; \"]},{\"header\":\"2.4 指数与对数\",\"slug\":\"_2-4-指数与对数\",\"contents\":[\"函数\",\"用法\",\"POW(x,y)，POWER(X,Y)\",\"返回x的y次方\",\"EXP(X)\",\"返回e的X次方，其中e是一个常数，2.718281828459045\",\"LN(X)，LOG(X)\",\"返回以e为底的X的对数，当X <= 0 时，返回的结果为NULL\",\"LOG10(X)\",\"返回以10为底的X的对数，当X <= 0 时，返回的结果为NULL\",\"LOG2(X)\",\"返回以2为底的X的对数，当X <= 0 时，返回NULL\",\"mysql> SELECT POW(2,5),POWER(2,4),EXP(2),LN(10),LOG10(10),LOG2(4) -> FROM DUAL; +----------+------------+------------------+-------------------+-----------+---------+ | POW(2,5) | POWER(2,4) | EXP(2) | LN(10) | LOG10(10) | LOG2(4) | +----------+------------+------------------+-------------------+-----------+---------+ | 32 | 16 | 7.38905609893065 | 2.302585092994046 | 1 | 2 | +----------+------------+------------------+-------------------+-----------+---------+ 1 row in set (0.00 sec) \"]},{\"header\":\"2.5 进制间的转换\",\"slug\":\"_2-5-进制间的转换\",\"contents\":[\"函数\",\"用法\",\"BIN(x)\",\"返回x的二进制编码\",\"HEX(x)\",\"返回x的十六进制编码\",\"OCT(x)\",\"返回x的八进制编码\",\"CONV(x,f1,f2)\",\"返回f1进制数变成f2进制数\",\"mysql> SELECT BIN(10),HEX(10),OCT(10),CONV(10,2,8) -> FROM DUAL; +---------+---------+---------+--------------+ | BIN(10) | HEX(10) | OCT(10) | CONV(10,2,8) | +---------+---------+---------+--------------+ | 1010 | A | 12 | 2 | +---------+---------+---------+--------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"3. 字符串函数\",\"slug\":\"_3-字符串函数\",\"contents\":[\"函数\",\"用法\",\"ASCII(S)\",\"返回字符串S中的第一个字符的ASCII码值\",\"CHAR_LENGTH(s)\",\"返回字符串s的字符数。作用与CHARACTER_LENGTH(s)相同\",\"LENGTH(s)\",\"返回字符串s的字节数，和字符集有关\",\"CONCAT(s1,s2,......,sn)\",\"连接s1,s2,......,sn为一个字符串\",\"CONCAT_WS(x, s1,s2,......,sn)\",\"同CONCAT(s1,s2,...)函数，但是每个字符串之间要加上x\",\"INSERT(str, idx, len, replacestr)\",\"将字符串str从第idx位置开始，len个字符长的子串替换为字符串replacestr\",\"REPLACE(str, a, b)\",\"用字符串b替换字符串str中所有出现的字符串a\",\"UPPER(s) 或 UCASE(s)\",\"将字符串s的所有字母转成大写字母\",\"LOWER(s) 或LCASE(s)\",\"将字符串s的所有字母转成小写字母\",\"LEFT(str,n)\",\"返回字符串str最左边的n个字符\",\"RIGHT(str,n)\",\"返回字符串str最右边的n个字符\",\"LPAD(str, len, pad)\",\"用字符串pad对str最左边进行填充，直到str的长度为len个字符\",\"RPAD(str ,len, pad)\",\"用字符串pad对str最右边进行填充，直到str的长度为len个字符\",\"LTRIM(s)\",\"去掉字符串s左侧的空格\",\"RTRIM(s)\",\"去掉字符串s右侧的空格\",\"TRIM(s)\",\"去掉字符串s开始与结尾的空格\",\"TRIM(s1 FROM s)\",\"去掉字符串s开始与结尾的s1\",\"TRIM(LEADING s1 FROM s)\",\"去掉字符串s开始处的s1\",\"TRIM(TRAILING s1 FROM s)\",\"去掉字符串s结尾处的s1\",\"REPEAT(str, n)\",\"返回str重复n次的结果\",\"SPACE(n)\",\"返回n个空格\",\"STRCMP(s1,s2)\",\"比较字符串s1,s2的ASCII码值的大小\",\"SUBSTR(s,index,len)\",\"返回从字符串s的index位置其len个字符，作用与SUBSTRING(s,n,len)、MID(s,n,len)相同\",\"LOCATE(substr,str)\",\"返回字符串substr在字符串str中首次出现的位置，作用于POSITION(substr IN str)、INSTR(str,substr)相同。未找到，返回0\",\"ELT(m,s1,s2,…,sn)\",\"返回指定位置的字符串，如果m=1，则返回s1，如果m=2，则返回s2，如果m=n，则返回sn\",\"FIELD(s,s1,s2,…,sn)\",\"返回字符串s在字符串列表中第一次出现的位置\",\"FIND_IN_SET(s1,s2)\",\"返回字符串s1在字符串s2中出现的位置。其中，字符串s2是一个以逗号分隔的字符串\",\"REVERSE(s)\",\"返回s反转后的字符串\",\"NULLIF(value1,value2)\",\"比较两个字符串，如果value1与value2相等，则返回NULL，否则返回value1\",\"注意：MySQL中，字符串的位置是从1开始的。\",\"举例：\",\"mysql> SELECT FIELD('mm','hello','msm','amma'),FIND_IN_SET('mm','hello,mm,amma') -> FROM DUAL; +----------------------------------+-----------------------------------+ | FIELD('mm','hello','msm','amma') | FIND_IN_SET('mm','hello,mm,amma') | +----------------------------------+-----------------------------------+ | 0 | 2 | +----------------------------------+-----------------------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT NULLIF('mysql','mysql'),NULLIF('mysql', ''); +-------------------------+---------------------+ | NULLIF('mysql','mysql') | NULLIF('mysql', '') | +-------------------------+---------------------+ | NULL | mysql | +-------------------------+---------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"4. 日期和时间函数\",\"slug\":\"_4-日期和时间函数\",\"contents\":[]},{\"header\":\"4.1 获取日期、时间\",\"slug\":\"_4-1-获取日期、时间\",\"contents\":[\"函数\",\"用法\",\"CURDATE() ，CURRENT_DATE()\",\"返回当前日期，只包含年、月、日\",\"CURTIME() ， CURRENT_TIME()\",\"返回当前时间，只包含时、分、秒\",\"NOW() / SYSDATE() / CURRENT_TIMESTAMP() / LOCALTIME() / LOCALTIMESTAMP()\",\"返回当前系统日期和时间\",\"UTC_DATE()\",\"返回UTC（世界标准时间）日期\",\"UTC_TIME()\",\"返回UTC（世界标准时间）时间\",\"举例：\",\"SELECT CURDATE(),CURTIME(),NOW(),SYSDATE()+0,UTC_DATE(),UTC_DATE()+0,UTC_TIME(),UTC_TIME()+0 FROM DUAL; \"]},{\"header\":\"4.2 日期与时间戳的转换\",\"slug\":\"_4-2-日期与时间戳的转换\",\"contents\":[\"函数\",\"用法\",\"UNIX_TIMESTAMP()\",\"以UNIX时间戳的形式返回当前时间。SELECT UNIX_TIMESTAMP() ->1634348884\",\"UNIX_TIMESTAMP(date)\",\"将时间date以UNIX时间戳的形式返回。\",\"FROM_UNIXTIME(timestamp)\",\"将UNIX时间戳的时间转换为普通格式的时间\",\"举例：\",\"mysql> SELECT UNIX_TIMESTAMP(now()); +-----------------------+ | UNIX_TIMESTAMP(now()) | +-----------------------+ | 1576380910 | +-----------------------+ 1 row in set (0.01 sec) mysql> SELECT UNIX_TIMESTAMP(CURDATE()); +---------------------------+ | UNIX_TIMESTAMP(CURDATE()) | +---------------------------+ | 1576339200 | +---------------------------+ 1 row in set (0.00 sec) mysql> SELECT UNIX_TIMESTAMP(CURTIME()); +---------------------------+ | UNIX_TIMESTAMP(CURTIME()) | +---------------------------+ | 1576380969 | +---------------------------+ 1 row in set (0.00 sec) mysql> SELECT UNIX_TIMESTAMP('2011-11-11 11:11:11') +---------------------------------------+ | UNIX_TIMESTAMP('2011-11-11 11:11:11') | +---------------------------------------+ | 1320981071 | +---------------------------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT FROM_UNIXTIME(1576380910); +---------------------------+ | FROM_UNIXTIME(1576380910) | +---------------------------+ | 2019-12-15 11:35:10 | +---------------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"4.3 获取月份、星期、星期数、天数等函数\",\"slug\":\"_4-3-获取月份、星期、星期数、天数等函数\",\"contents\":[\"函数\",\"用法\",\"YEAR(date) / MONTH(date) / DAY(date)\",\"返回具体的日期值\",\"HOUR(time) / MINUTE(time) / SECOND(time)\",\"返回具体的时间值\",\"MONTHNAME(date)\",\"返回月份：January，...\",\"DAYNAME(date)\",\"返回星期几：MONDAY，TUESDAY.....SUNDAY\",\"WEEKDAY(date)\",\"返回周几，注意，周1是0，周2是1，。。。周日是6\",\"QUARTER(date)\",\"返回日期对应的季度，范围为1～4\",\"WEEK(date) ， WEEKOFYEAR(date)\",\"返回一年中的第几周\",\"DAYOFYEAR(date)\",\"返回日期是一年中的第几天\",\"DAYOFMONTH(date)\",\"返回日期位于所在月份的第几天\",\"DAYOFWEEK(date)\",\"返回周几，注意：周日是1，周一是2，。。。周六是7\",\"举例：\",\"SELECT YEAR(CURDATE()),MONTH(CURDATE()),DAY(CURDATE()), HOUR(CURTIME()),MINUTE(NOW()),SECOND(SYSDATE()) FROM DUAL; \",\"SELECT MONTHNAME('2021-10-26'),DAYNAME('2021-10-26'),WEEKDAY('2021-10-26'), QUARTER(CURDATE()),WEEK(CURDATE()),DAYOFYEAR(NOW()), DAYOFMONTH(NOW()),DAYOFWEEK(NOW()) FROM DUAL; \"]},{\"header\":\"4.4 日期的操作函数\",\"slug\":\"_4-4-日期的操作函数\",\"contents\":[\"函数\",\"用法\",\"EXTRACT(type FROM date)\",\"返回指定日期中特定的部分，type指定返回的值\",\"EXTRACT(type FROM date)函数中type的取值与含义：\",\"SELECT EXTRACT(MINUTE FROM NOW()),EXTRACT( WEEK FROM NOW()), EXTRACT( QUARTER FROM NOW()),EXTRACT( MINUTE_SECOND FROM NOW()) FROM DUAL; \"]},{\"header\":\"4.5 时间和秒钟转换的函数\",\"slug\":\"_4-5-时间和秒钟转换的函数\",\"contents\":[\"函数\",\"用法\",\"TIME_TO_SEC(time)\",\"将 time 转化为秒并返回结果值。转化的公式为：小时*3600+分钟*60+秒\",\"SEC_TO_TIME(seconds)\",\"将 seconds 描述转化为包含小时、分钟和秒的时间\",\"举例：\",\"mysql> SELECT TIME_TO_SEC(NOW()); +--------------------+ | TIME_TO_SEC(NOW()) | +--------------------+ | 78774 | +--------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT SEC_TO_TIME(78774); +--------------------+ | SEC_TO_TIME(78774) | +--------------------+ | 21:52:54 | +--------------------+ 1 row in set (0.12 sec) \"]},{\"header\":\"4.6 计算日期和时间的函数\",\"slug\":\"_4-6-计算日期和时间的函数\",\"contents\":[\"第1组：\",\"函数\",\"用法\",\"DATE_ADD(datetime, INTERVAL expr type)，ADDDATE(date,INTERVAL expr type)\",\"返回与给定日期时间相差INTERVAL时间段的日期时间\",\"DATE_SUB(date,INTERVAL expr type)，SUBDATE(date,INTERVAL expr type)\",\"返回与date相差INTERVAL时间间隔的日期\",\"上述函数中type的取值：\",\"举例：\",\"SELECT DATE_ADD(NOW(), INTERVAL 1 DAY) AS col1,DATE_ADD('2021-10-21 23:32:12',INTERVAL 1 SECOND) AS col2, ADDDATE('2021-10-21 23:32:12',INTERVAL 1 SECOND) AS col3, DATE_ADD('2021-10-21 23:32:12',INTERVAL '1_1' MINUTE_SECOND) AS col4, DATE_ADD(NOW(), INTERVAL -1 YEAR) AS col5, #可以是负数 DATE_ADD(NOW(), INTERVAL '1_1' YEAR_MONTH) AS col6 #需要单引号 FROM DUAL; \",\"SELECT DATE_SUB('2021-01-21',INTERVAL 31 DAY) AS col1, SUBDATE('2021-01-21',INTERVAL 31 DAY) AS col2, DATE_SUB('2021-01-21 02:01:01',INTERVAL '1 1' DAY_HOUR) AS col3 FROM DUAL; \",\"第2组：\",\"函数\",\"用法\",\"ADDTIME(time1,time2)\",\"返回time1加上time2的时间。当time2为一个数字时，代表的是秒，可以为负数\",\"SUBTIME(time1,time2)\",\"返回time1减去time2后的时间。当time2为一个数字时，代表的是秒，可以为负数\",\"DATEDIFF(date1,date2)\",\"返回date1 - date2的日期间隔天数\",\"TIMEDIFF(time1, time2)\",\"返回time1 - time2的时间间隔\",\"FROM_DAYS(N)\",\"返回从0000年1月1日起，N天以后的日期\",\"TO_DAYS(date)\",\"返回日期date距离0000年1月1日的天数\",\"LAST_DAY(date)\",\"返回date所在月份的最后一天的日期\",\"MAKEDATE(year,n)\",\"针对给定年份与所在年份中的天数返回一个日期\",\"MAKETIME(hour,minute,second)\",\"将给定的小时、分钟和秒组合成时间并返回\",\"PERIOD_ADD(time,n)\",\"返回time加上n后的时间\",\"举例：\",\"SELECT ADDTIME(NOW(),20),SUBTIME(NOW(),30),SUBTIME(NOW(),'1:1:3'),DATEDIFF(NOW(),'2021-10-01'), TIMEDIFF(NOW(),'2021-10-25 22:10:10'),FROM_DAYS(366),TO_DAYS('0000-12-25'), LAST_DAY(NOW()),MAKEDATE(YEAR(NOW()),12),MAKETIME(10,21,23),PERIOD_ADD(20200101010101,10) FROM DUAL; \",\"mysql> SELECT ADDTIME(NOW(), 50); +---------------------+ | ADDTIME(NOW(), 50) | +---------------------+ | 2019-12-15 22:17:47 | +---------------------+ 1 row in set (0.00 sec) mysql> SELECT ADDTIME(NOW(), '1:1:1'); +-------------------------+ | ADDTIME(NOW(), '1:1:1') | +-------------------------+ | 2019-12-15 23:18:46 | +-------------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT SUBTIME(NOW(), '1:1:1'); +-------------------------+ | SUBTIME(NOW(), '1:1:1') | +-------------------------+ | 2019-12-15 21:23:50 | +-------------------------+ 1 row in set (0.00 sec) mysql> SELECT SUBTIME(NOW(), '-1:-1:-1'); +----------------------------+ | SUBTIME(NOW(), '-1:-1:-1') | +----------------------------+ | 2019-12-15 22:25:11 | +----------------------------+ 1 row in set, 1 warning (0.00 sec) \",\"mysql> SELECT FROM_DAYS(366); +----------------+ | FROM_DAYS(366) | +----------------+ | 0001-01-01 | +----------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT MAKEDATE(2020,1); +------------------+ | MAKEDATE(2020,1) | +------------------+ | 2020-01-01 | +------------------+ 1 row in set (0.00 sec) mysql> SELECT MAKEDATE(2020,32); +-------------------+ | MAKEDATE(2020,32) | +-------------------+ | 2020-02-01 | +-------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT MAKETIME(1,1,1); +-----------------+ | MAKETIME(1,1,1) | +-----------------+ | 01:01:01 | +-----------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT PERIOD_ADD(20200101010101,1); +------------------------------+ | PERIOD_ADD(20200101010101,1) | +------------------------------+ | 20200101010102 | +------------------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT TO_DAYS(NOW()); +----------------+ | TO_DAYS(NOW()) | +----------------+ | 737773 | +----------------+ 1 row in set (0.00 sec) \",\"举例：查询 7 天内的新增用户数有多少？\",\"SELECT COUNT(*) as num FROM new_user WHERE TO_DAYS(NOW())-TO_DAYS(regist_time)<=7 \"]},{\"header\":\"4.7 日期的格式化与解析\",\"slug\":\"_4-7-日期的格式化与解析\",\"contents\":[\"函数\",\"用法\",\"DATE_FORMAT(date,fmt)\",\"按照字符串fmt格式化日期date值\",\"TIME_FORMAT(time,fmt)\",\"按照字符串fmt格式化时间time值\",\"GET_FORMAT(date_type,format_type)\",\"返回日期字符串的显示格式\",\"STR_TO_DATE(str, fmt)\",\"按照字符串fmt对str进行解析，解析为一个日期\",\"上述非GET_FORMAT函数中fmt参数常用的格式符：\",\"格式符\",\"说明\",\"格式符\",\"说明\",\"%Y\",\"4位数字表示年份\",\"%y\",\"表示两位数字表示年份\",\"%M\",\"月名表示月份（January,....）\",\"%m\",\"两位数字表示月份（01,02,03。。。）\",\"%b\",\"缩写的月名（Jan.，Feb.，....）\",\"%c\",\"数字表示月份（1,2,3,...）\",\"%D\",\"英文后缀表示月中的天数（1st,2nd,3rd,...）\",\"%d\",\"两位数字表示月中的天数(01,02...)\",\"%e\",\"数字形式表示月中的天数（1,2,3,4,5.....）\",\"%H\",\"两位数字表示小数，24小时制（01,02..）\",\"%h和%I\",\"两位数字表示小时，12小时制（01,02..）\",\"%k\",\"数字形式的小时，24小时制(1,2,3)\",\"%l\",\"数字形式表示小时，12小时制（1,2,3,4....）\",\"%i\",\"两位数字表示分钟（00,01,02）\",\"%S和%s\",\"两位数字表示秒(00,01,02...)\",\"%W\",\"一周中的星期名称（Sunday...）\",\"%a\",\"一周中的星期缩写（Sun.，Mon.,Tues.，..）\",\"%w\",\"以数字表示周中的天数(0=Sunday,1=Monday....)\",\"%j\",\"以3位数字表示年中的天数(001,002...)\",\"%U\",\"以数字表示年中的第几周，（1,2,3。。）其中Sunday为周中第一天\",\"%u\",\"以数字表示年中的第几周，（1,2,3。。）其中Monday为周中第一天\",\"%T\",\"24小时制\",\"%r\",\"12小时制\",\"%p\",\"AM或PM\",\"%%\",\"表示%\",\"GET_FORMAT函数中date_type和format_type参数取值如下：\",\"举例：\",\"mysql> SELECT DATE_FORMAT(NOW(), '%H:%i:%s'); +--------------------------------+ | DATE_FORMAT(NOW(), '%H:%i:%s') | +--------------------------------+ | 22:57:34 | +--------------------------------+ 1 row in set (0.00 sec) \",\"SELECT STR_TO_DATE('09/01/2009','%m/%d/%Y') FROM DUAL; SELECT STR_TO_DATE('20140422154706','%Y%m%d%H%i%s') FROM DUAL; SELECT STR_TO_DATE('2014-04-22 15:47:06','%Y-%m-%d %H:%i:%s') FROM DUAL; \",\"mysql> SELECT GET_FORMAT(DATE, 'USA'); +-------------------------+ | GET_FORMAT(DATE, 'USA') | +-------------------------+ | %m.%d.%Y | +-------------------------+ 1 row in set (0.00 sec) SELECT DATE_FORMAT(NOW(),GET_FORMAT(DATE,'USA')), FROM DUAL; \",\"mysql> SELECT STR_TO_DATE('2020-01-01 00:00:00','%Y-%m-%d'); +-----------------------------------------------+ | STR_TO_DATE('2020-01-01 00:00:00','%Y-%m-%d') | +-----------------------------------------------+ | 2020-01-01 | +-----------------------------------------------+ 1 row in set, 1 warning (0.00 sec) \"]},{\"header\":\"5. 流程控制函数\",\"slug\":\"_5-流程控制函数\",\"contents\":[\"流程处理函数可以根据不同的条件，执行不同的处理流程，可以在SQL语句中实现不同的条件选择。MySQL中的流程处理函数主要包括IF()、IFNULL()和CASE()函数。\",\"函数\",\"用法\",\"IF(value,value1,value2)\",\"如果value的值为TRUE，返回value1，否则返回value2\",\"IFNULL(value1, value2)\",\"如果value1不为NULL，返回value1，否则返回value2\",\"CASE WHEN 条件1 THEN 结果1 WHEN 条件2 THEN 结果2 .... [ELSE resultn] END\",\"相当于Java的if...else if...else...\",\"CASE expr WHEN 常量值1 THEN 值1 WHEN 常量值1 THEN 值1 .... [ELSE 值n] END\",\"相当于Java的switch...case...\",\"SELECT IF(1 > 0,'正确','错误') ->正确 \",\"SELECT IFNULL(null,'Hello Word') ->Hello Word \",\"SELECT CASE WHEN 1 > 0 THEN '1 > 0' WHEN 2 > 0 THEN '2 > 0' ELSE '3 > 0' END ->1 > 0 \",\"SELECT CASE 1 WHEN 1 THEN '我是1' WHEN 2 THEN '我是2' ELSE '你是谁' \",\"SELECT employee_id,salary, CASE WHEN salary>=15000 THEN '高薪' WHEN salary>=10000 THEN '潜力股' WHEN salary>=8000 THEN '屌丝' ELSE '草根' END \\\"描述\\\" FROM employees; \",\"SELECT oid,`status`, CASE `status` WHEN 1 THEN '未付款' WHEN 2 THEN '已付款' WHEN 3 THEN '已发货' WHEN 4 THEN '确认收货' ELSE '无效订单' END FROM t_order; \",\"mysql> SELECT CASE WHEN 1 > 0 THEN 'yes' WHEN 1 <= 0 THEN 'no' ELSE 'unknown' END; +---------------------------------------------------------------------+ | CASE WHEN 1 > 0 THEN 'yes' WHEN 1 <= 0 THEN 'no' ELSE 'unknown' END | +---------------------------------------------------------------------+ | yes | +---------------------------------------------------------------------+ 1 row in set (0.00 sec) mysql> SELECT CASE WHEN 1 < 0 THEN 'yes' WHEN 1 = 0 THEN 'no' ELSE 'unknown' END; +--------------------------------------------------------------------+ | CASE WHEN 1 < 0 THEN 'yes' WHEN 1 = 0 THEN 'no' ELSE 'unknown' END | +--------------------------------------------------------------------+ | unknown | +--------------------------------------------------------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT CASE 1 WHEN 0 THEN 0 WHEN 1 THEN 1 ELSE -1 END; +------------------------------------------------+ | CASE 1 WHEN 0 THEN 0 WHEN 1 THEN 1 ELSE -1 END | +------------------------------------------------+ | 1 | +------------------------------------------------+ 1 row in set (0.00 sec) mysql> SELECT CASE -1 WHEN 0 THEN 0 WHEN 1 THEN 1 ELSE -1 END; +-------------------------------------------------+ | CASE -1 WHEN 0 THEN 0 WHEN 1 THEN 1 ELSE -1 END | +-------------------------------------------------+ | -1 | +-------------------------------------------------+ 1 row in set (0.00 sec) \",\"SELECT employee_id,12 * salary * (1 + IFNULL(commission_pct,0)) FROM employees; \",\"SELECT last_name, job_id, salary, CASE job_id WHEN 'IT_PROG' THEN 1.10*salary WHEN 'ST_CLERK' THEN 1.15*salary WHEN 'SA_REP' THEN 1.20*salary ELSE salary END \\\"REVISED_SALARY\\\" FROM employees; \",\"练习：查询部门号为 10,20, 30 的员工信息, 若部门号为 10, 则打印其工资的 1.1 倍, 20 号部门, 则打印其工资的 1.2 倍, 30 号部门打印其工资的 1.3 倍数。\"]},{\"header\":\"6. 加密与解密函数\",\"slug\":\"_6-加密与解密函数\",\"contents\":[\"加密与解密函数主要用于对数据库中的数据进行加密和解密处理，以防止数据被他人窃取。这些函数在保证数据库安全时非常有用。\",\"函数\",\"用法\",\"PASSWORD(str)\",\"返回字符串str的加密版本，41位长的字符串。加密结果不可逆，常用于用户的密码加密\",\"MD5(str)\",\"返回字符串str的md5加密后的值，也是一种加密方式。若参数为NULL，则会返回NULL\",\"SHA(str)\",\"从原明文密码str计算并返回加密后的密码字符串，当参数为NULL时，返回NULL。SHA加密算法比MD5更加安全。\",\"ENCODE(value,password_seed)\",\"返回使用password_seed作为加密密码加密value\",\"DECODE(value,password_seed)\",\"返回使用password_seed作为加密密码解密value\",\"可以看到，ENCODE(value,password_seed)函数与DECODE(value,password_seed)函数互为反函数。\",\"举例：\",\"mysql> SELECT PASSWORD('mysql'), PASSWORD(NULL); +-------------------------------------------+----------------+ | PASSWORD('mysql') | PASSWORD(NULL) | +-------------------------------------------+----------------+ | *E74858DB86EBA20BC33D0AECAE8A8108C56B17FA | | +-------------------------------------------+----------------+ 1 row in set, 1 warning (0.00 sec) \",\"SELECT md5('123') ->202cb962ac59075b964b07152d234b70 \",\"SELECT SHA('Tom123') ->c7c506980abc31cc390a2438c90861d0f1216d50 \",\"mysql> SELECT ENCODE('mysql', 'mysql'); +--------------------------+ | ENCODE('mysql', 'mysql') | +--------------------------+ | íg ¼ ìÉ | +--------------------------+ 1 row in set, 1 warning (0.01 sec) \",\"mysql> SELECT DECODE(ENCODE('mysql','mysql'),'mysql'); +-----------------------------------------+ | DECODE(ENCODE('mysql','mysql'),'mysql') | +-----------------------------------------+ | mysql | +-----------------------------------------+ 1 row in set, 2 warnings (0.00 sec) \"]},{\"header\":\"7. MySQL信息函数\",\"slug\":\"_7-mysql信息函数\",\"contents\":[\"MySQL中内置了一些可以查询MySQL信息的函数，这些函数主要用于帮助数据库开发或运维人员更好地对数据库进行维护工作。\",\"函数\",\"用法\",\"VERSION()\",\"返回当前MySQL的版本号\",\"CONNECTION_ID()\",\"返回当前MySQL服务器的连接数\",\"DATABASE()，SCHEMA()\",\"返回MySQL命令行当前所在的数据库\",\"USER()，CURRENT_USER()、SYSTEM_USER()，SESSION_USER()\",\"返回当前连接MySQL的用户名，返回结果格式为“主机名@用户名”\",\"CHARSET(value)\",\"返回字符串value自变量的字符集\",\"COLLATION(value)\",\"返回字符串value的比较规则\",\"举例：\",\"mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | test | +------------+ 1 row in set (0.00 sec) mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | test | +------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT USER(), CURRENT_USER(), SYSTEM_USER(),SESSION_USER(); +----------------+----------------+----------------+----------------+ | USER() | CURRENT_USER() | SYSTEM_USER() | SESSION_USER() | +----------------+----------------+----------------+----------------+ | root@localhost | root@localhost | root@localhost | root@localhost | +----------------+----------------+----------------+----------------+ \",\"mysql> SELECT CHARSET('ABC'); +----------------+ | CHARSET('ABC') | +----------------+ | utf8mb4 | +----------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT COLLATION('ABC'); +--------------------+ | COLLATION('ABC') | +--------------------+ | utf8mb4_general_ci | +--------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"8. 其他函数\",\"slug\":\"_8-其他函数\",\"contents\":[\"MySQL中有些函数无法对其进行具体的分类，但是这些函数在MySQL的开发和运维过程中也是不容忽视的。\",\"函数\",\"用法\",\"FORMAT(value,n)\",\"返回对数字value进行格式化后的结果数据。n表示四舍五入后保留到小数点后n位\",\"CONV(value,from,to)\",\"将value的值进行不同进制之间的转换\",\"INET_ATON(ipvalue)\",\"将以点分隔的IP地址转化为一个数字\",\"INET_NTOA(value)\",\"将数字形式的IP地址转化为以点分隔的IP地址\",\"BENCHMARK(n,expr)\",\"将表达式expr重复执行n次。用于测试MySQL处理expr表达式所耗费的时间\",\"CONVERT(value USING char_code)\",\"将value所使用的字符编码修改为char_code\",\"举例：\",\"## 如果n的值小于或者等于0，则只保留整数部分 mysql> SELECT FORMAT(123.123, 2), FORMAT(123.523, 0), FORMAT(123.123, -2); +--------------------+--------------------+---------------------+ | FORMAT(123.123, 2) | FORMAT(123.523, 0) | FORMAT(123.123, -2) | +--------------------+--------------------+---------------------+ | 123.12 | 124 | 123 | +--------------------+--------------------+---------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT CONV(16, 10, 2), CONV(8888,10,16), CONV(NULL, 10, 2); +-----------------+------------------+-------------------+ | CONV(16, 10, 2) | CONV(8888,10,16) | CONV(NULL, 10, 2) | +-----------------+------------------+-------------------+ | 10000 | 22B8 | NULL | +-----------------+------------------+-------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT INET_ATON('192.168.1.100'); +----------------------------+ | INET_ATON('192.168.1.100') | +----------------------------+ | 3232235876 | +----------------------------+ 1 row in set (0.00 sec) ## 以“192.168.1.100”为例，计算方式为192乘以256的3次方，加上168乘以256的2次方，加上1乘以256，再加上100。 \",\"mysql> SELECT INET_NTOA(3232235876); +-----------------------+ | INET_NTOA(3232235876) | +-----------------------+ | 192.168.1.100 | +-----------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT BENCHMARK(1, MD5('mysql')); +----------------------------+ | BENCHMARK(1, MD5('mysql')) | +----------------------------+ | 0 | +----------------------------+ 1 row in set (0.00 sec) mysql> SELECT BENCHMARK(1000000, MD5('mysql')); +----------------------------------+ | BENCHMARK(1000000, MD5('mysql')) | +----------------------------------+ | 0 | +----------------------------------+ 1 row in set (0.20 sec) \",\"mysql> SELECT CHARSET('mysql'), CHARSET(CONVERT('mysql' USING 'utf8')); +------------------+----------------------------------------+ | CHARSET('mysql') | CHARSET(CONVERT('mysql' USING 'utf8')) | +------------------+----------------------------------------+ | utf8mb4 | utf8 | +------------------+----------------------------------------+ 1 row in set, 1 warning (0.00 sec) \"]}]},\"/study-tutorial/database/mysql/operator.html\":{\"title\":\"4、运算符\",\"contents\":[{\"header\":\"1. 算术运算符\",\"slug\":\"_1-算术运算符\",\"contents\":[\"算术运算符主要用于数学运算，其可以连接运算符前后的两个数值或表达式，对数值或表达式进行加（+）、减（-）、乘（*）、除（/）和取模（%）运算。\",\"1．加法与减法运算符\",\"mysql> SELECT 100, 100 + 0, 100 - 0, 100 + 50, 100 + 50 -30, 100 + 35.5, 100 - 35.5 FROM dual; +-----+---------+---------+----------+--------------+------------+------------+ | 100 | 100 + 0 | 100 - 0 | 100 + 50 | 100 + 50 -30 | 100 + 35.5 | 100 - 35.5 | +-----+---------+---------+----------+--------------+------------+------------+ | 100 | 100 | 100 | 150 | 120 | 135.5 | 64.5 | +-----+---------+---------+----------+--------------+------------+------------+ 1 row in set (0.00 sec) \",\"由运算结果可以得出如下结论：\",\"一个整数类型的值对整数进行加法和减法操作，结果还是一个整数；\",\"一个整数类型的值对浮点数进行加法和减法操作，结果是一个浮点数；\",\"加法和减法的优先级相同，进行先加后减操作与进行先减后加操作的结果是一样的；\",\"在Java中，+的左右两边如果有字符串，那么表示字符串的拼接。但是在MySQL中+只表示数值相加。如果遇到非数值类型，先尝试转成数值，如果转失败，就按0计算。（补充：MySQL中字符串拼接要使用字符串函数CONCAT()实现）\",\"2．乘法与除法运算符\",\"mysql> SELECT 100, 100 * 1, 100 * 1.0, 100 / 1.0, 100 / 2,100 + 2 * 5 / 2,100 /3, 100 DIV 0 FROM dual; +-----+---------+-----------+-----------+---------+-----------------+---------+-----------+ | 100 | 100 * 1 | 100 * 1.0 | 100 / 1.0 | 100 / 2 | 100 + 2 * 5 / 2 | 100 /3 | 100 DIV 0 | +-----+---------+-----------+-----------+---------+-----------------+---------+-----------+ | 100 | 100 | 100.0 | 100.0000 | 50.0000 | 105.0000 | 33.3333 | NULL | +-----+---------+-----------+-----------+---------+-----------------+---------+-----------+ 1 row in set (0.00 sec) \",\"#计算出员工的年基本工资 SELECT employee_id,salary,salary * 12 annual_sal FROM employees; \",\"由运算结果可以得出如下结论：\",\"一个数乘以整数1和除以整数1后仍得原数；\",\"一个数乘以浮点数1和除以浮点数1后变成浮点数，数值与原数相等；\",\"一个数除以整数后，不管是否能除尽，结果都为一个浮点数；\",\"一个数除以另一个数，除不尽时，结果为一个浮点数，并保留到小数点后4位；\",\"乘法和除法的优先级相同，进行先乘后除操作与先除后乘操作，得出的结果相同。\",\"在数学运算中，0不能用作除数，在MySQL中，一个数除以0为NULL。\",\"3．求模（求余）运算符 将t22表中的字段i对3和5进行求模（求余）运算。\",\"mysql> SELECT 12 % 3, 12 MOD 5 FROM dual; +--------+----------+ | 12 % 3 | 12 MOD 5 | +--------+----------+ | 0 | 2 | +--------+----------+ 1 row in set (0.00 sec) \",\"#筛选出employee_id是偶数的员工 SELECT * FROM employees WHERE employee_id MOD 2 = 0; \",\"可以看到，100对3求模后的结果为3，对5求模后的结果为0。\"]},{\"header\":\"2. 比较运算符\",\"slug\":\"_2-比较运算符\",\"contents\":[\"比较运算符用来对表达式左边的操作数和右边的操作数进行比较，比较的结果为真则返回1，比较的结果为假则返回0，其他情况则返回NULL。\",\"比较运算符经常被用来作为SELECT查询语句的条件来使用，返回符合条件的结果记录。\",\"1．等号运算符\",\"等号运算符（=）判断等号两边的值、字符串或表达式是否相等，如果相等则返回1，不相等则返回0。\",\"在使用等号运算符时，遵循如下规则：\",\"如果等号两边的值、字符串或表达式都为字符串，则MySQL会按照字符串进行比较，其比较的是每个字符串中字符的ANSI编码是否相等。\",\"如果等号两边的值都是整数，则MySQL会按照整数来比较两个值的大小。\",\"如果等号两边的值一个是整数，另一个是字符串，则MySQL会将字符串转化为数字进行比较。\",\"如果等号两边的值、字符串或表达式中有一个为NULL，则比较结果为NULL。\",\"对比：SQL中赋值符号使用 :=\",\"mysql> SELECT 1 = 1, 1 = '1', 1 = 0, 'a' = 'a', (5 + 3) = (2 + 6), '' = NULL , NULL = NULL; +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 = 1 | 1 = '1' | 1 = 0 | 'a' = 'a' | (5 + 3) = (2 + 6) | '' = NULL | NULL = NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 | 1 | 0 | 1 | 1 | NULL | NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT 1 = 2, 0 = 'abc', 1 = 'abc' FROM dual; +-------+-----------+-----------+ | 1 = 2 | 0 = 'abc' | 1 = 'abc' | +-------+-----------+-----------+ | 0 | 1 | 0 | +-------+-----------+-----------+ 1 row in set, 2 warnings (0.00 sec) \",\"#查询salary=10000，注意在Java中比较是== SELECT employee_id,salary FROM employees WHERE salary = 10000; \",\"2．安全等于运算符 安全等于运算符（<=>）与等于运算符（=）的作用是相似的，唯一的区别是‘<=>’可以用来对NULL进行判断。在两个操作数均为NULL时，其返回值为1，而不为NULL；当一个操作数为NULL时，其返回值为0，而不为NULL。\",\"mysql> SELECT 1 <=> '1', 1 <=> 0, 'a' <=> 'a', (5 + 3) <=> (2 + 6), '' <=> NULL,NULL <=> NULL FROM dual; +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 <=> '1' | 1 <=> 0 | 'a' <=> 'a' | (5 + 3) <=> (2 + 6) | '' <=> NULL | NULL <=> NULL | +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 | 0 | 1 | 1 | 0 | 1 | +-----------+---------+-------------+---------------------+-------------+---------------+ 1 row in set (0.00 sec) \",\"#查询commission_pct等于0.40 SELECT employee_id,commission_pct FROM employees WHERE commission_pct = 0.40; SELECT employee_id,commission_pct FROM employees WHERE commission_pct <=> 0.40; #如果把0.40改成 NULL 呢？ \",\"可以看到，使用安全等于运算符时，两边的操作数的值都为NULL时，返回的结果为1而不是NULL，其他返回结果与等于运算符相同。\",\"3．不等于运算符 不等于运算符（<>和!=）用于判断两边的数字、字符串或者表达式的值是否不相等，如果不相等则返回1，相等则返回0。不等于运算符不能判断NULL值。如果两边的值有任意一个为NULL，或两边都为NULL，则结果为NULL。 SQL语句示例如下：\",\"mysql> SELECT 1 <> 1, 1 != 2, 'a' != 'b', (3+4) <> (2+6), 'a' != NULL, NULL <> NULL; +--------+--------+------------+----------------+-------------+--------------+ | 1 <> 1 | 1 != 2 | 'a' != 'b' | (3+4) <> (2+6) | 'a' != NULL | NULL <> NULL | +--------+--------+------------+----------------+-------------+--------------+ | 0 | 1 | 1 | 1 | NULL | NULL | +--------+--------+------------+----------------+-------------+--------------+ 1 row in set (0.00 sec) \",\"此外，还有非符号类型的运算符：\",\"4. 空运算符 空运算符（IS NULL或者ISNULL）判断一个值是否为NULL，如果为NULL则返回1，否则返回0。 SQL语句示例如下：\",\"mysql> SELECT NULL IS NULL, ISNULL(NULL), ISNULL('a'), 1 IS NULL; +--------------+--------------+-------------+-----------+ | NULL IS NULL | ISNULL(NULL) | ISNULL('a') | 1 IS NULL | +--------------+--------------+-------------+-----------+ | 1 | 1 | 0 | 0 | +--------------+--------------+-------------+-----------+ 1 row in set (0.00 sec) \",\"#查询commission_pct等于NULL。比较如下的四种写法 SELECT employee_id,commission_pct FROM employees WHERE commission_pct IS NULL; SELECT employee_id,commission_pct FROM employees WHERE commission_pct <=> NULL; SELECT employee_id,commission_pct FROM employees WHERE ISNULL(commission_pct); SELECT employee_id,commission_pct FROM employees WHERE commission_pct = NULL; \",\"SELECT last_name, manager_id FROM employees WHERE manager_id IS NULL; \",\"5. 非空运算符 非空运算符（IS NOT NULL）判断一个值是否不为NULL，如果不为NULL则返回1，否则返回0。 SQL语句示例如下：\",\"mysql> SELECT NULL IS NOT NULL, 'a' IS NOT NULL, 1 IS NOT NULL; +------------------+-----------------+---------------+ | NULL IS NOT NULL | 'a' IS NOT NULL | 1 IS NOT NULL | +------------------+-----------------+---------------+ | 0 | 1 | 1 | +------------------+-----------------+---------------+ 1 row in set (0.01 sec) \",\"#查询commission_pct不等于NULL SELECT employee_id,commission_pct FROM employees WHERE commission_pct IS NOT NULL; SELECT employee_id,commission_pct FROM employees WHERE NOT commission_pct <=> NULL; SELECT employee_id,commission_pct FROM employees WHERE NOT ISNULL(commission_pct); \",\"6. 最小值运算符 语法格式为：LEAST(值1，值2，...，值n)。其中，“值n”表示参数列表中有n个值。在有两个或多个参数的情况下，返回最小值。\",\"mysql> SELECT LEAST (1,0,2), LEAST('b','a','c'), LEAST(1,NULL,2); +---------------+--------------------+-----------------+ | LEAST (1,0,2) | LEAST('b','a','c') | LEAST(1,NULL,2) | +---------------+--------------------+-----------------+ | 0 | a | NULL | +---------------+--------------------+-----------------+ 1 row in set (0.00 sec) \",\"由结果可以看到，当参数是整数或者浮点数时，LEAST将返回其中最小的值；当参数为字符串时，返回字母表中顺序最靠前的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。\",\"7. 最大值运算符 语法格式为：GREATEST(值1，值2，...，值n)。其中，n表示参数列表中有n个值。当有两个或多个参数时，返回值为最大值。假如任意一个自变量为NULL，则GREATEST()的返回值为NULL。\",\"mysql> SELECT GREATEST(1,0,2), GREATEST('b','a','c'), GREATEST(1,NULL,2); +-----------------+-----------------------+--------------------+ | GREATEST(1,0,2) | GREATEST('b','a','c') | GREATEST(1,NULL,2) | +-----------------+-----------------------+--------------------+ | 2 | c | NULL | +-----------------+-----------------------+--------------------+ 1 row in set (0.00 sec) \",\"由结果可以看到，当参数中是整数或者浮点数时，GREATEST将返回其中最大的值；当参数为字符串时，返回字母表中顺序最靠后的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。\",\"8. BETWEEN AND运算符 BETWEEN运算符使用的格式通常为SELECT D FROM TABLE WHERE C BETWEEN A AND B，此时，当C大于或等于A，并且C小于或等于B时，结果为1，否则结果为0。\",\"mysql> SELECT 1 BETWEEN 0 AND 1, 10 BETWEEN 11 AND 12, 'b' BETWEEN 'a' AND 'c'; +-------------------+----------------------+-------------------------+ | 1 BETWEEN 0 AND 1 | 10 BETWEEN 11 AND 12 | 'b' BETWEEN 'a' AND 'c' | +-------------------+----------------------+-------------------------+ | 1 | 0 | 1 | +-------------------+----------------------+-------------------------+ 1 row in set (0.00 sec) \",\"SELECT last_name, salary FROM employees WHERE salary BETWEEN 2500 AND 3500; \",\"9. IN运算符 IN运算符用于判断给定的值是否是IN列表中的一个值，如果是则返回1，否则返回0。如果给定的值为NULL，或者IN列表中存在NULL，则结果为NULL。\",\"mysql> SELECT 'a' IN ('a','b','c'), 1 IN (2,3), NULL IN ('a','b'), 'a' IN ('a', NULL); +----------------------+------------+-------------------+--------------------+ | 'a' IN ('a','b','c') | 1 IN (2,3) | NULL IN ('a','b') | 'a' IN ('a', NULL) | +----------------------+------------+-------------------+--------------------+ | 1 | 0 | NULL | 1 | +----------------------+------------+-------------------+--------------------+ 1 row in set (0.00 sec) \",\"SELECT employee_id, last_name, salary, manager_id FROM employees WHERE manager_id IN (100, 101, 201); \",\"10. NOT IN运算符 NOT IN运算符用于判断给定的值是否不是IN列表中的一个值，如果不是IN列表中的一个值，则返回1，否则返回0。\",\"mysql> SELECT 'a' NOT IN ('a','b','c'), 1 NOT IN (2,3); +--------------------------+----------------+ | 'a' NOT IN ('a','b','c') | 1 NOT IN (2,3) | +--------------------------+----------------+ | 0 | 1 | +--------------------------+----------------+ 1 row in set (0.00 sec) \",\"11. LIKE运算符 LIKE运算符主要用来匹配字符串，通常用于模糊匹配，如果满足条件则返回1，否则返回0。如果给定的值或者匹配条件为NULL，则返回结果为NULL。\",\"LIKE运算符通常使用如下通配符：\",\"“%”：匹配0个或多个字符。 “_”：只能匹配一个字符。 \",\"SQL语句示例如下：\",\"mysql> SELECT NULL LIKE 'abc', 'abc' LIKE NULL; +-----------------+-----------------+ | NULL LIKE 'abc' | 'abc' LIKE NULL | +-----------------+-----------------+ | NULL | NULL | +-----------------+-----------------+ 1 row in set (0.00 sec) \",\"SELECT first_name FROM employees WHERE first_name LIKE 'S%'; \",\"SELECT last_name FROM employees WHERE last_name LIKE '_o%'; \",\"ESCAPE\",\"回避特殊符号的：使用转义符。例如：将[%]转为[]，然后再加上[ESCAPE‘$’]即可。\",\"SELECT job_id FROM jobs WHERE job_id LIKE ‘IT\\\\_%‘; \",\"如果使用\\\\表示转义，要省略ESCAPE。如果不是\\\\，则要加上ESCAPE。\",\"SELECT job_id FROM jobs WHERE job_id LIKE ‘IT$_%‘ escape ‘$‘; \",\"12. REGEXP运算符\",\"REGEXP运算符用来匹配字符串，语法格式为：expr REGEXP 匹配条件。如果expr满足匹配条件，返回1；如果不满足，则返回0。若expr或匹配条件任意一个为NULL，则结果为NULL。\",\"REGEXP运算符在进行匹配时，常用的有下面几种通配符：\",\"（1）‘^’匹配以该字符后面的字符开头的字符串。 （2）‘$’匹配以该字符前面的字符结尾的字符串。 （3）‘.’匹配任何一个单字符。 （4）“[...]”匹配在方括号内的任何字符。例如，“[abc]”匹配“a”或“b”或“c”。为了命名字符的范围，使用一个‘-’。“[a-z]”匹配任何字母，而“[0-9]”匹配任何数字。 （5）‘*’匹配零个或多个在它前面的字符。例如，“x*”匹配任何数量的‘x’字符，“[0-9]*”匹配任何数量的数字，而“*”匹配任何数量的任何字符。 \",\"SQL语句示例如下：\",\"mysql> SELECT 'shkstart' REGEXP '^s', 'shkstart' REGEXP 't);self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
, 'shkstart' REGEXP 'hk'; +------------------------+------------------------+-------------------------+ | 'shkstart' REGEXP '^s' | 'shkstart' REGEXP 't);self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
 | 'shkstart' REGEXP 'hk' | +------------------------+------------------------+-------------------------+ | 1 | 1 | 1 | +------------------------+------------------------+-------------------------+ 1 row in set (0.01 sec) \",\"mysql> SELECT 'atguigu' REGEXP 'gu.gu', 'atguigu' REGEXP '[ab]'; +--------------------------+-------------------------+ | 'atguigu' REGEXP 'gu.gu' | 'atguigu' REGEXP '[ab]' | +--------------------------+-------------------------+ | 1 | 1 | +--------------------------+-------------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"3. 逻辑运算符\",\"slug\":\"_3-逻辑运算符\",\"contents\":[\"逻辑运算符主要用来判断表达式的真假，在MySQL中，逻辑运算符的返回结果为1、0或者NULL。\",\"MySQL中支持4种逻辑运算符如下：\",\"1．逻辑非运算符 逻辑非（NOT或!）运算符表示当给定的值为0时返回1；当给定的值为非0值时返回0；当给定的值为NULL时，返回NULL。\",\"mysql> SELECT NOT 1, NOT 0, NOT(1+1), NOT !1, NOT NULL; +-------+-------+----------+--------+----------+ | NOT 1 | NOT 0 | NOT(1+1) | NOT !1 | NOT NULL | +-------+-------+----------+--------+----------+ | 0 | 1 | 0 | 1 | NULL | +-------+-------+----------+--------+----------+ 1 row in set, 1 warning (0.00 sec) \",\"SELECT last_name, job_id FROM employees WHERE job_id NOT IN ('IT_PROG', 'ST_CLERK', 'SA_REP'); \",\"2．逻辑与运算符 逻辑与（AND或&&）运算符是当给定的所有值均为非0值，并且都不为NULL时，返回1；当给定的一个值或者多个值为0时则返回0；否则返回NULL。\",\"mysql> SELECT 1 AND -1, 0 AND 1, 0 AND NULL, 1 AND NULL; +----------+---------+------------+------------+ | 1 AND -1 | 0 AND 1 | 0 AND NULL | 1 AND NULL | +----------+---------+------------+------------+ | 1 | 0 | 0 | NULL | +----------+---------+------------+------------+ 1 row in set (0.00 sec) \",\"SELECT employee_id, last_name, job_id, salary FROM employees WHERE salary >=10000 AND job_id LIKE '%MAN%'; \",\"3．逻辑或运算符 逻辑或（OR或||）运算符是当给定的值都不为NULL，并且任何一个值为非0值时，则返回1，否则返回0；当一个值为NULL，并且另一个值为非0值时，返回1，否则返回NULL；当两个值都为NULL时，返回NULL。\",\"mysql> SELECT 1 OR -1, 1 OR 0, 1 OR NULL, 0 || NULL, NULL || NULL; +---------+--------+-----------+-----------+--------------+ | 1 OR -1 | 1 OR 0 | 1 OR NULL | 0 || NULL | NULL || NULL | +---------+--------+-----------+-----------+--------------+ | 1 | 1 | 1 | NULL | NULL | +---------+--------+-----------+-----------+--------------+ 1 row in set, 2 warnings (0.00 sec) \",\"#查询基本薪资不在9000-12000之间的员工编号和基本薪资 SELECT employee_id,salary FROM employees WHERE NOT (salary >= 9000 AND salary <= 12000); SELECT employee_id,salary FROM employees WHERE salary <9000 OR salary > 12000; SELECT employee_id,salary FROM employees WHERE salary NOT BETWEEN 9000 AND 12000; \",\"SELECT employee_id, last_name, job_id, salary FROM employees WHERE salary >= 10000 OR job_id LIKE '%MAN%'; \",\"注意：\",\"OR可以和AND一起使用，但是在使用时要注意两者的优先级，由于AND的优先级高于OR，因此先对AND两边的操作数进行操作，再与OR中的操作数结合。\",\"4．逻辑异或运算符 逻辑异或（XOR）运算符是当给定的值中任意一个值为NULL时，则返回NULL；如果两个非NULL的值都是0或者都不等于0时，则返回0；如果一个值为0，另一个值不为0时，则返回1。\",\"mysql> SELECT 1 XOR -1, 1 XOR 0, 0 XOR 0, 1 XOR NULL, 1 XOR 1 XOR 1, 0 XOR 0 XOR 0; +----------+---------+---------+------------+---------------+---------------+ | 1 XOR -1 | 1 XOR 0 | 0 XOR 0 | 1 XOR NULL | 1 XOR 1 XOR 1 | 0 XOR 0 XOR 0 | +----------+---------+---------+------------+---------------+---------------+ | 0 | 1 | 0 | NULL | 1 | 0 | +----------+---------+---------+------------+---------------+---------------+ 1 row in set (0.00 sec) \",\"select last_name,department_id,salary from employees where department_id in (10,20) XOR salary > 8000; \"]},{\"header\":\"4. 位运算符\",\"slug\":\"_4-位运算符\",\"contents\":[\"位运算符是在二进制数上进行计算的运算符。位运算符会先将操作数变成二进制数，然后进行位运算，最后将计算结果从二进制变回十进制数。\",\"MySQL支持的位运算符如下：\",\"1．按位与运算符 按位与（&）运算符将给定值对应的二进制数逐位进行逻辑与运算。当给定值对应的二进制位的数值都为1时，则该位返回1，否则返回0。\",\"mysql> SELECT 1 & 10, 20 & 30; +--------+---------+ | 1 & 10 | 20 & 30 | +--------+---------+ | 0 | 20 | +--------+---------+ 1 row in set (0.00 sec) \",\"1的二进制数为0001，10的二进制数为1010，所以1 & 10的结果为0000，对应的十进制数为0。20的二进制数为10100，30的二进制数为11110，所以20 & 30的结果为10100，对应的十进制数为20。\",\"2. 按位或运算符 按位或（|）运算符将给定的值对应的二进制数逐位进行逻辑或运算。当给定值对应的二进制位的数值有一个或两个为1时，则该位返回1，否则返回0。\",\"mysql> SELECT 1 | 10, 20 | 30; +--------+---------+ | 1 | 10 | 20 | 30 | +--------+---------+ | 11 | 30 | +--------+---------+ 1 row in set (0.00 sec) \",\"1的二进制数为0001，10的二进制数为1010，所以1 | 10的结果为1011，对应的十进制数为11。20的二进制数为10100，30的二进制数为11110，所以20 | 30的结果为11110，对应的十进制数为30。\",\"3. 按位异或运算符 按位异或（^）运算符将给定的值对应的二进制数逐位进行逻辑异或运算。当给定值对应的二进制位的数值不同时，则该位返回1，否则返回0。\",\"mysql> SELECT 1 ^ 10, 20 ^ 30; +--------+---------+ | 1 ^ 10 | 20 ^ 30 | +--------+---------+ | 11 | 10 | +--------+---------+ 1 row in set (0.00 sec) \",\"1的二进制数为0001，10的二进制数为1010，所以1 ^ 10的结果为1011，对应的十进制数为11。20的二进制数为10100，30的二进制数为11110，所以20 ^ 30的结果为01010，对应的十进制数为10。\",\"再举例：\",\"mysql> SELECT 12 & 5, 12 | 5,12 ^ 5 FROM DUAL; +--------+--------+--------+ | 12 & 5 | 12 | 5 | 12 ^ 5 | +--------+--------+--------+ | 4 | 13 | 9 | +--------+--------+--------+ 1 row in set (0.00 sec) \",\"4. 按位取反运算符 按位取反（~）运算符将给定的值的二进制数逐位进行取反操作，即将1变为0，将0变为1。\",\"mysql> SELECT 10 & ~1; +---------+ | 10 & ~1 | +---------+ | 10 | +---------+ 1 row in set (0.00 sec) \",\"由于按位取反（~）运算符的优先级高于按位与（&）运算符的优先级，所以10 & ~1，首先，对数字1进行按位取反操作，结果除了最低位为0，其他位都为1，然后与10进行按位与操作，结果为10。\",\"5. 按位右移运算符 按位右移（>>）运算符将给定的值的二进制数的所有位右移指定的位数。右移指定的位数后，右边低位的数值被移出并丢弃，左边高位空出的位置用0补齐。\",\"mysql> SELECT 1 >> 2, 4 >> 2; +--------+--------+ | 1 >> 2 | 4 >> 2 | +--------+--------+ | 0 | 1 | +--------+--------+ 1 row in set (0.00 sec) \",\"1的二进制数为0000 0001，右移2位为0000 0000，对应的十进制数为0。4的二进制数为0000 0100，右移2位为0000 0001，对应的十进制数为1。\",\"6. 按位左移运算符 按位左移（<<）运算符将给定的值的二进制数的所有位左移指定的位数。左移指定的位数后，左边高位的数值被移出并丢弃，右边低位空出的位置用0补齐。\",\"mysql> SELECT 1 << 2, 4 << 2; +--------+--------+ | 1 << 2 | 4 << 2 | +--------+--------+ | 4 | 16 | +--------+--------+ 1 row in set (0.00 sec) \",\"1的二进制数为0000 0001，左移两位为0000 0100，对应的十进制数为4。4的二进制数为0000 0100，左移两位为0001 0000，对应的十进制数为16。\"]},{\"header\":\"5. 运算符的优先级\",\"slug\":\"_5-运算符的优先级\",\"contents\":[\"数字编号越大，优先级越高，优先级高的运算符先进行计算。可以看到，赋值运算符的优先级最低，使用“()”括起来的表达式的优先级最高。\"]},{\"header\":\"拓展：使用正则表达式查询\",\"slug\":\"拓展-使用正则表达式查询\",\"contents\":[\"正则表达式通常被用来检索或替换那些符合某个模式的文本内容，根据指定的匹配模式匹配文本中符合要求的特殊字符串。例如，从一个文本文件中提取电话号码，查找一篇文章中重复的单词或者替换用户输入的某些敏感词语等，这些地方都可以使用正则表达式。正则表达式强大而且灵活，可以应用于非常复杂的查询。\",\"MySQL中使用REGEXP关键字指定正则表达式的字符匹配模式。下表列出了REGEXP操作符中常用字符匹配列表。\",\"1. 查询以特定字符或字符串开头的记录 字符‘^’匹配以特定字符或者字符串开头的文本。\",\"在fruits表中，查询f_name字段以字母‘b’开头的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP '^b'; \",\"2. 查询以特定字符或字符串结尾的记录 字符‘$’匹配以特定字符或者字符串结尾的文本。\",\"在fruits表中，查询f_name字段以字母‘y’结尾的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'y);self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
; \",\"3. 用符号\\\".\\\"来替代字符串中的任意一个字符 字符‘.’匹配任意一个字符。 在fruits表中，查询f_name字段值包含字母‘a’与‘g’且两个字母之间只有一个字母的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'a.g'; \",\"4. 使用\\\"*\\\"和\\\"+\\\"来匹配多个字符 星号‘*’匹配前面的字符任意多次，包括0次。加号‘+’匹配前面的字符至少一次。\",\"在fruits表中，查询f_name字段值以字母‘b’开头且‘b’后面出现字母‘a’的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP '^ba*'; \",\"在fruits表中，查询f_name字段值以字母‘b’开头且‘b’后面出现字母‘a’至少一次的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP '^ba+'; \",\"5. 匹配指定字符串 正则表达式可以匹配指定字符串，只要这个字符串在查询文本中即可，如要匹配多个字符串，多个字符串之间使用分隔符‘|’隔开。\",\"在fruits表中，查询f_name字段值包含字符串“on”的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'on'; \",\"在fruits表中，查询f_name字段值包含字符串“on”或者“ap”的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'on|ap'; \",\"之前介绍过，LIKE运算符也可以匹配指定的字符串，但与REGEXP不同，LIKE匹配的字符串如果在文本中间出现，则找不到它，相应的行也不会返回。REGEXP在文本内进行匹配，如果被匹配的字符串在文本中出现，REGEXP将会找到它，相应的行也会被返回。对比结果如下所示。\",\"在fruits表中，使用LIKE运算符查询f_name字段值为“on”的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name like 'on'; Empty set(0.00 sec) \",\"6. 匹配指定字符中的任意一个 方括号“[]”指定一个字符集合，只匹配其中任何一个字符，即为所查找的文本。\",\"在fruits表中，查找f_name字段中包含字母‘o’或者‘t’的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP '[ot]'; \",\"在fruits表中，查询s_id字段中包含4、5或者6的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE s_id REGEXP '[456]'; \",\"7. 匹配指定字符以外的字符“[^字符集合]”匹配不在指定集合中的任何字符。\",\"在fruits表中，查询f_id字段中包含字母ae和数字12以外字符的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_id REGEXP '[^a-e1-2]'; \",\"8. 使用{n,}或者{n,m}来指定字符串连续出现的次数 “字符串{n,}”表示至少匹配n次前面的字符；“字符串{n,m}”表示匹配前面的字符串不少于n次，不多于m次。例如，a{2,}表示字母a连续出现至少2次，也可以大于2次；a{2,4}表示字母a连续出现最少2次，最多不能超过4次。\",\"在fruits表中，查询f_name字段值出现字母‘x’至少2次的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'x{2,}'; \",\"在fruits表中，查询f_name字段值出现字符串“ba”最少1次、最多3次的记录，SQL语句如下：\",\"mysql> SELECT * FROM fruits WHERE f_name REGEXP 'ba{1,3}'; \"]}]},\"/study-tutorial/database/mysql/overview.html\":{\"title\":\"1、数据库概述\",\"contents\":[{\"header\":\"1. 为什么要使用数据库\",\"slug\":\"_1-为什么要使用数据库\",\"contents\":[\"持久化(persistence)：把数据保存到可掉电式存储设备中以供之后使用。大多数情况下，特别是企业级应用，数据持久化意味着将内存中的数据保存到硬盘上加以”固化”，而持久化的实现过程大多通过各种关系数据库来完成。\",\"持久化的主要作用是将内存中的数据存储在关系型数据库中，当然也可以存储在磁盘文件、XML数据文件中。\",\"生活中的例子：\"]},{\"header\":\"2. 数据库与数据库管理系统\",\"slug\":\"_2-数据库与数据库管理系统\",\"contents\":[]},{\"header\":\"2.1 数据库的相关概念\",\"slug\":\"_2-1-数据库的相关概念\",\"contents\":[\"DB：数据库（Database）\",\"即存储数据的“仓库”，其本质是一个文件系统。它保存了一系列有组织的数据。\",\"DBMS：数据库管理系统（Database Management System）\",\"是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，对数据库进行统一管理和控制。用户通过数据库管理系统访问数据库中表内的数据。\",\"SQL：结构化查询语言（Structured Query Language）\",\"专门用来与数据库通信的语言。\"]},{\"header\":\"2.2 数据库与数据库管理系统的关系\",\"slug\":\"_2-2-数据库与数据库管理系统的关系\",\"contents\":[\"数据库管理系统(DBMS)可以管理多个数据库，一般开发人员会针对每一个应用创建一个数据库。为保存应用中实体的数据，一般会在数据库创建多个表，以保存程序中实体用户的数据。\",\"数据库管理系统、数据库和表的关系如图所示：\"]},{\"header\":\"2.3 常见的数据库管理系统排名(DBMS)\",\"slug\":\"_2-3-常见的数据库管理系统排名-dbms\",\"contents\":[\"目前互联网上常见的数据库管理软件有Oracle、MySQL、MS SQL Server、DB2、PostgreSQL、Access、Sybase、Informix这几种。以下是2021年DB-Engines Ranking 对各数据库受欢迎程度进行调查后的统计结果：（查看数据库最新排名:https://db-engines.com/en/ranking）\",\"。。。\",\"对应的走势图：（https://db-engines.com/en/ranking_trend）\"]},{\"header\":\"2.4 常见的数据库介绍\",\"slug\":\"_2-4-常见的数据库介绍\",\"contents\":[\"Oracle\",\"1979 年，Oracle 2 诞生，它是第一个商用的 RDBMS（关系型数据库管理系统）。随着 Oracle 软件的名气越来越大，公司也改名叫 Oracle 公司。\",\"2007年，总计85亿美金收购BEA Systems。\",\"2009年，总计74亿美金收购SUN。此前的2008年，SUN以10亿美金收购MySQL。意味着Oracle 同时拥有了 MySQL 的管理权，至此 Oracle 在数据库领域中成为绝对的领导者。\",\"2013年，甲骨文超越IBM，成为继Microsoft后全球第二大软件公司。\",\"如今 Oracle 的年收入达到了 400 亿美金，足以证明商用（收费）数据库软件的价值。\",\"SQL Server\",\"SQL Server 是微软开发的大型商业数据库，诞生于 1989 年。C#、.net等语言常使用，与WinNT完全集成，也可以很好地与Microsoft BackOffice产品集成。\",\"DB2\",\"IBM公司的数据库产品,收费的。常应用在银行系统中。\",\"PostgreSQL\",\"PostgreSQL 的稳定性极强，最符合SQL标准，开放源码，具备商业级DBMS质量。PG对数据量大的文本以及SQL处理较快。\",\"SyBase\",\"已经淡出历史舞台。提供了一个非常专业数据建模的工具PowerDesigner。\",\"SQLite\",\"嵌入式的小型数据库，应用在手机端。 零配置，SQlite3不用安装，不用配置，不用启动，关闭或者配置数据库实例。当系统崩溃后不用做任何恢复操作，再下次使用数据库的时候自动恢复。\",\"informix\",\"IBM公司出品，取自Information 和Unix的结合，它是第一个被移植到Linux上的商业数据库产品。仅运行于unix/linux平台，命令行操作。 性能较高，支持集群，适应于安全性要求极高的系统，尤其是银行，证券系统的应用。\"]},{\"header\":\"3. MySQL介绍\",\"slug\":\"_3-mysql介绍\",\"contents\":[]},{\"header\":\"3.1 概述\",\"slug\":\"_3-1-概述\",\"contents\":[\"MySQL是一个开放源代码的关系型数据库管理系统，由瑞典MySQL AB（创始人Michael Widenius）公司1995年开发，迅速成为开源数据库的 No.1。\",\"2008被Sun收购（10亿美金），2009年Sun被Oracle收购。MariaDB应运而生。（MySQL 的创造者担心 MySQL 有闭源的风险，因此创建了 MySQL 的分支项目 MariaDB）\",\"MySQL6.x 版本之后分为社区版和商业版。\",\"MySQL是一种关联数据库管理系统，将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。\",\"MySQL是开源的，所以你不需要支付额外的费用。\",\"MySQL是可以定制的，采用了GPL（GNU General Public License）协议，你可以修改源码来开发自己的MySQL系统。\",\"MySQL支持大型的数据库。可以处理拥有上千万条记录的大型数据库。\",\"MySQL支持大型数据库，支持5000万条记录的数据仓库，32位系统表文件最大可支持4GB，64位系统支持最大的表文件为8TB。\",\"MySQL使用标准的SQL数据语言形式。\",\"MySQL可以允许运行于多个系统上，并且支持多种语言。这些编程语言包括C、C++、Python、Java、Perl、PHP和Ruby等。\"]},{\"header\":\"3.2 MySQL发展史重大事件\",\"slug\":\"_3-2-mysql发展史重大事件\",\"contents\":[\"MySQL的历史就是整个互联网的发展史。互联网业务从社交领域、电商领域到金融领域的发展，推动着应用对数据库的需求提升，对传统的数据库服务能力提出了挑战。高并发、高性能、高可用、轻资源、易维护、易扩展的需求，促进了MySQL的长足发展。\"]},{\"header\":\"1.4 关于MySQL 8.0\",\"slug\":\"_1-4-关于mysql-8-0\",\"contents\":[\"MySQL从5.7版本直接跳跃发布了8.0版本，可见这是一个令人兴奋的里程碑版本。MySQL 8版本在功能上做了显著的改进与增强，开发者对MySQL的源代码进行了重构，最突出的一点是多MySQL Optimizer优化器进行了改进。不仅在速度上得到了改善，还为用户带来了更好的性能和更棒的体验。\"]},{\"header\":\"1.5 Why choose MySQL?\",\"slug\":\"_1-5-why-choose-mysql\",\"contents\":[\"为什么如此多的厂商要选用MySQL？大概总结的原因主要有以下几点：\",\"开放源代码，使用成本低。\",\"性能卓越，服务稳定。\",\"软件体积小，使用简单，并且易于维护。\",\"历史悠久，社区用户非常活跃，遇到问题可以寻求帮助。\",\"许多互联网公司在用，经过了时间的验证。\"]},{\"header\":\"1.6 Oracle vs MySQL\",\"slug\":\"_1-6-oracle-vs-mysql\",\"contents\":[\"Oracle 更适合大型跨国企业的使用，因为他们对费用不敏感，但是对性能要求以及安全性有更高的要求。\",\"MySQL 由于其体积小、速度快、总体拥有成本低，可处理上千万条记录的大型数据库，尤其是开放源码这一特点，使得很多互联网公司、中小型网站选择了MySQL作为网站数据库（Facebook，Twitter，YouTube，阿里巴巴/蚂蚁金服，去哪儿，美团外卖，腾讯）。\"]},{\"header\":\"4. RDBMS 与 非RDBMS\",\"slug\":\"_4-rdbms-与-非rdbms\",\"contents\":[\"从排名中我们能看出来，关系型数据库绝对是 DBMS 的主流，其中使用最多的 DBMS 分别是 Oracle、MySQL 和 SQL Server。这些都是关系型数据库（RDBMS）。\"]},{\"header\":\"4.1 关系型数据库(RDBMS)\",\"slug\":\"_4-1-关系型数据库-rdbms\",\"contents\":[]},{\"header\":\"4.1.1 实质\",\"slug\":\"_4-1-1-实质\",\"contents\":[\"这种类型的数据库是最古老的数据库类型，关系型数据库模型是把复杂的数据结构归结为简单的二元关系（即二维表格形式）。\",\"关系型数据库以行(row)和列(column)的形式存储数据，以便于用户理解。这一系列的行和列被称为表(table)，一组表组成了一个库(database)。\",\"表与表之间的数据记录有关系(relationship)。现实世界中的各种实体以及实体之间的各种联系均用关系模型来表示。关系型数据库，就是建立在关系模型基础上的数据库。\",\"SQL 就是关系型数据库的查询语言。\"]},{\"header\":\"4.1.2 优势\",\"slug\":\"_4-1-2-优势\",\"contents\":[\"复杂查询 可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。\",\"事务支持 使得对于安全性能很高的数据访问要求得以实现。\"]},{\"header\":\"4.2 非关系型数据库(非RDBMS)\",\"slug\":\"_4-2-非关系型数据库-非rdbms\",\"contents\":[]},{\"header\":\"4.2.1 介绍\",\"slug\":\"_4-2-1-介绍\",\"contents\":[\"非关系型数据库，可看成传统关系型数据库的功能阉割版本，基于键值对存储数据，不需要经过SQL层的解析，性能非常高。同时，通过减少不常用的功能，进一步提高性能。\",\"目前基本上大部分主流的非关系型数据库都是免费的。\"]},{\"header\":\"4.2.2 有哪些非关系型数据库\",\"slug\":\"_4-2-2-有哪些非关系型数据库\",\"contents\":[\"相比于 SQL，NoSQL 泛指非关系型数据库，包括了榜单上的键值型数据库、文档型数据库、搜索引擎和列存储等，除此以外还包括图形数据库。也只有用 NoSQL 一词才能将这些技术囊括进来。\",\"键值型数据库\",\"键值型数据库通过 Key-Value 键值的方式来存储数据，其中 Key 和 Value 可以是简单的对象，也可以是复杂的对象。Key 作为唯一的标识符，优点是查找速度快，在这方面明显优于关系型数据库，缺点是无法像关系型数据库一样使用条件过滤（比如 WHERE），如果你不知道去哪里找数据，就要遍历所有的键，这就会消耗大量的计算。\",\"键值型数据库典型的使用场景是作为内存缓存。Redis 是最流行的键值型数据库。\",\"文档型数据库\",\"此类数据库可存放并获取文档，可以是XML、JSON等格式。在数据库中文档作为处理信息的基本单位，一个文档就相当于一条记录。文档数据库所存放的文档，就相当于键值数据库所存放的“值”。MongoDB 是最流行的文档型数据库。此外，还有CouchDB等。\",\"搜索引擎数据库\",\"虽然关系型数据库采用了索引提升检索效率，但是针对全文索引效率却较低。搜索引擎数据库是应用在搜索引擎领域的数据存储形式，由于搜索引擎会爬取大量的数据，并以特定的格式进行存储，这样在检索的时候才能保证性能最优。核心原理是“倒排索引”。\",\"典型产品：Solr、Elasticsearch、Splunk 等。\",\"列式数据库\",\"列式数据库是相对于行式存储的数据库，Oracle、MySQL、SQL Server 等数据库都是采用的行式存储（Row-based），而列式数据库是将数据按照列存储到数据库中，这样做的好处是可以大量降低系统的 I/O，适合于分布式文件系统，不足在于功能相对有限。典型产品：HBase等。\",\"图形数据库\",\"图形数据库，利用了图这种数据结构存储了实体（对象）之间的关系。图形数据库最典型的例子就是社交网络中人与人的关系，数据模型主要是以节点和边（关系）来实现，特点在于能高效地解决复杂的关系问题。\",\"图形数据库顾名思义，就是一种存储图形关系的数据库。它利用了图这种数据结构存储了实体（对象）之间的关系。关系型数据用于存储明确关系的数据，但对于复杂关系的数据存储却有些力不从心。如社交网络中人物之间的关系，如果用关系型数据库则非常复杂，用图形数据库将非常简单。典型产品：Neo4J、InfoGrid等。\"]},{\"header\":\"4.2.3 NoSQL的演变\",\"slug\":\"_4-2-3-nosql的演变\",\"contents\":[\"由于 SQL 一直称霸 DBMS，因此许多人在思考是否有一种数据库技术能远离 SQL，于是 NoSQL 诞生了，但是随着发展却发现越来越离不开 SQL。到目前为止 NoSQL 阵营中的 DBMS 都会有实现类似 SQL 的功能。下面是“NoSQL”这个名词在不同时期的诠释，从这些释义的变化中可以看出 NoSQL 功能的演变：\",\"1970：NoSQL = We have no SQL\",\"1980：NoSQL = Know SQL\",\"2000：NoSQL = No SQL!\",\"2005：NoSQL = Not only SQL\",\"2013：NoSQL = No, SQL!\",\"NoSQL 对 SQL 做出了很好的补充，比如实际开发中，有很多业务需求，其实并不需要完整的关系型数据库功能，非关系型数据库的功能就足够使用了。这种情况下，使用性能更高、成本更低的非关系型数据库当然是更明智的选择。比如：日志收集、排行榜、定时器等。\"]},{\"header\":\"4.3 小结\",\"slug\":\"_4-3-小结\",\"contents\":[\"NoSQL 的分类很多，即便如此，在 DBMS 排名中，还是 SQL 阵营的比重更大，影响力前 5 的 DBMS 中有 4 个是关系型数据库，而排名前 20 的 DBMS 中也有 12 个是关系型数据库。所以说，掌握 SQL 是非常有必要的。整套课程将围绕 SQL 展开。\"]},{\"header\":\"5. 关系型数据库设计规则\",\"slug\":\"_5-关系型数据库设计规则\",\"contents\":[\"关系型数据库的典型数据结构就是数据表，这些数据表的组成都是结构化的（Structured）。\",\"将数据放到表中，表再放到库中。\",\"一个数据库中可以有多个表，每个表都有一个名字，用来标识自己。表名具有唯一性。\",\"表具有一些特性，这些特性定义了数据在表中如何存储，类似Java和Python中 “类”的设计。\"]},{\"header\":\"5.1 表、记录、字段\",\"slug\":\"_5-1-表、记录、字段\",\"contents\":[\"E-R（entity-relationship，实体-联系）模型中有三个主要概念是：实体集、属性、联系集。\",\"一个实体集（class）对应于数据库中的一个表（table），一个实体（instance）则对应于数据库表中的一行（row），也称为一条记录（record）。一个属性（attribute）对应于数据库表中的一列（column），也称为一个字段（field）。\",\"ORM思想 (Object Relational Mapping)体现： 数据库中的一个表 <---> Java或Python中的一个类 表中的一条数据 <---> 类中的一个对象（或实体） 表中的一个列 <----> 类中的一个字段、属性(field) \"]},{\"header\":\"5.2 表的关联关系\",\"slug\":\"_5-2-表的关联关系\",\"contents\":[\"表与表之间的数据记录有关系(relationship)。现实世界中的各种实体以及实体之间的各种联系均用关系模型来表示。\",\"四种：一对一关联、一对多关联、多对多关联、自我引用\"]},{\"header\":\"5.2.1 一对一关联（one-to-one）\",\"slug\":\"_5-2-1-一对一关联-one-to-one\",\"contents\":[\"在实际的开发中应用不多，因为一对一可以创建成一张表。\",\"举例：设计学生表：学号、姓名、手机号码、班级、系别、身份证号码、家庭住址、籍贯、紧急联系人、... \",\"拆为两个表：两个表的记录是一一对应关系。\",\"基础信息表（常用信息）：学号、姓名、手机号码、班级、系别\",\"档案信息表（不常用信息）：学号、身份证号码、家庭住址、籍贯、紧急联系人、...\",\"两种建表原则： \",\"外键唯一：主表的主键和从表的外键（唯一），形成主外键关系，外键唯一。\",\"外键是主键：主表的主键和从表的主键，形成主外键关系。\"]},{\"header\":\"5.2.2 一对多关系（one-to-many）\",\"slug\":\"_5-2-2-一对多关系-one-to-many\",\"contents\":[\"常见实例场景：客户表和订单表，分类表和商品表，部门表和员工表。\",\"举例： \",\"员工表：编号、姓名、...、所属部门\",\"部门表：编号、名称、简介\",\"一对多建表原则：在从表(多方)创建一个字段，字段作为外键指向主表(一方)的主键\"]},{\"header\":\"5.2.3 多对多（many-to-many）\",\"slug\":\"_5-2-3-多对多-many-to-many\",\"contents\":[\"要表示多对多关系，必须创建第三个表，该表通常称为联接表，它将多对多关系划分为两个一对多关系。将这两个表的主键都插入到第三个表中。\",\"举例1：学生-课程\",\"学生信息表：一行代表一个学生的信息（学号、姓名、手机号码、班级、系别...）\",\"课程信息表：一行代表一个课程的信息（课程编号、授课老师、简介...）\",\"选课信息表：一个学生可以选多门课，一门课可以被多个学生选择\",\"学号 课程编号 1 1001 2 1001 1 1002 \",\"举例2：产品-订单\",\"“订单”表和“产品”表有一种多对多的关系，这种关系是通过与“订单明细”表建立两个一对多关系来定义的。一个订单可以有多个产品，每个产品可以出现在多个订单中。\",\"产品表：“产品”表中的每条记录表示一个产品。\",\"订单表：“订单”表中的每条记录表示一个订单。\",\"订单明细表：每个产品可以与“订单”表中的多条记录对应，即出现在多个订单中。一个订单可以与“产品”表中的多条记录对应，即包含多个产品。\",\"举例3：用户-角色\",\"多对多关系建表原则：需要创建第三张表，中间表中至少两个字段，这两个字段分别作为外键指向各自一方的主键。\"]}]},\"/study-tutorial/database/mysql/process-control.html\":{\"title\":\"16、变量、流程控制与游标\",\"contents\":[{\"header\":\"1. 变量\",\"slug\":\"_1-变量\",\"contents\":[\"在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终的结果数据。\",\"在 MySQL 数据库中，变量分为系统变量以及用户自定义变量。\"]},{\"header\":\"1.1 系统变量\",\"slug\":\"_1-1-系统变量\",\"contents\":[]},{\"header\":\"1.1.1 系统变量分类\",\"slug\":\"_1-1-1-系统变量分类\",\"contents\":[\"变量由系统定义，不是用户定义，属于服务器层面。启动MySQL服务，生成MySQL服务实例期间，MySQL将为MySQL服务器内存中的系统变量赋值，这些系统变量定义了当前MySQL服务实例的属性、特征。这些系统变量的值要么是编译MySQL时参数的默认值，要么是配置文件（例如my.ini等）中的参数值。大家可以通过网址 https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html 查看MySQL文档的系统变量。\",\"系统变量分为全局系统变量（需要添加global 关键字）以及会话系统变量（需要添加 session 关键字），有时也把全局系统变量简称为全局变量，有时也把会话系统变量称为local变量。**如果不写，默认会话级别。**静态变量（在 MySQL 服务实例运行期间它们的值不能使用 set 动态修改）属于特殊的全局系统变量。\",\"每一个MySQL客户机成功连接MySQL服务器后，都会产生与之对应的会话。会话期间，MySQL服务实例会在MySQL服务器内存中生成与该会话对应的会话系统变量，这些会话系统变量的初始值是全局系统变量值的复制。如下图：\",\"全局系统变量针对于所有会话（连接）有效，但不能跨重启\",\"会话系统变量仅针对于当前会话（连接）有效。会话期间，当前会话对某个会话系统变量值的修改，不会影响其他会话同一个会话系统变量的值。\",\"会话1对某个全局系统变量值的修改会导致会话2中同一个全局系统变量值的修改。\",\"在MySQL中有些系统变量只能是全局的，例如 max_connections 用于限制服务器的最大连接数；有些系统变量作用域既可以是全局又可以是会话，例如 character_set_client 用于设置客户端的字符集；有些系统变量的作用域只能是当前会话，例如 pseudo_thread_id 用于标记当前会话的 MySQL 连接 ID。\"]},{\"header\":\"1.1.2 查看系统变量\",\"slug\":\"_1-1-2-查看系统变量\",\"contents\":[\"查看所有或部分系统变量\",\"#查看所有全局变量 SHOW GLOBAL VARIABLES; #查看所有会话变量 SHOW SESSION VARIABLES; 或 SHOW VARIABLES; \",\"#查看满足条件的部分系统变量。 SHOW GLOBAL VARIABLES LIKE '%标识符%'; #查看满足条件的部分会话变量 SHOW SESSION VARIABLES LIKE '%标识符%'; \",\"举例：\",\"SHOW GLOBAL VARIABLES LIKE 'admin_%'; \",\"查看指定系统变量\",\"作为 MySQL 编码规范，MySQL 中的系统变量以两个“@”开头，其中“@@global”仅用于标记全局系统变量，“@@session”仅用于标记会话系统变量。“@@”首先标记会话系统变量，如果会话系统变量不存在，则标记全局系统变量。\",\"#查看指定的系统变量的值 SELECT @@global.变量名; #查看指定的会话变量的值 SELECT @@session.变量名; #或者 SELECT @@变量名; \",\"修改系统变量的值\",\"有些时候，数据库管理员需要修改系统变量的默认值，以便修改当前会话或者MySQL服务实例的属性、特征。具体方法：\",\"方式1：修改MySQL配置文件，继而修改MySQL系统变量的值（该方法需要重启MySQL服务）\",\"方式2：在MySQL服务运行期间，使用“set”命令重新设置系统变量的值\",\"#为某个系统变量赋值 #方式1： SET @@global.变量名=变量值; #方式2： SET GLOBAL 变量名=变量值; #为某个会话变量赋值 #方式1： SET @@session.变量名=变量值; #方式2： SET SESSION 变量名=变量值; \",\"举例：\",\"SELECT @@global.autocommit; SET GLOBAL autocommit=0; \",\"SELECT @@session.tx_isolation; SET @@session.tx_isolation='read-uncommitted'; \",\"SET GLOBAL max_connections = 1000; SELECT @@global.max_connections; \"]},{\"header\":\"1.2 用户变量\",\"slug\":\"_1-2-用户变量\",\"contents\":[]},{\"header\":\"1.2.1 用户变量分类\",\"slug\":\"_1-2-1-用户变量分类\",\"contents\":[\"用户变量是用户自己定义的，作为 MySQL 编码规范，MySQL 中的用户变量以一个“@”开头。根据作用范围不同，又分为会话用户变量和局部变量。\",\"会话用户变量：作用域和会话变量一样，只对当前连接会话有效。\",\"局部变量：只在 BEGIN 和 END 语句块中有效。局部变量只能在存储过程和函数中使用。\"]},{\"header\":\"1.2.2 会话用户变量\",\"slug\":\"_1-2-2-会话用户变量\",\"contents\":[\"变量的定义\",\"#方式1：“=”或“:=” SET @用户变量 = 值; SET @用户变量 := 值; #方式2：“:=” 或 INTO关键字 SELECT @用户变量 := 表达式 [FROM 等子句]; SELECT 表达式 INTO @用户变量 [FROM 等子句]; \",\"查看用户变量的值 （查看、比较、运算等）\",\"SELECT @用户变量 \",\"举例\",\"SET @a = 1; SELECT @a; \",\"SELECT @num := COUNT(*) FROM employees; SELECT @num; \",\"SELECT AVG(salary) INTO @avgsalary FROM employees; SELECT @avgsalary; \",\"SELECT @big; #查看某个未声明的变量时，将得到NULL值 \"]},{\"header\":\"1.2.3 局部变量\",\"slug\":\"_1-2-3-局部变量\",\"contents\":[\"定义：可以使用DECLARE语句定义一个局部变量\",\"作用域：仅仅在定义它的 BEGIN ... END 中有效\",\"位置：只能放在 BEGIN ... END 中，而且只能放在第一句\",\"BEGIN #声明局部变量 DECLARE 变量名1 变量数据类型 [DEFAULT 变量默认值]; DECLARE 变量名2,变量名3,... 变量数据类型 [DEFAULT 变量默认值]; #为局部变量赋值 SET 变量名1 = 值; SELECT 值 INTO 变量名2 [FROM 子句]; #查看局部变量的值 SELECT 变量1,变量2,变量3; END \",\"1.定义变量\",\"DECLARE 变量名 类型 [default 值]; ## 如果没有DEFAULT子句，初始值为NULL \",\"举例：\",\"DECLARE myparam INT DEFAULT 100; \",\"2.变量赋值\",\"方式1：一般用于赋简单的值\",\"SET 变量名=值; SET 变量名:=值; \",\"方式2：一般用于赋表中的字段值\",\"SELECT 字段名或表达式 INTO 变量名 FROM 表; \",\"3.使用变量（查看、比较、运算等）\",\"SELECT 局部变量名; \",\"举例1：声明局部变量，并分别赋值为employees表中employee_id为102的last_name和salary\",\"DELIMITER // CREATE PROCEDURE set_value() BEGIN DECLARE emp_name VARCHAR(25); DECLARE sal DOUBLE(10,2); SELECT last_name,salary INTO emp_name,sal FROM employees WHERE employee_id = 102; SELECT emp_name,sal; END // DELIMITER ; \",\"举例2：声明两个变量，求和并打印 （分别使用会话用户变量、局部变量的方式实现）\",\"#方式1：使用用户变量 SET @m=1; SET @n=1; SET @sum=@m+@n; SELECT @sum; \",\"#方式2：使用局部变量 DELIMITER // CREATE PROCEDURE add_value() BEGIN #局部变量 DECLARE m INT DEFAULT 1; DECLARE n INT DEFAULT 3; DECLARE SUM INT; SET SUM = m+n; SELECT SUM; END // DELIMITER ; \",\"举例3：创建存储过程“different_salary”查询某员工和他领导的薪资差距，并用IN参数emp_id接收员工id，用OUT参数dif_salary输出薪资差距结果。\",\"#声明 DELIMITER // CREATE PROCEDURE different_salary(IN emp_id INT,OUT dif_salary DOUBLE) BEGIN #声明局部变量 DECLARE emp_sal,mgr_sal DOUBLE DEFAULT 0.0; DECLARE mgr_id INT; SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT manager_id INTO mgr_id FROM employees WHERE employee_id = emp_id; SELECT salary INTO mgr_sal FROM employees WHERE employee_id = mgr_id; SET dif_salary = mgr_sal - emp_sal; END // DELIMITER ; #调用 SET @emp_id = 102; CALL different_salary(@emp_id,@diff_sal); #查看 SELECT @diff_sal; \"]},{\"header\":\"1.2.4 对比会话用户变量与局部变量\",\"slug\":\"_1-2-4-对比会话用户变量与局部变量\",\"contents\":[\" 作用域 定义位置 语法 会话用户变量 当前会话 会话的任何地方 加@符号，不用指定类型 局部变量 定义它的BEGIN END中 BEGIN END的第一句话 一般不用加@,需要指定类型 \"]},{\"header\":\"2. 定义条件与处理程序\",\"slug\":\"_2-定义条件与处理程序\",\"contents\":[\"定义条件是事先定义程序执行过程中可能遇到的问题，处理程序定义了在遇到问题时应当采取的处理方式，并且保证存储过程或函数在遇到警告或错误时能继续执行。这样可以增强存储程序处理问题的能力，避免程序异常停止运行。\",\"说明：定义条件和处理程序在存储过程、存储函数中都是支持的。\"]},{\"header\":\"2.1 案例分析\",\"slug\":\"_2-1-案例分析\",\"contents\":[\"**案例分析：**创建一个名称为“UpdateDataNoCondition”的存储过程。代码如下：\",\"DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = 'Abel'; SET @x = 2; UPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel'; SET @x = 3; END // DELIMITER ; \",\"调用存储过程：\",\"mysql> CALL UpdateDataNoCondition(); ERROR 1048 (23000): Column 'email' cannot be null mysql> SELECT @x; +------+ | @x | +------+ | 1 | +------+ 1 row in set (0.00 sec) \",\"可以看到，此时@x变量的值为1。结合创建存储过程的SQL语句代码可以得出：在存储过程中未定义条件和处理程序，且当存储过程中执行的SQL语句报错时，MySQL数据库会抛出错误，并退出当前SQL逻辑，不再向下继续执行。\"]},{\"header\":\"2.2 定义条件\",\"slug\":\"_2-2-定义条件\",\"contents\":[\"定义条件就是给MySQL中的错误码命名，这有助于存储的程序代码更清晰。它将一个错误名字和指定的错误条件关联起来。这个名字可以随后被用在定义处理程序的DECLARE HANDLER语句中。\",\"定义条件使用DECLARE语句，语法格式如下：\",\"DECLARE 错误名称 CONDITION FOR 错误码（或错误条件） \",\"错误码的说明：\",\"MySQL_error_code和sqlstate_value都可以表示MySQL的错误。 \",\"MySQL_error_code是数值类型错误代码。\",\"sqlstate_value是长度为5的字符串类型错误代码。\",\"例如，在ERROR 1418 (HY000)中，1418是MySQL_error_code，'HY000'是sqlstate_value。\",\"例如，在ERROR 1142（42000）中，1142是MySQL_error_code，'42000'是sqlstate_value。\",\"**举例1：**定义“Field_Not_Be_NULL”错误名与MySQL中违反非空约束的错误类型是“ERROR 1048 (23000)”对应。\",\"#使用MySQL_error_code DECLARE Field_Not_Be_NULL CONDITION FOR 1048; #使用sqlstate_value DECLARE Field_Not_Be_NULL CONDITION FOR SQLSTATE '23000'; \",\"**举例2：**定义\\\"ERROR 1148(42000)\\\"错误，名称为command_not_allowed。\",\"#使用MySQL_error_code DECLARE command_not_allowed CONDITION FOR 1148; #使用sqlstate_value DECLARE command_not_allowed CONDITION FOR SQLSTATE '42000'; \"]},{\"header\":\"2.3 定义处理程序\",\"slug\":\"_2-3-定义处理程序\",\"contents\":[\"可以为SQL执行过程中发生的某种类型的错误定义特殊的处理程序。定义处理程序时，使用DECLARE语句的语法如下：\",\"DECLARE 处理方式 HANDLER FOR 错误类型 处理语句 \",\"处理方式：处理方式有3个取值：CONTINUE、EXIT、UNDO。 \",\"CONTINUE：表示遇到错误不处理，继续执行。\",\"EXIT：表示遇到错误马上退出。\",\"UNDO：表示遇到错误后撤回之前的操作。MySQL中暂时不支持这样的操作。\",\"错误类型（即条件）可以有如下取值： \",\"SQLSTATE '字符串错误码'：表示长度为5的sqlstate_value类型的错误代码；\",\"MySQL_error_code：匹配数值类型错误代码；\",\"错误名称：表示DECLARE ... CONDITION定义的错误条件名称。\",\"SQLWARNING：匹配所有以01开头的SQLSTATE错误代码；\",\"NOT FOUND：匹配所有以02开头的SQLSTATE错误代码；\",\"SQLEXCEPTION：匹配所有没有被SQLWARNING或NOT FOUND捕获的SQLSTATE错误代码；\",\"处理语句：如果出现上述条件之一，则采用对应的处理方式，并执行指定的处理语句。语句可以是像“SET 变量 = 值”这样的简单语句，也可以是使用BEGIN ... END编写的复合语句。\",\"定义处理程序的几种方式，代码如下：\",\"#方法1：捕获sqlstate_value DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02' SET @info = 'NO_SUCH_TABLE'; #方法2：捕获mysql_error_value DECLARE CONTINUE HANDLER FOR 1146 SET @info = 'NO_SUCH_TABLE'; #方法3：先定义条件，再调用 DECLARE no_such_table CONDITION FOR 1146; DECLARE CONTINUE HANDLER FOR NO_SUCH_TABLE SET @info = 'NO_SUCH_TABLE'; #方法4：使用SQLWARNING DECLARE EXIT HANDLER FOR SQLWARNING SET @info = 'ERROR'; #方法5：使用NOT FOUND DECLARE EXIT HANDLER FOR NOT FOUND SET @info = 'NO_SUCH_TABLE'; #方法6：使用SQLEXCEPTION DECLARE EXIT HANDLER FOR SQLEXCEPTION SET @info = 'ERROR'; \"]},{\"header\":\"2.4 案例解决\",\"slug\":\"_2-4-案例解决\",\"contents\":[\"在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到MySQL_error_code值为1048时，执行CONTINUE操作，并且将@proc_value的值设置为-1。\",\"DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN #定义处理程序 DECLARE CONTINUE HANDLER FOR 1048 SET @proc_value = -1; SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = 'Abel'; SET @x = 2; UPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel'; SET @x = 3; END // DELIMITER ; \",\"调用过程：\",\"mysql> CALL UpdateDataWithCondition(); Query OK, 0 rows affected (0.01 sec) mysql> SELECT @x,@proc_value; +------+-------------+ | @x | @proc_value | +------+-------------+ | 3 | -1 | +------+-------------+ 1 row in set (0.00 sec) \",\"举例：\",\"创建一个名称为“InsertDataWithCondition”的存储过程，代码如下。\",\"在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到sqlstate_value值为23000时，执行EXIT操作，并且将@proc_value的值设置为-1。\",\"#准备工作 CREATE TABLE departments AS SELECT * FROM atguigudb.`departments`; ALTER TABLE departments ADD CONSTRAINT uk_dept_name UNIQUE(department_id); \",\"DELIMITER // CREATE PROCEDURE InsertDataWithCondition() BEGIN DECLARE duplicate_entry CONDITION FOR SQLSTATE '23000' ; DECLARE EXIT HANDLER FOR duplicate_entry SET @proc_value = -1; SET @x = 1; INSERT INTO departments(department_name) VALUES('测试'); SET @x = 2; INSERT INTO departments(department_name) VALUES('测试'); SET @x = 3; END // DELIMITER ; \",\"调用存储过程：\",\"mysql> CALL InsertDataWithCondition(); Query OK, 0 rows affected (0.01 sec) mysql> SELECT @x,@proc_value; +------+-------------+ | @x | @proc_value | +------+-------------+ | 2 | -1 | +------+-------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"3. 流程控制\",\"slug\":\"_3-流程控制\",\"contents\":[\"解决复杂问题不可能通过一个 SQL 语句完成，我们需要执行多个 SQL 操作。流程控制语句的作用就是控制存储过程中 SQL 语句的执行顺序，是我们完成复杂操作必不可少的一部分。只要是执行的程序，流程就分为三大类：\",\"顺序结构：程序从上往下依次执行\",\"分支结构：程序按条件进行选择执行，从两条或多条路径中选择一条执行\",\"循环结构：程序满足一定条件下，重复执行一组语句\",\"针对于MySQL 的流程控制语句主要有 3 类。注意：只能用于存储程序。\",\"条件判断语句：IF 语句和 CASE 语句\",\"循环语句：LOOP、WHILE 和 REPEAT 语句\",\"跳转语句：ITERATE 和 LEAVE 语句\"]},{\"header\":\"3.1 分支结构之 IF\",\"slug\":\"_3-1-分支结构之-if\",\"contents\":[\"IF 语句的语法结构是：\",\"IF 表达式1 THEN 操作1 [ELSEIF 表达式2 THEN 操作2]…… [ELSE 操作N] END IF \",\"根据表达式的结果为TRUE或FALSE执行相应的语句。这里“[]”中的内容是可选的。\",\"特点：① 不同的表达式对应不同的操作 ② 使用在begin end中\",\"举例1：\",\"IF val IS NULL THEN SELECT 'val is null'; ELSE SELECT 'val is not null'; END IF; \",\"**举例2：**声明存储过程“update_salary_by_eid1”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于8000元并且入职时间超过5年，就涨薪500元；否则就不变。\",\"DELIMITER // CREATE PROCEDURE update_salary_by_eid1(IN emp_id INT) BEGIN DECLARE emp_salary DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year FROM employees WHERE employee_id = emp_id; IF emp_salary < 8000 AND hire_year > 5 THEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id; END IF; END // DELIMITER ; \",\"**举例3：**声明存储过程“update_salary_by_eid2”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元并且入职时间超过5年，就涨薪500元；否则就涨薪100元。\",\"DELIMITER // CREATE PROCEDURE update_salary_by_eid2(IN emp_id INT) BEGIN DECLARE emp_salary DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year FROM employees WHERE employee_id = emp_id; IF emp_salary < 8000 AND hire_year > 5 THEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary = salary + 100 WHERE employee_id = emp_id; END IF; END // DELIMITER ; \",\"**举例4：**声明存储过程“update_salary_by_eid3”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元，就更新薪资为9000元；薪资如果大于等于9000元且低于10000的，但是奖金比例为NULL的，就更新奖金比例为0.01；其他的涨薪100元。\",\"DELIMITER // CREATE PROCEDURE update_salary_by_eid3(IN emp_id INT) BEGIN DECLARE emp_salary DOUBLE; DECLARE bonus DECIMAL(3,2); SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id; IF emp_salary < 9000 THEN UPDATE employees SET salary = 9000 WHERE employee_id = emp_id; ELSEIF emp_salary < 10000 AND bonus IS NULL THEN UPDATE employees SET commission_pct = 0.01 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary = salary + 100 WHERE employee_id = emp_id; END IF; END // DELIMITER ; \"]},{\"header\":\"3.2 分支结构之 CASE\",\"slug\":\"_3-2-分支结构之-case\",\"contents\":[\"CASE 语句的语法结构1：\",\"#情况一：类似于switch CASE 表达式 WHEN 值1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 值2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） \",\"CASE 语句的语法结构2：\",\"#情况二：类似于多重if CASE WHEN 条件1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 条件2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） \",\"举例1：\",\"使用CASE流程控制语句的第1种格式，判断val值等于1、等于2，或者两者都不等。\",\"CASE val WHEN 1 THEN SELECT 'val is 1'; WHEN 2 THEN SELECT 'val is 2'; ELSE SELECT 'val is not 1 or 2'; END CASE; \",\"举例2：\",\"使用CASE流程控制语句的第2种格式，判断val是否为空、小于0、大于0或者等于0。\",\"CASE WHEN val IS NULL THEN SELECT 'val is null'; WHEN val < 0 THEN SELECT 'val is less than 0'; WHEN val > 0 THEN SELECT 'val is greater than 0'; ELSE SELECT 'val is 0'; END CASE; \",\"**举例3：**声明存储过程“update_salary_by_eid4”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元，就更新薪资为9000元；薪资大于等于9000元且低于10000的，但是奖金比例为NULL的，就更新奖金比例为0.01；其他的涨薪100元。\",\"DELIMITER // CREATE PROCEDURE update_salary_by_eid4(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE bonus DECIMAL(3,2); SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id; CASE WHEN emp_sal<9000 THEN UPDATE employees SET salary=9000 WHERE employee_id = emp_id; WHEN emp_sal<10000 AND bonus IS NULL THEN UPDATE employees SET commission_pct=0.01 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; \",\"举例4：声明存储过程update_salary_by_eid5，定义IN参数emp_id，输入员工编号。判断该员工的入职年限，如果是0年，薪资涨50；如果是1年，薪资涨100；如果是2年，薪资涨200；如果是3年，薪资涨300；如果是4年，薪资涨400；其他的涨薪500。\",\"DELIMITER // CREATE PROCEDURE update_salary_by_eid5(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT ROUND(DATEDIFF(CURDATE(),hire_date)/365) INTO hire_year FROM employees WHERE employee_id = emp_id; CASE hire_year WHEN 0 THEN UPDATE employees SET salary=salary+50 WHERE employee_id = emp_id; WHEN 1 THEN UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; WHEN 2 THEN UPDATE employees SET salary=salary+200 WHERE employee_id = emp_id; WHEN 3 THEN UPDATE employees SET salary=salary+300 WHERE employee_id = emp_id; WHEN 4 THEN UPDATE employees SET salary=salary+400 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+500 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; \"]},{\"header\":\"3.3 循环结构之LOOP\",\"slug\":\"_3-3-循环结构之loop\",\"contents\":[\"LOOP循环语句用来重复执行某些语句。LOOP内的语句一直重复执行直到循环被退出（使用LEAVE子句），跳出循环过程。\",\"LOOP语句的基本格式如下：\",\"[loop_label:] LOOP 循环执行的语句 END LOOP [loop_label] \",\"其中，loop_label表示LOOP语句的标注名称，该参数可以省略。\",\"举例1：\",\"使用LOOP语句进行循环操作，id值小于10时将重复执行循环过程。\",\"DECLARE id INT DEFAULT 0; add_loop:LOOP SET id = id +1; IF id >= 10 THEN LEAVE add_loop; END IF; END LOOP add_loop; \",\"**举例2：**当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程“update_salary_loop()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.1倍。直到全公司的平均薪资达到12000结束。并统计循环次数。\",\"DELIMITER // CREATE PROCEDURE update_salary_loop(OUT num INT) BEGIN DECLARE avg_salary DOUBLE; DECLARE loop_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_salary FROM employees; label_loop:LOOP IF avg_salary >= 12000 THEN LEAVE label_loop; END IF; UPDATE employees SET salary = salary * 1.1; SET loop_count = loop_count + 1; SELECT AVG(salary) INTO avg_salary FROM employees; END LOOP label_loop; SET num = loop_count; END // DELIMITER ; \"]},{\"header\":\"3.4 循环结构之WHILE\",\"slug\":\"_3-4-循环结构之while\",\"contents\":[\"WHILE语句创建一个带条件判断的循环过程。WHILE在执行语句执行时，先对指定的表达式进行判断，如果为真，就执行循环内的语句，否则退出循环。WHILE语句的基本格式如下：\",\"[while_label:] WHILE 循环条件 DO 循环体 END WHILE [while_label]; \",\"while_label为WHILE语句的标注名称；如果循环条件结果为真，WHILE语句内的语句或语句群被执行，直至循环条件为假，退出循环。\",\"举例1：\",\"WHILE语句示例，i值小于10时，将重复执行循环过程，代码如下：\",\"DELIMITER // CREATE PROCEDURE test_while() BEGIN DECLARE i INT DEFAULT 0; WHILE i < 10 DO SET i = i + 1; END WHILE; SELECT i; END // DELIMITER ; #调用 CALL test_while(); \",\"**举例2：**市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“update_salary_while()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家降薪，薪资降为原来的90%。直到全公司的平均薪资达到5000结束。并统计循环次数。\",\"DELIMITER // CREATE PROCEDURE update_salary_while(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE while_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; WHILE avg_sal > 5000 DO UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; SET num = while_count; END // DELIMITER ; \"]},{\"header\":\"3.5 循环结构之REPEAT\",\"slug\":\"_3-5-循环结构之repeat\",\"contents\":[\"REPEAT语句创建一个带条件判断的循环过程。与WHILE循环不同的是，REPEAT 循环首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。\",\"REPEAT语句的基本格式如下：\",\"[repeat_label:] REPEAT 循环体的语句 UNTIL 结束循环的条件表达式 END REPEAT [repeat_label] \",\"repeat_label为REPEAT语句的标注名称，该参数可以省略；REPEAT语句内的语句或语句群被重复，直至expr_condition为真。\",\"举例1：\",\"DELIMITER // CREATE PROCEDURE test_repeat() BEGIN DECLARE i INT DEFAULT 0; REPEAT SET i = i + 1; UNTIL i >= 10 END REPEAT; SELECT i; END // DELIMITER ; \",\"**举例2：**当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程“update_salary_repeat()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.15倍。直到全公司的平均薪资达到13000结束。并统计循环次数。\",\"DELIMITER // CREATE PROCEDURE update_salary_repeat(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE repeat_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; REPEAT UPDATE employees SET salary = salary * 1.15; SET repeat_count = repeat_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; UNTIL avg_sal >= 13000 END REPEAT; SET num = repeat_count; END // DELIMITER ; \",\"对比三种循环结构：\",\"1、这三种循环都可以省略名称，但如果循环中添加了循环控制语句（LEAVE或ITERATE）则必须添加名称。 2、 LOOP：一般用于实现简单的\\\"死\\\"循环 WHILE：先判断后执行 REPEAT：先执行后判断，无条件至少执行一次\"]},{\"header\":\"3.6 跳转语句之LEAVE语句\",\"slug\":\"_3-6-跳转语句之leave语句\",\"contents\":[\"LEAVE语句：可以用在循环语句内，或者以 BEGIN 和 END 包裹起来的程序体内，表示跳出循环或者跳出程序体的操作。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 break。\",\"基本格式如下：\",\"LEAVE 标记名 \",\"其中，label参数表示循环的标志。LEAVE和BEGIN ... END或循环一起被使用。\",\"**举例1：**创建存储过程 “leave_begin()”，声明INT类型的IN参数num。给BEGIN...END加标记名，并在BEGIN...END中使用IF语句判断num参数的值。\",\"如果num<=0，则使用LEAVE语句退出BEGIN...END；\",\"如果num=1，则查询“employees”表的平均薪资；\",\"如果num=2，则查询“employees”表的最低薪资；\",\"如果num>2，则查询“employees”表的最高薪资。\",\"IF语句结束后查询“employees”表的总人数。\",\"DELIMITER // CREATE PROCEDURE leave_begin(IN num INT) begin_label: BEGIN IF num<=0 THEN LEAVE begin_label; ELSEIF num=1 THEN SELECT AVG(salary) FROM employees; ELSEIF num=2 THEN SELECT MIN(salary) FROM employees; ELSE SELECT MAX(salary) FROM employees; END IF; SELECT COUNT(*) FROM employees; END // DELIMITER ; \",\"举例2：\",\"当市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“leave_while()”，声明OUT参数num，输出循环次数，存储过程中使用WHILE循环给大家降低薪资为原来薪资的90%，直到全公司的平均薪资小于等于10000，并统计循环次数。\",\"DELIMITER // CREATE PROCEDURE leave_while(OUT num INT) BEGIN # DECLARE avg_sal DOUBLE;#记录平均工资 DECLARE while_count INT DEFAULT 0; #记录循环次数 SELECT AVG(salary) INTO avg_sal FROM employees; #① 初始化条件 while_label:WHILE TRUE DO #② 循环条件 #③ 循环体 IF avg_sal <= 10000 THEN LEAVE while_label; END IF; UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; #④ 迭代条件 SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; #赋值 SET num = while_count; END // DELIMITER ; \"]},{\"header\":\"3.7 跳转语句之ITERATE语句\",\"slug\":\"_3-7-跳转语句之iterate语句\",\"contents\":[\"ITERATE语句：只能用在循环语句（LOOP、REPEAT和WHILE语句）内，表示重新开始循环，将执行顺序转到语句段开头处。如果你有面向过程的编程语言的使用经验，你可以把 ITERATE 理解为 continue，意思为“再次循环”。\",\"语句基本格式如下：\",\"ITERATE label \",\"label参数表示循环的标志。ITERATE语句必须跟在循环标志前面。\",\"举例： 定义局部变量num，初始值为0。循环结构中执行num + 1操作。\",\"如果num < 10，则继续执行循环；\",\"如果num > 15，则退出循环结构；\",\"DELIMITER // CREATE PROCEDURE test_iterate() BEGIN DECLARE num INT DEFAULT 0; my_loop:LOOP SET num = num + 1; IF num < 10 THEN ITERATE my_loop; ELSEIF num > 15 THEN LEAVE my_loop; END IF; SELECT '尚硅谷：让天下没有难学的技术'; END LOOP my_loop; END // DELIMITER ; \"]},{\"header\":\"4. 游标\",\"slug\":\"_4-游标\",\"contents\":[]},{\"header\":\"4.1 什么是游标（或光标）\",\"slug\":\"_4-1-什么是游标-或光标\",\"contents\":[\"虽然我们也可以通过筛选条件 WHERE 和 HAVING，或者是限定返回记录的关键字 LIMIT 返回一条记录，但是，却无法在结果集中像指针一样，向前定位一条记录、向后定位一条记录，或者是随意定位到某一条记录，并对记录的数据进行处理。\",\"这个时候，就可以用到游标。游标，提供了一种灵活的操作方式，让我们能够对结果集中的每一条记录进行定位，并对指向的记录中的数据进行操作的数据结构。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。\",\"在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用，我们可以通过操作游标来对数据行进行操作。\",\"MySQL中游标可以在存储过程和函数中使用。\",\"比如，我们查询了 employees 数据表中工资高于15000的员工都有哪些：\",\"SELECT employee_id,last_name,salary FROM employees WHERE salary > 15000; \",\"这里我们就可以通过游标来操作数据行，如图所示此时游标所在的行是“108”的记录，我们也可以在结果集上滚动游标，指向结果集中的任意一行。\"]},{\"header\":\"4.2 使用游标步骤\",\"slug\":\"_4-2-使用游标步骤\",\"contents\":[\"游标必须在声明处理程序之前被声明，并且变量和条件还必须在声明游标或处理程序之前被声明。\",\"如果我们想要使用游标，一般需要经历四个步骤。不同的 DBMS 中，使用游标的语法可能略有不同。\",\"第一步，声明游标\",\"在MySQL中，使用DECLARE关键字来声明游标，其语法的基本形式如下：\",\"DECLARE cursor_name CURSOR FOR select_statement; \",\"这个语法适用于 MySQL，SQL Server，DB2 和 MariaDB。如果是用 Oracle 或者 PostgreSQL，需要写成：\",\"DECLARE cursor_name CURSOR IS select_statement; \",\"要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句，返回一个用于创建游标的结果集。\",\"比如：\",\"DECLARE cur_emp CURSOR FOR SELECT employee_id,salary FROM employees; \",\"DECLARE cursor_fruit CURSOR FOR SELECT f_name, f_price FROM fruits ; \",\"第二步，打开游标\",\"打开游标的语法如下：\",\"OPEN cursor_name \",\"当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区，为后面游标的逐条读取结果集中的记录做准备。\",\"OPEN cur_emp ; \",\"第三步，使用游标（从游标中取得数据）\",\"语法如下：\",\"FETCH cursor_name INTO var_name [, var_name] ... \",\"这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。\",\"注意：var_name必须在声明游标之前就定义好。\",\"FETCH cur_emp INTO emp_id, emp_sal ; \",\"注意：游标的查询结果集中的字段数，必须跟 INTO 后面的变量数一致，否则，在存储过程执行的时候，MySQL 会提示错误。\",\"第四步，关闭游标\",\"CLOSE cursor_name \",\"有 OPEN 就会有 CLOSE，也就是打开和关闭游标。当我们使用完游标后需要关闭掉该游标。因为游标会占用系统资源，如果不及时关闭，游标会一直保持到存储过程结束，影响系统运行的效率。而关闭游标的操作，会释放游标占用的系统资源。\",\"关闭游标之后，我们就不能再检索查询结果中的数据行，如果需要检索只能再次打开游标。\",\"CLOSE cur_emp; \"]},{\"header\":\"4.3 举例\",\"slug\":\"_4-3-举例\",\"contents\":[\"创建存储过程“get_count_by_limit_total_salary()”，声明IN参数 limit_total_salary，DOUBLE类型；声明OUT参数total_count，INT类型。函数的功能可以实现累加薪资最高的几个员工的薪资值，直到薪资总和达到limit_total_salary参数的值，返回累加的人数给total_count。\",\"DELIMITER // CREATE PROCEDURE get_count_by_limit_total_salary(IN limit_total_salary DOUBLE,OUT total_count INT) BEGIN DECLARE sum_salary DOUBLE DEFAULT 0; #记录累加的总工资 DECLARE cursor_salary DOUBLE DEFAULT 0; #记录某一个工资值 DECLARE emp_count INT DEFAULT 0; #记录循环个数 #定义游标 DECLARE emp_cursor CURSOR FOR SELECT salary FROM employees ORDER BY salary DESC; #打开游标 OPEN emp_cursor; REPEAT #使用游标（从游标中获取数据） FETCH emp_cursor INTO cursor_salary; SET sum_salary = sum_salary + cursor_salary; SET emp_count = emp_count + 1; UNTIL sum_salary >= limit_total_salary END REPEAT; SET total_count = emp_count; #关闭游标 CLOSE emp_cursor; END // DELIMITER ; \"]},{\"header\":\"4.5 小结\",\"slug\":\"_4-5-小结\",\"contents\":[\"游标是 MySQL 的一个重要的功能，为逐条读取结果集中的数据，提供了完美的解决方案。跟在应用层面实现相同的功能相比，游标可以在存储程序中使用，效率高，程序也更加简洁。\",\"但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行加锁，这样在业务并发量大的时候，不仅会影响业务之间的效率，还会消耗系统资源，造成内存不足，这是因为游标是在内存中进行的处理。\",\"建议：养成用完之后就关闭的习惯，这样才能提高系统的整体效率。\"]},{\"header\":\"补充：MySQL 8.0的新特性—全局变量的持久化\",\"slug\":\"补充-mysql-8-0的新特性—全局变量的持久化\",\"contents\":[\"在MySQL数据库中，全局变量可以通过SET GLOBAL语句来设置。例如，设置服务器语句超时的限制，可以通过设置系统变量max_execution_time来实现：\",\"SET GLOBAL MAX_EXECUTION_TIME=2000; \",\"使用SET GLOBAL语句设置的变量值只会临时生效。数据库重启后，服务器又会从MySQL配置文件中读取变量的默认值。 MySQL 8.0版本新增了SET PERSIST命令。例如，设置服务器的最大连接数为1000：\",\"SET PERSIST global max_connections = 1000; \",\"MySQL会将该命令的配置保存到数据目录下的mysqld-auto.cnf文件中，下次启动时会读取该文件，用其中的配置来覆盖默认的配置文件。\",\"举例：\",\"查看全局变量max_connections的值，结果如下：\",\"mysql> show variables like '%max_connections%'; +------------------------+-------+ | Variable_name | Value | +------------------------+-------+ | max_connections | 151 | | mysqlx_max_connections | 100 | +------------------------+-------+ 2 rows in set, 1 warning (0.00 sec) \",\"设置全局变量max_connections的值：\",\"mysql> set persist max_connections=1000; Query OK, 0 rows affected (0.00 sec) \",\"重启MySQL服务器，再次查询max_connections的值：\",\"mysql> show variables like '%max_connections%'; +------------------------+-------+ | Variable_name | Value | +------------------------+-------+ | max_connections | 1000 | | mysqlx_max_connections | 100 | +------------------------+-------+ 2 rows in set, 1 warning (0.00 sec) \"]}]},\"/study-tutorial/database/mysql/sorting-and-paging.html\":{\"title\":\"5、排序与分页\",\"contents\":[{\"header\":\"1. 排序数据\",\"slug\":\"_1-排序数据\",\"contents\":[]},{\"header\":\"1.1 排序规则\",\"slug\":\"_1-1-排序规则\",\"contents\":[\"使用 ORDER BY 子句排序 \",\"ASC（ascend）: 升序\",\"DESC（descend）:降序\",\"ORDER BY 子句在SELECT语句的结尾。\"]},{\"header\":\"1.2 单列排序\",\"slug\":\"_1-2-单列排序\",\"contents\":[\"SELECT last_name, job_id, department_id, hire_date FROM employees ORDER BY hire_date ; \",\"SELECT last_name, job_id, department_id, hire_date FROM employees ORDER BY hire_date DESC ; \",\"SELECT employee_id, last_name, salary*12 annsal FROM employees ORDER BY annsal; \"]},{\"header\":\"1.3 多列排序\",\"slug\":\"_1-3-多列排序\",\"contents\":[\"SELECT last_name, department_id, salary FROM employees ORDER BY department_id, salary DESC; \",\"可以使用不在SELECT列表中的列排序。\",\"在对多列进行排序的时候，首先排序的第一列必须有相同的列值，才会对第二列进行排序。如果第一列数据中所有值都是唯一的，将不再对第二列进行排序。\"]},{\"header\":\"2. 分页\",\"slug\":\"_2-分页\",\"contents\":[]},{\"header\":\"2.1 背景\",\"slug\":\"_2-1-背景\",\"contents\":[\"背景1：查询返回的记录太多了，查看起来很不方便，怎么样能够实现分页查询呢？\",\"背景2：表里有 4 条数据，我们只想要显示第 2、3 条数据怎么办呢？\"]},{\"header\":\"2.2 实现规则\",\"slug\":\"_2-2-实现规则\",\"contents\":[\"分页原理\",\"所谓分页显示，就是将数据库中的结果集，一段一段显示出来需要的条件。\",\"MySQL中使用 LIMIT 实现分页\",\"格式：\",\"LIMIT [位置偏移量,] 行数 \",\"第一个“位置偏移量”参数指示MySQL从哪一行开始显示，是一个可选参数，如果不指定“位置偏移量”，将会从表中的第一条记录开始（第一条记录的位置偏移量是0，第二条记录的位置偏移量是1，以此类推）；第二个参数“行数”指示返回的记录条数。\",\"举例\",\"--前10条记录： SELECT * FROM 表名 LIMIT 0,10; 或者 SELECT * FROM 表名 LIMIT 10; --第11至20条记录： SELECT * FROM 表名 LIMIT 10,10; --第21至30条记录： SELECT * FROM 表名 LIMIT 20,10; \",\"MySQL 8.0中可以使用“LIMIT 3 OFFSET 4”，意思是获取从第5条记录开始后面的3条记录，和“LIMIT 4,3;”返回的结果相同。\",\"分页显式公式**：（当前页数-1）每页条数，每页条数*\",\"SELECT * FROM table LIMIT(PageNo - 1)*PageSize,PageSize; \",\"注意：LIMIT 子句必须放在整个SELECT语句的最后！\",\"使用 LIMIT 的好处\",\"约束返回结果的数量可以减少数据表的网络传输量，也可以提升查询效率。如果我们知道返回结果只有 1 条，就可以使用LIMIT 1，告诉 SELECT 语句只需要返回一条记录即可。这样的好处就是 SELECT 不需要扫描完整的表，只需要检索到一条符合条件的记录即可返回。\"]},{\"header\":\"2.3 拓展\",\"slug\":\"_2-3-拓展\",\"contents\":[\"在不同的 DBMS 中使用的关键字可能不同。在 MySQL、PostgreSQL、MariaDB 和 SQLite 中使用 LIMIT 关键字，而且需要放到 SELECT 语句的最后面。\",\"如果是 SQL Server 和 Access，需要使用 TOP 关键字，比如：\",\"SELECT TOP 5 name, hp_max FROM heros ORDER BY hp_max DESC \",\"如果是 DB2，使用FETCH FIRST 5 ROWS ONLY这样的关键字：\",\"SELECT name, hp_max FROM heros ORDER BY hp_max DESC FETCH FIRST 5 ROWS ONLY \",\"如果是 Oracle，你需要基于 ROWNUM 来统计行数：\",\"SELECT rownum,last_name,salary FROM employees WHERE rownum < 5 ORDER BY salary DESC; \",\"需要说明的是，这条语句是先取出来前 5 条数据行，然后再按照 hp_max 从高到低的顺序进行排序。但这样产生的结果和上述方法的并不一样。我会在后面讲到子查询，你可以使用\",\"SELECT rownum, last_name,salary FROM ( SELECT last_name,salary FROM employees ORDER BY salary DESC) WHERE rownum < 10; \",\"得到与上述方法一致的结果。\"]}]},\"/study-tutorial/database/mysql/stored-procedure.html\":{\"title\":\"15、存储过程与函数\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"MySQL从5.0版本开始支持存储过程和函数。存储过程和函数能够将复杂的SQL逻辑封装在一起，应用程序无须关注存储过程和函数内部复杂的SQL逻辑，而只需要简单地调用存储过程和函数即可。\"]},{\"header\":\"1. 存储过程概述\",\"slug\":\"_1-存储过程概述\",\"contents\":[]},{\"header\":\"1.1 理解\",\"slug\":\"_1-1-理解\",\"contents\":[\"含义：存储过程的英文是 Stored Procedure。它的思想很简单，就是一组经过预先编译的 SQL 语句的封装。\",\"执行过程：存储过程预先存储在 MySQL 服务器上，需要执行的时候，客户端只需要向服务器端发出调用存储过程的命令，服务器端就可以把预先存储好的这一系列 SQL 语句全部执行。\",\"好处：\",\"1、简化操作，提高了sql语句的重用性，减少了开发程序员的压力 2、减少操作过程中的失误，提高效率 3、减少网络传输量（客户端不需要把所有的 SQL 语句通过网络发给服务器） 4、减少了 SQL 语句暴露在网上的风险，也提高了数据查询的安全性\",\"和视图、函数的对比：\",\"它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。不过它和视图不同，视图是虚拟表，通常不对底层数据表直接操作，而存储过程是程序化的 SQL，可以直接操作底层数据表，相比于面向集合的操作方式，能够实现一些更复杂的数据处理。\",\"一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。相较于函数，存储过程是没有返回值的。\"]},{\"header\":\"1.2 分类\",\"slug\":\"_1-2-分类\",\"contents\":[\"存储过程的参数类型可以是IN、OUT和INOUT。根据这点分类如下：\",\"1、没有参数（无参数无返回） 2、仅仅带 IN 类型（有参数无返回） 3、仅仅带 OUT 类型（无参数有返回） 4、既带 IN 又带 OUT（有参数有返回） 5、带 INOUT（有参数有返回）\",\"注意：IN、OUT、INOUT 都可以在一个存储过程中带多个。\"]},{\"header\":\"2. 创建存储过程\",\"slug\":\"_2-创建存储过程\",\"contents\":[]},{\"header\":\"2.1 语法分析\",\"slug\":\"_2-1-语法分析\",\"contents\":[\"语法：\",\"CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN 存储过程体 END \",\"类似于Java中的方法：\",\"修饰符 返回类型 方法名(参数类型 参数名,...){ 方法体; } \",\"说明：\",\"1、参数前面的符号的意思\",\"IN：当前参数为输入参数，也就是表示入参；\",\"存储过程只是读取这个参数的值。如果没有定义参数种类，默认就是 IN，表示输入参数。\",\"OUT：当前参数为输出参数，也就是表示出参；\",\"执行完成之后，调用这个存储过程的客户端或者应用程序就可以读取这个参数返回的值了。\",\"INOUT：当前参数既可以为输入参数，也可以为输出参数。\",\"2、形参类型可以是 MySQL数据库中的任意类型。\",\"3、characteristics 表示创建存储过程时指定的对存储过程的约束条件，其取值信息如下：\",\"LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' \",\"LANGUAGE SQL：说明存储过程执行体是由SQL语句组成的，当前系统支持的语言为SQL。\",\"[NOT] DETERMINISTIC：指明存储过程执行的结果是否确定。DETERMINISTIC表示结果是确定的。每次执行存储过程时，相同的输入会得到相同的输出。NOT DETERMINISTIC表示结果是不确定的，相同的输入可能得到不同的输出。如果没有指定任意一个值，默认为NOT DETERMINISTIC。\",\"{ CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }：指明子程序使用SQL语句的限制。 \",\"CONTAINS SQL表示当前存储过程的子程序包含SQL语句，但是并不包含读写数据的SQL语句；\",\"NO SQL表示当前存储过程的子程序中不包含任何SQL语句；\",\"READS SQL DATA表示当前存储过程的子程序中包含读数据的SQL语句；\",\"MODIFIES SQL DATA表示当前存储过程的子程序中包含写数据的SQL语句。\",\"默认情况下，系统会指定为CONTAINS SQL。\",\"SQL SECURITY { DEFINER | INVOKER }：执行当前存储过程的权限，即指明哪些用户能够执行当前存储过程。 \",\"DEFINER表示只有当前存储过程的创建者或者定义者才能执行当前存储过程；\",\"INVOKER表示拥有当前存储过程的访问权限的用户能够执行当前存储过程。\",\"如果没有设置相关的值，则MySQL默认指定值为DEFINER。\",\"COMMENT 'string'：注释信息，可以用来描述存储过程。\",\"4、存储过程体中可以有多条 SQL 语句，如果仅仅一条SQL 语句，则可以省略 BEGIN 和 END\",\"编写存储过程并不是一件简单的事情，可能存储过程中需要复杂的 SQL 语句。\",\"1. BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。 2. DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进行变量的声明。 3. SET：赋值语句，用于对变量进行赋值。 4. SELECT… INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。 \",\"5、需要设置新的结束标记\",\"DELIMITER 新的结束标记 \",\"因为MySQL默认的语句结束符号为分号‘;’。为了避免与存储过程中SQL语句结束符相冲突，需要使用DELIMITER改变存储过程的结束符。\",\"比如：“DELIMITER //”语句的作用是将MySQL的结束符设置为//，并以“END //”结束存储过程。存储过程定义完毕之后再使用“DELIMITER ;”恢复默认结束符。DELIMITER也可以指定其他符号作为结束符。\",\"当使用DELIMITER命令时，应该避免使用反斜杠（‘\\\\’）字符，因为反斜线是MySQL的转义字符。\",\"示例：\",\"DELIMITER $ CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN sql语句1; sql语句2; END $ \"]},{\"header\":\"2.2 代码举例\",\"slug\":\"_2-2-代码举例\",\"contents\":[\"举例1：创建存储过程select_all_data()，查看 emps 表的所有数据\",\"DELIMITER $ CREATE PROCEDURE select_all_data() BEGIN SELECT * FROM emps; END $ DELIMITER ; \",\"举例2：创建存储过程avg_employee_salary()，返回所有员工的平均工资\",\"DELIMITER // CREATE PROCEDURE avg_employee_salary () BEGIN SELECT AVG(salary) AS avg_salary FROM emps; END // DELIMITER ; \",\"举例3：创建存储过程show_max_salary()，用来查看“emps”表的最高薪资值。\",\"CREATE PROCEDURE show_max_salary() LANGUAGE SQL NOT DETERMINISTIC CONTAINS SQL SQL SECURITY DEFINER COMMENT '查看最高薪资' BEGIN SELECT MAX(salary) FROM emps; END // DELIMITER ; \",\"举例4：创建存储过程show_min_salary()，查看“emps”表的最低薪资值。并将最低薪资通过OUT参数“ms”输出\",\"DELIMITER // CREATE PROCEDURE show_min_salary(OUT ms DOUBLE) BEGIN SELECT MIN(salary) INTO ms FROM emps; END // DELIMITER ; \",\"举例5：创建存储过程show_someone_salary()，查看“emps”表的某个员工的薪资，并用IN参数empname输入员工姓名。\",\"DELIMITER // CREATE PROCEDURE show_someone_salary(IN empname VARCHAR(20)) BEGIN SELECT salary FROM emps WHERE ename = empname; END // DELIMITER ; \",\"举例6：创建存储过程show_someone_salary2()，查看“emps”表的某个员工的薪资，并用IN参数empname输入员工姓名，用OUT参数empsalary输出员工薪资。\",\"DELIMITER // CREATE PROCEDURE show_someone_salary2(IN empname VARCHAR(20),OUT empsalary DOUBLE) BEGIN SELECT salary INTO empsalary FROM emps WHERE ename = empname; END // DELIMITER ; \",\"举例7：创建存储过程show_mgr_name()，查询某个员工领导的姓名，并用INOUT参数“empname”输入员工姓名，输出领导的姓名。\",\"DELIMITER // CREATE PROCEDURE show_mgr_name(INOUT empname VARCHAR(20)) BEGIN SELECT ename INTO empname FROM emps WHERE eid = (SELECT MID FROM emps WHERE ename=empname); END // DELIMITER ; \"]},{\"header\":\"3. 调用存储过程\",\"slug\":\"_3-调用存储过程\",\"contents\":[]},{\"header\":\"3.1 调用格式\",\"slug\":\"_3-1-调用格式\",\"contents\":[\"存储过程有多种调用方法。存储过程必须使用CALL语句调用，并且存储过程和数据库相关，如果要执行其他数据库中的存储过程，需要指定数据库名称，例如CALL dbname.procname。\",\"CALL 存储过程名(实参列表) \",\"格式：\",\"1、调用in模式的参数：\",\"CALL sp1('值'); \",\"2、调用out模式的参数：\",\"SET @name; CALL sp1(@name); SELECT @name; \",\"3、调用inout模式的参数：\",\"SET @name=值; CALL sp1(@name); SELECT @name; \"]},{\"header\":\"3.2 代码举例\",\"slug\":\"_3-2-代码举例\",\"contents\":[\"举例1：\",\"DELIMITER // CREATE PROCEDURE CountProc(IN sid INT,OUT num INT) BEGIN SELECT COUNT(*) INTO num FROM fruits WHERE s_id = sid; END // DELIMITER ; \",\"调用存储过程：\",\"mysql> CALL CountProc (101, @num); Query OK, 1 row affected (0.00 sec) \",\"查看返回结果：\",\"mysql> SELECT @num; \",\"该存储过程返回了指定 s_id=101 的水果商提供的水果种类，返回值存储在num变量中，使用SELECT查看，返回结果为3。\",\"**举例2：**创建存储过程，实现累加运算，计算 1+2+…+n 等于多少。具体的代码如下：\",\"DELIMITER // CREATE PROCEDURE `add_num`(IN n INT) BEGIN DECLARE i INT; DECLARE sum INT; SET i = 1; SET sum = 0; WHILE i <= n DO SET sum = sum + i; SET i = i +1; END WHILE; SELECT sum; END // DELIMITER ; \",\"如果你用的是 Navicat 工具，那么在编写存储过程的时候，Navicat 会自动设置 DELIMITER 为其他符号，我们不需要再进行 DELIMITER 的操作。\",\"直接使用 CALL add_num(50);即可。这里我传入的参数为 50，也就是统计 1+2+…+50 的积累之和。\"]},{\"header\":\"3.3 如何调试\",\"slug\":\"_3-3-如何调试\",\"contents\":[\"在 MySQL 中，存储过程不像普通的编程语言（比如 VC++、Java 等）那样有专门的集成开发环境。因此，你可以通过 SELECT 语句，把程序执行的中间结果查询出来，来调试一个 SQL 语句的正确性。调试成功之后，把 SELECT 语句后移到下一个 SQL 语句之后，再调试下一个 SQL 语句。这样逐步推进，就可以完成对存储过程中所有操作的调试了。当然，你也可以把存储过程中的 SQL 语句复制出来，逐段单独调试。\"]},{\"header\":\"4. 存储函数的使用\",\"slug\":\"_4-存储函数的使用\",\"contents\":[\"前面学习了很多函数，使用这些函数可以对数据进行的各种处理操作，极大地提高用户对数据库的管理效率。MySQL支持自定义函数，定义好之后，调用方式与调用MySQL预定义的系统函数一样。\"]},{\"header\":\"4.1 语法分析\",\"slug\":\"_4-1-语法分析\",\"contents\":[\"学过的函数：LENGTH、SUBSTR、CONCAT等\",\"语法格式：\",\"CREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回值类型 [characteristics ...] BEGIN 函数体 #函数体中肯定有 RETURN 语句 END \",\"说明：\",\"1、参数列表：指定参数为IN、OUT或INOUT只对PROCEDURE是合法的，FUNCTION中总是默认为IN参数。\",\"2、RETURNS type 语句表示函数返回数据的类型；\",\"RETURNS子句只能对FUNCTION做指定，对函数而言这是强制的。它用来指定函数的返回类型，而且函数体必须包含一个RETURN value语句。\",\"3、characteristic 创建函数时指定的对函数的约束。取值与创建存储过程时相同，这里不再赘述。\",\"4、函数体也可以用BEGIN…END来表示SQL代码的开始和结束。如果函数体只有一条语句，也可以省略BEGIN…END。\"]},{\"header\":\"4.2 调用存储函数\",\"slug\":\"_4-2-调用存储函数\",\"contents\":[\"在MySQL中，存储函数的使用方法与MySQL内部函数的使用方法是一样的。换言之，用户自己定义的存储函数与MySQL内部函数是一个性质的。区别在于，存储函数是用户自己定义的，而内部函数是MySQL的开发者定义的。\",\"SELECT 函数名(实参列表) \"]},{\"header\":\"4.3 代码举例\",\"slug\":\"_4-3-代码举例\",\"contents\":[\"举例1：\",\"创建存储函数，名称为email_by_name()，参数定义为空，该函数查询Abel的email，并返回，数据类型为字符串型。\",\"DELIMITER // CREATE FUNCTION email_by_name() RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE last_name = 'Abel'); END // DELIMITER ; \",\"调用：\",\"SELECT email_by_name(); \",\"举例2：\",\"创建存储函数，名称为email_by_id()，参数传入emp_id，该函数查询emp_id的email，并返回，数据类型为字符串型。\",\"DELIMITER // CREATE FUNCTION email_by_id(emp_id INT) RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE employee_id = emp_id); END // DELIMITER ; \",\"调用：\",\"SET @emp_id = 102; SELECT email_by_id(102); \",\"举例3：\",\"创建存储函数count_by_id()，参数传入dept_id，该函数查询dept_id部门的员工人数，并返回，数据类型为整型。\",\"DELIMITER // CREATE FUNCTION count_by_id(dept_id INT) RETURNS INT LANGUAGE SQL NOT DETERMINISTIC READS SQL DATA SQL SECURITY DEFINER COMMENT '查询部门平均工资' BEGIN RETURN (SELECT COUNT(*) FROM employees WHERE department_id = dept_id); END // DELIMITER ; \",\"调用：\",\"SET @dept_id = 50; SELECT count_by_id(@dept_id); \",\"注意：\",\"若在创建存储函数中报错“you might want to use the less safe log_bin_trust_function_creators variable”，有两种处理方法：\",\"方式1：加上必要的函数特性“[NOT] DETERMINISTIC”和“{CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA}”\",\"方式2：\",\"mysql> SET GLOBAL log_bin_trust_function_creators = 1; \"]},{\"header\":\"4.4 对比存储函数和存储过程\",\"slug\":\"_4-4-对比存储函数和存储过程\",\"contents\":[\"关键字\",\"调用语法\",\"返回值\",\"应用场景\",\"存储过程\",\"PROCEDURE\",\"CALL 存储过程()\",\"理解为有0个或多个\",\"一般用于更新\",\"存储函数\",\"FUNCTION\",\"SELECT 函数()\",\"只能是一个\",\"一般用于查询结果为一个值并返回时\",\"此外，存储函数可以放在查询语句中使用，存储过程不行。反之，存储过程的功能更加强大，包括能够执行对表的操作（比如创建表，删除表等）和事务操作，这些功能是存储函数不具备的。\"]},{\"header\":\"5. 存储过程和函数的查看、修改、删除\",\"slug\":\"_5-存储过程和函数的查看、修改、删除\",\"contents\":[]},{\"header\":\"5.1 查看\",\"slug\":\"_5-1-查看\",\"contents\":[\"创建完之后，怎么知道我们创建的存储过程、存储函数是否成功了呢？\",\"MySQL存储了存储过程和函数的状态信息，用户可以使用SHOW STATUS语句或SHOW CREATE语句来查看，也可直接从系统的information_schema数据库中查询。这里介绍3种方法。\",\"1. 使用SHOW CREATE语句查看存储过程和函数的创建信息\",\"基本语法结构如下：\",\"SHOW CREATE {PROCEDURE | FUNCTION} 存储过程名或函数名 \",\"举例：\",\"SHOW CREATE FUNCTION test_db.CountProc \\\\G \",\"2. 使用SHOW STATUS语句查看存储过程和函数的状态信息\",\"基本语法结构如下：\",\"SHOW {PROCEDURE | FUNCTION} STATUS [LIKE 'pattern'] \",\"这个语句返回子程序的特征，如数据库、名字、类型、创建者及创建和修改日期。\",\"[LIKE 'pattern']：匹配存储过程或函数的名称，可以省略。当省略不写时，会列出MySQL数据库中存在的所有存储过程或函数的信息。 举例：SHOW STATUS语句示例，代码如下：\",\"mysql> SHOW PROCEDURE STATUS LIKE 'SELECT%' \\\\G *************************** 1. row *************************** Db: test_db Name: SelectAllData Type: PROCEDURE Definer: root@localhost Modified: 2021-10-16 15:55:07 Created: 2021-10-16 15:55:07 Security_type: DEFINER Comment: character_set_client: utf8mb4 collation_connection: utf8mb4_general_ci Database Collation: utf8mb4_general_ci 1 row in set (0.00 sec) \",\"3. 从information_schema.Routines表中查看存储过程和函数的信息\",\"MySQL中存储过程和函数的信息存储在information_schema数据库下的Routines表中。可以通过查询该表的记录来查询存储过程和函数的信息。其基本语法形式如下：\",\"SELECT * FROM information_schema.Routines WHERE ROUTINE_NAME='存储过程或函数的名' [AND ROUTINE_TYPE = {'PROCEDURE|FUNCTION'}]; \",\"说明：如果在MySQL数据库中存在存储过程和函数名称相同的情况，最好指定ROUTINE_TYPE查询条件来指明查询的是存储过程还是函数。\",\"举例：从Routines表中查询名称为CountProc的存储函数的信息，代码如下：\",\"SELECT * FROM information_schema.Routines WHERE ROUTINE_NAME='count_by_id' AND ROUTINE_TYPE = 'FUNCTION' \\\\G \"]},{\"header\":\"5.2 修改\",\"slug\":\"_5-2-修改\",\"contents\":[\"修改存储过程或函数，不影响存储过程或函数功能，只是修改相关特性。使用ALTER语句实现。\",\"ALTER {PROCEDURE | FUNCTION} 存储过程或函数的名 [characteristic ...] \",\"其中，characteristic指定存储过程或函数的特性，其取值信息与创建存储过程、函数时的取值信息略有不同。\",\"{ CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' \",\"CONTAINS SQL，表示子程序包含SQL语句，但不包含读或写数据的语句。\",\"NO SQL，表示子程序中不包含SQL语句。\",\"READS SQL DATA，表示子程序中包含读数据的语句。\",\"MODIFIES SQL DATA，表示子程序中包含写数据的语句。\",\"SQL SECURITY { DEFINER | INVOKER }，指明谁有权限来执行。 \",\"DEFINER，表示只有定义者自己才能够执行。\",\"INVOKER，表示调用者可以执行。\",\"COMMENT 'string'，表示注释信息。\",\"修改存储过程使用ALTER PROCEDURE语句，修改存储函数使用ALTER FUNCTION语句。但是，这两个语句的结构是一样的，语句中的所有参数也是一样的。\",\"举例1：\",\"修改存储过程CountProc的定义。将读写权限改为MODIFIES SQL DATA，并指明调用者可以执行，代码如下：\",\"ALTER PROCEDURE CountProc MODIFIES SQL DATA SQL SECURITY INVOKER ; \",\"查询修改后的信息：\",\"SELECT specific_name,sql_data_access,security_type FROM information_schema.`ROUTINES` WHERE routine_name = 'CountProc' AND routine_type = 'PROCEDURE'; \",\"结果显示，存储过程修改成功。从查询的结果可以看出，访问数据的权限（SQL_DATA_ ACCESS）已经变成MODIFIES SQL DATA，安全类型（SECURITY_TYPE）已经变成INVOKER。\",\"举例2：\",\"修改存储函数CountProc的定义。将读写权限改为READS SQL DATA，并加上注释信息“FIND NAME”，代码如下：\",\"ALTER FUNCTION CountProc READS SQL DATA COMMENT 'FIND NAME' ; \",\"存储函数修改成功。从查询的结果可以看出，访问数据的权限（SQL_DATA_ACCESS）已经变成READS SQL DATA，函数注释（ROUTINE_COMMENT）已经变成FIND NAME。\"]},{\"header\":\"5.3 删除\",\"slug\":\"_5-3-删除\",\"contents\":[\"删除存储过程和函数，可以使用DROP语句，其语法结构如下：\",\"DROP {PROCEDURE | FUNCTION} [IF EXISTS] 存储过程或函数的名 \",\"IF EXISTS：如果程序或函数不存储，它可以防止发生错误，产生一个用SHOW WARNINGS查看的警告。\",\"举例：\",\"DROP PROCEDURE CountProc; \",\"DROP FUNCTION CountProc; \"]},{\"header\":\"6. 关于存储过程使用的争议\",\"slug\":\"_6-关于存储过程使用的争议\",\"contents\":[\"尽管存储过程有诸多优点，但是对于存储过程的使用，一直都存在着很多争议，比如有些公司对于大型项目要求使用存储过程，而有些公司在手册中明确禁止使用存储过程，为什么这些公司对存储过程的使用需求差别这么大呢？\"]},{\"header\":\"6.1 优点\",\"slug\":\"_6-1-优点\",\"contents\":[\"**1、存储过程可以一次编译多次使用。**存储过程只在创建时进行编译，之后的使用都不需要重新编译，这就提升了 SQL 的执行效率。\",\"**2、可以减少开发工作量。**将代码封装成模块，实际上是编程的核心思想之一，这样可以把复杂的问题拆解成不同的模块，然后模块之间可以重复使用，在减少开发工作量的同时，还能保证代码的结构清晰。\",\"**3、存储过程的安全性强。**我们在设定存储过程的时候可以设置对用户的使用权限，这样就和视图一样具有较强的安全性。\",\"**4、可以减少网络传输量。**因为代码封装到存储过程中，每次使用只需要调用存储过程即可，这样就减少了网络传输量。\",\"**5、良好的封装性。**在进行相对复杂的数据库操作时，原本需要使用一条一条的 SQL 语句，可能要连接多次数据库才能完成的操作，现在变成了一次存储过程，只需要连接一次即可。\"]},{\"header\":\"6.2 缺点\",\"slug\":\"_6-2-缺点\",\"contents\":[\"基于上面这些优点，不少大公司都要求大型项目使用存储过程，比如微软、IBM 等公司。但是国内的阿里并不推荐开发人员使用存储过程，这是为什么呢？\"]},{\"header\":\"阿里开发规范\",\"slug\":\"阿里开发规范\",\"contents\":[\"【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\",\"存储过程虽然有诸如上面的好处，但缺点也是很明显的。\",\"**1、可移植性差。**存储过程不能跨数据库移植，比如在 MySQL、Oracle 和 SQL Server 里编写的存储过程，在换成其他数据库时都需要重新编写。\",\"**2、调试困难。**只有少数 DBMS 支持存储过程的调试。对于复杂的存储过程来说，开发和维护都不容易。虽然也有一些第三方工具可以对存储过程进行调试，但要收费。\",\"**3、存储过程的版本管理很困难。**比如数据表索引发生变化了，可能会导致存储过程失效。我们在开发软件的时候往往需要进行版本管理，但是存储过程本身没有版本控制，版本迭代更新的时候很麻烦。\",\"**4、它不适合高并发的场景。**高并发的场景需要减少数据库的压力，有时数据库会采用分库分表的方式，而且对可扩展性要求很高，在这种情况下，存储过程会变得难以维护，增加数据库的压力，显然就不适用了。\",\"小结：\",\"存储过程既方便，又有局限性。尽管不同的公司对存储过程的态度不一，但是对于我们开发人员来说，不论怎样，掌握存储过程都是必备的技能之一。\"]}]},\"/study-tutorial/database/mysql/subquery.html\":{\"title\":\"9、子查询\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"子查询指一个查询语句嵌套在另一个查询语句内部的查询，这个特性从MySQL 4.1开始引入。\",\"SQL 中子查询的使用大大增强了 SELECT 查询的能力，因为很多时候查询需要从结果集中获取数据，或者需要从同一个表中先计算得出一个数据结果，然后与这个数据结果（可能是某个标量，也可能是某个集合）进行比较。\"]},{\"header\":\"1. 需求分析与问题解决\",\"slug\":\"_1-需求分析与问题解决\",\"contents\":[]},{\"header\":\"1.1 实际问题\",\"slug\":\"_1-1-实际问题\",\"contents\":[\"现有解决方式：\",\"#方式一： SELECT salary FROM employees WHERE last_name = 'Abel'; SELECT last_name,salary FROM employees WHERE salary > 11000; #方式二：自连接 SELECT e2.last_name,e2.salary FROM employees e1,employees e2 WHERE e1.last_name = 'Abel' AND e1.`salary` < e2.`salary` \",\"#方式三：子查询 SELECT last_name,salary FROM employees WHERE salary > ( SELECT salary FROM employees WHERE last_name = 'Abel' ); \"]},{\"header\":\"1.2 子查询的基本使用\",\"slug\":\"_1-2-子查询的基本使用\",\"contents\":[\"子查询的基本语法结构：\",\"子查询（内查询）在主查询之前一次执行完成。\",\"子查询的结果被主查询（外查询）使用 。\",\"注意事项\",\"子查询要包含在括号内\",\"将子查询放在比较条件的右侧\",\"单行操作符对应单行子查询，多行操作符对应多行子查询\"]},{\"header\":\"1.3 子查询的分类\",\"slug\":\"_1-3-子查询的分类\",\"contents\":[\"分类方式1：\",\"我们按内查询的结果返回一条还是多条记录，将子查询分为单行子查询、多行子查询。\",\"单行子查询\",\"多行子查询\",\"分类方式2：\",\"我们按内查询是否被执行多次，将子查询划分为相关(或关联)子查询和不相关(或非关联)子查询。\",\"子查询从数据表中查询了数据结果，如果这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行，那么这样的子查询叫做不相关子查询。\",\"同样，如果子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，这种嵌套的执行方式就称为相关子查询。\"]},{\"header\":\"2. 单行子查询\",\"slug\":\"_2-单行子查询\",\"contents\":[]},{\"header\":\"2.1 单行比较操作符\",\"slug\":\"_2-1-单行比较操作符\",\"contents\":[\"操作符\",\"含义\",\"=\",\"equal to\",\">\",\"greater than\",\">=\",\"greater than or equal to\",\"<\",\"less than\",\"<=\",\"less than or equal to\",\"<>\",\"not equal to\"]},{\"header\":\"2.2 代码示例\",\"slug\":\"_2-2-代码示例\",\"contents\":[\"题目：查询工资大于149号员工工资的员工的信息\",\"题目：返回job_id与141号员工相同，salary比143号员工多的员工姓名，job_id和工资\",\"SELECT last_name, job_id, salary FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE employee_id = 141) AND salary > (SELECT salary FROM employees WHERE employee_id = 143); \",\"题目：返回公司工资最少的员工的last_name,job_id和salary\",\"SELECT last_name, job_id, salary FROM employees WHERE salary = (SELECT MIN(salary) FROM employees); \",\"题目：查询与141号或174号员工的manager_id和department_id相同的其他员工的employee_id，manager_id，department_id\",\"实现方式1：不成对比较\",\"SELECT employee_id, manager_id, department_id FROM employees WHERE manager_id IN (SELECT manager_id FROM employees WHERE employee_id IN (174,141)) AND department_id IN (SELECT department_id FROM employees WHERE employee_id IN (174,141)) AND employee_id NOT IN(174,141); \",\"实现方式2：成对比较\",\"SELECT employee_id, manager_id, department_id FROM employees WHERE (manager_id, department_id) IN (SELECT manager_id, department_id FROM employees WHERE employee_id IN (141,174)) AND employee_id NOT IN (141,174); \"]},{\"header\":\"2.3 HAVING 中的子查询\",\"slug\":\"_2-3-having-中的子查询\",\"contents\":[\"首先执行子查询。\",\"向主查询中的HAVING 子句返回结果。\",\"题目：查询最低工资大于50号部门最低工资的部门id和其最低工资\",\"SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) > (SELECT MIN(salary) FROM employees WHERE department_id = 50); \"]},{\"header\":\"2.4 CASE中的子查询\",\"slug\":\"_2-4-case中的子查询\",\"contents\":[\"在CASE表达式中使用单列子查询：\",\"题目：显式员工的employee_id,last_name和location。其中，若员工department_id与location_id为1800的department_id相同，则location为’Canada’，其余则为’USA’。\",\"SELECT employee_id, last_name, (CASE department_id WHEN (SELECT department_id FROM departments WHERE location_id = 1800) THEN 'Canada' ELSE 'USA' END) location FROM employees; \"]},{\"header\":\"2.5 子查询中的空值问题\",\"slug\":\"_2-5-子查询中的空值问题\",\"contents\":[\"SELECT last_name, job_id FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE last_name = 'Haas'); \",\"子查询不返回任何行\"]},{\"header\":\"2.5 非法使用子查询\",\"slug\":\"_2-5-非法使用子查询\",\"contents\":[\"SELECT employee_id, last_name FROM employees WHERE salary = (SELECT MIN(salary) FROM employees GROUP BY department_id); \",\"多行子查询使用单行比较符\"]},{\"header\":\"3. 多行子查询\",\"slug\":\"_3-多行子查询\",\"contents\":[\"也称为集合比较子查询\",\"内查询返回多行\",\"使用多行比较操作符\"]},{\"header\":\"3.1 多行比较操作符\",\"slug\":\"_3-1-多行比较操作符\",\"contents\":[\"操作符\",\"含义\",\"IN\",\"等于列表中的任意一个\",\"ANY\",\"需要和单行比较操作符一起使用，和子查询返回的某一个值比较\",\"ALL\",\"需要和单行比较操作符一起使用，和子查询返回的所有值比较\",\"SOME\",\"实际上是ANY的别名，作用相同，一般常使用ANY\",\"体会 ANY 和 ALL 的区别\"]},{\"header\":\"3.2 代码示例\",\"slug\":\"_3-2-代码示例\",\"contents\":[\"题目：返回其它job_id中比job_id为‘IT_PROG’部门任一工资低的员工的员工号、姓名、job_id 以及salary\",\"题目：返回其它job_id中比job_id为‘IT_PROG’部门所有工资都低的员工的员工号、姓名、job_id以及salary\",\"题目：查询平均工资最低的部门id\",\"#方式1： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) dept_avg_sal ) \",\"#方式2： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) <= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) \"]},{\"header\":\"3.3 空值问题\",\"slug\":\"_3-3-空值问题\",\"contents\":[\"SELECT last_name FROM employees WHERE employee_id NOT IN ( SELECT manager_id FROM employees ); \"]},{\"header\":\"4. 相关子查询\",\"slug\":\"_4-相关子查询\",\"contents\":[]},{\"header\":\"4.1 相关子查询执行流程\",\"slug\":\"_4-1-相关子查询执行流程\",\"contents\":[\"如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为关联子查询。\",\"相关子查询按照一行接一行的顺序执行，主查询的每一行都执行一次子查询。\",\"说明：子查询中使用主查询中的列\"]},{\"header\":\"4.2 代码示例\",\"slug\":\"_4-2-代码示例\",\"contents\":[\"题目：查询员工中工资大于本部门平均工资的员工的last_name,salary和其department_id\",\"方式一：相关子查询\",\"方式二：在 FROM 中使用子查询\",\"SELECT last_name,salary,e1.department_id FROM employees e1,(SELECT department_id,AVG(salary) dept_avg_sal FROM employees GROUP BY department_id) e2 WHERE e1.`department_id` = e2.department_id AND e2.dept_avg_sal < e1.`salary`; \",\"from型的子查询：子查询是作为from的一部分，子查询要用()引起来，并且要给这个子查询取别名， 把它当成一张“临时的虚拟的表”来使用。\",\"在ORDER BY 中使用子查询：\",\"题目：查询员工的id,salary,按照department_name 排序\",\"SELECT employee_id,salary FROM employees e ORDER BY ( SELECT department_name FROM departments d WHERE e.`department_id` = d.`department_id` ); \",\"题目：若employees表中employee_id与job_history表中employee_id相同的数目不小于2，输出这些相同id的员工的employee_id,last_name和其job_id\",\"SELECT e.employee_id, last_name,e.job_id FROM employees e WHERE 2 <= (SELECT COUNT(*) FROM job_history WHERE employee_id = e.employee_id); \"]},{\"header\":\"4.3 EXISTS 与 NOT EXISTS关键字\",\"slug\":\"_4-3-exists-与-not-exists关键字\",\"contents\":[\"关联子查询通常也会和 EXISTS操作符一起来使用，用来检查在子查询中是否存在满足条件的行。\",\"如果在子查询中不存在满足条件的行：\",\"条件返回 FALSE\",\"继续在子查询中查找\",\"如果在子查询中存在满足条件的行：\",\"不在子查询中继续查找\",\"条件返回 TRUE\",\"NOT EXISTS关键字表示如果不存在某种条件，则返回TRUE，否则返回FALSE。\",\"题目：查询公司管理者的employee_id，last_name，job_id，department_id信息\",\"方式一：\",\"SELECT employee_id, last_name, job_id, department_id FROM employees e1 WHERE EXISTS ( SELECT * FROM employees e2 WHERE e2.manager_id = e1.employee_id); \",\"方式二：自连接\",\"SELECT DISTINCT e1.employee_id, e1.last_name, e1.job_id, e1.department_id FROM employees e1 JOIN employees e2 WHERE e1.employee_id = e2.manager_id; \",\"方式三：\",\"SELECT employee_id,last_name,job_id,department_id FROM employees WHERE employee_id IN ( SELECT DISTINCT manager_id FROM employees ); \",\"题目：查询departments表中，不存在于employees表中的部门的department_id和department_name\",\"SELECT department_id, department_name FROM departments d WHERE NOT EXISTS (SELECT 'X' FROM employees WHERE department_id = d.department_id); \"]},{\"header\":\"4.4 相关更新\",\"slug\":\"_4-4-相关更新\",\"contents\":[\"UPDATE table1 alias1 SET column = (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); \",\"使用相关子查询依据一个表中的数据更新另一个表的数据。\",\"题目：在employees中增加一个department_name字段，数据为员工对应的部门名称\",\"## 1） ALTER TABLE employees ADD(department_name VARCHAR2(14)); ## 2） UPDATE employees e SET department_name = (SELECT department_name FROM departments d WHERE e.department_id = d.department_id); \"]},{\"header\":\"4.4 相关删除\",\"slug\":\"_4-4-相关删除\",\"contents\":[\" DELETE FROM table1 alias1 WHERE column operator (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); \",\"使用相关子查询依据一个表中的数据删除另一个表的数据。\",\"题目：删除表employees中，其与emp_history表皆有的数据\",\"DELETE FROM employees e WHERE employee_id in (SELECT employee_id FROM emp_history WHERE employee_id = e.employee_id); \"]},{\"header\":\"5. 抛一个思考题\",\"slug\":\"_5-抛一个思考题\",\"contents\":[\"**问题：**谁的工资比Abel的高？\",\"解答：\",\"#方式1：自连接 SELECT e2.last_name,e2.salary FROM employees e1,employees e2 WHERE e1.last_name = 'Abel' AND e1.`salary` < e2.`salary` \",\"#方式2：子查询 SELECT last_name,salary FROM employees WHERE salary > ( SELECT salary FROM employees WHERE last_name = 'Abel' ); \",\"**问题：**以上两种方式有好坏之分吗？\",\"**解答：**自连接方式好！\",\"题目中可以使用子查询，也可以使用自连接。一般情况建议你使用自连接，因为在许多 DBMS 的处理过程中，对于自连接的处理速度要比子查询快得多。\",\"可以这样理解：子查询实际上是通过未知表进行查询后的条件判断，而自连接是通过已知的自身数据表进行条件判断，因此在大部分 DBMS 中都对自连接处理进行了优化。\"]}]},\"/study-tutorial/database/mysql/type-of-data.html\":{\"title\":\"12、MySQL数据类型精讲\",\"contents\":[{\"header\":\"1. MySQL中的数据类型\",\"slug\":\"_1-mysql中的数据类型\",\"contents\":[\"类型\",\"类型举例\",\"整数类型\",\"TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT\",\"浮点类型\",\"FLOAT、DOUBLE\",\"定点数类型\",\"DECIMAL\",\"位类型\",\"BIT\",\"日期时间类型\",\"YEAR、TIME、DATE、DATETIME、TIMESTAMP\",\"文本字符串类型\",\"CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT\",\"枚举类型\",\"ENUM\",\"集合类型\",\"SET\",\"二进制字符串类型\",\"BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB\",\"JSON类型\",\"JSON对象、JSON数组\",\"空间数据类型\",\"单值类型：GEOMETRY、POINT、LINESTRING、POLYGON；集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION\",\"常见数据类型的属性，如下：\",\"MySQL关键字\",\"含义\",\"NULL\",\"数据列可包含NULL值\",\"NOT NULL\",\"数据列不允许包含NULL值\",\"DEFAULT\",\"默认值\",\"PRIMARY KEY\",\"主键\",\"AUTO_INCREMENT\",\"自动递增，适用于整数类型\",\"UNSIGNED\",\"无符号\",\"CHARACTER SET name\",\"指定一个字符集\"]},{\"header\":\"2. 整数类型\",\"slug\":\"_2-整数类型\",\"contents\":[]},{\"header\":\"2.1 类型介绍\",\"slug\":\"_2-1-类型介绍\",\"contents\":[\"整数类型一共有 5 种，包括 TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和 BIGINT。\",\"它们的区别如下表所示：\",\"整数类型\",\"字节\",\"有符号数取值范围\",\"无符号数取值范围\",\"TINYINT\",\"1\",\"-128~127\",\"0~255\",\"SMALLINT\",\"2\",\"-32768~32767\",\"0~65535\",\"MEDIUMINT\",\"3\",\"-8388608~8388607\",\"0~16777215\",\"INT、INTEGER\",\"4\",\"-2147483648~2147483647\",\"0~4294967295\",\"BIGINT\",\"8\",\"-9223372036854775808~9223372036854775807\",\"0~18446744073709551615\"]},{\"header\":\"2.2 可选属性\",\"slug\":\"_2-2-可选属性\",\"contents\":[\"整数类型的可选属性有三个：\"]},{\"header\":\"2.2.1 M\",\"slug\":\"_2-2-1-m\",\"contents\":[\"M: 表示显示宽度，M的取值范围是(0, 255)。例如，int(5)：当数据宽度小于5位的时候在数字前面需要用字符填满宽度。该项功能需要配合“ZEROFILL”使用，表示用“0”填满宽度，否则指定显示宽度无效。\",\"如果设置了显示宽度，那么插入的数据宽度超过显示宽度限制，会不会截断或插入失败？\",\"答案：不会对插入的数据有任何影响，还是按照类型的实际宽度进行保存，即显示宽度与类型可以存储的值范围无关。从MySQL 8.0.17开始，整数数据类型不推荐使用显示宽度属性。\",\"整型数据类型可以在定义表结构时指定所需要的显示宽度，如果不指定，则系统为每一种类型指定默认的宽度值。\",\"举例：\",\"CREATE TABLE test_int1 ( x TINYINT, y SMALLINT, z MEDIUMINT, m INT, n BIGINT ); \",\"查看表结构 （MySQL5.7中显式如下，MySQL8中不再显式范围）\",\"mysql> desc test_int1; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | x | tinyint(4) | YES | | NULL | | | y | smallint(6) | YES | | NULL | | | z | mediumint(9) | YES | | NULL | | | m | int(11) | YES | | NULL | | | n | bigint(20) | YES | | NULL | | +-------+--------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) \",\"TINYINT有符号数和无符号数的取值范围分别为-128127和0255，由于负号占了一个数字位，因此TINYINT默认的显示宽度为4。同理，其他整数类型的默认显示宽度与其有符号数的最小值的宽度相同。\",\"举例：\",\"CREATE TABLE test_int2( f1 INT, f2 INT(5), f3 INT(5) ZEROFILL ) DESC test_int2; INSERT INTO test_int2(f1,f2,f3) VALUES(1,123,123); INSERT INTO test_int2(f1,f2) VALUES(123456,123456); INSERT INTO test_int2(f1,f2,f3) VALUES(123456,123456,123456); \",\"mysql> SELECT * FROM test_int2; +--------+--------+--------+ | f1 | f2 | f3 | +--------+--------+--------+ | 1 | 123 | 00123 | | 123456 | 123456 | NULL | | 123456 | 123456 | 123456 | +--------+--------+--------+ 3 rows in set (0.00 sec) \"]},{\"header\":\"2.2.2 UNSIGNED\",\"slug\":\"_2-2-2-unsigned\",\"contents\":[\"UNSIGNED: 无符号类型（非负），所有的整数类型都有一个可选的属性UNSIGNED（无符号属性），无符号整数类型的最小取值为0。所以，如果需要在MySQL数据库中保存非负整数值时，可以将整数类型设置为无符号类型。\",\"int类型默认显示宽度为int(11)，无符号int类型默认显示宽度为int(10)。\",\"CREATE TABLE test_int3( f1 INT UNSIGNED ); mysql> desc test_int3; +-------+------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+------------------+------+-----+---------+-------+ | f1 | int(10) unsigned | YES | | NULL | | +-------+------------------+------+-----+---------+-------+ 1 row in set (0.00 sec) \"]},{\"header\":\"2.2.3 ZEROFILL\",\"slug\":\"_2-2-3-zerofill\",\"contents\":[\"ZEROFILL: 0填充,（如果某列是ZEROFILL，那么MySQL会自动为当前列添加UNSIGNED属性），如果指定了ZEROFILL只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可。\",\"原来，在 int(M) 中，M 的值跟 int(M) 所占多少存储空间并无任何关系。 int(3)、int(4)、int(8) 在磁盘上都是占用 4 bytes 的存储空间。也就是说，**int(M)，必须和UNSIGNED ZEROFILL一起使用才有意义。**如果整数值超过M位，就按照实际位数存储。只是无须再用字符 0 进行填充。\"]},{\"header\":\"2.3 适用场景\",\"slug\":\"_2-3-适用场景\",\"contents\":[\"TINYINT：一般用于枚举数据，比如系统设定取值范围很小且固定的场景。\",\"SMALLINT：可以用于较小范围的统计数据，比如统计工厂的固定资产库存数量等。\",\"MEDIUMINT：用于较大整数的计算，比如车站每日的客流量等。\",\"INT、INTEGER：取值范围足够大，一般情况下不用考虑超限问题，用得最多。比如商品编号。\",\"BIGINT：只有当你处理特别巨大的整数时才会用到。比如双十一的交易量、大型门户网站点击量、证券公司衍生产品持仓等。\"]},{\"header\":\"2.4 如何选择？\",\"slug\":\"_2-4-如何选择\",\"contents\":[\"在评估用哪种整数类型的时候，你需要考虑存储空间和可靠性的平衡问题：一方 面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间， 使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起系统错误，影响可靠性。\",\"举个例子，商品编号采用的数据类型是 INT。原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。\",\"如果使用 SMALLINT 类型，虽然占用字节数比 INT 类型的整数少，但是却不能保证数据不会超出范围 65535。相反，使用 INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。\",\"你要注意的是，在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，我建议你首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。\"]},{\"header\":\"3. 浮点类型\",\"slug\":\"_3-浮点类型\",\"contents\":[]},{\"header\":\"3.1 类型介绍\",\"slug\":\"_3-1-类型介绍\",\"contents\":[\"浮点数和定点数类型的特点是可以处理小数，你可以把整数看成小数的一个特例。因此，浮点数和定点数的使用场景，比整数大多了。 MySQL支持的浮点数类型，分别是 FLOAT、DOUBLE、REAL。\",\"FLOAT 表示单精度浮点数；\",\"DOUBLE 表示双精度浮点数；\",\"REAL默认就是 DOUBLE。如果你把 SQL 模式设定为启用“REAL_AS_FLOAT”，那 么，MySQL 就认为 REAL 是 FLOAT。如果要启用“REAL_AS_FLOAT”，可以通过以下 SQL 语句实现：\",\"SET sql_mode = “REAL_AS_FLOAT”; \",\"**问题1：**FLOAT 和 DOUBLE 这两种数据类型的区别是啥呢？\",\"FLOAT 占用字节数少，取值范围小；DOUBLE 占用字节数多，取值范围也大。\",\"**问题2：**为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？\",\"MySQL 存储浮点数的格式为：符号(S)、尾数(M)和 阶码(E)。因此，无论有没有符号，MySQL 的浮点数都会存储表示符号的部分。因此， 所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。\"]},{\"header\":\"3.2 数据精度说明\",\"slug\":\"_3-2-数据精度说明\",\"contents\":[\"对于浮点类型，在MySQL中单精度值使用4个字节，双精度值使用8个字节。\",\"MySQL允许使用非标准语法（其他数据库未必支持，因此如果涉及到数据迁移，则最好不要这么用）：FLOAT(M,D)或DOUBLE(M,D)。这里，M称为精度，D称为标度。(M,D)中 M=整数位+小数位，D=小数位。 D<=M<=255，0<=D<=30。\",\"例如，定义为FLOAT(5,2)的一个列可以显示为-999.99-999.99。如果超过这个范围会报错。\",\"FLOAT和DOUBLE类型在不指定(M,D)时，默认会按照实际的精度（由实际的硬件和操作系统决定）来显示。\",\"说明：浮点类型，也可以加UNSIGNED，但是不会改变数据范围，例如：FLOAT(3,2) UNSIGNED仍然只能表示0-9.99的范围。\",\"不管是否显式设置了精度(M,D)，这里MySQL的处理方案如下：\",\"如果存储时，整数部分超出了范围，MySQL就会报错，不允许存这样的值\",\"如果存储时，小数点部分若超出范围，就分以下情况：\",\"若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。例如在FLOAT(5,2)列内插入999.009，近似结果是999.01。\",\"若四舍五入后，整数部分超出范围，则MySQL报错，并拒绝处理。如FLOAT(5,2)列内插入999.995和-999.995都会报错。\",\"从MySQL 8.0.17开始，FLOAT(M,D) 和DOUBLE(M,D)用法在官方文档中已经明确不推荐使用，将来可能被移除。另外，关于浮点型FLOAT和DOUBLE的UNSIGNED也不推荐使用了，将来也可能被移除。\",\"举例\",\"CREATE TABLE test_double1( f1 FLOAT, f2 FLOAT(5,2), f3 DOUBLE, f4 DOUBLE(5,2) ); DESC test_double1; INSERT INTO test_double1 VALUES(123.456,123.456,123.4567,123.45); #Out of range value for column 'f2' at row 1 INSERT INTO test_double1 VALUES(123.456,1234.456,123.4567,123.45); SELECT * FROM test_double1; \"]},{\"header\":\"3.3 精度误差说明\",\"slug\":\"_3-3-精度误差说明\",\"contents\":[\"浮点数类型有个缺陷，就是不精准。下面我来重点解释一下为什么 MySQL 的浮点数不够精准。比如，我们设计一个表，有f1这个字段，插入值分别为0.47,0.44,0.19，我们期待的运行结果是：0.47 + 0.44 + 0.19 = 1.1。而使用sum之后查询：\",\"CREATE TABLE test_double2( f1 DOUBLE ); INSERT INTO test_double2 VALUES(0.47),(0.44),(0.19); \",\"mysql> SELECT SUM(f1) -> FROM test_double2; +--------------------+ | SUM(f1) | +--------------------+ | 1.0999999999999999 | +--------------------+ 1 row in set (0.00 sec) \",\"mysql> SELECT SUM(f1) = 1.1,1.1 = 1.1 -> FROM test_double2; +---------------+-----------+ | SUM(f1) = 1.1 | 1.1 = 1.1 | +---------------+-----------+ | 0 | 1 | +---------------+-----------+ 1 row in set (0.00 sec) \",\"查询结果是 1.0999999999999999。看到了吗？虽然误差很小，但确实有误差。 你也可以尝试把数据类型改成 FLOAT，然后运行求和查询，得到的是， 1.0999999940395355。显然，误差更大了。\",\"那么，为什么会存在这样的误差呢？问题还是出在 MySQL 对浮点类型数据的存储方式上。\",\"MySQL 用 4 个字节存储 FLOAT 类型数据，用 8 个字节来存储 DOUBLE 类型数据。无论哪个，都是采用二进制的方式来进行存储的。比如 9.625，用二进制来表达，就是 1001.101，或者表达成 1.001101×2^3。如果尾数不是 0 或 5（比如 9.624），你就无法用一个二进制数来精确表达。进而，就只好在取值允许的范围内进行四舍五入。\",\"在编程中，如果用到浮点数，要特别注意误差问题，**因为浮点数是不准确的，所以我们要避免使用“=”来判断两个数是否相等。**同时，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。那么，MySQL 有没有精准的数据类型呢？当然有，这就是定点数类型：DECIMAL。\"]},{\"header\":\"4. 定点数类型\",\"slug\":\"_4-定点数类型\",\"contents\":[]},{\"header\":\"4.1 类型介绍\",\"slug\":\"_4-1-类型介绍\",\"contents\":[\"MySQL中的定点数类型只有 DECIMAL 一种类型。\",\"| 数据类型 | 字节数 | 含义 | | ------------------------ | ------- | ------------------ | | DECIMAL(M,D),DEC,NUMERIC | M+2字节 | 有效范围由M和D决定 |\",\"使用 DECIMAL(M,D) 的方式表示高精度小数。其中，M被称为精度，D被称为标度。0<=M<=65，0<=D<=30，D<M。例如，定义DECIMAL（5,2）的类型，表示该列取值范围是-999.99~999.99。\",\"DECIMAL(M,D)的最大取值范围与DOUBLE类型一样，但是有效的数据范围是由M和D决定的。DECIMAL 的存储空间并不是固定的，由精度值M决定，总共占用的存储空间为M+2个字节。也就是说，在一些对精度要求不高的场景下，比起占用同样字节长度的定点数，浮点数表达的数值范围可以更大一些。\",\"定点数在MySQL内部是以字符串的形式进行存储，这就决定了它一定是精准的。\",\"当DECIMAL类型不指定精度和标度时，其默认为DECIMAL(10,0)。当数据的精度超出了定点数类型的精度范围时，则MySQL同样会进行四舍五入处理。\",\"浮点数 vs 定点数\",\"浮点数相对于定点数的优点是在长度一定的情况下，浮点类型取值范围大，但是不精准，适用于需要取值范围大，又可以容忍微小误差的科学计算场景（比如计算化学、分子建模、流体动力学等）\",\"定点数类型取值范围相对小，但是精准，没有误差，适合于对精度要求极高的场景 （比如涉及金额计算的场景）\",\"举例\",\"CREATE TABLE test_decimal1( f1 DECIMAL, f2 DECIMAL(5,2) ); DESC test_decimal1; INSERT INTO test_decimal1(f1,f2) VALUES(123.123,123.456); #Out of range value for column 'f2' at row 1 INSERT INTO test_decimal1(f2) VALUES(1234.34); \",\"mysql> SELECT * FROM test_decimal1; +------+--------+ | f1 | f2 | +------+--------+ | 123 | 123.46 | +------+--------+ 1 row in set (0.00 sec) \",\"举例\",\"我们运行下面的语句，把test_double2表中字段“f1”的数据类型修改为 DECIMAL(5,2)：\",\"ALTER TABLE test_double2 MODIFY f1 DECIMAL(5,2); \",\"然后，我们再一次运行求和语句：\",\"mysql> SELECT SUM(f1) -> FROM test_double2; +---------+ | SUM(f1) | +---------+ | 1.10 | +---------+ 1 row in set (0.00 sec) \",\"mysql> SELECT SUM(f1) = 1.1 -> FROM test_double2; +---------------+ | SUM(f1) = 1.1 | +---------------+ | 1 | +---------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"4.2 开发中经验\",\"slug\":\"_4-2-开发中经验\",\"contents\":[\"“由于 DECIMAL 数据类型的精准性，在我们的项目中，除了极少数（比如商品编号）用到整数类型外，其他的数值都用的是 DECIMAL，原因就是这个项目所处的零售行业，要求精准，一分钱也不能差。 ” ——来自某项目经理\"]},{\"header\":\"5. 位类型：BIT\",\"slug\":\"_5-位类型-bit\",\"contents\":[\"BIT类型中存储的是二进制值，类似010110。\",\"二进制字符串类型\",\"长度\",\"长度范围\",\"占用空间\",\"BIT(M)\",\"M\",\"1 <= M <= 64\",\"约为(M + 7)/8个字节\",\"BIT类型，如果没有指定(M)，默认是1位。这个1位，表示只能存1位的二进制值。这里(M)是表示二进制的位数，位数最小值为1，最大值为64。\",\"CREATE TABLE test_bit1( f1 BIT, f2 BIT(5), f3 BIT(64) ); INSERT INTO test_bit1(f1) VALUES(1); #Data too long for column 'f1' at row 1 INSERT INTO test_bit1(f1) VALUES(2); INSERT INTO test_bit1(f2) VALUES(23); \",\"注意：在向BIT类型的字段中插入数据时，一定要确保插入的数据在BIT类型支持的范围内。\",\"使用SELECT命令查询位字段时，可以用BIN()或HEX()函数进行读取。\",\"mysql> SELECT * FROM test_bit1; +------------+------------+------------+ | f1 | f2 | f3 | +------------+------------+------------+ | 0x01 | NULL | NULL | | NULL | 0x17 | NULL | +------------+------------+------------+ 2 rows in set (0.00 sec) \",\"mysql> SELECT BIN(f2),HEX(f2) -> FROM test_bit1; +---------+---------+ | BIN(f2) | HEX(f2) | +---------+---------+ | NULL | NULL | | 10111 | 17 | +---------+---------+ 2 rows in set (0.00 sec) \",\"mysql> SELECT f2 + 0 -> FROM test_bit1; +--------+ | f2 + 0 | +--------+ | NULL | | 23 | +--------+ 2 rows in set (0.00 sec) \",\"可以看到，使用b+0查询数据时，可以直接查询出存储的十进制数据的值。\"]},{\"header\":\"6. 日期与时间类型\",\"slug\":\"_6-日期与时间类型\",\"contents\":[\"日期与时间是重要的信息，在我们的系统中，几乎所有的数据表都用得到。原因是客户需要知道数据的时间标签，从而进行数据查询、统计和处理。\",\"MySQL有多种表示日期和时间的数据类型，不同的版本可能有所差异，MySQL8.0版本支持的日期和时间类型主要有：YEAR类型、TIME类型、DATE类型、DATETIME类型和TIMESTAMP类型。\",\"YEAR类型通常用来表示年\",\"DATE类型通常用来表示年、月、日\",\"TIME类型通常用来表示时、分、秒\",\"DATETIME类型通常用来表示年、月、日、时、分、秒\",\"TIMESTAMP类型通常用来表示带时区的年、月、日、时、分、秒\",\"类型\",\"名称\",\"字节\",\"日期格式\",\"最小值\",\"最大值\",\"YEAR\",\"年\",\"1\",\"YYYY或YY\",\"1901\",\"2155\",\"TIME\",\"时间\",\"3\",\"HH:MM:SS\",\"-838:59:59\",\"838:59:59\",\"DATE\",\"日期\",\"3\",\"YYYY-MM-DD\",\"1000-01-01\",\"9999-12-03\",\"DATETIME\",\"日期时间\",\"8\",\"YYYY-MM-DD HH:MM:SS\",\"1000-01-01 00:00:00\",\"9999-12-31 23:59:59\",\"TIMESTAMP\",\"日期时间\",\"4\",\"YYYY-MM-DD HH:MM:SS\",\"1970-01-01 00:00:00 UTC\",\"2038-01-19 03:14:07UTC\",\"可以看到，不同数据类型表示的时间内容不同、取值范围不同，而且占用的字节数也不一样，你要根据实际需要灵活选取。\",\"为什么时间类型 TIME 的取值范围不是 -23:59:59～23:59:59 呢？原因是 MySQL 设计的 TIME 类型，不光表示一天之内的时间，而且可以用来表示一个时间间隔，这个时间间隔可以超过 24 小时。\"]},{\"header\":\"6.1 YEAR类型\",\"slug\":\"_6-1-year类型\",\"contents\":[\"YEAR类型用来表示年份，在所有的日期时间类型中所占用的存储空间最小，只需要1个字节的存储空间。\",\"在MySQL中，YEAR有以下几种存储格式：\",\"以4位字符串或数字格式表示YEAR类型，其格式为YYYY，最小值为1901，最大值为2155。\",\"以2位字符串格式表示YEAR类型，最小值为00，最大值为99。 \",\"当取值为01到69时，表示2001到2069；\",\"当取值为70到99时，表示1970到1999；\",\"当取值整数的0或00添加的话，那么是0000年；\",\"当取值是日期/字符串的'0'添加的话，是2000年。\",\"从MySQL5.5.27开始，2位格式的YEAR已经不推荐使用。YEAR默认格式就是“YYYY”，没必要写成YEAR(4)，从MySQL 8.0.19开始，不推荐使用指定显示宽度的YEAR(4)数据类型。\",\"CREATE TABLE test_year( f1 YEAR, f2 YEAR(4) ); \",\"mysql> DESC test_year; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | f1 | year(4) | YES | | NULL | | | f2 | year(4) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 2 rows in set (0.00 sec) \",\"INSERT INTO test_year VALUES('2020','2021'); mysql> SELECT * FROM test_year; +------+------+ | f1 | f2 | +------+------+ | 2020 | 2021 | +------+------+ 1 rows in set (0.00 sec) \",\"INSERT INTO test_year VALUES('45','71'); INSERT INTO test_year VALUES(0,'0'); mysql> SELECT * FROM test_year; +------+------+ | f1 | f2 | +------+------+ | 2020 | 2021 | | 2045 | 1971 | | 0000 | 2000 | +------+------+ 3 rows in set (0.00 sec) \"]},{\"header\":\"6.2 DATE类型\",\"slug\":\"_6-2-date类型\",\"contents\":[\"DATE类型表示日期，没有时间部分，格式为YYYY-MM-DD，其中，YYYY表示年份，MM表示月份，DD表示日期。需要3个字节的存储空间。在向DATE类型的字段插入数据时，同样需要满足一定的格式条件。\",\"以YYYY-MM-DD格式或者YYYYMMDD格式表示的字符串日期，其最小取值为1000-01-01，最大取值为9999-12-03。YYYYMMDD格式会被转化为YYYY-MM-DD格式。\",\"以YY-MM-DD格式或者YYMMDD格式表示的字符串日期，此格式中，年份为两位数值或字符串满足YEAR类型的格式条件为：当年份取值为00到69时，会被转化为2000到2069；当年份取值为70到99时，会被转化为1970到1999。\",\"使用CURRENT_DATE()或者NOW()函数，会插入当前系统的日期。\",\"举例：\",\"创建数据表，表中只包含一个DATE类型的字段f1。\",\"CREATE TABLE test_date1( f1 DATE ); Query OK, 0 rows affected (0.13 sec) \",\"插入数据：\",\"INSERT INTO test_date1 VALUES ('2020-10-01'), ('20201001'),(20201001); INSERT INTO test_date1 VALUES ('00-01-01'), ('000101'), ('69-10-01'), ('691001'), ('70-01-01'), ('700101'), ('99-01-01'), ('990101'); INSERT INTO test_date1 VALUES (000301), (690301), (700301), (990301); INSERT INTO test_date1 VALUES (CURRENT_DATE()), (NOW()); SELECT * FROM test_date1; \"]},{\"header\":\"6.3 TIME类型\",\"slug\":\"_6-3-time类型\",\"contents\":[\"TIME类型用来表示时间，不包含日期部分。在MySQL中，需要3个字节的存储空间来存储TIME类型的数据，可以使用“HH:MM:SS”格式来表示TIME类型，其中，HH表示小时，MM表示分钟，SS表示秒。\",\"在MySQL中，向TIME类型的字段插入数据时，也可以使用几种不同的格式。 （1）可以使用带有冒号的字符串，比如'D HH:MM:SS'、'HH:MM:SS'、'HH:MM'、'D HH:MM'、'D HH'或'SS'格式，都能被正确地插入TIME类型的字段中。其中D表示天，其最小值为0，最大值为34。如果使用带有D格式的字符串插入TIME类型的字段时，D会被转化为小时，计算格式为D*24+HH。当使用带有冒号并且不带D的字符串表示时间时，表示当天的时间，比如12:10表示12:10:00，而不是00:12:10。 （2）可以使用不带有冒号的字符串或者数字，格式为'HHMMSS'或者HHMMSS。如果插入一个不合法的字符串或者数字，MySQL在存储数据时，会将其自动转化为00:00:00进行存储。比如1210，MySQL会将最右边的两位解析成秒，表示00:12:10，而不是12:10:00。 （3）使用CURRENT_TIME()或者NOW()，会插入当前系统的时间。\",\"举例：\",\"创建数据表，表中包含一个TIME类型的字段f1。\",\"CREATE TABLE test_time1( f1 TIME ); Query OK, 0 rows affected (0.02 sec) \",\"INSERT INTO test_time1 VALUES('2 12:30:29'), ('12:35:29'), ('12:40'), ('2 12:40'),('1 05'), ('45'); INSERT INTO test_time1 VALUES ('123520'), (124011),(1210); INSERT INTO test_time1 VALUES (NOW()), (CURRENT_TIME()); SELECT * FROM test_time1; \"]},{\"header\":\"6.4 DATETIME类型\",\"slug\":\"_6-4-datetime类型\",\"contents\":[\"DATETIME类型在所有的日期时间类型中占用的存储空间最大，总共需要8个字节的存储空间。在格式上为DATE类型和TIME类型的组合，可以表示为YYYY-MM-DD HH:MM:SS，其中YYYY表示年份，MM表示月份，DD表示日期，HH表示小时，MM表示分钟，SS表示秒。\",\"在向DATETIME类型的字段插入数据时，同样需要满足一定的格式条件。\",\"以YYYY-MM-DD HH:MM:SS格式或者YYYYMMDDHHMMSS格式的字符串插入DATETIME类型的字段时，最小值为1000-01-01 00:00:00，最大值为9999-12-03 23:59:59。 \",\"以YYYYMMDDHHMMSS格式的数字插入DATETIME类型的字段时，会被转化为YYYY-MM-DD HH:MM:SS格式。\",\"以YY-MM-DD HH:MM:SS格式或者YYMMDDHHMMSS格式的字符串插入DATETIME类型的字段时，两位数的年份规则符合YEAR类型的规则，00到69表示2000到2069；70到99表示1970到1999。\",\"使用函数CURRENT_TIMESTAMP()和NOW()，可以向DATETIME类型的字段插入系统的当前日期和时间。\",\"举例：\",\"创建数据表，表中包含一个DATETIME类型的字段dt。\",\"CREATE TABLE test_datetime1( dt DATETIME ); Query OK, 0 rows affected (0.02 sec) \",\"插入数据：\",\"INSERT INTO test_datetime1 VALUES ('2021-01-01 06:50:30'), ('20210101065030'); INSERT INTO test_datetime1 VALUES ('99-01-01 00:00:00'), ('990101000000'), ('20-01-01 00:00:00'), ('200101000000'); INSERT INTO test_datetime1 VALUES (20200101000000), (200101000000), (19990101000000), (990101000000); INSERT INTO test_datetime1 VALUES (CURRENT_TIMESTAMP()), (NOW()); \"]},{\"header\":\"6.5 TIMESTAMP类型\",\"slug\":\"_6-5-timestamp类型\",\"contents\":[\"TIMESTAMP类型也可以表示日期时间，其显示格式与DATETIME类型相同，都是YYYY-MM-DD HH:MM:SS，需要4个字节的存储空间。但是TIMESTAMP存储的时间范围比DATETIME要小很多，只能存储“1970-01-01 00:00:01 UTC”到“2038-01-19 03:14:07 UTC”之间的时间。其中，UTC表示世界统一时间，也叫作世界标准时间。\",\"存储数据的时候需要对当前时间所在的时区进行转换，查询数据的时候再将时间转换回当前的时区。因此，使用TIMESTAMP存储的同一个时间值，在不同的时区查询时会显示不同的时间。\",\"向TIMESTAMP类型的字段插入数据时，当插入的数据格式满足YY-MM-DD HH:MM:SS和YYMMDDHHMMSS时，两位数值的年份同样符合YEAR类型的规则条件，只不过表示的时间范围要小很多。\",\"如果向TIMESTAMP类型的字段插入的时间超出了TIMESTAMP类型的范围，则MySQL会抛出错误信息。\",\"举例：\",\"创建数据表，表中包含一个TIMESTAMP类型的字段ts。\",\"CREATE TABLE test_timestamp1( ts TIMESTAMP ); \",\"插入数据：\",\"INSERT INTO test_timestamp1 VALUES ('1999-01-01 03:04:50'), ('19990101030405'), ('99-01-01 03:04:05'), ('990101030405'); INSERT INTO test_timestamp1 VALUES ('2020@01@01@00@00@00'), ('20@01@01@00@00@00'); INSERT INTO test_timestamp1 VALUES (CURRENT_TIMESTAMP()), (NOW()); #Incorrect datetime value INSERT INTO test_timestamp1 VALUES ('2038-01-20 03:14:07'); \",\"TIMESTAMP和DATETIME的区别：\",\"TIMESTAMP存储空间比较小，表示的日期时间范围也比较小\",\"底层存储方式不同，TIMESTAMP底层存储的是毫秒值，距离1970-1-1 0:0:0 0毫秒的毫秒值。\",\"两个日期比较大小或日期计算时，TIMESTAMP更方便、更快。\",\"TIMESTAMP和时区有关。TIMESTAMP会根据用户的时区不同，显示不同的结果。而DATETIME则只能反映出插入时当地的时区，其他时区的人查看数据必然会有误差的。\",\"CREATE TABLE temp_time( d1 DATETIME, d2 TIMESTAMP ); \",\"INSERT INTO temp_time VALUES('2021-9-2 14:45:52','2021-9-2 14:45:52'); INSERT INTO temp_time VALUES(NOW(),NOW()); \",\"mysql> SELECT * FROM temp_time; +---------------------+---------------------+ | d1 | d2 | +---------------------+---------------------+ | 2021-09-02 14:45:52 | 2021-09-02 14:45:52 | | 2021-11-03 17:38:17 | 2021-11-03 17:38:17 | +---------------------+---------------------+ 2 rows in set (0.00 sec) \",\"#修改当前的时区 SET time_zone = '+9:00'; \",\"mysql> SELECT * FROM temp_time; +---------------------+---------------------+ | d1 | d2 | +---------------------+---------------------+ | 2021-09-02 14:45:52 | 2021-09-02 15:45:52 | | 2021-11-03 17:38:17 | 2021-11-03 18:38:17 | +---------------------+---------------------+ 2 rows in set (0.00 sec) \"]},{\"header\":\"6.6 开发中经验\",\"slug\":\"_6-6-开发中经验\",\"contents\":[\"用得最多的日期时间类型，就是 DATETIME。虽然 MySQL 也支持 YEAR（年）、 TIME（时间）、DATE（日期），以及 TIMESTAMP 类型，但是在实际项目中，尽量用 DATETIME 类型。因为这个数据类型包括了完整的日期和时间信息，取值范围也最大，使用起来比较方便。毕竟，如果日期时间信息分散在好几个字段，很不容易记，而且查询的时候，SQL 语句也会更加复杂。\",\"此外，一般存注册时间、商品发布时间等，不建议使用DATETIME存储，而是使用时间戳，因为DATETIME虽然直观，但不便于计算。\",\"mysql> SELECT UNIX_TIMESTAMP(); +------------------+ | UNIX_TIMESTAMP() | +------------------+ | 1635932762 | +------------------+ 1 row in set (0.00 sec) \"]},{\"header\":\"7. 文本字符串类型\",\"slug\":\"_7-文本字符串类型\",\"contents\":[\"在实际的项目中，我们还经常遇到一种数据，就是字符串数据。\",\"MySQL中，文本字符串总体上分为CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT、ENUM、SET等类型。\"]},{\"header\":\"7.1 CHAR与VARCHAR类型\",\"slug\":\"_7-1-char与varchar类型\",\"contents\":[\"CHAR和VARCHAR类型都可以存储比较短的字符串。\",\"字符串(文本)类型\",\"特点\",\"长度\",\"长度范围\",\"占用的存储空间\",\"CHAR(M)\",\"固定长度\",\"M\",\"0 <= M <= 255\",\"M个字节\",\"VARCHAR(M)\",\"可变长度\",\"M\",\"0 <= M <= 65535\",\"(实际长度 + 1) 个字节\",\"CHAR类型：\",\"CHAR(M) 类型一般需要预先定义字符串长度。如果不指定(M)，则表示长度默认是1个字符。\",\"如果保存时，数据的实际长度比CHAR类型声明的长度小，则会在右侧填充空格以达到指定的长度。当MySQL检索CHAR类型的数据时，CHAR类型的字段会去除尾部的空格。\",\"定义CHAR类型字段时，声明的字段长度即为CHAR类型字段所占的存储空间的字节数。\",\"CREATE TABLE test_char1( c1 CHAR, c2 CHAR(5) ); DESC test_char1; \",\"INSERT INTO test_char1 VALUES('a','Tom'); SELECT c1,CONCAT(c2,'***') FROM test_char1; \",\"INSERT INTO test_char1(c2) VALUES('a '); SELECT CHAR_LENGTH(c2) FROM test_char1; \",\"VARCHAR类型：\",\"VARCHAR(M) 定义时，必须指定长度M，否则报错。\",\"MySQL4.0版本以下，varchar(20)：指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） ；MySQL5.0版本以上，varchar(20)：指的是20字符。\",\"检索VARCHAR类型的字段数据时，会保留数据尾部的空格。VARCHAR类型的字段所占用的存储空间为字符串实际长度加1个字节。\",\"CREATE TABLE test_varchar1( NAME VARCHAR #错误 ); \",\"#Column length too big for column 'NAME' (max = 21845); CREATE TABLE test_varchar2( NAME VARCHAR(65535) #错误 ); \",\"CREATE TABLE test_varchar3( NAME VARCHAR(5) ); INSERT INTO test_varchar3 VALUES('尚硅谷'),('尚硅谷教育'); #Data too long for column 'NAME' at row 1 INSERT INTO test_varchar3 VALUES('尚硅谷IT教育'); \",\"哪些情况使用 CHAR 或 VARCHAR 更好\",\"类型\",\"特点\",\"空间上\",\"时间上\",\"适用场景\",\"CHAR(M)\",\"固定长度\",\"浪费存储空间\",\"效率高\",\"存储不大，速度要求高\",\"VARCHAR(M)\",\"可变长度\",\"节省存储空间\",\"效率低\",\"非CHAR的情况\",\"情况1：存储很短的信息。比如门牌号码101，201……这样很短的信息应该用char，因为varchar还要占个byte用于存储信息长度，本来打算节约存储的，结果得不偿失。\",\"情况2：固定长度的。比如使用uuid作为主键，那用char应该更合适。因为他固定长度，varchar动态根据长度的特性就消失了，而且还要占个长度信息。\",\"情况3：十分频繁改变的column。因为varchar每次存储都要有额外的计算，得到长度等工作，如果一个非常频繁改变的，那就要有很多的精力用于计算，而这些对于char来说是不需要的。\",\"情况4：具体存储引擎中的情况：\",\"MyISAM 数据存储引擎和数据列：MyISAM数据表，最好使用固定长度(CHAR)的数据列代替可变长度(VARCHAR)的数据列。这样使得整个表静态化，从而使数据检索更快，用空间换时间。\",\"MEMORY 存储引擎和数据列：MEMORY数据表目前都使用固定长度的数据行存储，因此无论使用CHAR或VARCHAR列都没有关系，两者都是作为CHAR类型处理的。\",\"InnoDB存储引擎，建议使用VARCHAR类型。因为对于InnoDB数据表，内部的行存储格式并没有区分固定长度和可变长度列（所有数据行都使用指向数据列值的头指针），而且主要影响性能的因素是数据行使用的存储总量，由于char平均占用的空间多于varchar，所以除了简短并且固定长度的，其他考虑varchar。这样节省空间，对磁盘I/O和数据存储总量比较好。\"]},{\"header\":\"7.2 TEXT类型\",\"slug\":\"_7-2-text类型\",\"contents\":[\"在MySQL中，TEXT用来保存文本类型的字符串，总共包含4种类型，分别为TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT 类型。\",\"在向TEXT类型的字段保存和查询数据时，系统自动按照实际长度存储，不需要预先定义长度。这一点和 VARCHAR类型相同。\",\"每种TEXT类型保存的数据长度和所占用的存储空间不同，如下：\",\"文本字符串类型\",\"特点\",\"长度\",\"长度范围\",\"占用的存储空间\",\"TINYTEXT\",\"小文本、可变长度\",\"L\",\"0 <= L <= 255\",\"L + 2 个字节\",\"TEXT\",\"文本、可变长度\",\"L\",\"0 <= L <= 65535\",\"L + 2 个字节\",\"MEDIUMTEXT\",\"中等文本、可变长度\",\"L\",\"0 <= L <= 16777215\",\"L + 3 个字节\",\"LONGTEXT\",\"大文本、可变长度\",\"L\",\"0 <= L<= 4294967295（相当于4GB）\",\"L + 4 个字节\",\"由于实际存储的长度不确定，MySQL 不允许 TEXT 类型的字段做主键。遇到这种情况，你只能采用 CHAR(M)，或者 VARCHAR(M)。\",\"举例：\",\"创建数据表：\",\"CREATE TABLE test_text( tx TEXT ); \",\"INSERT INTO test_text VALUES('atguigu '); SELECT CHAR_LENGTH(tx) FROM test_text; #10 \",\"说明在保存和查询数据时，并没有删除TEXT类型的数据尾部的空格。\",\"开发中经验：\",\"TEXT文本类型，可以存比较大的文本段，搜索速度稍慢，因此如果不是特别大的内容，建议使用CHAR，VARCHAR来代替。还有TEXT类型不用加默认值，加了也没用。而且text和blob类型的数据删除后容易导致“空洞”，使得文件碎片比较多，所以频繁使用的表不建议包含TEXT类型字段，建议单独分出去，单独用一个表。\"]},{\"header\":\"8. ENUM类型\",\"slug\":\"_8-enum类型\",\"contents\":[\"ENUM类型也叫作枚举类型，ENUM类型的取值范围需要在定义字段时进行指定。设置字段值时，ENUM类型只允许从成员中选取单个值，不能一次选取多个值。\",\"其所需要的存储空间由定义ENUM类型时指定的成员个数决定。\",\"文本字符串类型\",\"长度\",\"长度范围\",\"占用的存储空间\",\"ENUM\",\"L\",\"1 <= L <= 65535\",\"1或2个字节\",\"当ENUM类型包含1～255个成员时，需要1个字节的存储空间；\",\"当ENUM类型包含256～65535个成员时，需要2个字节的存储空间。\",\"ENUM类型的成员个数的上限为65535个。\",\"举例：\",\"创建表如下：\",\"CREATE TABLE test_enum( season ENUM('春','夏','秋','冬','unknow') ); \",\"添加数据：\",\"INSERT INTO test_enum VALUES('春'),('秋'); ## 忽略大小写 INSERT INTO test_enum VALUES('UNKNOW'); ## 允许按照角标的方式获取指定索引位置的枚举值 INSERT INTO test_enum VALUES('1'),(3); ## Data truncated for column 'season' at row 1 INSERT INTO test_enum VALUES('ab'); ## 当ENUM类型的字段没有声明为NOT NULL时，插入NULL也是有效的 INSERT INTO test_enum VALUES(NULL); \"]},{\"header\":\"9. SET类型\",\"slug\":\"_9-set类型\",\"contents\":[\"SET表示一个字符串对象，可以包含0个或多个成员，但成员个数的上限为64。设置字段值时，可以取取值范围内的 0 个或多个值。\",\"当SET类型包含的成员个数不同时，其所占用的存储空间也是不同的，具体如下：\",\"成员个数范围（L表示实际成员个数）\",\"占用的存储空间\",\"1 <= L <= 8\",\"1个字节\",\"9 <= L <= 16\",\"2个字节\",\"17 <= L <= 24\",\"3个字节\",\"25 <= L <= 32\",\"4个字节\",\"33 <= L <= 64\",\"8个字节\",\"SET类型在存储数据时成员个数越多，其占用的存储空间越大。注意：SET类型在选取成员时，可以一次选择多个成员，这一点与ENUM类型不同。\",\"举例：\",\"创建表：\",\"CREATE TABLE test_set( s SET ('A', 'B', 'C') ); \",\"向表中插入数据：\",\"INSERT INTO test_set (s) VALUES ('A'), ('A,B'); #插入重复的SET类型成员时，MySQL会自动删除重复的成员 INSERT INTO test_set (s) VALUES ('A,B,C,A'); #向SET类型的字段插入SET成员中不存在的值时，MySQL会抛出错误。 INSERT INTO test_set (s) VALUES ('A,B,C,D'); SELECT * FROM test_set; \",\"举例：\",\"CREATE TABLE temp_mul( gender ENUM('男','女'), hobby SET('吃饭','睡觉','打豆豆','写代码') ); \",\"INSERT INTO temp_mul VALUES('男','睡觉,打豆豆'); #成功 ## Data truncated for column 'gender' at row 1 INSERT INTO temp_mul VALUES('男,女','睡觉,写代码'); #失败 ## Data truncated for column 'gender' at row 1 INSERT INTO temp_mul VALUES('妖','睡觉,写代码');#失败 INSERT INTO temp_mul VALUES('男','睡觉,写代码,吃饭'); #成功 \"]},{\"header\":\"10. 二进制字符串类型\",\"slug\":\"_10-二进制字符串类型\",\"contents\":[\"MySQL中的二进制字符串类型主要存储一些二进制数据，比如可以存储图片、音频和视频等二进制数据。\",\"MySQL中支持的二进制字符串类型主要包括BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB类型。\"]},{\"header\":\"BINARY与VARBINARY类型\",\"slug\":\"binary与varbinary类型\",\"contents\":[\"BINARY和VARBINARY类似于CHAR和VARCHAR，只是它们存储的是二进制字符串。\",\"BINARY (M)为固定长度的二进制字符串，M表示最多能存储的字节数，取值范围是0~255个字符。如果未指定(M)，表示只能存储1个字节。例如BINARY (8)，表示最多能存储8个字节，如果字段值不足(M)个字节，将在右边填充'\\\\0'以补齐指定长度。\",\"VARBINARY (M)为可变长度的二进制字符串，M表示最多能存储的字节数，总字节数不能超过行的字节长度限制65535，另外还要考虑额外字节开销，VARBINARY类型的数据除了存储数据本身外，还需要1或2个字节来存储数据的字节数。VARBINARY类型必须指定(M)，否则报错。\",\"二进制字符串类型\",\"特点\",\"值的长度\",\"占用空间\",\"BINARY(M)\",\"固定长度\",\"M （0 <= M <= 255）\",\"M个字节\",\"VARBINARY(M)\",\"可变长度\",\"M（0 <= M <= 65535）\",\"M+1个字节\",\"举例：\",\"创建表：\",\"CREATE TABLE test_binary1( f1 BINARY, f2 BINARY(3), ## f3 VARBINARY, f4 VARBINARY(10) ); \",\"添加数据：\",\"INSERT INTO test_binary1(f1,f2) VALUES('a','a'); INSERT INTO test_binary1(f1,f2) VALUES('尚','尚');#失败 \",\"INSERT INTO test_binary1(f2,f4) VALUES('ab','ab'); mysql> SELECT LENGTH(f2),LENGTH(f4) -> FROM test_binary1; +------------+------------+ | LENGTH(f2) | LENGTH(f4) | +------------+------------+ | 3 | NULL | | 3 | 2 | +------------+------------+ 2 rows in set (0.00 sec) \"]},{\"header\":\"BLOB类型\",\"slug\":\"blob类型\",\"contents\":[\"BLOB是一个二进制大对象，可以容纳可变数量的数据。\",\"MySQL中的BLOB类型包括TINYBLOB、BLOB、MEDIUMBLOB和LONGBLOB 4种类型，它们可容纳值的最大长度不同。可以存储一个二进制的大对象，比如图片、音频和视频等。\",\"需要注意的是，在实际工作中，往往不会在MySQL数据库中使用BLOB类型存储大对象数据，通常会将图片、音频和视频文件存储到服务器的磁盘上，并将图片、音频和视频的访问路径存储到MySQL中。\",\"二进制字符串类型\",\"值的长度\",\"长度范围\",\"占用空间\",\"TINYBLOB\",\"L\",\"0 <= L <= 255\",\"L + 1 个字节\",\"BLOB\",\"L\",\"0 <= L <= 65535（相当于64KB）\",\"L + 2 个字节\",\"MEDIUMBLOB\",\"L\",\"0 <= L <= 16777215 （相当于16MB）\",\"L + 3 个字节\",\"LONGBLOB\",\"L\",\"0 <= L <= 4294967295（相当于4GB）\",\"L + 4 个字节\",\"举例：\",\"CREATE TABLE test_blob1( id INT, img MEDIUMBLOB ); \",\"TEXT和BLOB的使用注意事项：\",\"在使用text和blob字段类型时要注意以下几点，以便更好的发挥数据库的性能。\",\"① BLOB和TEXT值也会引起自己的一些问题，特别是执行了大量的删除或更新操作的时候。删除这种值会在数据表中留下很大的\\\"空洞\\\"，以后填入这些\\\"空洞\\\"的记录可能长度不同。为了提高性能，建议定期使用 OPTIMIZE TABLE 功能对这类表进行碎片整理。\",\"② 如果需要对大文本字段进行模糊查询，MySQL 提供了前缀索引。但是仍然要在不必要的时候避免检索大型的BLOB或TEXT值。例如，SELECT * 查询就不是很好的想法，除非你能够确定作为约束条件的WHERE子句只会找到所需要的数据行。否则，你可能毫无目的地在网络上传输大量的值。\",\"③ 把BLOB或TEXT列分离到单独的表中。在某些环境中，如果把这些数据列移动到第二张数据表中，可以让你把原数据表中的数据列转换为固定长度的数据行格式，那么它就是有意义的。这会减少主表中的碎片，使你得到固定长度数据行的性能优势。它还使你在主数据表上运行 SELECT * 查询的时候不会通过网络传输大量的BLOB或TEXT值。\"]},{\"header\":\"11. JSON 类型\",\"slug\":\"_11-json-类型\",\"contents\":[\"JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。它易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。JSON 可以将 JavaScript 对象中表示的一组数据转换为字符串，然后就可以在网络或者程序之间轻松地传递这个字符串，并在需要的时候将它还原为各编程语言所支持的数据格式。\",\"在MySQL 5.7中，就已经支持JSON数据类型。在MySQL 8.x版本中，JSON类型提供了可以进行自动验证的JSON文档和优化的存储结构，使得在MySQL中存储和读取JSON类型的数据更加方便和高效。 创建数据表，表中包含一个JSON类型的字段 js 。\",\"CREATE TABLE test_json( js json ); \",\"向表中插入JSON数据。\",\"INSERT INTO test_json (js) VALUES ('{\\\"name\\\":\\\"songhk\\\", \\\"age\\\":18, \\\"address\\\":{\\\"province\\\":\\\"beijing\\\", \\\"city\\\":\\\"beijing\\\"}}'); \",\"查询t19表中的数据。\",\"mysql> SELECT * -> FROM test_json; \",\"当需要检索JSON类型的字段中数据的某个具体值时，可以使用“->”和“->>”符号。\",\"mysql> SELECT js -> '$.name' AS NAME,js -> '$.age' AS age ,js -> '$.address.province' AS province, js -> '$.address.city' AS city -> FROM test_json; +----------+------+-----------+-----------+ | NAME | age | province | city | +----------+------+-----------+-----------+ | \\\"songhk\\\" | 18 | \\\"beijing\\\" | \\\"beijing\\\" | +----------+------+-----------+-----------+ 1 row in set (0.00 sec) \",\"通过“->”和“->>”符号，从JSON字段中正确查询出了指定的JSON数据的值。\"]},{\"header\":\"12. 空间类型\",\"slug\":\"_12-空间类型\",\"contents\":[\"MySQL 空间类型扩展支持地理特征的生成、存储和分析。这里的地理特征表示世界上具有位置的任何东西，可以是一个实体，例如一座山；可以是空间，例如一座办公楼；也可以是一个可定义的位置，例如一个十字路口等等。MySQL中使用Geometry（几何）来表示所有地理特征。Geometry指一个点或点的集合，代表世界上任何具有位置的事物。\",\"MySQL的空间数据类型（Spatial Data Type）对应于OpenGIS类，包括单值类型：GEOMETRY、POINT、LINESTRING、POLYGON以及集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION 。\",\"Geometry是所有空间集合类型的基类，其他类型如POINT、LINESTRING、POLYGON都是Geometry的子类。 \",\"Point，顾名思义就是点，有一个坐标值。例如POINT(121.213342 31.234532)，POINT(30 10)，坐标值支持DECIMAL类型，经度（longitude）在前，维度（latitude）在后，用空格分隔。\",\"LineString，线，由一系列点连接而成。如果线从头至尾没有交叉，那就是简单的（simple）；如果起点和终点重叠，那就是封闭的（closed）。例如LINESTRING(30 10,10 30,40 40)，点与点之间用逗号分隔，一个点中的经纬度用空格分隔，与POINT格式一致。\",\"Polygon，多边形。可以是一个实心平面形，即没有内部边界，也可以有空洞，类似纽扣。最简单的就是只有一个外边界的情况，例如POLYGON((0 0,10 0,10 10, 0 10))。\",\"下面展示几种常见的几何图形元素：\",\"MultiPoint、MultiLineString、MultiPolygon、GeometryCollection 这4种类型都是集合类，是多个Point、LineString或Polygon组合而成。\",\"下面展示的是多个同类或异类几何图形元素的组合：\"]},{\"header\":\"13. 小结及选择建议\",\"slug\":\"_13-小结及选择建议\",\"contents\":[\"在定义数据类型时，如果确定是整数，就用 INT； 如果是小数，一定用定点数类型 DECIMAL(M,D)； 如果是日期与时间，就用 DATETIME。\",\"这样做的好处是，首先确保你的系统不会因为数据类型定义出错。不过，凡事都是有两面的，可靠性好，并不意味着高效。比如，TEXT 虽然使用方便，但是效率不如 CHAR(M) 和 VARCHAR(M)。\",\"关于字符串的选择，建议参考如下阿里巴巴的《Java开发手册》规范：\",\"阿里巴巴《Java开发手册》之MySQL数据库：\",\"任何字段如果为非负数，必须是 UNSIGNED\",\"【强制】小数类型为 DECIMAL，禁止使用 FLOAT 和 DOUBLE。 \",\"说明：在存储的时候，FLOAT 和 DOUBLE 都存在精度损失的问题，很可能在比较值的时候，得到不正确的结果。如果存储的数据范围超过 DECIMAL 的范围，建议将数据拆成整数和小数并分开存储。\",\"【强制】如果存储的字符串长度几乎相等，使用 CHAR 定长字符串类型。\",\"【强制】VARCHAR 是可变长字符串，不预先分配存储空间，长度不要超过 5000。如果存储长度大于此值，定义字段类型为 TEXT，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\"]}]},\"/study-tutorial/database/mysql/view.html\":{\"title\":\"14、视图\",\"contents\":[{\"header\":\"1. 常见的数据库对象\",\"slug\":\"_1-常见的数据库对象\",\"contents\":[\"对象\",\"描述\",\"表(TABLE)\",\"表是存储数据的逻辑单元，以行和列的形式存在，列就是字段，行就是记录\",\"数据字典\",\"就是系统表，存放数据库相关信息的表。系统表的数据通常由数据库系统维护，程序员通常不应该修改，只可查看\",\"约束(CONSTRAINT)\",\"执行数据校验的规则，用于保证数据完整性的规则\",\"视图(VIEW)\",\"一个或者多个数据表里的数据的逻辑显示，视图并不存储数据\",\"索引(INDEX)\",\"用于提高查询性能，相当于书的目录\",\"存储过程(PROCEDURE)\",\"用于完成一次完整的业务处理，没有返回值，但可通过传出参数将多个值传给调用环境\",\"存储函数(FUNCTION)\",\"用于完成一次特定的计算，具有一个返回值\",\"触发器(TRIGGER)\",\"相当于一个事件监听器，当数据库发生特定事件后，触发器被触发，完成相应的处理\"]},{\"header\":\"2. 视图概述\",\"slug\":\"_2-视图概述\",\"contents\":[]},{\"header\":\"2.1 为什么使用视图？\",\"slug\":\"_2-1-为什么使用视图\",\"contents\":[\"视图一方面可以帮我们使用表的一部分而不是所有的表，另一方面也可以针对不同的用户制定不同的查询视图。比如，针对一个公司的销售人员，我们只想给他看部分数据，而某些特殊的数据，比如采购的价格，则不会提供给他。再比如，人员薪酬是个敏感的字段，那么只给某个级别以上的人员开放，其他人的查询视图中则不提供这个字段。\",\"刚才讲的只是视图的一个使用场景，实际上视图还有很多作用。最后，我们总结视图的优点。\"]},{\"header\":\"2.2 视图的理解\",\"slug\":\"_2-2-视图的理解\",\"contents\":[\"视图是一种虚拟表，本身是不具有数据的，占用很少的内存空间，它是 SQL 中的一个重要概念。\",\"视图建立在已有表的基础上, 视图赖以建立的这些表称为基表。\",\"视图的创建和删除只影响视图本身，不影响对应的基表。但是当对视图中的数据进行增加、删除和修改操作时，数据表中的数据会相应地发生变化，反之亦然。\",\"向视图提供数据内容的语句为 SELECT 语句, 可以将视图理解为存储起来的SELECT语句\",\"在数据库中，视图不会保存数据，数据真正保存在数据表中。当对视图中的数据进行增加、删除和修改操作时，数据表中的数据会相应地发生变化；反之亦然。\",\"视图，是向用户提供基表数据的另一种表现形式。通常情况下，小型项目的数据库可以不使用视图，但是在大型项目中，以及数据表比较复杂的情况下，视图的价值就凸显出来了，它可以帮助我们把经常查询的结果集放到虚拟表中，提升使用效率。理解和使用起来都非常方便。\"]},{\"header\":\"3. 创建视图\",\"slug\":\"_3-创建视图\",\"contents\":[\"在CREATE VIEW语句中嵌入子查询\",\"CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW 视图名称 [(字段列表)] AS 查询语句 [WITH [CASCADED|LOCAL] CHECK OPTION] \",\"精简版\",\"CREATE VIEW 视图名称 AS 查询语句 \"]},{\"header\":\"3.1 创建单表视图\",\"slug\":\"_3-1-创建单表视图\",\"contents\":[\"举例：\",\"CREATE VIEW empvu80 AS SELECT employee_id, last_name, salary FROM employees WHERE department_id = 80; \",\"查询视图：\",\"SELECT * FROM salvu80; \",\"举例：\",\"CREATE VIEW emp_year_salary (ename,year_salary) AS SELECT ename,salary*12*(1+IFNULL(commission_pct,0)) FROM t_employee; \",\"举例：\",\"CREATE VIEW salvu50 AS SELECT employee_id ID_NUMBER, last_name NAME,salary*12 ANN_SALARY FROM employees WHERE department_id = 50; \",\"说明1：实际上就是我们在 SQL 查询语句的基础上封装了视图 VIEW，这样就会基于 SQL 语句的结果集形成一张虚拟表。\",\"说明2：在创建视图时，没有在视图名后面指定字段列表，则视图中字段列表默认和SELECT语句中的字段列表一致。如果SELECT语句中给字段取了别名，那么视图中的字段名和别名相同。\"]},{\"header\":\"3.2 创建多表联合视图\",\"slug\":\"_3-2-创建多表联合视图\",\"contents\":[\"举例：\",\"CREATE VIEW empview AS SELECT employee_id emp_id,last_name NAME,department_name FROM employees e,departments d WHERE e.department_id = d.department_id; \",\"CREATE VIEW emp_dept AS SELECT ename,dname FROM t_employee LEFT JOIN t_department ON t_employee.did = t_department.did; \",\"CREATE VIEW dept_sum_vu (name, minsal, maxsal, avgsal) AS SELECT d.department_name, MIN(e.salary), MAX(e.salary),AVG(e.salary) FROM employees e, departments d WHERE e.department_id = d.department_id GROUP BY d.department_name; \",\"利用视图对数据进行格式化\",\"我们经常需要输出某个格式的内容，比如我们想输出员工姓名和对应的部门名，对应格式为 emp_name(department_name)，就可以使用视图来完成数据格式化的操作：\",\"CREATE VIEW emp_depart AS SELECT CONCAT(last_name,'(',department_name,')') AS emp_dept FROM employees e JOIN departments d WHERE e.department_id = d.department_id \"]},{\"header\":\"3.3 基于视图创建视图\",\"slug\":\"_3-3-基于视图创建视图\",\"contents\":[\"当我们创建好一张视图之后，还可以在它的基础上继续创建视图。\",\"举例：联合“emp_dept”视图和“emp_year_salary”视图查询员工姓名、部门名称、年薪信息创建 “emp_dept_ysalary”视图。\",\"CREATE VIEW emp_dept_ysalary AS SELECT emp_dept.ename,dname,year_salary FROM emp_dept INNER JOIN emp_year_salary ON emp_dept.ename = emp_year_salary.ename; \"]},{\"header\":\"4. 查看视图\",\"slug\":\"_4-查看视图\",\"contents\":[\"语法1：查看数据库的表对象、视图对象\",\"SHOW TABLES; \",\"语法2：查看视图的结构\",\"DESC / DESCRIBE 视图名称; \",\"语法3：查看视图的属性信息\",\"## 查看视图信息（显示数据表的存储引擎、版本、数据行数和数据大小等） SHOW TABLE STATUS LIKE '视图名称'\\\\G \",\"执行结果显示，注释Comment为VIEW，说明该表为视图，其他的信息为NULL，说明这是一个虚表。\",\"语法4：查看视图的详细定义信息\",\"SHOW CREATE VIEW 视图名称; \"]},{\"header\":\"5. 更新视图的数据\",\"slug\":\"_5-更新视图的数据\",\"contents\":[]},{\"header\":\"5.1 一般情况\",\"slug\":\"_5-1-一般情况\",\"contents\":[\"MySQL支持使用INSERT、UPDATE和DELETE语句对视图中的数据进行插入、更新和删除操作。当视图中的数据发生变化时，数据表中的数据也会发生变化，反之亦然。\",\"举例：UPDATE操作\",\"mysql> SELECT ename,tel FROM emp_tel WHERE ename = '孙洪亮'; +---------+-------------+ | ename | tel | +---------+-------------+ | 孙洪亮 | 13789098765 | +---------+-------------+ 1 row in set (0.01 sec) mysql> UPDATE emp_tel SET tel = '13789091234' WHERE ename = '孙洪亮'; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql> SELECT ename,tel FROM emp_tel WHERE ename = '孙洪亮'; +---------+-------------+ | ename | tel | +---------+-------------+ | 孙洪亮 | 13789091234 | +---------+-------------+ 1 row in set (0.00 sec) mysql> SELECT ename,tel FROM t_employee WHERE ename = '孙洪亮'; +---------+-------------+ | ename | tel | +---------+-------------+ | 孙洪亮 | 13789091234 | +---------+-------------+ 1 row in set (0.00 sec) \",\"举例：DELETE操作\",\"mysql> SELECT ename,tel FROM emp_tel WHERE ename = '孙洪亮'; +---------+-------------+ | ename | tel | +---------+-------------+ | 孙洪亮 | 13789091234 | +---------+-------------+ 1 row in set (0.00 sec) mysql> DELETE FROM emp_tel WHERE ename = '孙洪亮'; Query OK, 1 row affected (0.01 sec) mysql> SELECT ename,tel FROM emp_tel WHERE ename = '孙洪亮'; Empty set (0.00 sec) mysql> SELECT ename,tel FROM t_employee WHERE ename = '孙洪亮'; Empty set (0.00 sec) \"]},{\"header\":\"5.2 不可更新的视图\",\"slug\":\"_5-2-不可更新的视图\",\"contents\":[\"要使视图可更新，视图中的行和底层基本表中的行之间必须存在一对一的关系。另外当视图定义出现如下情况时，视图不支持更新操作：\",\"在定义视图的时候指定了“ALGORITHM = TEMPTABLE”，视图将不支持INSERT和DELETE操作；\",\"视图中不包含基表中所有被定义为非空又未指定默认值的列，视图将不支持INSERT操作；\",\"在定义视图的SELECT语句中使用了JOIN联合查询，视图将不支持INSERT和DELETE操作；\",\"在定义视图的SELECT语句后的字段列表中使用了数学表达式或子查询，视图将不支持INSERT，也不支持UPDATE使用了数学表达式、子查询的字段值；\",\"在定义视图的SELECT语句后的字段列表中使用DISTINCT、聚合函数、GROUP BY、HAVING、UNION等，视图将不支持INSERT、UPDATE、DELETE；\",\"在定义视图的SELECT语句中包含了子查询，而子查询中引用了FROM后面的表，视图将不支持INSERT、UPDATE、DELETE；\",\"视图定义基于一个不可更新视图；\",\"常量视图。\",\"举例：\",\"mysql> CREATE OR REPLACE VIEW emp_dept -> (ename,salary,birthday,tel,email,hiredate,dname) -> AS SELECT ename,salary,birthday,tel,email,hiredate,dname -> FROM t_employee INNER JOIN t_department -> ON t_employee.did = t_department.did ; Query OK, 0 rows affected (0.01 sec) \",\"mysql> INSERT INTO emp_dept(ename,salary,birthday,tel,email,hiredate,dname) -> VALUES('张三',15000,'1995-01-08','18201587896', -> 'zs@atguigu.com','2022-02-14','新部门'); #ERROR 1393 (HY000): Can not modify more than one base table through a join view 'atguigu_chapter9.emp_dept' \",\"从上面的SQL执行结果可以看出，在定义视图的SELECT语句中使用了JOIN联合查询，视图将不支持更新操作。\",\"虽然可以更新视图数据，但总的来说，视图作为虚拟表，主要用于方便查询，不建议更新视图的数据。对视图数据的更改，都是通过对实际数据表里数据的操作来完成的。\"]},{\"header\":\"6. 修改、删除视图\",\"slug\":\"_6-修改、删除视图\",\"contents\":[]},{\"header\":\"6.1 修改视图\",\"slug\":\"_6-1-修改视图\",\"contents\":[\"方式1：使用CREATE OR REPLACE VIEW 子句修改视图\",\"CREATE OR REPLACE VIEW empvu80 (id_number, name, sal, department_id) AS SELECT employee_id, first_name || ' ' || last_name, salary, department_id FROM employees WHERE department_id = 80; \",\"说明：CREATE VIEW 子句中各列的别名应和子查询中各列相对应。\",\"方式2：ALTER VIEW\",\"修改视图的语法是：\",\"ALTER VIEW 视图名称 AS 查询语句 \"]},{\"header\":\"6.2 删除视图\",\"slug\":\"_6-2-删除视图\",\"contents\":[\"删除视图只是删除视图的定义，并不会删除基表的数据。\",\"删除视图的语法是：\",\"DROP VIEW IF EXISTS 视图名称; \",\"DROP VIEW IF EXISTS 视图名称1,视图名称2,视图名称3,..; \",\"举例：\",\"DROP VIEW empvu80; \",\"说明：基于视图a、b创建了新的视图c，如果将视图a或者视图b删除，会导致视图c的查询失败。这样的视图c需要手动删除或修改，否则影响使用。\"]},{\"header\":\"7. 总结\",\"slug\":\"_7-总结\",\"contents\":[]},{\"header\":\"7.1 视图优点\",\"slug\":\"_7-1-视图优点\",\"contents\":[\"1. 操作简单\",\"将经常使用的查询操作定义为视图，可以使开发人员不需要关心视图对应的数据表的结构、表与表之间的关联关系，也不需要关心数据表之间的业务逻辑和查询条件，而只需要简单地操作视图即可，极大简化了开发人员对数据库的操作。\",\"2. 减少数据冗余\",\"视图跟实际数据表不一样，它存储的是查询语句。所以，在使用的时候，我们要通过定义视图的查询语句来获取结果集。而视图本身不存储数据，不占用数据存储的资源，减少了数据冗余。\",\"3. 数据安全\",\"MySQL将用户对数据的访问限制在某些数据的结果集上，而这些数据的结果集可以使用视图来实现。用户不必直接查询或操作数据表。这也可以理解为视图具有隔离性。视图相当于在用户和实际的数据表之间加了一层虚拟表。\",\"同时，MySQL可以根据权限将用户对数据的访问限制在某些视图上，用户不需要查询数据表，可以直接通过视图获取数据表中的信息。这在一定程度上保障了数据表中数据的安全性。\",\"4. 适应灵活多变的需求 当业务系统的需求发生变化后，如果需要改动数据表的结构，则工作量相对较大，可以使用视图来减少改动的工作量。这种方式在实际工作中使用得比较多。\",\"5. 能够分解复杂的查询逻辑 数据库中如果存在复杂的查询逻辑，则可以将问题进行分解，创建多个视图获取数据，再将创建的多个视图结合起来，完成复杂的查询逻辑。\"]},{\"header\":\"7.2 视图不足\",\"slug\":\"_7-2-视图不足\",\"contents\":[\"如果我们在实际数据表的基础上创建了视图，那么，如果实际数据表的结构变更了，我们就需要及时对相关的视图进行相应的维护。特别是嵌套的视图（就是在视图的基础上创建视图），维护会变得比较复杂，可读性不好，容易变成系统的潜在隐患。因为创建视图的 SQL 查询可能会对字段重命名，也可能包含复杂的逻辑，这些都会增加维护的成本。\",\"实际项目中，如果视图过多，会导致数据库维护成本的问题。\",\"所以，在创建视图的时候，你要结合实际项目需求，综合考虑视图的优点和不足，这样才能正确使用视图，使系统整体达到最优。\"]}]},\"/study-tutorial/devTools/idea/code_template.html\":{\"title\":\"5、代码模板的使用\",\"contents\":[{\"header\":\"1、查看Postfix Completion模板(后缀补全)\",\"slug\":\"_1、查看postfix-completion模板-后缀补全\",\"contents\":[]},{\"header\":\"2、查看Live Templates模板(实时模板)\",\"slug\":\"_2、查看live-templates模板-实时模板\",\"contents\":[]},{\"header\":\"3、常用代码模板\",\"slug\":\"_3、常用代码模板\",\"contents\":[]},{\"header\":\"1、非空判断\",\"slug\":\"_1、非空判断\",\"contents\":[\"变量.null：if(变量 == null)\",\"变量.nn：if(变量 != null)\",\"变量.notnull：if(变量 != null) ifn：if(xx == null)\",\"inn：if(xx != null)\"]},{\"header\":\"2、遍历数组和集合\",\"slug\":\"_2、遍历数组和集合\",\"contents\":[\"数组或集合变量.fori：for循环\",\"数组或集合变量.for：增强for循环\",\"数组或集合变量.forr：反向for循环\",\"数组或集合变量.iter：增强for循环遍历数组或集合\"]},{\"header\":\"3、输出语句\",\"slug\":\"_3、输出语句\",\"contents\":[\"sout： 相 当 于 System.out.println\",\"soutm： 打 印 当 前 方 法 的 名 称\",\"soutp：打印当前方法的形参及形参对应的实参值\",\"soutv：打印方法中声明的最近的变量的值\",\"变量.sout：打印当前变量值\",\"变量.soutv：打印当前变量名及变量值\"]},{\"header\":\"4、对象操作\",\"slug\":\"_4、对象操作\",\"contents\":[\"创建对象\",\"Xxx.new .var ：创建Xxx类的对象，并赋给相应的变量\",\"Xxx.new .field：会将方法内刚创建的Xxx对象抽取为一个属性\",\"强转\",\"对象.cast：将对象进行强转\",\"对象.castvar：将对象强转后，并赋给一个变量\"]},{\"header\":\"5、静态常量声明\",\"slug\":\"_5、静态常量声明\",\"contents\":[\"psf：public static final\",\"psfi：public static final int\",\"psfs：public static final String\",\"prsf：private static final\"]},{\"header\":\"4、自定义代码模板\",\"slug\":\"_4、自定义代码模板\",\"contents\":[]},{\"header\":\"自定义后缀补全模板\",\"slug\":\"自定义后缀补全模板\",\"contents\":[]},{\"header\":\"自定义Live Templates\",\"slug\":\"自定义live-templates\",\"contents\":[\"例如：定义sop代表System.out.print();语句\",\"①在Live Templates中增加模板\",\"②先定义一个模板的组，这样方便管理所有自定义的代码模板\",\"③在模板组里新建模板\",\"④定义模板（以输出语句为例）\",\"Abbreviation：模板的缩略名称\",\"Description：模板的描述\",\"Template text：模板的代码片段\",\"模板应用范围。比如点击Define。选择如下：应用在java代码中。\"]},{\"header\":\"其它模板 1 ：单元测试模板：\",\"slug\":\"其它模板1-单元测试模板\",\"contents\":[\"其它模板2：创建多线程\",\"@Test public void test$var1$(){ $var2$ } \",\"new Thread(){ public void run(){ $var$ } }; \"]},{\"header\":\"其它模板3：冒泡排序\",\"slug\":\"其它模板3-冒泡排序\",\"contents\":[\"for(int $INDEX$ = 1 ; $INDEX$ < $ARRAY$.length; $INDEX$++) { for(int $INDEX2$ = 0 ; $INDEX2$ < $ARRAY$.length-$INDEX$; $INDEX2$++) { if($ARRAY$[$INDEX2$] > $ARRAY$[$INDEX2$+ 1 ]){ $ELEMENT_TYPE$ temp = $ARRAY$[$INDEX2$]; $ARRAY$[$INDEX2$] = $ARRAY$[$INDEX2$+ 1 ]; $ARRAY$[$INDEX2$+ 1 ] = temp; } } } \"]}]},\"/study-tutorial/devTools/idea/database.html\":{\"title\":\"9、关联数据库\",\"contents\":[{\"header\":\"关联方式\",\"slug\":\"关联方式\",\"contents\":[\"找到数据库选项：\",\"添加指定数据库：\",\"配置MySQL数据库的详细信息：\",\"表面上很多人认为配置 Database 就是为了有一个 GUI 管理数据库功能，但是这并不是 IntelliJ IDEA 的\",\"Database 最重要特性。数据库的 GUI 工具有很多，IntelliJ IDEA 的 Database 也没有太明显的优势。\",\"IntelliJ IDEA 的 Database 最大特性就是对于 Java Web 项目来讲，常使用的 ORM 框架，如 Hibernate、Mybatis 有很好的支持，比如配置好了 Database 之后，IntelliJ IDEA 会自动识别 domain 对象与数据表的关系，也可以通过 Database 的数据表直接生成 domain 对象等。\"]},{\"header\":\"常用操作\",\"slug\":\"常用操作\",\"contents\":[\"展示ER图：\",\"可以导出文件：\"]}]},\"/study-tutorial/devTools/idea/debug.html\":{\"title\":\"7、IDEA断点调试(Debug)\",\"contents\":[{\"header\":\"为什么需要Debug\",\"slug\":\"为什么需要debug\",\"contents\":[\"编好的程序在执行过程中如果出现错误，该如何查找或定位错误呢？简单的代码直接就可以看出来，但 如果代码比较复杂，就需要借助程序调试来查找错误了。\",\"运行编写好的程序时，可能出现的几种情况：\",\"情况1：没有任何bug,程序执行正确！ 如果出现如下的三种情况，都又必要使用debug\",\"情况2：运行以后，出现了错误或异常信息。但是通过日志文件或控制台，显示了异常信息的位置。 情况3：运行以后，得到了结果，但是结果不是我们想要的。 情况4：运行以后，得到了结果，结果大概率是我们想要的。但是多次运行的话，可能会出现不是我们想要的情况。\",\"比如：多线程情况下，处理线程安全问题。\"]},{\"header\":\"Debug的步骤\",\"slug\":\"debug的步骤\",\"contents\":[\"Debug(调试)程序步骤如下：\",\"1、添加断点\",\"2、启动调试\",\"3、单步执行\",\"4、观察变量和执行流程，找到并解决问题\"]},{\"header\":\"1 、添加断点\",\"slug\":\"_1、添加断点\",\"contents\":[\"在源代码文件中，在想要设置断点的代码行的前面的标记行处，单击鼠标左键就可以设置断点，在相同 位置再次单击即可取消断点。\"]},{\"header\":\"2、启动调试\",\"slug\":\"_2、启动调试\",\"contents\":[\"IDEA提供多种方式来启动程序(Launch)的调试，分别是通过菜单(Run –> Debug)、图标(“绿色臭虫”等等\"]},{\"header\":\"3、单步调试工具介绍\",\"slug\":\"_3、单步调试工具介绍\",\"contents\":[\"或\",\"：Step Over（F8)：进入下一步，如果当前行断点是调用一个方法，则不进入当前方法体内\",\"：Step Into（F7)：进入下一步，如果当前行断点是调用一个自定义方法，则进入该方法体内\",\"：Force Step Into（Alt +Shift + F7)：进入下一步，如果当前行断点是调用一个核心类库方法，则进入该方法体内\",\"：Step Out（Shift + F8)：跳出当前方法体\",\"：Run to Cursor（Alt + F9)：直接跳到光标处继续调试\",\"：Resume Program（F9)：恢复程序运行，但如果该断点下面代码还有断点则停在下一个断点上\",\"：Stop（Ctrl + F2)：结束调试\",\"：View Breakpoints（Ctrl + Shift + F8)：查看所有断点\",\"：Mute Breakpoints：使得当前代码后面所有的断点失效， 一下执行到底\"]},{\"header\":\"3、多种Debug情况介绍\",\"slug\":\"_3、多种debug情况介绍\",\"contents\":[]},{\"header\":\"行断点\",\"slug\":\"行断点\",\"contents\":[\"断点打在代码所在的行上。执行到此行时，会停下来。\",\"package com.atguigu.debug; /** * ClassName: Debug01 * Package: com.atguigu.debug * Description: 演示1：行断点 & 测试debug各个常见操作按钮 * * @Author: 尚硅谷-宋红康 * @Create: 2022/10/20 18:44 * @Version 1.0 */ public class Debug01 { public static void main(String[] args) { int m = 10; int n = 20; System.out.println(\\\"m = \\\" + m + \\\",n = \\\" + n); swap(m, n); System.out.println(\\\"m = \\\" + m + \\\",n = \\\" + n); //2. int[] arr = new int[] {1,2,3,4,5}; System.out.println(arr);//地址值 char[] arr1 = new char[] {'a','b','c'}; System.out.println(arr1);//abc } public static void swap(int m,int n){ int temp = m; m = n; n = temp; } } \"]},{\"header\":\"方法断点\",\"slug\":\"方法断点\",\"contents\":[\"断点设置在方法的签名上，默认当进入时，断点可以被唤醒。\",\"也可以设置在方法退出时，断点也被唤醒\",\"在多态的场景下，在父类或接口的方法上打断点，会自动调入到子类或实现类的方法\"]},{\"header\":\"字段断点\",\"slug\":\"字段断点\",\"contents\":[\"在类的属性声明上打断点，默认对属性的修改操作进行监控\"]},{\"header\":\"条件断点\",\"slug\":\"条件断点\",\"contents\":[\"针对上述代码，在满足arr[i] % 3 == 0的条件下，执行断点。\"]},{\"header\":\"异常断点\",\"slug\":\"异常断点\",\"contents\":[\"对异常进行跟踪。如果程序出现指定异常，程序就会执行断点，自动停住。\",\"通过下图的方式，对指定的异常进行监控：\"]},{\"header\":\"线程调试\",\"slug\":\"线程调试\",\"contents\":[]},{\"header\":\"强制结束\",\"slug\":\"强制结束\",\"contents\":[]},{\"header\":\"自定义调试数据视图\",\"slug\":\"自定义调试数据视图\",\"contents\":[\"设置如下：\"]},{\"header\":\"常见问题\",\"slug\":\"常见问题\",\"contents\":[\"问题：使用Step Into时，会出现无法进入源码的情况。如何解决？ 方案1：使用 force step into 即可\",\"方案2：点击Setting -> Build,Execution,Deployment -> Debugger -> Stepping\",\"把Do not step into the classess中的 java.* 、 取消勾选即可。\",\"小结：\",\"经验：初学者对于在哪里加断点，缺乏经验，这也是调试程序最麻烦的地方，需要一定的经验。\\n简单来说，在可能发生错误的代码的前面加断点。如果不会判断，就在程序执行的起点处加断点。\"]}]},\"/study-tutorial/devTools/idea/detailed_settings.html\":{\"title\":\"3、详细设置\",\"contents\":[{\"header\":\"1、如何打开详细配置界面\",\"slug\":\"_1、如何打开详细配置界面\",\"contents\":[\"1、显示工具栏\",\"2、选择详细配置菜单或按钮\"]},{\"header\":\"2、系统设置\",\"slug\":\"_2、系统设置\",\"contents\":[]},{\"header\":\"1 、默认启动项目配置\",\"slug\":\"_1、默认启动项目配置\",\"contents\":[\"启动IDEA时，默认自动打开上次开发的项目？还是自己选择？\",\"如果去掉Reopen last project on startup前面的对勾，每次启动IDEA就会出现如下界面：\"]},{\"header\":\"2、取消自动更新\",\"slug\":\"_2、取消自动更新\",\"contents\":[\"Settings-->Appearance & Behavior->System Settings -> Updates\",\"默认都打√了，建议检查IDE更新的√去掉，检查插件更新的√选上。\"]},{\"header\":\"3、设置整体主题\",\"slug\":\"_3、设置整体主题\",\"contents\":[]},{\"header\":\"1、选择主题\",\"slug\":\"_1、选择主题\",\"contents\":[]},{\"header\":\"2、设置菜单和窗口字体和大小\",\"slug\":\"_2、设置菜单和窗口字体和大小\",\"contents\":[]},{\"header\":\"3、设置IDEA背景图\",\"slug\":\"_3、设置idea背景图\",\"contents\":[\"选择一张合适的图片作为背景，即可。\"]},{\"header\":\"4、设置编辑器主题样式\",\"slug\":\"_4、设置编辑器主题样式\",\"contents\":[]},{\"header\":\"1 、编辑器主题\",\"slug\":\"_1、编辑器主题\",\"contents\":[]},{\"header\":\"2、字体大小\",\"slug\":\"_2、字体大小\",\"contents\":[\"更详细的字体与颜色如下：\",\"温馨提示：如果选择某个font字体，中文乱码，可以在fallback font（备选字体）中选择一个支持中文的字体。\"]},{\"header\":\"3、注释的字体颜色\",\"slug\":\"_3、注释的字体颜色\",\"contents\":[\"Block comment：修改多行注释的字体颜色\",\"Doc Comment –> Text：修改文档注释的字体颜色\",\"Line comment：修改单行注释的字体颜色\"]},{\"header\":\"5、显示行号与方法分隔符\",\"slug\":\"_5、显示行号与方法分隔符\",\"contents\":[]},{\"header\":\"6、代码智能提示功能\",\"slug\":\"_6、代码智能提示功能\",\"contents\":[\"IntelliJ IDEA 的代码提示和补充功能有一个特性： 区分大小写 。 如果想不区分大小写的话，就把这个对勾去掉。 建议去掉勾选 。\"]},{\"header\":\"7、自动导包配置\",\"slug\":\"_7、自动导包配置\",\"contents\":[\"默认需要自己手动导包，Alt+Enter快捷键\",\"自动导包设置\",\"动态导入明确的包：Add unambiguous imports on the fly，该设置具有全局性；\",\"优化动态导入的包：Optimize imports on the fly，该设置只对当前项目有效；\"]},{\"header\":\"8、设置项目文件编码（一定要改)\",\"slug\":\"_8、设置项目文件编码-一定要改\",\"contents\":[\"说明： Transparent native-to-ascii conversion主要用于转换ascii，显式原生内容。一般都要勾选。\"]},{\"header\":\"9、设置控制台的字符编码\",\"slug\":\"_9、设置控制台的字符编码\",\"contents\":[]},{\"header\":\"10、修改类头的文档注释信息\",\"slug\":\"_10、修改类头的文档注释信息\",\"contents\":[\"/** * ClassName: ${NAME} * Package: ${PACKAGE_NAME} * Description: * @Author 小熊学Java * @Create ${DATE} ${TIME} * @Version 1.0 */ \",\"比如：常用的预设的变量，这里直接贴出官网给的：\",\"${PACKAGE_NAME} - the name of the target package where the new class or interface will be created. ${PROJECT_NAME} - the name of the current project. ${FILE_NAME} - the name of the PHP file that will be created. ${NAME} - the name of the new file which you specify in the New File dialog box during the file creation. ${USER} - the login name of the current user. ${DATE} - the current system date. ${TIME} - the current system time. ${YEAR} - the current year. ${MONTH} - the current month. ${DAY} - the current day of the month. ${HOUR} - the current hour. ${MINUTE} - the current minute. ${PRODUCT_NAME} - the name of the IDE in which the file will be created. ${MONTH_NAME_SHORT} - the first 3 letters of the month name. Example: Jan, Feb, etc. ${MONTH_NAME_FULL} - full name of a month. Example: January, February, etc. \"]},{\"header\":\"11、设置自动编译\",\"slug\":\"_11、设置自动编译\",\"contents\":[\"Settings-->Build,Execution,Deployment-->Compiler\"]},{\"header\":\"12、设置为省电模式 (可忽略)\",\"slug\":\"_12、设置为省电模式-可忽略\",\"contents\":[\"IntelliJ IDEA 有一种叫做 省电模式 的状态，开启这种模式之后 IntelliJ IDEA 会 关掉代码检查 和 代码提示 等功能。所以一般也可认为这是一种 阅读模式 ，如果你在开发过程中遇到突然代码文件不能进行检查和提示，可以来看看这里是否有开启该功能。\"]},{\"header\":\"13、取消双击shift搜索\",\"slug\":\"_13、取消双击shift搜索\",\"contents\":[\"因为我们按shift切换中英文输入方式，经常被按到，总是弹出搜索框，太麻烦了。可以取消它。\",\"方式1：适用于IDEA 2022.1.2版本\",\"在2022.1版本中，采用如下方式消双击shift出现搜索框：搜索double即可，勾选Disable double modifier key shortcuts，禁用这个选项。\",\"方式2：适用于IDEA 2022.1.2之前版本\",\"双击shift 或 ctrl + shift + a，打开如下搜索窗口：\",\"选择registry...，找到\\\"ide.suppress.double.click.handler\\\"，把复选框打上勾就可以取消双击shift出现搜索 框了。\"]},{\"header\":\"14、其它设置\",\"slug\":\"_14、其它设置\",\"contents\":[\"是否在单行显式编辑器选项卡（建议去掉勾选）\",\"设置代码样式：比如，设置import显示\\\"*\\\"时的个数\",\"总结：以上这些设置看似只是针对当前Project设置的，但是新建的其它Project也是同样适用的。\"]}]},\"/study-tutorial/devTools/idea/diff_project.html\":{\"title\":\"8、创建不同类型的工程\",\"contents\":[{\"header\":\"创建Java工程\",\"slug\":\"创建java工程\",\"contents\":[\"在工程上，右键- New - Module，如下：\",\"指明Java工程的名称及使用的JDK版本：\",\"创建包：\",\"提供包名：\",\"在包下创建类，即可：\",\"提供类名：\",\"测试代码：\",\"点击运行即可：\"]},{\"header\":\"创建Java Web工程\",\"slug\":\"创建java-web工程\",\"contents\":[]},{\"header\":\"IDEA中配置Tomcat\",\"slug\":\"idea中配置tomcat\",\"contents\":[\"在IDEA中配置Tomcat之前，需要保证已经安装并配置了Tomcat的环境变量。如果没有安装并配置，可以 参考《 尚硅谷_宋红康_Tomcat8.5快速部署.docx 》配置完成以后，在命令行输入：catalina run 。能够启动tomcat，则证明安装配置成功。\",\"下面看如何在IDEA中配置：\",\"配置Tomcat Server的位置：\",\"这里配置Tomcat的名称以及配置应用服务器的位置。根据自己Tomcat的安装位置决定。\",\"配置好后，如下图所示：\"]},{\"header\":\"创建Web工程\",\"slug\":\"创建web工程\",\"contents\":[\"选择New Module，指明当前工程的名称：\",\"选中当前创建的工程，添加框架支持：\",\"选择：Web Application，选择Create web.xml，如下：\"]},{\"header\":\"配置web工程并运行\",\"slug\":\"配置web工程并运行\",\"contents\":[\"部署当前的web项目：\",\"选择第2项：\",\"可以修改Application context，如下：\",\"配置当前web工程的详细信息，如下：\",\"配置好后，可以直接运行：\"]},{\"header\":\"乱码的解决\",\"slug\":\"乱码的解决\",\"contents\":[\"如果Tomcat日志出现乱码，需要配置：\",\"解决方案：\",\"点击Help => Edit custom VM Options，在最后面添加\",\"-Dfile.encoding=UTF-8 \",\"在当前Tomcat实例中配置 VM option，添加\",\"-Dfile.encoding=UTF-8 \",\"在第二步的Startup/Connection页签的Run和Debug添加一个key为 JAVA_TOOL_OPTIONS ， value为“ -\",\"Dfile.encoding=UTF-8 ”的环境变量\",\"保存后重启IDEA，可以发现控制台中文乱码显示正常了。\"]},{\"header\":\"创建Maven Java工程\",\"slug\":\"创建maven-java工程\",\"contents\":[]},{\"header\":\"Maven的介绍\",\"slug\":\"maven的介绍\",\"contents\":[\"Maven是一款自动化构建工具，专注服务于Java平台的 项目构建 和 依赖管理 。在JavaEE开发的历史上构建工具的发展也经历了一系列的演化和变迁：\",\"Make→Ant→Maven→Gradle→其他……\",\"构建环节：\",\"①清理：删除以前的编译结果，为重新编译做好准备。\",\"②编译：将Java源程序编译为字节码文件。 ③测试：运行单元测试用例程序，确保项目在迭代开发过程中关键点的正确性。 ④报告：测试程序的结果。 ⑤打包：将java项目打成jar包；将Web项目打成war包。 ⑥安装：将jar包或war包安装到本地仓库中。 ⑦部署：将jar或war从Maven仓库中部署到Web服务器上运行。\"]},{\"header\":\"Maven的配置\",\"slug\":\"maven的配置\",\"contents\":[\"maven的下载 – 解压 – 环境变量的配置这里就不赘述了，需要的参考03-资料\\\\05-Maven的配置中的《 尚 硅谷_Maven的配置_V2.0.docx 》。下面直接整合Maven。选择自己Maven的目录，和settings文件，然后配置自己的仓库reposiroty。\"]},{\"header\":\"Maven Java工程的创建\",\"slug\":\"maven-java工程的创建\",\"contents\":[\"指明当前maven工程的名称、模板等信息。这里要求一个项目组的jdk版本必须一致。 通过坐标，就可以定位仓库中具体的jar包。如下：\",\"新创建的maven 的java工程缺少相应的resources文件目录，需要创建如下：\",\"指明main下resources的文件目录类型：\",\"类似的操作test目录下，提供resources即可。这里说明Maven的java工程的目录结构：\",\"工程名 src ----main --------java --------resources ----test --------java --------resources pom.xml \",\"main目录用于存放主程序。\",\"test目录用于存放测试程序。\",\"java目录用于存放源代码文件。\",\"resources目录用于存放配置文件和资源文件\"]},{\"header\":\"编写代码及测试\",\"slug\":\"编写代码及测试\",\"contents\":[]},{\"header\":\"第1步：创建Maven的核心配置文件pom.xml\",\"slug\":\"第1步-创建maven的核心配置文件pom-xml\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <modelVersion>4.0.0</modelVersion> <groupId>com.atguigu.maven</groupId> <artifactId>maven-01</artifactId> <version>1.0-SNAPSHOT</version> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>8.0.18</version> </dependency> </dependencies> </project> \"]},{\"header\":\"第2步：编写主程序代码\",\"slug\":\"第2步-编写主程序代码\",\"contents\":[\"在src/main/java/com/atguigu/java目录下新建文件HelloMaven.java\",\"package com.atguigu.java; /** * ClassName: HelloMaven * Package: com.atguigu.java * Description: * * @Author: 尚硅谷-宋红康 * @Create: 2022/10/20 18:20 * @Version 1.0 */ public class HelloMaven { public String sayHello(String message) { return \\\"Hello, \\\" + message + \\\"!\\\"; } } \"]},{\"header\":\"第3步：编写测试代码\",\"slug\":\"第3步-编写测试代码\",\"contents\":[\"在/src/test/java/com/atguigu/java目录下新建测试文件HelloMavenTest.java\",\"package com.atguigu.java; import org.junit.Test; /** * ClassName: HelloMavenTest * Package: com.atguigu.java * Description: * * @Author: 尚硅谷-宋红康 * @Create: 2022/10/20 18:21 * @Version 1.0 */ public class HelloMavenTest { @Test public void testHelloMaven() { HelloMaven helloMaven = new HelloMaven(); System.out.println(helloMaven.sayHello(\\\"Maven\\\")); } } \"]},{\"header\":\"第4步：运行几个基本的Maven命令\",\"slug\":\"第4步-运行几个基本的maven命令\",\"contents\":[\"目录下也会有对应的生命周期。其中常用的是：clean、compile、package、install。\",\"比如这里install，如果其他项目需要将这里的模块作为依赖使用，那就可以install。安装到本地仓库的位 置。\"]},{\"header\":\"创建Maven Web工程\",\"slug\":\"创建maven-web工程\",\"contents\":[]},{\"header\":\"创建Maven的Web工程步骤\",\"slug\":\"创建maven的web工程步骤\",\"contents\":[\"指明Maven的web工程的名称和模板。如下：\",\"在Tomcat上进行部署：\",\"配置部署的详细信息：\"]},{\"header\":\"开发jsp依赖jar包\",\"slug\":\"开发jsp依赖jar包\",\"contents\":[]},{\"header\":\"1、找不到HttpServlet错误\",\"slug\":\"_1、找不到httpservlet错误\",\"contents\":[\"如果看到JSP报错： The superclass \\\"javax.servlet.http.HttpServlet\\\" was not found onthe Java Build Path 可以加入如下依赖解决。\",\"<dependency> <groupId>javax.servlet</groupId> <artifactId>servlet-api</artifactId> <version>2.5</version> <scope>provided</scope> </dependency> \"]},{\"header\":\"2、EL表达式没有提示问题\",\"slug\":\"_2、el表达式没有提示问题\",\"contents\":[\"同时，针对index.jsp文件，修改一下文件头信息为：\",\"<%@page language=\\\"java\\\" pageEncoding=\\\"utf-8\\\" contentType=\\\"text/html;UTF-8\\\" %> \"]}]},\"/study-tutorial/devTools/idea/hello_world.html\":{\"title\":\"2、HelloWorld的实现\",\"contents\":[{\"header\":\"新建Project - Class\",\"slug\":\"新建project-class\",\"contents\":[\"选择\\\"New Project\\\"：\",\"指名工程名、使用的JDK版本等信息。如下所示：\",\"接着创建Java类：\"]},{\"header\":\"编写代码\",\"slug\":\"编写代码\",\"contents\":[\"public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello,World!\\\"); } } \"]},{\"header\":\"运行\",\"slug\":\"运行\",\"contents\":[\"****\"]},{\"header\":\"项目的JDK设置\",\"slug\":\"项目的jdk设置\",\"contents\":[\"File-->Project Structure...-->Platform Settings -->SDKs\",\"注1：SDKs全称是Software Development Kit ，这里一定是选择JDK的安装根目录，不是JRE的目录。注2：这里还可以从本地添加多个JDK。使用“+”即可实现。\"]},{\"header\":\"out目录和编译版本\",\"slug\":\"out目录和编译版本\",\"contents\":[\"File-->Project Structure...-->Project Settings -->Project\"]}]},\"/study-tutorial/devTools/idea/know.html\":{\"title\":\"1、认识IntelliJ IDEA\",\"contents\":[{\"header\":\"JetBrains 公司介绍\",\"slug\":\"jetbrains-公司介绍\",\"contents\":[\"IDEA，是 JetBrains ( https://www.jetbrains.com/ )公司的产品，该公司成立于2000年，总部位于捷克的布拉格，致力于为开发者打造最高效智能的开发工具。\",\"公司旗下还有其它产品，比如：\",\"WebStorm：用于开发 JavaScript、HTML5、CSS3 等前端技术\",\"PyCharm： 用 于 开 发 python PhpStorm： 用 于 开 发 PHP RubyMine： 用 于 开 发 Ruby/Rails AppCode：用于开发 Objective - C/Swift CLion： 用 于 开 发 C/C++ DataGrip：用于开发数据库和 SQL Rider：用于开发.NET\",\"GoLand：用于开发 Go\",\"用于开发 Android的Android Studio，也是Google 基于 IDEA 社区版进行迭代的。\"]},{\"header\":\"IntelliJ IDEA 介绍\",\"slug\":\"intellij-idea-介绍\",\"contents\":[\"IDEA，全称 IntelliJ IDEA ，是 Java 语言的集成开发环境，目前已经（基本） 代替 了Eclipse的使用。IDEA 在业界被公认为是最好的 Java 开发工具（之一），因其 功能强悍 、 设置人性化 ，而深受Java、大数据、移动端程序员的喜爱。尤其在智能代码助手、代码自动提示、重构、J2EE支持、Ant、JUnit、CVS 整合、代码审查、创新的 GUI 设计等方面的功能可以说是超常的。\",\"IntelliJ IDEA 在 2015 年的官网上这样介绍自己：\"]},{\"header\":\"IDEA的主要优势：(vs Eclipse)\",\"slug\":\"idea的主要优势-vs-eclipse\",\"contents\":[]},{\"header\":\"功能强大：\",\"slug\":\"功能强大\",\"contents\":[\"强大的整合能力。比如：Git、Maven、Spring等\",\"开箱即用的体验（集成版本控制系统、多语言支持的框架随时可用，无需额外安装插件）\"]},{\"header\":\"符合人体工程学：\",\"slug\":\"符合人体工程学\",\"contents\":[\"高度智能（快速的智能代码补全、实时代码分析、可靠的重构工具）\",\"提示功能的快速、便捷、范围广\",\"好用的快捷键和代码模板\",\"精准搜索\"]},{\"header\":\"IDEA 的下载\",\"slug\":\"idea-的下载\",\"contents\":[\"下载网址： https://www.jetbrains.com/idea/download/#section=windows\",\"IDEA 分为两个版本： 旗舰版(Ultimate) 和 。\",\"IDEA的大版本每年迭代一次，大版本下的小版本（如：2022.x）迭代时间不固定，一般每年3个小版 本。\",\"两个不同版本的详细对比，可以参照官网： https://www.jetbrains.com/idea/features/editions_comparis on_matrix.html\",\"官网提供的详细使用文档： https://www.jetbrains.com/help/idea/meet-intellij-idea.html\"]},{\"header\":\"卸载过程\",\"slug\":\"卸载过程\",\"contents\":[\"这里以卸载2022.1.2版本为例说明。在【控制面板】找到【卸载程序】\",\"右键点击或左键双击IntelliJ IDEA 2022.1.2进行卸载：\",\"如果需要保留下述数据，就不要打√。如果想彻底删除IDEA所有数据，那就打上√。\",\"软件卸载完以后，还需要删除其它几个位置的残留：\"]},{\"header\":\"安装前的准备\",\"slug\":\"安装前的准备\",\"contents\":[\"64 位 Microsoft Windows 11、10、8\",\"最低 2 GB 可用 RAM，推荐 8 GB 系统总 RAM\",\"2.5 GB 硬盘空间，推荐 SSD\",\"最低屏幕分辨率 1024x768\",\"从安装上来看，IntelliJ IDEA 对硬件的要求 似乎不是很高 。可是在实际开发中并不是这样的，因为 IntelliJ IDEA 执行时会有大量的缓存、索引文件，所以如果你正在使用 Eclipse / MyEclipse，想通过 IntelliJ IDEA 来解决计算机的卡、慢等问题，这基本上是不可能的，本质上你应该对自己的硬件设备进行升级。\"]},{\"header\":\"安装过程\",\"slug\":\"安装过程\",\"contents\":[\"1、下载完安装包，双击直接安装\",\"2、欢迎安装\",\"3、是否删除电脑上低版本的IDEA（如果有，可以选择忽略）\",\"如果电脑上有低版本的IDEA，可以选择删除或保留。\",\"这里没有卸载旧版本，如果需要卸载，记得勾选下面的保留旧的设置和配置。\",\"4、选择安装目录\",\"选择安装目录，目录中要避免中文和空格。\",\"5、创建桌面快捷图标等\",\"确认是否与.java、.groovy、.kt 格式文件进行关联。这里建议不关联。\",\"6、在【开始】菜单新建一个文件夹（这里需要确认文件夹的名称），来管理IDEA的相关内容。\",\"7、完成安装\",\"重启以后，单击登录：\"]},{\"header\":\"注册\",\"slug\":\"注册\",\"contents\":[\"首先，需要通过用户协议：\",\"是否同意发送用户数据（特性、使用的插件、硬件与软件配置等），建议选择：不发送。\",\"接着，会提示我们进行注册。\",\"选择1：适用30天。在IDEA2022.1.2版本中，需要先登录，才能开启适用。\",\"选择2：付费购买旗舰版\",\"选择3：（推荐）\",\"大家参照操作即可。\",\"由于存在时效性，如果失效，大家可以自行搜索注册方式即可。\"]},{\"header\":\"闪退问题\",\"slug\":\"闪退问题\",\"contents\":[\"问题描述1：2022.1启动不了，双击桌面图标，没有响应。\",\"问题描述2：进入到安装目录...\\\\IntelliJ IDEA 2022.1.2\\\\bin，打开CMD。输入idea，发现报错。解决办法：\",\"打开C:\\\\Users\\\\songhk\\\\AppData\\\\Roaming\\\\JetBrains\\\\IntelliJIdea2022.1\\\\idea64.exe.vmoptions这个文件。\",\"内容如下所示：\",\"删除红框的数据以后，再登录即可正常进入。\",\"原因：之前使用过的比如2021.2.2版本，pojie了。新版IEDA太智能了，把现有的启运参数也都复制过去 了 。 又 因 为 最 新 的 IDEA， 不 兼 容 pojie 程 序 -javaagent:D:\\\\develop_tools\\\\IDEA\\\\IntelliJ IDEA 2021.2.2\\\\bin\\\\jetbrains-agent.jar了，所以报错了，所以JVM结束了，所以没有启动画面，凉凉了。\"]}]},\"/study-tutorial/devTools/idea/module.html\":{\"title\":\"4、工程与模块管理\",\"contents\":[{\"header\":\"1、IDEA项目结构\",\"slug\":\"_1、idea项目结构\",\"contents\":[]},{\"header\":\"层级关系：\",\"slug\":\"层级关系\",\"contents\":[\"project(工程) - module(模块) - package(包) - class(类)\",\"具体的：\",\"一个project中可以创建多个module\",\"一个module中可以创建多个package\",\"一个package中可以创建多个class\",\"这些结构的划分，是为了方便管理功能代码。\"]},{\"header\":\"2、Project和Module的概念\",\"slug\":\"_2、project和module的概念\",\"contents\":[\"在 IntelliJ IDEA 中，提出了Project和Module这两个概念。\",\"在 IntelliJ IDEA 中Project是 最顶级的结构单元 ，然后就是Module。目前，主流的大型项目结构基本都是多Module的结构，这类项目一般是 按功能划分 的，比如：user-core-module、user-facade-module和user- hessian-module等等，模块之间彼此可以 相互依赖 ，有着不可分割的业务关系。因此，对于一个Project 来说：\",\"当为单Module项目的时候，这个单独的Module实际上就是一个Project。\",\"当为多Module项目的时候，多个模块处于同一个Project之中，此时彼此之间具有 互相依赖 的关联关系。\",\"当然多个模块没有建立依赖关系的话，也可以作为单独一个“小项目”运行。\"]},{\"header\":\"3、创建Module\",\"slug\":\"_3、创建module\",\"contents\":[\"建议创建“Empty空工程”，然后创建多模块，每一个模块可以独立运行，相当于一个小项目。JavaSE阶段 不涉及到模块之间的依赖。后期再学习模块之间的依赖。\",\"步骤：\",\"选择创建模块\",\"选择模块类型：这里选择创建Java模块，给模块命名，确定存放位置\",\"模块声明在工程下面\"]},{\"header\":\"4、删除模块\",\"slug\":\"_4、删除模块\",\"contents\":[\"移除模块\",\"彻底删除模块\"]},{\"header\":\"5、导入模块\",\"slug\":\"_5、导入模块\",\"contents\":[\"将模块 teacher_chapter04 整个的复制到自己IDEA项目的路径下\",\"接着打开自己IDEA的项目，会在项目目录下看到拷贝过来的module，只不过不是以模块的方式呈现。\",\"查看Project Structure，选择import module\",\"选择要导入的module：\",\"接着可以一路Next下去，最后选择Overwrite\",\"最后点击OK即可了。\"]},{\"header\":\"6、同时打开两个IDEA项目工程\",\"slug\":\"_6、同时打开两个idea项目工程\",\"contents\":[]},{\"header\":\"1、两个IDEA项目工程效果\",\"slug\":\"_1、两个idea项目工程效果\",\"contents\":[\"有些同学想要把上课练习代码和作业代码分开两个IDEA项目工程。\"]},{\"header\":\"2、新建一个IDEA项目\",\"slug\":\"_2、新建一个idea项目\",\"contents\":[\"注意：第一次需要新建，之后直接打开项目工程即可\",\"3、打开两个IDEA项目\"]},{\"header\":\"7、导入前几章非IDEA工程代码\",\"slug\":\"_7、导入前几章非idea工程代码\",\"contents\":[]},{\"header\":\"1、创建chapter01、chapter02、chapter03等章节的module\",\"slug\":\"_1、创建chapter01、chapter02、chapter03等章节的module\",\"contents\":[\"将相应章节的源文件粘贴到module的src下。\",\"打开其中各个源文件，会发现有乱码。比如：\"]},{\"header\":\"2、设置编码\",\"slug\":\"_2、设置编码\",\"contents\":[\"当前项目是UTF-8。如果原来的.java文件都是GBK的（如果原来.java文件有的是GBK，有的是UTF-8就比较 麻烦了）。\",\"可以单独把这两个模块设置为GBK编码的。\",\"改为GBK，确认即可。如图：\"]}]},\"/study-tutorial/devTools/idea/plugins.html\":{\"title\":\"10、IDEA常用插件\",\"contents\":[{\"header\":\"推荐1：Alibaba Java Coding Guidelines\",\"slug\":\"推荐1-alibaba-java-coding-guidelines\",\"contents\":[\"阿里巴巴Java编码规范检查插件，检测代码是否存在问题，以及是否符合规范。\",\"使用：在类中，右键，选择编码规约扫描，在下方显示扫描规约和提示。根据提示规范代码，提高代码 质量。\"]},{\"header\":\"推荐2：jclasslib bytecode viewer\",\"slug\":\"推荐2-jclasslib-bytecode-viewer\",\"contents\":[\"可视化的字节码查看器。使用：\",\"在 IDEA 打开想研究的类。\",\"编译该类或者直接编译整个项目（ 如果想研究的类在 jar 包中，此步可略过）。\",\"打开“view” 菜单，选择“Show Bytecode With jclasslib” 选项。\",\"选择上述菜单项后 IDEA 中会弹出 jclasslib 工具窗口。\",\"英文设置：\",\"在 Help -> Edit Custom VM Options …，加上\"]},{\"header\":\"推荐3：Translation\",\"slug\":\"推荐3-translation\",\"contents\":[\"注册翻译服务（有道智云、百度翻译开放平台、阿里云机器翻译）帐号，开通翻译服务并获取其应用ID 和密钥 绑定应用ID和密钥：偏好设置（设置） > 工具 > 翻译 > 常规 > 翻译引擎 > 配置…\",\"使用：鼠标选中文本，点击右键即可自动翻译成多国语言。注：请注意保管好你的应用密钥，防止其泄露。\"]},{\"header\":\"推荐4：GenerateAllSetter\",\"slug\":\"推荐4-generateallsetter\",\"contents\":[\"实际开发中还有一个非常常见的场景： 我们创建一个对象后，想依次调用 Setter 函数对属性赋值，如果属性较多很容易遗漏或者重复。\",\"可以使用这 GenerateAllSetter 提供的功能，快速生成对象的所有 Setter 函数（可填充默认值），然后自己再跟进实际需求设置属性值。\"]},{\"header\":\"插件5：Rainbow Brackets\",\"slug\":\"插件5-rainbow-brackets\",\"contents\":[\"给括号添加彩虹色，使开发者通过颜色区分括号嵌套层级，便于阅读\"]},{\"header\":\"推荐6：CodeGlance Pro\",\"slug\":\"推荐6-codeglance-pro\",\"contents\":[\"在编辑器右侧生成代码小地图，可以拖拽小地图光标快速定位代码，阅读行数很多的代码文件时非常实 用。\"]},{\"header\":\"推荐7：Statistic\",\"slug\":\"推荐7-statistic\",\"contents\":[\"代码统计工具。\"]},{\"header\":\"推荐8：Presentation Assistant\",\"slug\":\"推荐8-presentation-assistant\",\"contents\":[\"显示快捷键操作的按键\"]},{\"header\":\"推荐9：Key Promoter X\",\"slug\":\"推荐9-key-promoter-x\",\"contents\":[\"快捷键提示插件。当你执行鼠标操作时，如果该操作可被快捷键代替，会给出提示，帮助你自然形成使 用快捷键的习惯，告别死记硬背。\"]},{\"header\":\"推荐10：JavaDoc\",\"slug\":\"推荐10-javadoc\",\"contents\":[\"按 alt+insert ，执行操作：\"]},{\"header\":\"推荐11： LeetCode Editor\",\"slug\":\"推荐11-leetcode-editor\",\"contents\":[\"在 IDEA 里刷力扣算法题\"]},{\"header\":\"推荐12：GsonFormatPlus\",\"slug\":\"推荐12-gsonformatplus\",\"contents\":[\"根据 json 生成对象。\",\"使用：使用alt + s 或 alt + insert调取。\",\"举例：\"]},{\"header\":\"插件13：Material Theme UI\",\"slug\":\"插件13-material-theme-ui\",\"contents\":[\"对于很多人而言，写代码时略显枯燥的，如果能够安装自己喜欢的主题将为开发工作带来些许乐趣。\",\"IDEA 支持各种主题插件，其中最出名的当属 Material Theme UI。\",\"安装后，可以从该插件内置的各种风格个选择自己最喜欢的一种。\"]}]},\"/study-tutorial/devTools/idea/shortcut_key.html\":{\"title\":\"6、快捷键的使用\",\"contents\":[{\"header\":\"常用快捷键\",\"slug\":\"常用快捷键\",\"contents\":[]},{\"header\":\"查看快捷键\",\"slug\":\"查看快捷键\",\"contents\":[]},{\"header\":\"1 、已知快捷键操作名，未知快捷键\",\"slug\":\"_1、已知快捷键操作名-未知快捷键\",\"contents\":[]},{\"header\":\"2、已知快捷键，不知道对应的操作名\",\"slug\":\"_2、已知快捷键-不知道对应的操作名\",\"contents\":[]},{\"header\":\"3、自定义快捷键\",\"slug\":\"_3、自定义快捷键\",\"contents\":[]},{\"header\":\"4、 使用其它平台快捷键\",\"slug\":\"_4、使用其它平台快捷键\",\"contents\":[\"苹果电脑或者是用惯Eclipse快捷的，可以选择其他快捷键插件。\"]}]},\"/study-tutorial/distributed/elasticsearch/get_started.html\":{\"title\":\"2、Elasticsearch入门\",\"contents\":[{\"header\":\"1、Elasticsearch 安装\",\"slug\":\"_1、elasticsearch-安装\",\"contents\":[]},{\"header\":\"1、下载\",\"slug\":\"_1、下载\",\"contents\":[\"Elasticsearch 的官方地址： https://www.elastic.co/cn/\",\"下载地址：https://www.elastic.co/cn/downloads/past-releases#elasticsearch\",\"Elasticsearch 分为 Linux 和 WindowWindows版本，基于我们主要学习的是 ElasticElasticsearch 的 Java客户端的使用，所以 课程 中使用的是安装较为简便的 WindowWindows版本 。\"]},{\"header\":\"2、安装\",\"slug\":\"_2、安装\",\"contents\":[\"Windows版的 ElasticElasticsearch 的安装很简单，解压即安装完毕，解压后的 ElasticElasticsearch 的 目录结构如下\",\"目录\",\"含义\",\"bin\",\"可执行脚本目录\",\"config\",\"配置目录\",\"jdk\",\"内置JDK 目录\",\"lib\",\"类库\",\"logs\",\"日志目录\",\"modules\",\"模块目录\",\"plugins\",\"插件目录\",\"解压后，进入bin 文件目录，点击 elasticsearch.bat 文件启动 ES 服务\",\"注意：\",\"9300 端口为 Elastic s earch 集群间组件的通信端口，\",\"9200 端口为浏览器访问的 http协议 RESTful 端口。\",\"打开浏览器（推荐使用谷歌浏览器），输入地址：http://localhost:9200 ，测试结果\"]},{\"header\":\"3、问题解决\",\"slug\":\"_3、问题解决\",\"contents\":[\"ElasticSearch 是使用 java 开发的，且 7.13.2版本的 ES 需要 JDK 版本 1.8 以上，默认安装包带有 jdk 环境，如果系统配置 JAVA_HOME ，那么使用系统默认的 JDK ，如果没有配置使用自带的 JDK ，一般建议使用系统配置的 JDK 。\",\"双击启动窗口闪退，通过路径访问追踪错误，如果是“空间不足”，请修改config/jvm.options 配置文件\",\"设置 JVM 初始内存为 1G 。此值可以设置与 Xmx 相同，以避免每次垃圾回收完成后 JVM 重新分配内存\",\"Xms represents the initial size of total heap space\",\"设置 JVM 最大可用内存为 1G\",\"Xmx represents the maximum size of total heap space\",\"Xms1g Xmx1g\"]},{\"header\":\"2、基本操作\",\"slug\":\"_2、基本操作\",\"contents\":[]},{\"header\":\"1、RESTful\",\"slug\":\"_1、restful\",\"contents\":[\"​ REST指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful 。 Web 应用程序最重要的 REST 原则是，客户端和服务器之间的交互在请求之间是无状态的。从客户端到服务器的每个请求都必须包含理解请求所必需的信息。如果服务器在请求之间的任何时间点重启，客户端不会得到通知。此外，无状态请求可以由任何可用服务器回答，这十分适合云计算之类的环境。客户端可以缓存数据以改进性能。\",\"​ 在服务器端，应用程序状态和功能可以分为各种资源。资源是一个有趣的概念实体，它向客户端公开。资源的例子有：应用程序对象、数据库记录、算法等等。每个资源都使用 URI(Universal Resource Identifier) 得到一个唯一的地址。所有资源都共享统一的接口，以便在客户端和服务器之间传输状态。使用的是标准的 HTTP 方法，比如 GET 、 PUT 、 POST 和DELETE 。\",\"​ 在REST 样式的 Web 服务中，每个资源都有一个地址。资源本身都是方法调用的目标，方法列表对所有资源都是一样的。这些方法都是标准方法，包括 HTTP GET 、 POST 、PUT 、 DELETE ，还可能包括 HEAD 和 OPTIONS 。简单的理解就是，如果想要访问互联网上的资源，就必须向资源所在的服务器发出请求，请求体中必须包含资源的网络路径， 以及对资源进行的操作 增删改查 。\"]},{\"header\":\"2、Postman安装\",\"slug\":\"_2、postman安装\",\"contents\":[\"​ 如果直接通过浏览器向Elasticsearch 服务器发请求，那么需要在发送的请求中包含HTTP 标准的方法，而 HTTP 的大部分特性且仅支持 GET 和 POST 方法。所以为了能方便地进行客户端的访问，可以使用 Postman 软件 ​ Postman是一款强大 的 网页调试工具，提供功能强大的 Web API 和 HTTP 请求调试。软件功能强大，界面简洁明晰、操作方便快捷，设计得很人性化。 Postman 中文版能够发送任何类型的 HTTP 请求 (GET, POST, PUT..) PUT..)，不仅能够表单提交，且可以附带任意类型请求体。\",\"Postman官网： https://www.getpostman.com Postman下载 https://www.getpostman.com/apps\"]},{\"header\":\"3、数据格式\",\"slug\":\"_3、数据格式\",\"contents\":[\"Elasticsearch是面向文档型数据库，一条数据在这里就是一个文档。 为了方便大家理解，我们将 Elastic s earch 里 存储 文档 数据和关系型数据库 MySQL 存储数据的概念进行一个类比\",\"ES里的 Index 可以看做一个库，而 Types 相当于表， Documents 则相当于表的行。这里Types 的概念已经被逐渐弱化， Elasticsearch 6.X 中，一个 index 下已经只能包含一个type Elasticsearch 7.X 中 , Type 的概念已经被删除了。\"]},{\"header\":\"4、HTTP操作\",\"slug\":\"_4、http操作\",\"contents\":[]},{\"header\":\"1、索引操作\",\"slug\":\"_1、索引操作\",\"contents\":[]},{\"header\":\"1、创建索引\",\"slug\":\"_1、创建索引\",\"contents\":[\"对比关系型数据库，创建索引就等同于创建数据库\",\"在Postman 中，向 ES 服务器发 PUT 请求 http://127.0.0.1:9200/start\",\"请求后，服务器返回响应\",\"\\\" 【响应结果】 : true, ## true 操作成功 \\\"shards_ 【分片结果】 : true, ## 分片操作成功 \\\" 【索引名称】 : \\\" ## 注意：创建索引库的分片数默认 1 片，在 7.0.0 之前的 Elasticsearch 版本中，默认 5 片 \",\"如果重复添加索引，会返回错误信息\"]},{\"header\":\"2、查看索引\",\"slug\":\"_2、查看索引\",\"contents\":[\"在Postman 中，向 ES 服务器发 GET 请求 http://127.0.0.1:9200/_cat/indices?v\",\"这里请求路径中的**_cat 表示查看的意思**， indices 表示索引，所以整体含义就是查看当前 ES服务器中的所有索引，就好像 MySQL 中的 show tables 的感觉，服务器响应结果如下\"]},{\"header\":\"3、查看单个索引\",\"slug\":\"_3、查看单个索引\",\"contents\":[\"在Postman 中，向 ES 服务器发 GET 请求 http://127.0.0.1:9200/start\",\"查看索引向ES 服务器发送的请求路径和创建索引是一致的。但是 HTTP 方法不一致。这里可以体会一下 RESTful 的意义，请求后，服务器响应结果如下：\",\"{ \\\"start\\\"【索引名】: { \\\"aliases\\\"【别名】: {}, \\\"mappings\\\"【映射】: {}, \\\"settings\\\"【设置】: { \\\"index\\\"【设置-索引】: { \\\"routing\\\": { \\\"allocation\\\": { \\\"include\\\": { \\\"_tier_preference\\\": \\\"data_content\\\" } } }, \\\"number_of_shards\\\"【设置-索引-主分片数量】: \\\"1\\\", \\\"provided_name\\\"【名称】: \\\"start\\\", \\\"creation_date\\\"【创建时间】: \\\"1624440317651\\\", \\\"number_of_replicas\\\"【设置-索引-副分片数量】: \\\"1\\\", \\\"uuid\\\"【唯一标识】: \\\"5tI3rmvvQsKJISZ8GDR-YQ\\\", \\\"version\\\"【设置-索引-版本】: { \\\"created\\\": \\\"7130299\\\" } } } } } \"]},{\"header\":\"4、删除索引\",\"slug\":\"_4、删除索引\",\"contents\":[\"在Postman 中，向 ES 服务器发 DELETE 请求 http://127.0.0.1:9200/start\",\"重新访问索引时，服务器返回响应：索引不存在\"]},{\"header\":\"2、文档操作\",\"slug\":\"_2、文档操作\",\"contents\":[]},{\"header\":\"1、创建文档\",\"slug\":\"_1、创建文档\",\"contents\":[\"索引已经创建好了，接下来我们来创建文档，并添加数据。这里的文档可以类比为关系型数据库中的表数据，添加的数据格式为 JSON 格式\",\"在Postman 中，向 ES 服务器发 POST 请求 http://127.0.0.1:9200/start/doc\",\"此处发送请求的方式必须为POST ，不能是 PUT ，否则会发生错误\",\"{ \\\"_index\\\"【索引】: \\\"start\\\", \\\"_type\\\"【类型-文档】: \\\"doc\\\", \\\"_id\\\"【唯一标识】: \\\"iY9GOHoBucAyibLJ1Bbq\\\",#可以类比为 MySQL 中的主键，随机生成 \\\"_version\\\"【版本号】: 1, \\\"result\\\"【结果】: \\\"created\\\", #这里的 create 表示创建成功 \\\"_shards\\\"【分片】: { \\\"total\\\"【分片-总数】: 2, \\\"successful\\\"【分片-成功】: 1, \\\"failed\\\"【分片-s】: 0 }, \\\"_seq_no\\\": 1, \\\"_primary_term\\\": 1 } \",\"上面的数据创建后，由于没有指定数据唯一性标识（ID ），默认情况下 ES 服务器会随机生成一个 。\",\"如果想要自定义唯一性标识，需要在创建时指定http://127.0.0.1:9200/start/doc/1 or http://127.0.0.1:9200/start/_doc/1\",\"此处需要注意：如果增加数据时明确数据主键，那么请求方式也可以为PUT\"]},{\"header\":\"2、查看文档\",\"slug\":\"_2、查看文档\",\"contents\":[\"查看文档时，需要指明文档的唯一性标识，类似于MySQL 中数据的主键查询\",\"在Postman 中，向 ES 服务器发 GET 请求 http://127.0.0.1:9200/start/_doc/1\",\"查询成功后，服务器响应结果：\",\"{ \\\"_index\\\"【索引】: \\\"start\\\", \\\"_type\\\"【文档类型】: \\\"_doc\\\", \\\"_id\\\": \\\"1\\\", \\\"_version\\\": 1, \\\"_seq_no\\\": 2, \\\"_primary_term\\\": 2, \\\"found\\\"【查询结果】: true,## true 表示查找到， false 表示未查找到 \\\"_source\\\"【文档源信息】: { \\\"title\\\": \\\"小米手机\\\", \\\"category\\\": \\\"小米\\\", \\\"images\\\": \\\"www.xiaobear.cn\\\", \\\"price\\\": 3999.00 } } \"]},{\"header\":\"3、修改文档\",\"slug\":\"_3、修改文档\",\"contents\":[\"和新增文档一样，输入相同的URL 地址请求，如果请求体变化，会将原有的数据内容覆盖 在Postman 中，向 ES 服 务器发 POST 请求 http://127.0.0.1:9200/start/_doc/1\",\"请求体内容为:\",\"{ \\\"title\\\":\\\"华为手机\\\", \\\"category\\\":\\\"小米\\\", \\\"images\\\":\\\"www.xiaobear.cn\\\", \\\"price\\\":4999.00 } \",\"修改成功后，服务器响应结果：\",\"{ \\\"_index\\\": \\\"start\\\", \\\"_type\\\": \\\"_doc\\\", \\\"_id\\\": \\\"1\\\", \\\"_version\\\"【版本】: 2, \\\"result\\\"【结果】: \\\"updated\\\",## updated 表示数据被更新 \\\"_shards\\\": { \\\"total\\\": 2, \\\"successful\\\": 1, \\\"failed\\\": 0 }, \\\"_seq_no\\\": 3, \\\"_primary_term\\\": 2 } \"]},{\"header\":\"4、修改字段\",\"slug\":\"_4、修改字段\",\"contents\":[\"修改数据时，也可以只修改某一给条数据的局部信息\",\"在Postman 中，向 ES 服务器发 POST 请求 http://127.0.0.1:9200/start/_update/1\",\"请求体内容为：\",\"{ \\\"doc\\\": { \\\"price\\\":5000.00 } } \",\"修改成功后，服务器响应结果：\",\"根据唯一性标识，查询文档数据，文档数据已经更新\"]},{\"header\":\"5、删除文档\",\"slug\":\"_5、删除文档\",\"contents\":[\"删除一个文档不会立即从磁盘上移除，它只是被标记成已删除（逻辑删除）。\",\"在Postman 中，向 ES 服务器发 DELETE 请求 http://127.0.0.1:9200/start/_doc/1\",\"删除成功，服务器响应结果：\",\"version：对数据的操作，都会更新版本\",\"删除后再查询当前文档信息\",\"如果删除一个并不存在的文档\"]},{\"header\":\"6、条件删除文档\",\"slug\":\"_6、条件删除文档\",\"contents\":[\"一般删除数据都是根据文档的唯一性标识进行删除，实际操作时，也可以根据条件对多条数据进行删除\",\"首先分别增加多条数据：\",\"{ \\\"title\\\":\\\"小米手机\\\", \\\"category\\\":\\\"小米\\\", \\\"images\\\":\\\"www.xiaobear.cn\\\", \\\"price\\\":3999.00 } { \\\"title\\\":\\\"华为手机\\\", \\\"category\\\":\\\"华为\\\", \\\"images\\\":\\\"www.xiaobear.cn\\\", \\\"price\\\":3999.00 } \",\"向ES 服务器发 POST 请求 http://127.0.0.1:9200/start/_delete_by_query\",\"请求体内容为：\",\"{ \\\"query\\\":{ \\\"match\\\":{ \\\"price\\\": 3999.00 } } } \",\"删除成功后，服务器响应结果：\"]},{\"header\":\"3、映射操作\",\"slug\":\"_3、映射操作\",\"contents\":[\"有了索引库，等于有了数据库中的database 。\",\"接下来就需要建索引库(index)中的映射了，类似于数据库 (database)中的表结构 (table)。创建数据库表需要设置字段名称，类型，长度，约束等；索引库也一样，需要知道这个类型下有哪些字段，每个字段有哪些约束信息，这就叫做映射 (mapping)。\"]},{\"header\":\"1、创建映射\",\"slug\":\"_1、创建映射\",\"contents\":[\"在Postman 中，向 ES 服务器发 PUT 请求http://127.0.0.1:9200/user/_mapping\",\"请求内容为：\",\"{ \\\"properties\\\":{ \\\"name\\\":{ \\\"type\\\": \\\"text\\\", \\\"index\\\": true }, \\\"sex\\\":{ \\\"type\\\": \\\"keyword\\\", \\\"index\\\": true }, \\\"phone\\\":{ \\\"type\\\": \\\"keyword\\\", \\\"index\\\": false } } } \",\"服务器的响应\",\"映射数据说明\",\"字段名：任意填写，下面指定许多属性，例如： title 、 subtitle 、 images 、 price\",\"type ：类型 Elasticsearch 中支持的数据类型非常丰富，说几个关键的：\",\"String 类型，又分两种： \",\"text：可分词\",\"keyword：不可分词，数据会作为完整字段进行匹配\",\"Numerical ：数值类型，分两类 \",\"基本数据类型：long 、 integer 、 short 、 byte 、 double 、 float 、 half_float\",\"浮点数的高精度类型：scaled_float\",\"Date ：日期类型\",\"Array ：数组类型\",\"Object ：对象\",\"index ：是否索引，默认为 true ，也就是说你不进行任何配置，所有字段都会被索引。\",\"true：字段会被索引，则可以用来进行搜索\",\"false：字段不会被索引，不能用来搜索\",\"store ：是否将数据进行独立存储，默认为 false\",\"原始的文本会存储在_source 里面，默认情况下其他提取出来的字段都不是独立存储的，是从 _source 里面提取出来的。当然你也可以独立的存储某个字段，只要设置\\\"store\\\": true 即可，获取独立存储的字段要比从 _source 中解析快得多，但是也会占用更多的空间，所以要根据实际业务需求来设置。\",\"analyzer ：分词器，这里的 ik_max_word 即使用 ik 分词器\"]},{\"header\":\"2、查看映射\",\"slug\":\"_2、查看映射\",\"contents\":[\"在Postman 中，向 ES 服务器发 GET 请求http://127.0.0.1:9200/user/_mapping\"]},{\"header\":\"3、索引映射关联\",\"slug\":\"_3、索引映射关联\",\"contents\":[\"在Postman 中，向 ES 服务器发 PUT 请求 http://127.0.0.1:9200/user1\"]}]},\"/study-tutorial/distributed/elasticsearch/java_api.html\":{\"title\":\"3、Java API\",\"contents\":[{\"header\":\"1、环境准备\",\"slug\":\"_1、环境准备\",\"contents\":[\"创建maven项目\",\"导入相关依赖\",\" <dependencies> <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch</artifactId> <version>7.8.0</version> </dependency> <!--es的客户端--> <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>elasticsearch-rest-high-level-client</artifactId> <version>7.8.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.14.1</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>2.14.1</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-databind</artifactId> <version>2.12.4</version> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.1</version> <scope>test</scope> </dependency> </dependencies> \",\"创建测试\",\"public class ElasticSearchTest { public static void main(String[] args) throws IOException { //9200 端口为 Elastic s earch 的 Web 通信端口 localhost 为启动 ES 服务的主机名 RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); client.close(); } } \",\"没有任何输出或报错信息即是成功\"]},{\"header\":\"2、索引操作\",\"slug\":\"_2、索引操作\",\"contents\":[]},{\"header\":\"1、创建索引\",\"slug\":\"_1、创建索引\",\"contents\":[\"ES服务器正常启动后，可以通过 Java API 客户端对象对 ES 索引进行操作\",\"public class ElasticsearchDocCreate { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //创建索引 CreateIndexRequest request = new CreateIndexRequest(\\\"user\\\"); CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT); //创建索引的响应状态 boolean acknowledged = response.isAcknowledged(); System.out.println(\\\"响应状态为：\\\" + acknowledged); client.close(); } } \",\"输出结果\"]},{\"header\":\"2、查看索引\",\"slug\":\"_2、查看索引\",\"contents\":[\"public class ElasticsearchDocSearch { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //查询索引 GetIndexRequest request = new GetIndexRequest(\\\"user\\\"); GetIndexResponse response = client.indices().get(request, RequestOptions.DEFAULT); //查询索引的响应状态 System.out.println(response); System.out.println(response.getSettings()); System.out.println(response.getAliases()); System.out.println(response.getMappings()); client.close(); } } \",\"输出结果\"]},{\"header\":\"3、删除索引\",\"slug\":\"_3、删除索引\",\"contents\":[\"public class ElasticsearchDocDelete { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //删除索引 DeleteIndexRequest request = new DeleteIndexRequest(\\\"user\\\"); AcknowledgedResponse response = client.indices().delete(request, RequestOptions.DEFAULT); //删除索引的响应状态 System.out.println(\\\"删除状态为：\\\" + response.isAcknowledged()); client.close(); } } \"]},{\"header\":\"3、文档操作\",\"slug\":\"_3、文档操作\",\"contents\":[\"创建数据模型\",\"public class User { private String name; private Integer age; private String sex; public User() { } public User(String name, Integer age, String sex) { this.name = name; this.age = age; this.sex = sex; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } @Override public String toString() { return new StringJoiner(\\\", \\\", User.class.getSimpleName() + \\\"[\\\", \\\"]\\\") .add(\\\"name='\\\" + name + \\\"'\\\") .add(\\\"age=\\\" + age) .add(\\\"sex='\\\" + sex + \\\"'\\\") .toString(); } } \"]},{\"header\":\"1、创建数据，添加到文档\",\"slug\":\"_1、创建数据-添加到文档\",\"contents\":[\"public class ElasticsearchDocInsert { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); IndexRequest indexRequest = new IndexRequest(); indexRequest.index(\\\"user\\\").id(\\\"1001\\\"); //创建数据对象 User user = new User(\\\"xiaobear\\\",18,\\\"boy\\\"); //数据对象转为JSON ObjectMapper mapper = new ObjectMapper(); String userJson = mapper.writeValueAsString(user); indexRequest.source(userJson, XContentType.JSON); //获取响应对象 IndexResponse response = client.index(indexRequest, RequestOptions.DEFAULT); System.out.println(\\\"_index：\\\" + response.getIndex()); System.out.println(\\\"_id：\\\" + response.getId()); System.out.println(\\\"_result：\\\" + response.getResult()); client.close(); } } \",\"输出结果\"]},{\"header\":\"2、修改文档\",\"slug\":\"_2、修改文档\",\"contents\":[\"public class ElasticsearchDocUpdate { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //修改文档 UpdateRequest request = new UpdateRequest(); request.index(\\\"user\\\").id(\\\"1001\\\"); // 设置请求体，对数据进行修改 request.doc(XContentType.JSON,\\\"sex\\\",\\\"girl\\\"); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\\\"_index：\\\" + response.getIndex()); System.out.println(\\\"_id：\\\" + response.getId()); System.out.println(\\\"_result：\\\" + response.getResult()); client.close(); } } \",\"输出结果\"]},{\"header\":\"3、查询文档\",\"slug\":\"_3、查询文档\",\"contents\":[\"public class ElasticsearchDocGet { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //创建请求对象 GetRequest request = new GetRequest().index(\\\"user\\\").id(\\\"1001\\\"); //创建响应对象 GetResponse response = client.get(request, RequestOptions.DEFAULT); // 打印结果信息 System.out.println(\\\"_index:\\\" + response.getIndex()); System.out.println(\\\"_type:\\\" + response.getType()); System.out.println(\\\"_id:\\\" + response.getId()); System.out.println(\\\"source:\\\" + response.getSourceAsString()); client.close(); } } \",\"输出结果\"]},{\"header\":\"4、删除文档\",\"slug\":\"_4、删除文档\",\"contents\":[\"public class ElasticsearchDoc_Delete { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\\\"localhost\\\",9200))); //创建请求对象 DeleteRequest request = new DeleteRequest().index(\\\"user\\\").id(\\\"1\\\"); //创建响应对象 DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); //打印信息 System.out.println(response.toString()); client.close(); } } \",\"输出结果\"]},{\"header\":\"5、批量操作\",\"slug\":\"_5、批量操作\",\"contents\":[]},{\"header\":\"1、批量新增\",\"slug\":\"_1、批量新增\",\"contents\":[\"public class ElasticSearchBatchInsert { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建批量新增请求对象 BulkRequest request = new BulkRequest(); request.add(new IndexRequest().index(\\\"user\\\").id(\\\"1004\\\").source(XContentType.JSON,\\\"name\\\",\\\"xiaohuahua\\\")); request.add(new IndexRequest().index(\\\"user\\\").id(\\\"1005\\\").source(XContentType.JSON,\\\"name\\\",\\\"zhangsan\\\")); request.add(new IndexRequest().index(\\\"user\\\").id(\\\"1006\\\").source(XContentType.JSON,\\\"name\\\",\\\"lisi\\\")); //创建响应对象 BulkResponse response = client.bulk(request, RequestOptions.DEFAULT); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"items：\\\" + response.getItems()); } } \",\"输出结果\",\"查询文档：\",\"http://127.0.0.1:9200/user/_doc/1005\",\"http://127.0.0.1:9200/user/_doc/1004\",\"http://127.0.0.1:9200/user/_doc/1006\"]},{\"header\":\"2、批量删除\",\"slug\":\"_2、批量删除\",\"contents\":[\"public class ElasticSearchBatchDelete { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建批量新增请求对象 BulkRequest request = new BulkRequest(); request.add(new DeleteRequest().index(\\\"user\\\").id(\\\"1001\\\")); request.add(new DeleteRequest().index(\\\"user\\\").id(\\\"1002\\\")); request.add(new DeleteRequest().index(\\\"user\\\").id(\\\"1003\\\")); //创建响应对象 BulkResponse response = client.bulk(request, RequestOptions.DEFAULT); System.out.println(\\\"took：\\\" + response.getTook()); Arrays.stream(response.getItems()).forEach(System.out::println); System.out.println(\\\"items：\\\" + Arrays.toString(response.getItems())); System.out.println(\\\"status：\\\" + response.status()); System.out.println(\\\"失败消息：\\\" + response.buildFailureMessage()); } } \",\"输出结果\"]},{\"header\":\"4、高级查询\",\"slug\":\"_4、高级查询\",\"contents\":[]},{\"header\":\"1、请求体查询\",\"slug\":\"_1、请求体查询\",\"contents\":[]},{\"header\":\"1、查询所有索引数据\",\"slug\":\"_1、查询所有索引数据\",\"contents\":[\"public class RequestBodyQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询所有对象 sourceBuilder.query(QueryBuilders.matchAllQuery()); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"2、term查询\",\"slug\":\"_2、term查询\",\"contents\":[\"public class TremQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询所有对象 sourceBuilder.query(QueryBuilders.termQuery(\\\"name\\\",\\\"zhangsan\\\")); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"3、分页查询\",\"slug\":\"_3、分页查询\",\"contents\":[\"public class PageQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询所有对象 sourceBuilder.query(QueryBuilders.matchAllQuery()); //分页查询 当前页其实索引 第一条数据的顺序号 from sourceBuilder.from(0); //每页显示多少条 sourceBuilder.size(2); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"4、数据排序\",\"slug\":\"_4、数据排序\",\"contents\":[\"public class DataSorting { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询所有对象 sourceBuilder.query(QueryBuilders.matchAllQuery()); //数据排序 sourceBuilder.sort(\\\"age\\\", SortOrder.DESC); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \"]},{\"header\":\"5、过滤字段\",\"slug\":\"_5、过滤字段\",\"contents\":[\"public class FilterFiled { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询所有对象 sourceBuilder.query(QueryBuilders.matchAllQuery()); //数据排序 sourceBuilder.sort(\\\"age\\\", SortOrder.DESC); //查询过滤字段 String[] excludes = {}; //过滤掉name属性 String[] includes = {\\\"age\\\"}; sourceBuilder.fetchSource(includes,excludes); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"6、Bool查询\",\"slug\":\"_6、bool查询\",\"contents\":[\"public class BoolSearch { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); //必须包含 boolQuery.must(QueryBuilders.matchQuery(\\\"age\\\",18)); //一定不包含 boolQuery.mustNot(QueryBuilders.matchQuery(\\\"name\\\",\\\"lisi\\\")); //可能包含 boolQuery.should(QueryBuilders.matchQuery(\\\"name\\\",\\\"zhangsan\\\")); //查询所有对象 sourceBuilder.query(boolQuery); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"7、范围查询\",\"slug\":\"_7、范围查询\",\"contents\":[\"public class RangeSearch { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //范围查询 RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(\\\"age\\\"); //大于等于 rangeQuery.gte(\\\"19\\\"); //小于等于 rangeQuery.lte(\\\"40\\\"); //查询所有对象 sourceBuilder.query(rangeQuery); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"8、模糊查询\",\"slug\":\"_8、模糊查询\",\"contents\":[\"public class FuzzySearch { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest(); request.indices(\\\"user\\\"); //构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //模糊查询 FuzzyQueryBuilder fuzzyQuery = QueryBuilders.fuzzyQuery(\\\"name\\\", \\\"zhangsan\\\"); fuzzyQuery.fuzziness(Fuzziness.ONE); sourceBuilder.query(fuzzyQuery); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //查询匹配 SearchHits hits = response.getHits(); System.out.println(\\\"took：\\\" + response.getTook()); System.out.println(\\\"是否超时：\\\" + response.isTimedOut()); System.out.println(\\\"TotalHits：\\\" + hits.getTotalHits()); System.out.println(\\\"MaxScore：\\\" + hits.getMaxScore()); for (SearchHit hit : hits) { System.out.println(hit.getSourceAsString()); } } } \",\"输出结果\"]},{\"header\":\"5、高亮查询\",\"slug\":\"_5、高亮查询\",\"contents\":[\"public class HighlightQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //高亮查询 SearchRequest request = new SearchRequest(\\\"user\\\"); //创建查询请求体构建器 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //构建查询方式，高亮查询 TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(\\\"name\\\", \\\"zhangsan\\\"); //设置查询方式 sourceBuilder.query(termQueryBuilder); //构建高亮字段 HighlightBuilder highlightBuilder = new HighlightBuilder(); //设置标签前缀 highlightBuilder.preTags(\\\"<font color='red'\\\"); //设置标签后缀 highlightBuilder.postTags(\\\"</font>\\\"); //设置高亮字段 highlightBuilder.field(\\\"name\\\"); //设置高亮构建对象 sourceBuilder.highlighter(highlightBuilder); //设置请求体 request.source(sourceBuilder); //客户端发送请求，获取响应对象 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 打印响应结果 SearchHits hits = response.getHits(); System.out.println(\\\"took::\\\"+response.getTook()); System.out.println(\\\"time_out::\\\"+response.isTimedOut()); System.out.println(\\\"total::\\\"+hits.getTotalHits()); System.out.println(\\\"max_s core::\\\"+hits.getMaxScore()); System.out.println(\\\"hits::::>>\\\"); for (SearchHit hit : hits) { String sourceAsString = hit.getSourceAsString(); System.out.println(sourceAsString); //打印高亮结果 Map<String, HighlightField> highlightFields = hit.getHighlightFields(); System.out.println(highlightFields); System.out.println(\\\"<<::::\\\"); } } } \",\"输出结果\"]},{\"header\":\"6、聚合查询\",\"slug\":\"_6、聚合查询\",\"contents\":[]},{\"header\":\"1、最大年龄\",\"slug\":\"_1、最大年龄\",\"contents\":[\"public class AggregateQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); SearchRequest request = new SearchRequest().indices(\\\"user\\\"); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(AggregationBuilders.max(\\\"maxAge\\\").field(\\\"age\\\")); //设置请求体 request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); //打印响应结果 SearchHits hits = response.getHits(); System.out.println(\\\"hits = \\\" + hits); System.out.println(response); } } \",\"输出结果\"]},{\"header\":\"2、分组查询\",\"slug\":\"_2、分组查询\",\"contents\":[\"public class GroupQuery { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(\\\"localhost\\\", 9200))); //创建搜索对象 SearchRequest request = new SearchRequest().indices(\\\"user\\\"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.aggregation(AggregationBuilders.terms(\\\"age_groupby\\\").field(\\\"age\\\")); //设置请求体 request.source(searchSourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); System.out.println(response.getHits()); System.out.println(response); } } \",\"输出结果\"]}]},\"/study-tutorial/distributed/elasticsearch/overview.html\":{\"title\":\"1、Elasticsearch概述\",\"contents\":[{\"header\":\"1、什么是Elasticsearch\",\"slug\":\"_1、什么是elasticsearch\",\"contents\":[\"​ The Elastic Stack, 包括 Elasticsearch、Kibana、Beats 和 Logstash（也称为 ELK Stack）。能够安全可靠地获取任何来源、任何格式的数据，然后实时地对数据进行搜索、分析和可视化。Elaticsearch，简称为ES， ES是一个开源的高扩展的分布式全文搜索引擎，是整个Elastic Stack技术栈的核心。它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。\"]},{\"header\":\"2、全文搜索引擎\",\"slug\":\"_2、全文搜索引擎\",\"contents\":[\"​ Google，百度类的网站搜索，它们都是根据网页中的关键字生成索引，我们在搜索的时候输入关键字，它们会将该关键字即索引匹配到的所有网页返回；还有常见的项目中应用日志的搜索等等。对于这些非结构化的数据文本，关系型数据库搜索不是能很好的支持。\",\"​ 一般传统数据库，全文检索都实现的很鸡肋，因为一般也没人用数据库存文本字段。进行全文检索需要扫描整个表，如果数据量大的话即使对SQL的语法优化，也收效甚微。建立了索引，但是维护起来也很麻烦，对于 insert 和 update 操作都会重新构建索引。\",\"​ 基于以上原因可以分析得出，在一些生产环境中，使用常规的搜索方式，性能是非常差的：\",\"搜索的数据对象是大量的非结构化的文本数据。\",\"文件记录量达到数十万或数百万个甚至更多。\",\"支持大量基于交互式文本的查询。\",\"需求非常灵活的全文搜索查询。\",\"对高度相关的搜索结果的有特殊需求，但是没有可用的关系数据库可以满足。\",\"对不同记录类型、非文本数据操作或安全事务处理的需求相对较少的情况。\",\"为了解决结构化数据搜索和非结构化数据搜索性能问题，我们就需要专业，健壮，强大的全文搜索引擎，这里说到的全文搜索引擎指的是目前广泛应用的主流搜索引擎。它的工作原理是计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。\"]},{\"header\":\"3、Elasticsearch And Solr\",\"slug\":\"_3、elasticsearch-and-solr\",\"contents\":[\"​ Lucene是Apache软件基金会Jakarta项目组的一个子项目，提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。但Lucene只是一个提供全文搜索功能类库的核心工具包，而真正使用它还需要一个完善的服务框架搭建起来进行应用。\",\"目前市面上流行的搜索引擎软件，主流的就两款：Elasticsearch和Solr,这两款都是基于Lucene搭建的，可以独立部署启动的搜索引擎服务软件。由于内核相同，所以两者除了服务器安装、部署、管理、集群以外，对于数据的操作 修改、添加、保存、查询等等都十分类似。\"]},{\"header\":\"4、Elasticsearch Or Solr\",\"slug\":\"_4、elasticsearch-or-solr\",\"contents\":[\"Elasticsearch和Solr都是开源搜索引擎，那么我们在使用时该如何选择呢？\",\"Google 搜索趋势结果表明，与 Solr 相比， Elasticsearch 具有很大的吸引力，但这并不意味着 Apache Solr 已经死亡。虽然有些人可能不这么认为，但 Solr 仍然是最受欢迎的搜索引擎之一，拥有强大的社区和开源支持。\",\"与 Solr 相比， Elasticsearch 易于安装且非常轻巧。此外，你可以在几分钟内安装并运行Elasticsearch 。但是，如果 Elasticsearch 管理不当，这种易于部署和使用可能会成为一个问题。基于 JSON 的配置很简单，但如果要为文件中的每个配置指定注释，那么它不适合您。总的来说，如果你的应用使用 的是 JSON ，那么 Elasticsearch 是一个更好的选择。否则，请使用 Solr ，因为它的 schema.xml 和 solrconfig.xml 都有很好的文档记录。\",\"Solr 拥有更大，更成熟的用户，开发者和贡献者社区。 ES 虽拥有的规模较小但活跃的用户社区以及不断增长的贡献者社区。Solr贡献者和提交者来自许多不同的组织，而 Elasticsearch 提交者来自单个公司。\",\"Solr 更成熟，但 ES 增长迅速，更稳定。\",\"Solr 是一个非常有据可查的产品，具有清晰的示例和 API 用例场景。 Elasticsearch 的文档组织良好，但它缺乏好的示例和清晰的配置说明。\",\"那么，到底是Solr 还是 Elasticsearch？\",\"由于易于使用， Elasticsearch 在新开发者中更受欢迎。一个下载和一个命令就可以启动一切。\",\"如果除了搜索文本之外还需要它来处理分析查询， Elasticsearch 是更好的选择\",\"如果需要分布式索引，则需要选择 Elasticsearch 。对于需要良好可伸缩性和 以及 性能分布式环境 Elasticsearch 是更好的选择。\",\"Elasticsearch 在开源日志管理用例中占据主导地位，许多组织在 Elasticsearch 中索引它们的日志以使其可搜索。\",\"如果你喜欢监控和指标，那么请使用Elasticsearch ，因为相对于Solr，Elasticsearch 暴露了更多的关键指标\"]}]},\"/study-tutorial/distributed/docker/common_commands.html\":{\"title\":\"3、Docker常用命令\",\"contents\":[{\"header\":\"1、帮助类启动命令\",\"slug\":\"_1、帮助类启动命令\",\"contents\":[\"启动docker： systemctl start docker\",\"停止docker： systemctl stop docker\",\"重启docker： systemctl restart docker\",\"查看docker状态： systemctl status docker\",\"开机启动： systemctl enable docker\",\"查看docker概要信息： docker info\",\"查看docker总体帮助文档： docker --help\",\"查看docker命令帮助文档： docker 具体命令 --help\"]},{\"header\":\"2、镜像命令\",\"slug\":\"_2、镜像命令\",\"contents\":[]},{\"header\":\"1、列出本地主机上的镜像\",\"slug\":\"_1、列出本地主机上的镜像\",\"contents\":[\"docker images \",\"各个选项说明:\",\"REPOSITORY：表示镜像的仓库源\",\"TAG：镜像的标签版本号\",\"IMAGE ID：镜像ID\",\"CREATED：镜像创建时间\",\"SIZE：镜像大小\",\"同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像\",\"OPTIONS说明：\",\"-a :列出本地所有的镜像（含历史映像层） docker ../../images -a -q :只显示镜像ID。 docker ../../images -q \"]},{\"header\":\"2、搜索镜像\",\"slug\":\"_2、搜索镜像\",\"contents\":[\"官方搜索网址：https://hub.docker.com/\",\"命令\",\"docker search [OPTIONS] 镜像名字 \",\"[root@docker ~]## docker search mysql NAME DESCRIPTION STARS OFFICIAL AUTOMATED mysql MySQL is a widely used, open-source relation… 12378 [OK] mariadb MariaDB Server is a high performing open sou… 4762 [OK] mysql/mysql-server Optimized MySQL Server Docker ../../images. Create… 917 [OK] percona Percona Server is a fork of the MySQL relati… 572 [OK] phpmyadmin phpMyAdmin - A web interface for MySQL and M… 494 [OK] mysql/mysql-cluster Experimental MySQL Cluster Docker ../../images. Cr… 93 centos/mysql-57-centos7 MySQL 5.7 SQL database server 92 bitnami/mysql Bitnami MySQL Docker Image 68 [OK] ubuntu/mysql MySQL open source fast, stable, multi-thread… 29 circleci/mysql MySQL is a widely used, open-source relation… 25 mysql/mysql-router MySQL Router provides transparent routing be… 23 centos/mysql-56-centos7 MySQL 5.6 SQL database server 22 google/mysql MySQL server for Google Compute Engine 21 [OK] vmware/harbor-db Mysql container for Harbor 10 mysqlboy/docker-mydumper docker-mydumper containerizes MySQL logical … 3 mysqlboy/mydumper mydumper for mysql logcial backups 3 bitnami/mysqld-exporter 2 ibmcom/mysql-s390x Docker image for mysql-s390x 2 mysql/mysql-operator MySQL Operator for Kubernetes 0 ibmcom/tidb-ppc64le TiDB is a distributed NewSQL database compat… 0 mysqlboy/elasticsearch 0 mysqleatmydata/mysql-eatmydata 0 cimg/mysql 0 mysql/ndb-operator MySQL NDB Operator for Kubernetes 0 mirantis/mysql \"]},{\"header\":\"OPTIONS说明：\",\"slug\":\"options说明\",\"contents\":[\"--limit : 只列出N个镜像，默认25个\",\"docker search --limit 5 redis \"]},{\"header\":\"3、拉取（下载）镜像\",\"slug\":\"_3、拉取-下载-镜像\",\"contents\":[\"docker pull 某个XXX镜像名字 \",\"docker pull 某个XXX镜像名字[:TAG]\",\"docker pull 镜像名字\",\"没有TAG就是最新版\",\"等价于：docker pull 镜像名字:latest\"]},{\"header\":\"4、查看镜像\",\"slug\":\"_4、查看镜像\",\"contents\":[\"docker system df 查看镜像/容器/数据卷所占的空间 \",\"[root@docker ~]## docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE ../../images 1 1 13.26kB 0B (0%) Containers 1 0 0B 0B Local Volumes 0 0 0B 0B Build Cache 0 0 0B 0B [root@docker ~]## \"]},{\"header\":\"5、删除镜像\",\"slug\":\"_5、删除镜像\",\"contents\":[\"docker rmi 某个XXX镜像名字ID \"]},{\"header\":\"1、删除单个\",\"slug\":\"_1、删除单个\",\"contents\":[\"docker rmi -f 镜像ID \"]},{\"header\":\"2、删除多个\",\"slug\":\"_2、删除多个\",\"contents\":[\"docker rmi -f 镜像名1:TAG 镜像名2:TAG \"]},{\"header\":\"3、删除全部\",\"slug\":\"_3、删除全部\",\"contents\":[\"docker rmi -f $(docker ../../images -qa) \"]},{\"header\":\"6、面试题：谈谈docker虚悬镜像是什么？\",\"slug\":\"_6、面试题-谈谈docker虚悬镜像是什么\",\"contents\":[\"仓库名、标签都是<none>的镜像，俗称虚悬镜像dangling image\"]},{\"header\":\"3、容器命令\",\"slug\":\"_3、容器命令\",\"contents\":[\"有镜像才能创建容器\",\"本次用centos进行演示\"]},{\"header\":\"1、新建+启动命令\",\"slug\":\"_1、新建-启动命令\",\"contents\":[\"docker run [OPTIONS] IMAGE [COMMAND] [ARG...] \",\"OPTIONS说明（常用）：有些是一个减号，有些是两个减号\",\"--name=\\\"容器新名字\\\" 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)；\",\"-i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)；\",\"-P: 随机端口映射，大写P -p: 指定端口映射，小写p\",\"#使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。\",\"docker run -it centos /bin/bash \",\"参数说明： -i: 交互式操作。 -t: 终端。 centos : centos 镜像。 /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。 要退出终端，直接输入 exit:\"]},{\"header\":\"2、列出当前所有正在运行的容器\",\"slug\":\"_2、列出当前所有正在运行的容器\",\"contents\":[\"docker ps [OPTIONS] \",\"OPTIONS说明（常用）：\",\"-a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。\"]},{\"header\":\"3、退出容器\",\"slug\":\"_3、退出容器\",\"contents\":[]},{\"header\":\"1、 exec 退出\",\"slug\":\"_1、exec退出\",\"contents\":[\"run进去容器，exit退出，容器停止\"]},{\"header\":\"2、 ctrl+p+q\",\"slug\":\"_2、ctrl-p-q\",\"contents\":[\"run进去容器，ctrl+p+q退出，容器不停止\"]},{\"header\":\"4、启动已停止运行的容器\",\"slug\":\"_4、启动已停止运行的容器\",\"contents\":[\"docker start 容器ID或者容器名 \"]},{\"header\":\"5、重启容器\",\"slug\":\"_5、重启容器\",\"contents\":[\"docker restart 容器ID或者容器名 \"]},{\"header\":\"6、停止容器\",\"slug\":\"_6、停止容器\",\"contents\":[\"docker stop 容器ID或者容器名 \"]},{\"header\":\"7、强制停止容器\",\"slug\":\"_7、强制停止容器\",\"contents\":[\"docker kill 容器ID或容器名 \"]},{\"header\":\"8、删除已停止的容器\",\"slug\":\"_8、删除已停止的容器\",\"contents\":[\"docker rm 容器ID \"]},{\"header\":\"一次性删除多个容器实例\",\"slug\":\"一次性删除多个容器实例\",\"contents\":[\"docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm \",\"docker ps -a -q：查询进行的容器ID\"]},{\"header\":\"4、实操\",\"slug\":\"_4、实操\",\"contents\":[]},{\"header\":\"1、启动守护式容器(后台服务器)\",\"slug\":\"_1、启动守护式容器-后台服务器\",\"contents\":[\"在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 -d 指定容器的后台运行模式。\",\"docker run -d 容器名 \",\"#使用镜像centos:latest以后台模式启动一个容器\",\"docker run -d centos \",\"问题：然后docker ps -a 进行查看, 会发现容器已经退出很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程.容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。\",\"这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如service nginx start但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,这样的容器后台启动后,会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是,将你要运行的程序以前台进程的形式运行，常见就是命令行模式，表示我还有交互操作，别中断，O(∩_∩)O哈哈~\"]},{\"header\":\"案例演示\",\"slug\":\"案例演示\",\"contents\":[\"redis 前后台启动演示case（下载一个Redis6.0.8镜像演示）\",\"前台交互式启动\",\"docker run -it redis \",\"后台守护式启动\",\"docker run -d redis \",\"PS：启动容器后，先查看容器是否启动正常，往往伴随着docker ps\"]},{\"header\":\"2、查看容器日志\",\"slug\":\"_2、查看容器日志\",\"contents\":[\"docker logs 容器ID \",\"PS：容器ID可省略为前三位\",\"[root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94b526c73db2 redis \\\"docker-entrypoint.s…\\\" 4 seconds ago Up 3 seconds 6379/tcp admiring_poincare [root@docker ~]## docker logs 94b 1:C 10 Apr 2022 09:50:03.396 ## oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 10 Apr 2022 09:50:03.396 ## Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started 1:C 10 Apr 2022 09:50:03.396 ## Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 10 Apr 2022 09:50:03.397 * monotonic clock: POSIX clock_gettime 1:M 10 Apr 2022 09:50:03.398 * Running mode=standalone, port=6379. 1:M 10 Apr 2022 09:50:03.399 ## WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 10 Apr 2022 09:50:03.399 ## Server initialized 1:M 10 Apr 2022 09:50:03.399 ## WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 10 Apr 2022 09:50:03.399 * Ready to accept connections [root@docker ~]## \"]},{\"header\":\"3、查看容器内运行的进程\",\"slug\":\"_3、查看容器内运行的进程\",\"contents\":[\"docker top 容器ID \",\"[root@docker ~]## docker top 94b UID PID PPID C STIME TTY TIME CMD polkitd 35360 35341 0 17:50 ? 00:00:00 redis-server *:6379 \"]},{\"header\":\"4、查看容器内部细节\",\"slug\":\"_4、查看容器内部细节\",\"contents\":[\"docker inspect 容器ID \"]},{\"header\":\"5、进入容器内部\",\"slug\":\"_5、进入容器内部\",\"contents\":[]},{\"header\":\"1、使用 exec 进入容器\",\"slug\":\"_1、使用exec进入容器\",\"contents\":[\"docker exec -it 容器ID bashShell \",\"[root@docker ~]## docker exec -it 94b /bin/bash root@94b526c73db2:/data## redis-c redis-check-aof redis-check-rdb redis-cli root@94b526c73db2:/data## redis-cli 127.0.0.1:6379> ping PONG 127.0.0.1:6379> set k1 v1 OK 127.0.0.1:6379> get k1 \\\"v1\\\" 127.0.0.1:6379> exit \",\"exec帮助命令\",\"[root@docker ~]## docker exec --help Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container Options: -d, --detach Detached mode: run command in the background --detach-keys string Override the key sequence for detaching a container -e, --env list Set environment variables --env-file list Read in a file of environment variables -i, --interactive Keep STDIN open even if not attached --privileged Give extended privileges to the command -t, --tty Allocate a pseudo-TTY -u, --user string Username or UID (format: <name|uid>[:<group|gid>]) -w, --workdir string Working directory inside the container [root@docker ~]## \"]},{\"header\":\"2、使用 attach 进入容器\",\"slug\":\"_2、使用attach-进入容器\",\"contents\":[\"docker attach 容器ID \"]},{\"header\":\"3、两者的区别\",\"slug\":\"_3、两者的区别\",\"contents\":[\"attach 直接进入容器启动命令的终端，不会启动新的进程，用exit退出，会导致容器的停止。\",\"exec 是在容器中打开新的终端，并且可以启动新的进程，用exit退出，不会导致容器的停止。\",\"PS：推荐大家使用 docker exec 命令，因为退出容器终端，不会导致容器的停止。\"]},{\"header\":\"5、从容器内拷贝文件到主机上\",\"slug\":\"_5、从容器内拷贝文件到主机上\",\"contents\":[\"把文件从容器内部复制到主机上\",\"docker cp 容器ID:容器内路径 目的主机路径 \",\" docker cp 3065f084c80d:a.txt a.txt \"]},{\"header\":\"6、导入和导出容器\",\"slug\":\"_6、导入和导出容器\",\"contents\":[]},{\"header\":\"1、导出容器\",\"slug\":\"_1、导出容器\",\"contents\":[\"export 导出容器的内容留作为一个tar归档文件[对应import命令]\",\"docker export 容器ID > 文件名.tar \",\"[root@docker /]## docker export 3065f084c80d > centos.tar.gz [root@docker /]## ll 总用量 233004 -rw-r--r--. 1 root root 0 4月 10 18:16 a.txt lrwxrwxrwx. 1 root root 7 4月 9 15:59 bin -> usr/bin dr-xr-xr-x. 5 root root 4096 4月 9 16:06 boot -rw-r--r--. 1 root root 238572032 4月 10 19:04 centos.tar.gz drwxr-xr-x. 20 root root 3220 4月 9 16:29 dev drwxr-xr-x. 86 root root 8192 4月 10 11:03 etc drwxr-xr-x. 3 root root 22 4月 9 16:05 home lrwxrwxrwx. 1 root root 7 4月 9 15:59 lib -> usr/lib lrwxrwxrwx. 1 root root 9 4月 9 15:59 lib64 -> usr/lib64 drwxr-xr-x. 2 root root 6 4月 11 2018 media drwxr-xr-x. 2 root root 6 4月 11 2018 mnt drwxr-xr-x. 4 root root 34 4月 9 20:38 opt dr-xr-xr-x. 124 root root 0 4月 9 16:29 proc dr-xr-x---. 5 root root 176 4月 9 20:36 root drwxr-xr-x. 31 root root 940 4月 9 21:01 run lrwxrwxrwx. 1 root root 8 4月 9 15:59 sbin -> usr/sbin drwxr-xr-x. 2 root root 6 4月 11 2018 srv dr-xr-xr-x. 13 root root 0 4月 9 16:29 sys drwxrwxrwt. 13 root root 4096 4月 10 18:16 tmp drwxr-xr-x. 13 root root 155 4月 9 15:59 usr \"]},{\"header\":\"2、导入容器\",\"slug\":\"_2、导入容器\",\"contents\":[\"import 从tar包中的内容创建一个新的文件系统再导入为镜像[对应export]\",\"cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号 \",\"[root@docker /]## cat centos.tar.gz | docker import - xiaobear/centos:8.8 sha256:12e7a58fc36a755fa63a82f78364291de7271c3ade90f2f5581cc50bc2416cb2 [root@docker /]## docker ../../images REPOSITORY TAG IMAGE ID CREATED SIZE xiaobear/centos 8.8 12e7a58fc36a About a minute ago 231MB redis latest 7614ae9453d1 3 months ago 113MB hello-world latest feb5d9fea6a5 6 months ago 13.3kB centos latest 5d0da3dc9764 6 months ago 231MB [root@docker /]## \"]},{\"header\":\"7、常用命令汇总\",\"slug\":\"_7、常用命令汇总\",\"contents\":[\"命令\",\"英文\",\"中文\",\"attach\",\"Attach to a running container\",\"当前 shell 下 attach 连接指定运行镜像\",\"build\",\"Build an image from a Dockerfile\",\"通过 Dockerfile 定制镜像\",\"commit\",\"Create a new image from a container changes\",\"提交当前容器为新的镜像\",\"cp\",\"Copy files/folders from the containers filesystem to the host path\",\"容器中拷贝指定文件或者目录到宿主机中\",\"create\",\"Create a new container\",\"创建一个新的容器，同 run，但不启动容器\",\"diff\",\"Inspect changes on a container's filesystem\",\"查看 docker 容器变化\",\"events\",\"Get real time events from the server\",\"从 docker 服务获取容器实时事件\",\"exec\",\"Run a command in an existing container\",\"在已存在的容器上运行命令\",\"export\",\"Stream the contents of a container as a tar archive\",\"导出容器的内容流作为一个 tar 归档文件[对应 import ]\",\"history\",\"Show the history of an image\",\"展示一个镜像形成历史\",\"../../images\",\"List ../../images\",\"列出系统当前镜像\",\"import\",\"Create a new filesystem image from the contents of a tarball\",\"从tar包中的内容创建一个新的文件系统映像[对应export]\",\"info\",\"Display system-wide information\",\"显示系统相关信息\",\"inspect\",\"Return low-level information on a container\",\"查看容器详细信息\",\"kill\",\"Kill a running container\",\"kill 指定 docker 容器\",\"load\",\"Load an image from a tar archive\",\"从一个 tar 包中加载一个镜像[对应 save]\",\"login\",\"Register or Login to the docker registry server\",\"注册或者登陆一个 docker 源服务器\",\"logout\",\"Log out from a Docker registry server\",\"从当前 Docker registry 退出\",\"logs\",\"Fetch the logs of a container\",\"输出当前容器日志信息\",\"port\",\"Lookup the public-facing port which is NAT-ed to PRIVATE_PORT\",\"查看映射端口对应的容器内部源端口\",\"pause\",\"Pause all processes within a container\",\"暂停容器\",\"ps\",\"List containers\",\"列出容器列表\"]}]},\"/study-tutorial/distributed/docker/complex_install.html\":{\"title\":\"9、复杂安装\",\"contents\":[{\"header\":\"1、MySQL主从复制\",\"slug\":\"_1、mysql主从复制\",\"contents\":[\"原理：\"]},{\"header\":\"1、搭建步骤\",\"slug\":\"_1、搭建步骤\",\"contents\":[]},{\"header\":\"1、新建主服务器示例3307\",\"slug\":\"_1、新建主服务器示例3307\",\"contents\":[\"docker run -p 3307:3306 --name mysql-master \\\\ -v /mydata/mysql-master/log:/var/log/mysql \\\\ -v /mydata/mysql-master/data:/var/lib/mysql \\\\ -v /mydata/mysql-master/conf:/etc/mysql \\\\ -e MYSQL_ROOT_PASSWORD=root \\\\ -d mysql:5.7 \"]},{\"header\":\"2、进入/mydata/mysql-master/conf目录下新建my.cnf\",\"slug\":\"_2、进入-mydata-mysql-master-conf目录下新建my-cnf\",\"contents\":[\"vim my.cnf \",\"[mysqld] ### 设置server_id，同一局域网中需要唯一 server_id=101 ### 指定不需要同步的数据库名称 binlog-ignore-db=mysql ### 开启二进制日志功能 log-bin=mall-mysql-bin ### 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ### 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ### 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ### 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ### 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 \"]},{\"header\":\"3、重启实例\",\"slug\":\"_3、重启实例\",\"contents\":[\"docker restart mysql-master \"]},{\"header\":\"4、进入容器\",\"slug\":\"_4、进入容器\",\"contents\":[\"docker exec -it mysql-master /bin/bash #登录mysql mysql -u root -p \"]},{\"header\":\"5、master容器实例内创建数据同步用户\",\"slug\":\"_5、master容器实例内创建数据同步用户\",\"contents\":[\"#创建用户名和密码 CREATE USER 'slave'@'%' IDENTIFIED BY '123456'; #授权 GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; \"]},{\"header\":\"6、创建从服务器实例3308\",\"slug\":\"_6、创建从服务器实例3308\",\"contents\":[\"docker run -p 3308:3306 --name mysql-slave \\\\ -v /mydata/mysql-slave/log:/var/log/mysql \\\\ -v /mydata/mysql-slave/data:/var/lib/mysql \\\\ -v /mydata/mysql-slave/conf:/etc/mysql \\\\ -e MYSQL_ROOT_PASSWORD=root \\\\ -d mysql:5.7 \"]},{\"header\":\"7、进入/mydata/mysql-slave/conf目录下新建my.cnf\",\"slug\":\"_7、进入-mydata-mysql-slave-conf目录下新建my-cnf\",\"contents\":[\"[mysqld] ### 设置server_id，同一局域网中需要唯一 server_id=102 ### 指定不需要同步的数据库名称 binlog-ignore-db=mysql ### 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 log-bin=mall-mysql-slave1-bin ### 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ### 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ### 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ### 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ### 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ### relay_log配置中继日志 relay_log=mall-mysql-relay-bin ### log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ### slave设置为只读（具有super权限的用户除外） read_only=1 \"]},{\"header\":\"8、重启mysql-slave\",\"slug\":\"_8、重启mysql-slave\",\"contents\":[\"docker restart mysql-slave \"]},{\"header\":\"9、在主数据库中查看主从同步状态\",\"slug\":\"_9、在主数据库中查看主从同步状态\",\"contents\":[\"show master status; \"]},{\"header\":\"10、进入从容器\",\"slug\":\"_10、进入从容器\",\"contents\":[\"docker exec -it mysql-slave /bin/bash mysql -uroot -proot \"]},{\"header\":\"11、在从数据库中配置主从复制\",\"slug\":\"_11、在从数据库中配置主从复制\",\"contents\":[\"change master to master_host='192.168.130.132', master_user='slave', master_password='123456', master_port=3307, master_log_file='mall-mysql-bin.000001', master_log_pos=617, master_connect_retry=30; \",\"master_host：主数据库的IP地址；\",\"master_port：主数据库的运行端口；\",\"master_user：在主数据库创建的用于同步数据的用户账号；\",\"master_password：在主数据库创建的用于同步数据的用户密码；\",\"master_log_file：指定从数据库要复制数据的日志文件，通过查看主数据的状态，获取File参数；\",\"master_log_pos：指定从数据库从哪个位置开始复制数据，通过查看主数据的状态，获取Position参数；\",\"master_connect_retry：连接失败重试的时间间隔，单位为秒。\"]},{\"header\":\"12、在从数据库中查看主从同步状态\",\"slug\":\"_12、在从数据库中查看主从同步状态\",\"contents\":[\"show slave status \\\\G; \"]},{\"header\":\"13、在从数据库中开启主从同步\",\"slug\":\"_13、在从数据库中开启主从同步\",\"contents\":[\"mysql> start slave; Query OK, 0 rows affected (0.06 sec) \"]},{\"header\":\"14、查看从数据库状态\",\"slug\":\"_14、查看从数据库状态\",\"contents\":[\"show slave status \\\\G; \"]},{\"header\":\"15、主从复制测试\",\"slug\":\"_15、主从复制测试\",\"contents\":[]},{\"header\":\"1、主机新建库-使用库-新建表-插入数据\",\"slug\":\"_1、主机新建库-使用库-新建表-插入数据\",\"contents\":[\"mysql> create database test; Query OK, 1 row affected (0.00 sec) mysql> use test Database changed mysql> create table t1(id int, name varchar(20)); Query OK, 0 rows affected (0.04 sec) mysql> insert into t1(id, name) values (1,'hello'); Query OK, 1 row affected (0.01 sec) mysql> select * from t1; +------+-------+ | id | name | +------+-------+ | 1 | hello | +------+-------+ 1 row in set (0.00 sec) mysql> \"]},{\"header\":\"2、从机使用库-查看记录\",\"slug\":\"_2、从机使用库-查看记录\",\"contents\":[\"mysql> use test; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> select * from t1; +------+-------+ | id | name | +------+-------+ | 1 | hello | +------+-------+ 1 row in set (0.00 sec) mysql> \"]},{\"header\":\"2、redis集群\",\"slug\":\"_2、redis集群\",\"contents\":[\"cluster(集群)模式-docker版 哈希槽分区进行亿级数据存储\"]},{\"header\":\"1、场景\",\"slug\":\"_1、场景\",\"contents\":[\"1~2亿条数据需要缓存，请问如何设计这个存储案例？\",\"单机单台100%不可能，肯定是分布式存储，用redis如何落地？\"]},{\"header\":\"解决方案\",\"slug\":\"解决方案\",\"contents\":[]},{\"header\":\"1、哈希取余分区\",\"slug\":\"_1、哈希取余分区\",\"contents\":[\"2亿条记录就是2亿个k,v，我们单机不行必须要分布式多机，假设有3台机器构成一个集群，用户每次读写操作都是根据公式： hash(key) % N个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。\",\"优点：\",\"简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。 \",\"缺点：\",\" 原来规划好的节点，进行扩容或者缩容就比较麻烦了额，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3会变成Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。 某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。 \"]},{\"header\":\"2、一致性哈希算法分区\",\"slug\":\"_2、一致性哈希算法分区\",\"contents\":[\"一致性哈希算法在1997年由麻省理工学院中提出的，设计目标是为了解决分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不OK了。\",\"作用：\",\"提出一致性Hash解决方案。目的是当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系\",\"步骤：\",\"算法构建一致性哈希环\",\"一致性哈希环\",\"​ 一致性哈希算法必然有个hash函数并按照算法产生hash值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个hash空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。\",\"它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性Hash算法是对2^32 取模，简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1 （即哈希值是一个32位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、……直到2^32-1 ，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1 在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。\",\"服务器IP节点映射\",\"节点映射\",\"将集群中各个IP节点映射到环上的某一个位置。 将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如4个节点NodeA、B、C、D，经过IP地址的哈希函数计算(hash(ip))，使用IP地址哈希后在环空间的位置如下：\",\"key落到服务器的落键规则\",\"当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。 如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。\",\"优点：\",\"1.一致性哈希算法的容错性\",\"容错性 假设Node C宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。简单说，就是C挂了，受到影响的只是B、C之间的数据，并且这些数据会转移到D进行存储。\",\"2.一致性哈希算法的扩展性\",\"扩展性 数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可，不会导致hash取余全部数据重新洗牌。\",\"缺点：\",\"一致性哈希算法的数据倾斜问题\",\"一致性Hash算法在服务节点太少时，容易因为节点分布不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题， 例如系统中只有两台服务器：\",\"总结：\",\"为了在节点数目发生改变时尽可能少的迁移数据\",\"将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会顺时针找到临近的存储节点存放。 而当有节点加入或退出时仅影响该节点在Hash环上顺时针相邻的后续节点。\",\"优点 加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。\",\"缺点 数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。\"]},{\"header\":\"3、哈希槽分区\",\"slug\":\"_3、哈希槽分区\",\"contents\":[\"哈希槽实质就是一个数组，数组[0,2^14 -1]形成hash slot空间。\",\"**作用：**解决一致性哈希算法的数据倾斜问题\",\"解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。\",\"槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。 哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。\",\"####### 哈希槽的计算\",\"Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key之A 、B在Node2， key之C落在Node3上\"]},{\"header\":\"2、3主3从集群配置\",\"slug\":\"_2、3主3从集群配置\",\"contents\":[]},{\"header\":\"1、启动docker\",\"slug\":\"_1、启动docker\",\"contents\":[\"systemctl start docker \"]},{\"header\":\"2、新建6个docker容器redis实例\",\"slug\":\"_2、新建6个docker容器redis实例\",\"contents\":[\"docker run -d --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis --cluster-enabled yes --appendonly yes --port 6381 docker run -d --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis --cluster-enabled yes --appendonly yes --port 6382 docker run -d --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis --cluster-enabled yes --appendonly yes --port 6383 docker run -d --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis --cluster-enabled yes --appendonly yes --port 6384 docker run -d --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis --cluster-enabled yes --appendonly yes --port 6385 docker run -d --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis --cluster-enabled yes --appendonly yes --port 6386 \",\"[root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6fc3ef2855b redis \\\"docker-entrypoint.s…\\\" 3 seconds ago Up 2 seconds redis-node-6 9c8868d69a50 redis \\\"docker-entrypoint.s…\\\" 3 seconds ago Up 3 seconds redis-node-5 7fbb5345951a redis \\\"docker-entrypoint.s…\\\" 4 seconds ago Up 3 seconds redis-node-4 d53b9d5af1ac redis \\\"docker-entrypoint.s…\\\" 4 seconds ago Up 4 seconds redis-node-3 fe0e430cb940 redis \\\"docker-entrypoint.s…\\\" 6 seconds ago Up 4 seconds redis-node-2 ee03a7ec212e redis \\\"docker-entrypoint.s…\\\" 8 seconds ago Up 6 seconds redis-node-1 \"]},{\"header\":\"分步解释\",\"slug\":\"分步解释\",\"contents\":[\"docker run：创建并运行docker容器实例\",\"--name redis-node-6：容器名字\",\"--net host ：使用宿主机的IP和端口，默认、\",\"--privileged=true：获取宿主机root用户权限\",\"-v /data/redis/share/redis-node-6:/data：容器卷，宿主机地址:docker内部地址\",\"redis：redis镜像和版本号\",\"--cluster-enabled yes：开启redis集群\",\"--appendonly yes：开启持久化\",\"--port 6386：redis端口号\"]},{\"header\":\"3、进入容器redis-node-1并为6台机器构建集群关系\",\"slug\":\"_3、进入容器redis-node-1并为6台机器构建集群关系\",\"contents\":[]},{\"header\":\"1、进入容器\",\"slug\":\"_1、进入容器\",\"contents\":[\"docker exec -it redis-node-1 /bin/bash \"]},{\"header\":\"2、构建主从关系\",\"slug\":\"_2、构建主从关系\",\"contents\":[\"PS：注意自己的真实IP地址\",\"redis-cli --cluster create 192.168.130.132:6381 192.168.130.132:6382 192.168.130.132:6383 192.168.130.132:6384 192.168.130.132:6385 192.168.130.132:6386 --cluster-replicas 1 \",\"--cluster-replicas 1：表示为每个master创建一个slave节点\",\"[root@docker ~]## docker exec -it redis-node-1 /bin/bash root@docker:/data## redis-cli --cluster create 192.168.130.132:6381 192.168.130.132:6382 192.168.130.132:6383 192.168.130.132:6384 192.168.130.132:6385 192.168.130.132:6386 --cluster-replicas 1 >>> Performing hash slots allocation on 6 nodes... Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 192.168.130.132:6385 to 192.168.130.132:6381 Adding replica 192.168.130.132:6386 to 192.168.130.132:6382 Adding replica 192.168.130.132:6384 to 192.168.130.132:6383 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-5460] (5461 slots) master M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[5461-10922] (5462 slots) master M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[10923-16383] (5461 slots) master S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 replicates 8dbe8b347410cf87d62933382b73693405535ba1 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 replicates 8335b5349d781c11745ee129f5dbae370dbd3394 Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join ... >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \",\"到这里，3主3从就构建完成了。\"]},{\"header\":\"4、链接进入6381作为切入点，查看集群状态\",\"slug\":\"_4、链接进入6381作为切入点-查看集群状态\",\"contents\":[\"root@docker:/data## redis-cli -p 6381 127.0.0.1:6381> cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:663 cluster_stats_messages_pong_sent:671 cluster_stats_messages_sent:1334 cluster_stats_messages_ping_received:666 cluster_stats_messages_pong_received:663 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:1334 127.0.0.1:6381> cluster nodes b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385@16385 slave 8dbe8b347410cf87d62933382b73693405535ba1 0 1651152474000 3 connected 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381@16381 myself,master - 0 1651152472000 1 connected 0-5460 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383@16383 master - 0 1651152474000 3 connected 10923-16383 c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384@16384 slave 60fa7e084483feca3af41f269de5a57b526c0ad7 0 1651152476585 2 connected 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382@16382 master - 0 1651152475573 2 connected 5461-10922 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386@16386 slave 8335b5349d781c11745ee129f5dbae370dbd3394 0 1651152474566 1 connected 127.0.0.1:6381> \"]},{\"header\":\"1、 cluster info ：查看集群状态\",\"slug\":\"_1、cluster-info-查看集群状态\",\"contents\":[\"127.0.0.1:6381> cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:663 cluster_stats_messages_pong_sent:671 cluster_stats_messages_sent:1334 cluster_stats_messages_ping_received:666 cluster_stats_messages_pong_received:663 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:1334 \",\"cluster_state: ok状态表示集群可以正常接受查询请求。fail 状态表示，至少有一个哈希槽没有被绑定（说明有哈希槽没有被绑定到任意一个节点），或者在错误的状态（节点可以提供服务但是带有FAIL 标记），或者该节点无法联系到多数master节点。.\",\"cluster_slots_assigned: 已分配到集群节点的哈希槽数量（不是没有被绑定的数量）。16384个哈希槽全部被分配到集群节点是集群正常运行的必要条件.\",\"cluster_slots_ok: 哈希槽状态不是FAIL 和 PFAIL 的数量.\",\"cluster_slots_pfail: 哈希槽状态是 PFAIL的数量。只要哈希槽状态没有被升级到FAIL状态，这些哈希槽仍然可以被正常处理。PFAIL状态表示我们当前不能和节点进行交互，但这种状态只是临时的错误状态。\",\"cluster_slots_fail: 哈希槽状态是FAIL的数量。如果值不是0，那么集群节点将无法提供查询服务，除非cluster-require-full-coverage被设置为no .\",\"cluster_known_nodes: 集群中节点数量，包括处于握手状态还没有成为集群正式成员的节点.\",\"cluster_size: 至少包含一个哈希槽且能够提供服务的master节点数量.\",\"cluster_current_epoch: 集群本地Current Epoch变量的值。这个值在节点故障转移过程时有用，它总是递增和唯一的。\",\"cluster_my_epoch: 当前正在使用的节点的Config Epoch值. 这个是关联在本节点的版本值.\",\"cluster_stats_messages_sent: 通过node-to-node二进制总线发送的消息数量.\",\"cluster_stats_messages_received: 通过node-to-node二进制总线接收的消息数量.\"]},{\"header\":\"2、 cluster nodes ：提供了当前连接节点所属集群的配置信息，信息格式和Redis集群在磁盘上存储使用的序列化格式完全一样（在磁盘存储信息的结尾还存储了一些额外信息）\",\"slug\":\"_2、cluster-nodes-提供了当前连接节点所属集群的配置信息-信息格式和redis集群在磁盘上存储使用的序列化格式完全一样-在磁盘存储信息的结尾还存储了一些额外信息\",\"contents\":[\"127.0.0.1:6381> cluster nodes b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385@16385 slave 8dbe8b347410cf87d62933382b73693405535ba1 0 1651152474000 3 connected 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381@16381 myself,master - 0 1651152472000 1 connected 0-5460 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383@16383 master - 0 1651152474000 3 connected 10923-16383 c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384@16384 slave 60fa7e084483feca3af41f269de5a57b526c0ad7 0 1651152476585 2 connected 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382@16382 master - 0 1651152475573 2 connected 5461-10922 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386@16386 slave 8335b5349d781c11745ee129f5dbae370dbd3394 0 1651152474566 1 connected 127.0.0.1:6381> \",\"主从关系图：\",\"每行的组成：\",\"<id> <ip:port> <flags> <master> <ping-sent> <pong-recv> <config-epoch> <link-state> <slot> <slot> ... <slot> \",\"id: 节点ID,是一个40字节的随机字符串，这个值在节点启动的时候创建，并且永远不会改变（除非使用CLUSTER RESET HARD命令）。\",\"ip:port: 客户端与节点通信使用的地址.\",\"flags: 逗号分割的标记位，可能的值有: myself, master, slave, fail?, fail, handshake, noaddr, noflags. 下一部分将详细介绍这些标记. \",\"myself: 当前连接的节点.\",\"master: 节点是master.\",\"slave: 节点是slave.\",\"fail?: 节点处于PFAIL 状态。 当前节点无法联系，但逻辑上是可达的 (非 FAIL 状态).\",\"fail: 节点处于FAIL 状态. 大部分节点都无法与其取得联系将会将改节点由 PFAIL 状态升级至FAIL状态。\",\"handshake: 还未取得信任的节点，当前正在与其进行握手.\",\"noaddr: 没有地址的节点（No address known for this node）.\",\"noflags: 连个标记都没有（No flags at all）.\",\"master: 如果节点是slave，并且已知master节点，则这里列出master节点ID,否则的话这里列出”-“。\",\"ping-sent: 最近一次发送ping的时间，这个时间是一个unix毫秒时间戳，0代表没有发送过.\",\"pong-recv: 最近一次收到pong的时间，使用unix时间戳表示.\",\"config-epoch: 节点的epoch值（or of the current master if the node is a slave）。每当节点发生失败切换时，都会创建一个新的，独特的，递增的epoch。如果多个节点竞争同一个哈希槽时，epoch值更高的节点会抢夺到。\",\"link-state: node-to-node集群总线使用的链接的状态，我们使用这个链接与集群中其他节点进行通信.值可以是 connected 和 disconnected.\",\"slot: 哈希槽值或者一个哈希槽范围. 从第9个参数开始，后面最多可能有16384个 数(limit never reached)。代表当前节点可以提供服务的所有哈希槽值。如果只是一个值,那就是只有一个槽会被使用。如果是一个范围，这个值表示为起始槽-结束槽，节点将处理包括起始槽和结束槽在内的所有哈希槽。\",\"官方地址：http://www.redis.cn/commands/cluster-info.html\"]},{\"header\":\"3、主从容错切换迁移案例\",\"slug\":\"_3、主从容错切换迁移案例\",\"contents\":[]},{\"header\":\"1、数据读写存储\",\"slug\":\"_1、数据读写存储\",\"contents\":[]},{\"header\":\"1、启动6机构成的集群并通过exec进入\",\"slug\":\"_1、启动6机构成的集群并通过exec进入\",\"contents\":[\"root@docker:/data## redis-cli -p 6381 127.0.0.1:6381> set k1 v1 (error) MOVED 12706 192.168.130.132:6383 127.0.0.1:6381> set k2 v2\\\\ OK 127.0.0.1:6381> set k3 v3 OK 127.0.0.1:6381> set k4 v4 (error) MOVED 8455 192.168.130.132:6382 127.0.0.1:6381> \",\"显示k1和k4没有存储进去\",\"(error) MOVED 12706 192.168.130.132:6383：请转到6383的redis进行存储\"]},{\"header\":\"2、防止路由失效加参数-c并新增两个key\",\"slug\":\"_2、防止路由失效加参数-c并新增两个key\",\"contents\":[\"root@docker:/data## redis-cli -p 6381 -c 127.0.0.1:6381> set k1 v1 -> Redirected to slot [12706] located at 192.168.130.132:6383 OK 192.168.130.132:6383> set k4 v4 -> Redirected to slot [8455] located at 192.168.130.132:6382 OK 192.168.130.132:6382> get k4 \\\"v4\\\" 192.168.130.132:6382> \",\" Redirected to slot [8455] located at 192.168.130.132:6382：重定向到6382\"]},{\"header\":\"3、查看集群状态\",\"slug\":\"_3、查看集群状态\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 2 keys | 5461 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 5461 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 5462 slots | 1 slaves. [OK] 4 keys in 3 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \"]},{\"header\":\"2、容错切换迁移\",\"slug\":\"_2、容错切换迁移\",\"contents\":[]},{\"header\":\"1、主6381和从机切换，先停止主机6381\",\"slug\":\"_1、主6381和从机切换-先停止主机6381\",\"contents\":[\"[root@docker ~]## docker stop redis-node-1 redis-node-1 [root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6fc3ef2855b redis \\\"docker-entrypoint.s…\\\" 47 hours ago Up 47 hours redis-node-6 9c8868d69a50 redis \\\"docker-entrypoint.s…\\\" 47 hours ago Up 47 hours redis-node-5 7fbb5345951a redis \\\"docker-entrypoint.s…\\\" 47 hours ago Up 47 hours redis-node-4 d53b9d5af1ac redis \\\"docker-entrypoint.s…\\\" 47 hours ago Up 47 hours redis-node-3 fe0e430cb940 redis \\\"docker-entrypoint.s…\\\" 47 hours ago Up 47 hours redis-node-2 [root@docker ~]## \",\"6381主机停了，对应的真实从机上位，也就是6号机变成了主机器\"]},{\"header\":\"2、查看集群情况\",\"slug\":\"_2、查看集群情况\",\"contents\":[\"127.0.0.1:6382> cluster nodes 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386@16386 master - 0 1651158299456 7 connected 0-5460 c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384@16384 slave 60fa7e084483feca3af41f269de5a57b526c0ad7 0 1651158300472 2 connected 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382@16382 myself,master - 0 1651158298000 2 connected 5461-10922 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383@16383 master - 0 1651158298444 3 connected 10923-16383 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381@16381 master,fail - 1651154146064 1651154140969 1 disconnected b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385@16385 slave 8dbe8b347410cf87d62933382b73693405535ba1 0 1651158297000 3 connected 127.0.0.1:6382> \"]},{\"header\":\"3、恢复3主3从\",\"slug\":\"_3、恢复3主3从\",\"contents\":[\"重新启动1号机之后，6号机还是主机，1号机器从之前的主机变成了从机\",\"docker start redis-node-1 \",\"停掉6号机，再启动6号机\",\"docker stop redis-node-6 docker start redis-node-6 \",\"查看集群状态\",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 2 keys | 5461 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 5462 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 5461 slots | 1 slaves. [OK] 4 keys in 3 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \"]},{\"header\":\"4、主从扩容案例\",\"slug\":\"_4、主从扩容案例\",\"contents\":[]},{\"header\":\"1、新建6387、6388两个节点+新建后启动+查看是否8节点\",\"slug\":\"_1、新建6387、6388两个节点-新建后启动-查看是否8节点\",\"contents\":[\"docker run -d --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis --cluster-enabled yes --appendonly yes --port 6387 docker run -d --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis --cluster-enabled yes --appendonly yes --port 6388 \"]},{\"header\":\"2、进入6387容器实例内部\",\"slug\":\"_2、进入6387容器实例内部\",\"contents\":[\"docker exec -it redis-node-7 /bin/bash \"]},{\"header\":\"3、将新增的6387节点(空槽号)作为master节点加入原集群\",\"slug\":\"_3、将新增的6387节点-空槽号-作为master节点加入原集群\",\"contents\":[\"将新增的6387作为master节点加入集群 redis-cli --cluster add-node 自己实际IP地址:6387 自己实际IP地址:6381 6387 就是将要作为master新增节点 6381 就是原来集群节点里面的领路人，相当于6387拜拜6381的码头从而找到组织加入集群\",\"redis-cli --cluster add-node 192.168.130.132:6387 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster add-node 192.168.130.132:6387 192.168.130.132:6381 >>> Adding node 192.168.130.132:6387 to cluster 192.168.130.132:6381 >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. >>> Send CLUSTER MEET to node 192.168.130.132:6387 to make it join the cluster. [OK] New node added correctly. root@docker:/data## \"]},{\"header\":\"4、检查集群情况\",\"slug\":\"_4、检查集群情况\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 2 keys | 5461 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 5462 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 5461 slots | 1 slaves. 192.168.130.132:6387 (34b689b7...) -> 0 keys | 0 slots | 0 slaves. \",\"很明显，6387没有槽号\"]},{\"header\":\"5、重新分配槽号\",\"slug\":\"_5、重新分配槽号\",\"contents\":[\"重新分派槽号 命令：redis-cli --cluster reshard IP地址:端口号\",\"redis-cli --cluster reshard 192.168.130.:6381 \",\"输入需要迁移的槽数量，此处我们输入4096。\",\"目标节点ID，只能指定一个，因为我们需要迁移到6387中，因此下面输入6387的ID。\",\"之后输入源节点的ID，redis会从这些源节点中平均取出对应数量的槽，然后迁移到6385中。最后要输入done表示结束。\",\"最后输入yes即可。\"]},{\"header\":\"6、检查集群情况\",\"slug\":\"_6、检查集群情况\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6387 (34b689b7...) -> 1 keys | 4096 slots | 0 slaves. [OK] 4 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \\\\ \",\"M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master \",\"为什么6387是3个新的区间，以前的还是连续？ 重新分配成本太高，所以前3家各自匀出来一部分，从6381/6382/6383三个旧节点分别匀出1364个坑位给新节点6387\"]},{\"header\":\"7、为主节点6387分配从节点6388\",\"slug\":\"_7、为主节点6387分配从节点6388\",\"contents\":[\"命令：redis-cli --cluster add-node ip:新slave端口 ip:新master端口 --cluster-slave --cluster-master-id 新主机节点ID\",\"redis-cli --cluster add-node 192.168.130.132:6388 192.168.130.132:6387 --cluster-slave --cluster-master-id 34b689b791d9945a0b761349f1bc7b64f0be876f \"]},{\"header\":\"8、检查集群情况\",\"slug\":\"_8、检查集群情况\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6387 (34b689b7...) -> 1 keys | 4096 slots | 1 slaves. [OK] 4 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) S: 4b4b4a8a4d50548e954b46e921ff8085ed555c39 192.168.130.132:6388 slots: (0 slots) slave replicates 34b689b791d9945a0b761349f1bc7b64f0be876f M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \",\"6387存在一个子机器\"]},{\"header\":\"5、主从缩容案例\",\"slug\":\"_5、主从缩容案例\",\"contents\":[\"目的：6387和6388下线\"]},{\"header\":\"1、检查集群情况1获得6388的节点ID\",\"slug\":\"_1、检查集群情况1获得6388的节点id\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6382 \",\"S: 4b4b4a8a4d50548e954b46e921ff8085ed555c39 192.168.130.132:6388 slots: (0 slots) slave replicates 34b689b791d9945a0b761349f1bc7b64f0be876f M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \",\"节点ID为：4b4b4a8a4d50548e954b46e921ff8085ed555c39\"]},{\"header\":\"2、从集群中将节点6388删除\",\"slug\":\"_2、从集群中将节点6388删除\",\"contents\":[\"命令：redis-cli --cluster del-node ip:从机端口 从机6388节点ID\",\"redis-cli --cluster del-node 192.168.130.132:6388 4b4b4a8a4d50548e954b46e921ff8085ed555c39 \",\"oot@docker:/data## redis-cli --cluster del-node 192.168.130.132:6388 4b4b4a8a4d50548e954b46e921ff8085ed555c39 >>> Removing node 4b4b4a8a4d50548e954b46e921ff8085ed555c39 from cluster 192.168.130.132:6388 >>> Sending CLUSTER FORGET messages to the cluster... >>> Sending CLUSTER RESET SOFT to the deleted node. root@docker:/data## \",\"检查集群情况\",\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6387 (34b689b7...) -> 1 keys | 4096 slots | 0 slaves. [OK] 4 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[1365-5460] (4096 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \",\"很明显，6387的从机器已经被没了，6388机器也已经被删除了，只剩下7台机器了。\"]},{\"header\":\"3、将6387的槽号清空，重新分配，本例将清出来的槽号都给6381\",\"slug\":\"_3、将6387的槽号清空-重新分配-本例将清出来的槽号都给6381\",\"contents\":[\"redis-cli --cluster reshard 192.168.130.132:6381 \",\"这里我没有截到图，以阳哥的截图步骤\"]},{\"header\":\"4、查看集群状态\",\"slug\":\"_4、查看集群状态\",\"contents\":[\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 2 keys | 8192 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6387 (34b689b7...) -> 0 keys | 0 slots | 0 slaves. [OK] 4 keys in 4 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-6826],[10923-12287] (8192 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) M: 34b689b791d9945a0b761349f1bc7b64f0be876f 192.168.130.132:6387 slots: (0 slots) master S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. \",\"6381拥有8192个槽位\"]},{\"header\":\"5、将6387删除\",\"slug\":\"_5、将6387删除\",\"contents\":[\"命令：redis-cli --cluster del-node ip:端口 6387节点ID\",\"redis-cli --cluster del-node 192.168.130.132:6387 34b689b791d9945a0b761349f1bc7b64f0be876f \",\"再次检查集群情况\",\"redis-cli --cluster check 192.168.130.132:6381 \",\"root@docker:/data## redis-cli --cluster check 192.168.130.132:6381 192.168.130.132:6381 (8335b534...) -> 2 keys | 8192 slots | 1 slaves. 192.168.130.132:6382 (60fa7e08...) -> 1 keys | 4096 slots | 1 slaves. 192.168.130.132:6383 (8dbe8b34...) -> 1 keys | 4096 slots | 1 slaves. [OK] 4 keys in 3 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.130.132:6381) M: 8335b5349d781c11745ee129f5dbae370dbd3394 192.168.130.132:6381 slots:[0-6826],[10923-12287] (8192 slots) master 1 additional replica(s) S: c366905ca5ec2472275bbea9b2ae9b642b92a737 192.168.130.132:6384 slots: (0 slots) slave replicates 60fa7e084483feca3af41f269de5a57b526c0ad7 S: 4051766aa375f0ed4533cb729afa8daf8649f5d2 192.168.130.132:6386 slots: (0 slots) slave replicates 8335b5349d781c11745ee129f5dbae370dbd3394 M: 60fa7e084483feca3af41f269de5a57b526c0ad7 192.168.130.132:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: 8dbe8b347410cf87d62933382b73693405535ba1 192.168.130.132:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) S: b5fd469dd1f8b5a64cacd5ecaed9dd396e1b9217 192.168.130.132:6385 slots: (0 slots) slave replicates 8dbe8b347410cf87d62933382b73693405535ba1 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. root@docker:/data## \",\"发现已经删除了\"]}]},\"/study-tutorial/distributed/docker/data_volume.html\":{\"title\":\"7、Docker容器数据卷\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"容器卷记得加入：--privileged=true\",\"Docker挂载主机目录访问如果出现cannot open directory .: Permission denied\",\"**解决办法：**在挂载目录后多 加一个--privileged=true参数即可\",\"如果是CentOS7安全模块会比之前系统版本加强，不安全的会先禁止，所以目录挂载的情况被默认为不安全的行为， 在SELinux里面挂载目录被禁止掉了额，如果要开启，我们一般使用--privileged=true命令，扩大容器的权限解决挂载目录没有权限的问题，也即 使用该参数，container内的root拥有真正的root权限，否则，container内的root只是外部的一个普通用户权限。\"]},{\"header\":\"1、什么是容器卷\",\"slug\":\"_1、什么是容器卷\",\"contents\":[\"卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性： 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷\",\"一句话：有点类似我们Redis里面的rdb和aof文件\",\"将docker容器内的数据保存进宿主机的磁盘中\",\"运行一个带有容器卷存储功能的容器实例\",\" docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 \"]},{\"header\":\"2、作用\",\"slug\":\"_2、作用\",\"contents\":[\"将运用与运行的环境打包镜像，run后形成容器实例运行 ，但是我们对数据的要求希望是持久化的；Docker容器产生的数据，如果不备份，那么当容器实例删除后，容器内的数据自然也就没有了。为了能保存数据在docker中我们使用卷。\",\"特点：\",\"数据卷可在容器之间共享或重用数据\",\"卷中的更改可以直接实时生效\",\"数据卷中的更改不会包含在镜像的更新中\",\"数据卷的生命周期一直持续到没有容器使用它为止\"]},{\"header\":\"3、案例\",\"slug\":\"_3、案例\",\"contents\":[]},{\"header\":\"1、宿主vs容器之间映射添加容器卷\",\"slug\":\"_1、宿主vs容器之间映射添加容器卷\",\"contents\":[\" docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 \",\"## 公式：docker run -it -v /宿主机目录:/容器内目录 centos /bin/bash [root@docker ~]## docker run -it --name mycentos --privileged=true -v /tmp/myHostData:/tmp/myDockerData centos /bin/bash [root@77429a055609 /]## \",\"容器内目录结构\",\"[root@77429a055609 /]## cd tmp [root@77429a055609 tmp]## ls ks-script-4luisyla ks-script-o23i7rc2 ks-script-x6ei4wuu myDockerData \",\"宿主机内\",\"[root@docker /]## cd tmp/ [root@docker tmp]## ls ks-script-DUUAUC vmware-root_671-3988556280 myHostData vmware-root_679-3988687326 systemd-private-d8aa05bbc4754bae8de67cf03b16ed6c-chronyd.service-duHBa7 vmware-root_688-2688750615 systemd-private-dfa20a5865b348c4bb8885cff1f38f6a-chronyd.service-Kxsvca yum.log vmware-root_662-2689143848 [root@docker tmp]## \"]},{\"header\":\"1、查看是否挂载成功\",\"slug\":\"_1、查看是否挂载成功\",\"contents\":[\"docker inspect 容器ID \",\"\\\"Mounts\\\": [ { \\\"Type\\\": \\\"bind\\\", \\\"Source\\\": \\\"/tmp/myHostData\\\", \\\"Destination\\\": \\\"/tmp/myDockerData\\\", \\\"Mode\\\": \\\"\\\", \\\"RW\\\": true, \\\"Propagation\\\": \\\"rprivate\\\" } ], \"]},{\"header\":\"2、容器与宿主机数据共享\",\"slug\":\"_2、容器与宿主机数据共享\",\"contents\":[\"docker修改，主机同步获取\",\"主机修改，docker同步获得\",\"docker容器stop，主机修改，docker重启后数据同步获得\"]},{\"header\":\"2、读写规则映射添加说明\",\"slug\":\"_2、读写规则映射添加说明\",\"contents\":[\"默认就是rw， rw = read + write\",\" docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名 \"]},{\"header\":\"1、只读\",\"slug\":\"_1、只读\",\"contents\":[\"容器实例内部被限制，只能读取不能写 ro = read only\",\" docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 \",\"/容器目录:ro 镜像名 就能完成功能，此时容器自己只能读取不能写\",\"此时如果宿主机写入内容，可以同步给容器内，容器可以读取到。\"]},{\"header\":\"3、卷的继承和共享\",\"slug\":\"_3、卷的继承和共享\",\"contents\":[\"案例场景：容器1完成与主机的映射，容器2继承容器1的卷规则\",\"docker run -it --privileged=true --volumes-from 父类 --name u2 ubuntu \"]}]},\"/study-tutorial/distributed/docker/dockerFile.html\":{\"title\":\"10、DockerFile解析\",\"contents\":[{\"header\":\"1、什么是DockerFile\",\"slug\":\"_1、什么是dockerfile\",\"contents\":[\"DockerFile是用来构建Docker镜像的文本文件，是由一条条构建镜像所需的指令和参数构成的脚本。\",\"官网：https://docs.docker.com/engine/reference/builder/\",\"构建步骤：\",\"编写Dockerfile文件\",\"docker build命令构建镜像\",\"docker run依镜像运行容器实例\"]},{\"header\":\"2、DockerFile构建过程解析\",\"slug\":\"_2、dockerfile构建过程解析\",\"contents\":[]},{\"header\":\"1、Dockerfile内容基础知识\",\"slug\":\"_1、dockerfile内容基础知识\",\"contents\":[\"每条保留字指令都必须为大写字母且后面要跟随至少一个参数\",\"指令按照从上到下，顺序执行\",\"表示注释\",\"每条指令都会创建一个新的镜像层并对镜像进行提交\"]},{\"header\":\"2、Docker执行Dockerfile的大致流程\",\"slug\":\"_2、docker执行dockerfile的大致流程\",\"contents\":[\"docker从基础镜像运行一个容器\",\"执行一条指令并对容器作出修改\",\"执行类似docker commit的操作提交一个新的镜像层\",\"docker再基于刚提交的镜像运行一个新容器\",\"执行dockerfile中的下一条指令直到所有指令都执行完成\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段，\",\"Dockerfile是软件的原材料\",\"Docker镜像是软件的交付品\",\"Docker容器则可以认为是软件镜像的运行态，也即依照镜像运行的容器实例\",\"Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。\",\"Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等;\",\"Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时会真正开始提供服务;\",\"Docker容器，容器是直接提供服务的。\"]},{\"header\":\"3、Docker常用保留字指令\",\"slug\":\"_3、docker常用保留字指令\",\"contents\":[\"Docker中文文档 Dockerfile介绍-DockerInfo\",\"FROM：基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是from\",\"MAINTAINER：镜像维护者的姓名和邮箱地址\",\"RUN：容器构建时需要运行的命令\",\"RUN是在 docker build时运行\",\"shell格式：例如 RUN yum -y install vim\",\"exec格式：\",\"EXPOSE：当前容器对外暴露出的端口\",\"WORKDIR：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点\",\"USER：指定该镜像以什么样的用户去执行，如果都不指定，默认是root\",\"ENV：用来在构建镜像过程中设置环境变量\",\"ENV MY_PATH /usr/mytest 这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样； 也可以在其它指令中直接使用这些环境变量，\",\"比如：WORKDIR $MY_PATH\",\"ADD：将宿主机目录下的文件拷贝进镜像且会自动处理URL和解压tar压缩包\",\"COPY：类似ADD，拷贝文件和目录到镜像中。\",\"将从构建上下文目录中 <源路径> 的文件/目录复制到新的一层的镜像内的 <目标路径> 位置\",\"COPY src dest COPY [\\\"src\\\", \\\"dest\\\"] <src源路径>：源文件或者源目录 <dest目标路径>：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 \",\"VOLUME：容器数据卷，用于数据保存和持久化工作\",\"CMD：指定容器启动后的要干的事情\",\"Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换\",\"它和前面RUN命令的区别 \",\"CMD是在docker run 时运行。\",\"RUN是在 docker build时运行。\",\"ENTRYPOINT：也是用来指定一个容器启动时要运行的命令\",\"类似于 CMD 指令，但是ENTRYPOINT不会被docker run后面的命令覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序\",\"命令格式： ENTRYPOINT可以和CMD一起用，一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。 当指定了ENTRYPOINT后，CMD的含义就发生了变化，不再是直接运行其命令而是将CMD的内容作为参数传递给ENTRYPOINT指令，他两个组合会变成\",\"案例如下：假设已通过 Dockerfile 构建了 nginx:test 镜像：\",\"是否传参\",\"按照dockerfile编写执行\",\"传参运行\",\"Docker命令\",\"docker run nginx:test\",\"docker run nginx:test -c /etc/nginx/new.conf\",\"衍生出的实际命令\",\"nginx -c /etc/nginx/nginx.conf\",\"nginx -c /etc/nginx/new.conf\",\"优点：在执行docker run的时候可以指定 ENTRYPOINT 运行所需的参数。\",\"注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。\"]},{\"header\":\"4、构建案例\",\"slug\":\"_4、构建案例\",\"contents\":[]},{\"header\":\"1、实现目标\",\"slug\":\"_1、实现目标\",\"contents\":[\"Centos7镜像具备vim+ifconfig+jdk8\",\"准备JDK：下载地址\"]},{\"header\":\"2、编写DockerFile\",\"slug\":\"_2、编写dockerfile\",\"contents\":[\"FROM centos MAINTAINER xiaobear<xiaobear@qq.com> ENV MYPATH /usr/local WORKDIR $MYPATH #centos8 需加上 RUN sed -i -e \\\"s|mirrorlist=|#mirrorlist=|g\\\" /etc/yum.repos.d/CentOS-* RUN sed -i -e \\\"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\\\" /etc/yum.repos.d/CentOS-* #安装vim编辑器 RUN yum -y install vim #安装ifconfig命令查看网络IP RUN yum -y install net-tools #安装java8及lib库 RUN yum -y install glibc.i686 RUN mkdir /usr/local/java #ADD 是相对路径jar,把jdk-8u333-linux-x64.tar.gz添加到容器中,安装包必须要和Dockerfile文件在同一位置 ADD jdk-8u333-linux-x64.tar.gz /usr/local/java/ #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_333 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH #对外暴露端口 80 EXPOSE 80 CMD echo $MYPATH CMD echo \\\"success--------------ok\\\" CMD /bin/bash \"]},{\"header\":\"3、构建\",\"slug\":\"_3、构建\",\"contents\":[\"docker build -t 新镜像名字:TAG .\",\"PS：上面TAG后面有个空格，有个点\",\"docker build -t centos_java8:2.0 . \"]},{\"header\":\"4、运行\",\"slug\":\"_4、运行\",\"contents\":[\"docker run -it 新镜像名字:TAG \",\"[root@docker myDockerFile]## docker run -it 1ca6615ca841 /bin/bash [root@c73761eb0626 local]## ls bin etc games include java lib lib64 libexec sbin share src [root@c73761eb0626 local]## cd java [root@c73761eb0626 java]## ls jdk1.8.0_333 [root@c73761eb0626 java]## java -version java version \\\"1.8.0_333\\\" Java(TM) SE Runtime Environment (build 1.8.0_333-b02) Java HotSpot(TM) 64-Bit Server VM (build 25.333-b02, mixed mode) [root@c73761eb0626 java]## vim a.txt [root@c73761eb0626 java]## cat a.txt hello Dockerfile [root@c73761eb0626 java]## \"]},{\"header\":\"5、体会 UnionFS\",\"slug\":\"_5、体会unionfs\",\"contents\":[\"UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。\",\"特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录\"]},{\"header\":\"5、虚悬镜像\",\"slug\":\"_5、虚悬镜像\",\"contents\":[\"仓库名、标签都是<none>的镜像，俗称dangling image\"]},{\"header\":\"1、编写Dockerfile\",\"slug\":\"_1、编写dockerfile\",\"contents\":[\"from ubuntu CMD echo 'action is success' \",\"构建\",\"docker build . \",\"查看镜像\",\"docker ../../images \"]},{\"header\":\"2、查看\",\"slug\":\"_2、查看\",\"contents\":[\"docker image ls -f dangling=true \"]},{\"header\":\"3、删除\",\"slug\":\"_3、删除\",\"contents\":[\"docker image prune \",\"虚悬镜像已经失去存在价值，可以删除\"]}]},\"/study-tutorial/distributed/docker/docker_compose.html\":{\"title\":\"13、Docker-compose容器编排\",\"contents\":[{\"header\":\"1、什么是compose\",\"slug\":\"_1、什么是compose\",\"contents\":[\"Compose 是 Docker 公司推出的一个工具软件，可以管理多个 Docker 容器组成一个应用。你需要定义一个 YAML 格式的配置文件docker-compose.yml，写好多个容器之间的调用关系。然后，只要一个命令，就能同时启动/关闭这些容器.\",\"Docker-Compose是Docker官方的开源项目，负责实现对Docker容器集群的快速编排。\"]},{\"header\":\"2、作用\",\"slug\":\"_2、作用\",\"contents\":[\"docker建议我们每一个容器中只运行一个服务,因为docker容器本身占用资源极少,所以最好是将每个服务单独的分割开来但是这样我们又面临了一个问题？\",\"如果我需要同时部署好多个服务,难道要每个服务单独写Dockerfile然后在构建镜像,构建容器,这样累都累死了,所以docker官方给我们提供了docker-compose多服务部署的工具\",\"例如要实现一个Web微服务项目，除了Web服务容器本身，往往还需要再加上后端的数据库mysql服务容器，redis服务器，注册中心eureka，甚至还包括负载均衡容器等等。。。。。。\",\"Compose允许用户通过一个单独的docker-compose.yml模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\",\"可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-Compose 解决了容器与容器之间如何管理编排的问题。\"]},{\"header\":\"3、安装\",\"slug\":\"_3、安装\",\"contents\":[\"官网地址：https://docs.docker.com/compose/compose-file/compose-file-v3/、\",\"官网下载：https://docs.docker.com/compose/install/\",\"安装步骤：\",\"官网教程：https://docs.docker.com/compose/install/\",\"下载二进制文件\",\"curl -L \\\"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\\\" -o /usr/local/bin/docker-compose \",\"对二进制文件应用可执行权限\",\"## 为所有用户安装 chmod +x /usr/local/bin/docker-compose \",\"测试\",\"docker-compose --version \"]},{\"header\":\"卸载 🔗\",\"slug\":\"卸载🔗\",\"contents\":[\"如果您使用以下方式安装 Docker Compose，请卸载curl：\",\"$ rm $DOCKER_CONFIG/cli-plugins/docker-compose \",\"或者如果您选择为所有用户安装 Compose\",\"$ sudo rm /usr/local/lib/docker/cli-plugins/docker-compose \"]},{\"header\":\"4、Compose核心概念\",\"slug\":\"_4、compose核心概念\",\"contents\":[\"主要是：一文件和两要素\",\"一文件：docker-compose.yml\",\"两要素：\",\"服务（service）：一个个应用容器实例，比如订单微服务、库存微服务、mysql容器、nginx容器或者redis容器\",\"工程（project）：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\"]},{\"header\":\"5、Compose使用的三个步骤\",\"slug\":\"_5、compose使用的三个步骤\",\"contents\":[\"编写Dockerfile定义各个微服务应用并构建出对应的镜像文件\",\"使用 docker-compose.yml 定义一个完整业务单元，安排好整体应用中的各个容器服务。\",\"最后，执行docker-compose up命令 来启动并运行整个应用程序，完成一键部署上线\"]},{\"header\":\"6、Compose常用命令\",\"slug\":\"_6、compose常用命令\",\"contents\":[\"docker-compose -h ## 查看帮助 docker-compose up ## 启动所有docker-compose服务 docker-compose up -d ## 启动所有docker-compose服务并后台运行 docker-compose down ## 停止并删除容器、网络、卷、镜像。 docker-compose exec yml里面的服务id ## 进入容器实例内部 docker-compose exec docker-compose.yml文件中写的服务id /bin/bash docker-compose ps ## 展示当前docker-compose编排过的运行的所有容器 docker-compose top ## 展示当前docker-compose编排过的容器进程 docker-compose logs yml里面的服务id ## 查看容器输出日志 docker-compose config ## 检查配置 docker-compose config -q ## 检查配置，有问题才有输出 docker-compose restart ## 重启服务 docker-compose start ## 启动服务 docker-compose stop ## 停止服务 \"]},{\"header\":\"7、Compose编排微服务\",\"slug\":\"_7、compose编排微服务\",\"contents\":[]},{\"header\":\"1、修改之前的微服务项目\",\"slug\":\"_1、修改之前的微服务项目\",\"contents\":[]},{\"header\":\"1、SQL建库建表\",\"slug\":\"_1、sql建库建表\",\"contents\":[\"CREATE TABLE `t_user` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `username` varchar(50) NOT NULL DEFAULT '' COMMENT '用户名', `password` varchar(50) NOT NULL DEFAULT '' COMMENT '密码', `sex` tinyint(4) NOT NULL DEFAULT '0' COMMENT '性别 0=女 1=男 ', `deleted` tinyint(4) unsigned NOT NULL DEFAULT '0' COMMENT '删除标志，默认0不删除，1删除', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='用户表' \"]},{\"header\":\"2、利用MybatisX一键生成代码\",\"slug\":\"_2、利用mybatisx一键生成代码\",\"contents\":[]},{\"header\":\"3、改pom\",\"slug\":\"_3、改pom\",\"contents\":[\" <properties> <java.version>1.8</java.version> <druid.version>1.1.16</druid.version> <mybatis.spring.boot.version>1.3.0</mybatis.spring.boot.version> <log4j.version>1.2.17</log4j.version> <lombok.version>1.16.18</lombok.version> </properties> <dependencies> <!--guava Google 开源的 Guava 中自带的布隆过滤器--> <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>23.0</version> </dependency> <!-- redisson --> <dependency> <groupId>org.redisson</groupId> <artifactId>redisson</artifactId> <version>3.13.4</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>io.springfox</groupId> <artifactId>springfox-swagger2</artifactId> <version>2.9.2</version> </dependency> <dependency> <groupId>io.springfox</groupId> <artifactId>springfox-swagger-ui</artifactId> <version>2.9.2</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-redis</artifactId> </dependency> <!--springCache--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-cache</artifactId> </dependency> <!--SpringBoot集成druid连接池--> <dependency> <groupId>com.alibaba</groupId> <artifactId>druid-spring-boot-starter</artifactId> <version>1.1.10</version> </dependency> <dependency> <groupId>com.alibaba</groupId> <artifactId>druid</artifactId> <version>${druid.version}</version> </dependency> <!--mybatis和springboot整合--> <!-- <dependency>--> <!-- <groupId>org.mybatis.spring.boot</groupId>--> <!-- <artifactId>mybatis-spring-boot-starter</artifactId>--> <!-- <version>${mybatis.spring.boot.version}</version>--> <!-- </dependency>--> <!--mybatis-plus--> <dependency> <groupId>com.baomidou</groupId> <artifactId>mybatis-plus-boot-starter</artifactId> <version>3.5.1</version> </dependency> <!--需要添加这个依耐--> <dependency> <groupId>com.baomidou</groupId> <artifactId>mybatis-plus-extension</artifactId> <version>3.5.1</version> </dependency> <!-- 添加springboot对amqp的支持 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-amqp</artifactId> </dependency> <dependency> <groupId>commons-codec</groupId> <artifactId>commons-codec</artifactId> <version>1.10</version> </dependency> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <version>${lombok.version}</version> <optional>true</optional> </dependency> <!--hutool--> <dependency> <groupId>cn.hutool</groupId> <artifactId>hutool-all</artifactId> <version>5.2.3</version> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>${junit.version}</version> </dependency> <!--springCache连接池依赖包--> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-pool2</artifactId> </dependency> <!-- jedis --> <dependency> <groupId>redis.clients</groupId> <artifactId>jedis</artifactId> <version>3.1.0</version> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> </plugin> </plugins> </build> \"]},{\"header\":\"4、写yaml\",\"slug\":\"_4、写yaml\",\"contents\":[\"server: port: 6688 #数据库连接配置 spring: datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://192.168.130.132:3306/docker-compose?useUnicode=true&characterEncoding=utf-8&useSSL=false username: root password: 123456 druid: test-while-idle: false #redis配置 redis: database: 0 host: 192.168.130.132 port: 6379 password: lettuce: pool: max-active: 8 max-wait: -1ms max-idle: 8 min-idle: 0 swagger2: enabled: true #myabtis配置 #mybatis: ## mapper-locations: classpath:mapper/*.xml ## type-aliases-package: com.xiaobear.dockerboot.domain \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[]},{\"header\":\"1、配置类\",\"slug\":\"_1、配置类\",\"contents\":[\"RedisConfig\",\"@Configuration public class RedisConfig { @Bean public RedisTemplate<String, Serializable> redisTemplate(LettuceConnectionFactory connectionFactory) { RedisTemplate<String,Serializable> redisTemplate = new RedisTemplate<>(); redisTemplate.setConnectionFactory(connectionFactory); //设置key序列化方式string redisTemplate.setKeySerializer(new StringRedisSerializer()); //设置value序列化方式string redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setHashValueSerializer(new GenericJackson2JsonRedisSerializer()); return redisTemplate; } } \",\"SwaggerConfig\",\"@Configuration @EnableSwagger2 public class SwaggerConfig extends WebMvcConfigurationSupport { @Value(\\\"${spring.swagger2.enabled}\\\") private Boolean enable; @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\\\"doc.html\\\") .addResourceLocations(\\\"classpath:/META-INF/resources/\\\"); registry.addResourceHandler(\\\"swagger-ui.html\\\") .addResourceLocations(\\\"classpath:/META-INF/resources/\\\"); registry.addResourceHandler(\\\"/webjars/**\\\") .addResourceLocations(\\\"classpath:/META-INF/resources/webjars/\\\"); super.addResourceHandlers(registry); } @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .enable(enable) .select() //你自己的package .apis(RequestHandlerSelectors.basePackage(\\\"com.xiaobear.dockerboot.controller\\\")) .paths(PathSelectors.any()) .build(); } public ApiInfo apiInfo() { return new ApiInfoBuilder() .title(\\\"xiaobear学Java\\\"+\\\"\\\\t\\\"+new SimpleDateFormat(\\\"yyyy-MM-dd\\\").format(new Date())) .description(\\\"docker-compose\\\") .version(\\\"1.0\\\") .termsOfServiceUrl(\\\"https://www.xiaobear.com/\\\") .build(); } } \",\"SpringBoot集成swagger后出现: Failed to start bean ‘documentationPluginsBootstrapper‘的解决方法\",\"解决办法：在启动类加一个注解：@EnableWebMvc\",\"访问swagger出现404，后端没有异常报错信息\",\"原因：swagger版本与spring boot版本不兼容 建议降低版本\"]},{\"header\":\"2、domain\",\"slug\":\"_2、domain\",\"contents\":[\"@TableName(value =\\\"t_user\\\") @Data @AllArgsConstructor @NoArgsConstructor public class TUser implements Serializable { /** * */ @TableId(type = IdType.AUTO) private Integer id; /** * 用户名 */ private String username; /** * 密码 */ private String password; /** * 性别 0=女 1=男 */ private Integer sex; /** * 删除标志，默认0不删除，1删除 */ private Integer deleted; /** * 更新时间 */ private Date updateTime; /** * 创建时间 */ private Date createTime; @TableField(exist = false) private static final long serialVersionUID = 1L; } \"]},{\"header\":\"3、service\",\"slug\":\"_3、service\",\"contents\":[\"public interface TUserService extends IService<TUser> { /** * 新增用户 * @param user */ public void insertUser(TUser user); /** * 查询用户通过Id * @param id * @return */ public TUser selectUserById(Integer id); } \",\"@Service public class TUserServiceImpl extends ServiceImpl<TUserMapper, TUser> implements TUserService{ public static final String CACHE_KEY_USER = \\\"user:\\\"; @Resource private RedisTemplate redisTemplate; @Resource private TUserMapper userMapper; @Override public void insertUser(TUser user) { //1 先插入mysql并成功 int i = userMapper.insert(user); if(i > 0) { //2 需要再次查询一下mysql将数据捞回来并ok user = userMapper.selectById(user.getId()); //3 将捞出来的user存进redis，完成新增功能的数据一致性。 String key = CACHE_KEY_USER+user.getId(); redisTemplate.opsForValue().set(key,user); } } @Override public TUser selectUserById(Integer id) { TUser user = null; String key = CACHE_KEY_USER+id; //1 先从redis里面查询，如果有直接返回结果，如果没有再去查询mysql user = (TUser) redisTemplate.opsForValue().get(key); if(user == null) { //2 redis里面无，继续查询mysql user = userMapper.selectById(id); if(user == null) { //3.1 redis+mysql 都无数据 //你具体细化，防止多次穿透，我们规定，记录下导致穿透的这个key回写redis return user; }else{ //3.2 mysql有，需要将数据写回redis，保证下一次的缓存命中率 redisTemplate.opsForValue().set(key,user); } } return user; } } \"]},{\"header\":\"4、Mapper.xml\",\"slug\":\"_4、mapper-xml\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <!DOCTYPE mapper PUBLIC \\\"-//mybatis.org//DTD Mapper 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\\\"> <mapper namespace=\\\"com.xiaobear.dockerboot.mapper.TUserMapper\\\"> <resultMap id=\\\"BaseResultMap\\\" type=\\\"com.xiaobear.dockerboot.domain.TUser\\\"> <id property=\\\"id\\\" column=\\\"id\\\" jdbcType=\\\"INTEGER\\\"/> <result property=\\\"username\\\" column=\\\"username\\\" jdbcType=\\\"VARCHAR\\\"/> <result property=\\\"password\\\" column=\\\"password\\\" jdbcType=\\\"VARCHAR\\\"/> <result property=\\\"sex\\\" column=\\\"sex\\\" jdbcType=\\\"TINYINT\\\"/> <result property=\\\"deleted\\\" column=\\\"deleted\\\" jdbcType=\\\"TINYINT\\\"/> <result property=\\\"updateTime\\\" column=\\\"update_time\\\" jdbcType=\\\"TIMESTAMP\\\"/> <result property=\\\"createTime\\\" column=\\\"create_time\\\" jdbcType=\\\"TIMESTAMP\\\"/> </resultMap> <sql id=\\\"Base_Column_List\\\"> id,username,password, sex,deleted,update_time, create_time </sql> </mapper> \"]},{\"header\":\"5、controller\",\"slug\":\"_5、controller\",\"contents\":[\"@Api(description = \\\"用户User接口\\\") @RestController public class DockerController { @Value(\\\"${server.port}\\\") private String port; @Resource private TUserService userService; @GetMapping(\\\"/hello\\\") public String hello(){ return \\\"hello, docker-boot\\\" + port + \\\" \\\" + UUID.randomUUID(); } @ApiOperation(\\\"新增用户\\\") @PostMapping(\\\"/insert/user\\\") public void insertUser(){ for (int i = 1; i <= 3; i++) { TUser user = new TUser(); user.setUsername(\\\"xiaobear-\\\"+i); user.setPassword(IdUtil.simpleUUID().substring(0,6)); user.setSex(new Random().nextInt(2)); userService.insertUser(user); } } @ApiOperation(\\\"查询1条记录\\\") @GetMapping(value = \\\"/user/find/{id}\\\") public TUser findUserById(@PathVariable Integer id) { return userService.selectUserById(id); } } \"]},{\"header\":\"6、主启动类\",\"slug\":\"_6、主启动类\",\"contents\":[\"@SpringBootApplication @EnableWebMvc @MapperScan(\\\"com.xiaobear.dockerboot.mapper\\\") public class DockerBootApplication { public static void main(String[] args) { SpringApplication.run(DockerBootApplication.class, args); } } \"]},{\"header\":\"6、本地测试完进行打包\",\"slug\":\"_6、本地测试完进行打包\",\"contents\":[\"mvn package命令将微服务形成新的jar包 并上传到Linux服务器/mydocker目录下\"]},{\"header\":\"7、编写Dockerfile\",\"slug\":\"_7、编写dockerfile\",\"contents\":[\"## 基础镜像使用java FROM java:8 ## 作者 MAINTAINER xiaobear ## VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp ## 将jar包添加到容器中并更名为xiaobear_docker.jar ADD Docker-Boot-0.0.1-SNAPSHOT.jar xiaobear_docker.jar ## 运行jar包 RUN bash -c 'touch /xiaobear_docker.jar' ENTRYPOINT [\\\"java\\\",\\\"-jar\\\",\\\"/xiaobear_docker.jar\\\"] #暴露6688端口作为微服务 EXPOSE 6688 \"]},{\"header\":\"8、构建镜像\",\"slug\":\"_8、构建镜像\",\"contents\":[\"docker build -t xiaobear-docker-boot:2.0 . \"]},{\"header\":\"2、不适用Compose编排\",\"slug\":\"_2、不适用compose编排\",\"contents\":[]},{\"header\":\"1、启动mysql实例\",\"slug\":\"_1、启动mysql实例\",\"contents\":[\"docker run -p 3306:3306 --name mysql-master \\\\ -v /mydata/mysql-master/log:/var/log/mysql \\\\ -v /mydata/mysql-master/data:/var/lib/mysql \\\\ -v /mydata/mysql-master/conf:/etc/mysql \\\\ -e MYSQL_ROOT_PASSWORD=root \\\\ -d mysql:5.7 \",\"进入mysql容器实例并新建库docker-compose+新建表t_user\"]},{\"header\":\"2、启动redis实例\",\"slug\":\"_2、启动redis实例\",\"contents\":[\"docker run -p 6379:6379 --name myredis --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis redis-server /etc/redis/redis.conf \"]},{\"header\":\"3、启动微服务实例\",\"slug\":\"_3、启动微服务实例\",\"contents\":[\"docker run -it -d -p 6688:6688 xiaobear-docker-boot:2.0 \"]},{\"header\":\"4、测试微服务\",\"slug\":\"_4、测试微服务\",\"contents\":[\"http://localhost:你的微服务端口/swagger-ui.html#/\"]},{\"header\":\"5、存在的问题\",\"slug\":\"_5、存在的问题\",\"contents\":[\"先后顺序要求固定，先mysql+redis才能微服务访问成功\",\"多个run命令......\",\"容器间的启停或宕机，有可能导致IP地址对应的容器实例变化，映射出错，要么生产IP写死(可以但是不推荐)，要么通过服务调用\"]},{\"header\":\"3、使用Compose\",\"slug\":\"_3、使用compose\",\"contents\":[\"服务编排，一套带走，安排\"]},{\"header\":\"1、编写docker-compose.yml\",\"slug\":\"_1、编写docker-compose-yml\",\"contents\":[\"version: \\\"3\\\" services: microService: image: xiaobear-docker-boot:2.0 container_name: ms01 ports: - \\\"6688:6688\\\" volumes: - /app/microService:/data networks: - xiaobear-network depends_on: - redis - mysql redis: image: redis ports: - \\\"6379:6379\\\" volumes: - /app/redis/redis.conf:/etc/redis/redis.conf - /app/redis/data:/data networks: - xiaobear-network command: redis-server /etc/redis/redis.conf mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: '123456' MYSQL_ALLOW_EMPTY_PASSWORD: 'no' MYSQL_DATABASE: 'docker-compose' MYSQL_USER: 'xiaobear' MYSQL_PASSWORD: 'zzyy123' ports: - \\\"3306:3306\\\" volumes: - /app/mysql/db:/var/lib/mysql - /app/mysql/conf/my.cnf:/etc/my.cnf - /app/mysql/init:/docker-entrypoint-initdb.d networks: - xiaobear-network command: --default-authentication-plugin=mysql_native_password #解决外部无法访问 networks: xiaobear-network: \"]},{\"header\":\"2、改造微服务\",\"slug\":\"_2、改造微服务\",\"contents\":[\"通过服务名访问，IP无关\",\"server: port: 6688 #数据库连接配置 spring: datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://mysql:3306/docker-compose?useUnicode=true&characterEncoding=utf-8&useSSL=false username: root password: 123456 druid: test-while-idle: false #redis配置 redis: database: 0 host: redis port: 6379 password: lettuce: pool: max-active: 8 max-wait: -1ms max-idle: 8 min-idle: 0 swagger2: enabled: true #myabtis配置 #mybatis: ## mapper-locations: classpath:mapper/*.xml ## type-aliases-package: com.xiaobear.dockerboot.domain \",\"上传构建镜像\"]},{\"header\":\"3、执行docker-compose\",\"slug\":\"_3、执行docker-compose\",\"contents\":[\"docker-compose up \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"访问：http://localhost:你的微服务端口/swagger-ui.html#/\"]},{\"header\":\"5、关停\",\"slug\":\"_5、关停\",\"contents\":[\"[root@docker mydocker]## docker-compose stop Stopping ms01 ... done Stopping mydocker_mysql_1 ... done Stopping mydocker_redis_1 ... done [root@docker mydocker]## \"]}]},\"/study-tutorial/distributed/docker/image.html\":{\"title\":\"4、Docker镜像\",\"contents\":[{\"header\":\"1、什么是Docker镜像\",\"slug\":\"_1、什么是docker镜像\",\"contents\":[\"镜像：是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。\",\"只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。\"]},{\"header\":\"1、分层镜像\",\"slug\":\"_1、分层镜像\",\"contents\":[\"docker pull我们拉取镜像的时候，下载的时候就是多线程在下载，一层一层的下载\"]},{\"header\":\"2、联合文件系统\",\"slug\":\"_2、联合文件系统\",\"contents\":[\"UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。\",\"特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录\"]},{\"header\":\"3、镜像加速的原理\",\"slug\":\"_3、镜像加速的原理\",\"contents\":[\"平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？\",\"[root@docker /]## docker ../../images centos REPOSITORY TAG IMAGE ID CREATED SIZE centos latest 5d0da3dc9764 6 months ago 231MB \",\"对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。\"]},{\"header\":\"4、Docker为什么采用分层镜像\",\"slug\":\"_4、docker为什么采用分层镜像\",\"contents\":[\"镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。\",\"比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\"]},{\"header\":\"2、理解\",\"slug\":\"_2、理解\",\"contents\":[\"Docker镜像层都是只读的，容器层是可写的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。\",\"所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。\"]},{\"header\":\"3、 commit 操作\",\"slug\":\"_3、commit操作\",\"contents\":[\"docker commit -m=\\\"提交的描述信息\\\" -a=\\\"作者\\\" 容器ID 要创建的目标镜像名:[标签名]\",\"docker commit提交容器副本使之成为一个新的镜像\"]},{\"header\":\"案例演示\",\"slug\":\"案例演示\",\"contents\":[]},{\"header\":\"1、启动centos容器\",\"slug\":\"_1、启动centos容器\",\"contents\":[\"[root@docker /]## docker run -it centos /bin/bash [root@c7f841ff540c /]## vim a.txt bash: vim: command not found \"]},{\"header\":\"2、安装Vim\",\"slug\":\"_2、安装vim\",\"contents\":[\"[root@c7f841ff540c /]## yum update Failed to set locale, defaulting to C.UTF-8 CentOS Linux 8 - AppStream 54 B/s | 38 B 00:00 Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist \",\"失败原因：\",\"在2022年1月31日，CentOS团队终于从官方镜像中移除CentOS 8的所有包。\",\"CentOS 8已于2021年12月31日寿终正非，但软件包仍在官方镜像上保留了一段时间。现在他们被转移到https://vault.centos.org\",\"解决方法\",\"如果你仍然需要运行CentOS 8，你可以在/etc/yum.repos.d中更新一下源。使用vault.centos.org代替mirror.centos.org。\",\"sed -i -e \\\"s|mirrorlist=|#mirrorlist=|g\\\" /etc/yum.repos.d/CentOS-* sed -i -e \\\"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\\\" /etc/yum.repos.d/CentOS-* \",\"yum install -y vim \"]},{\"header\":\"3、提交新镜像\",\"slug\":\"_3、提交新镜像\",\"contents\":[\"docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] \",\"[root@docker /]## docker commit -m=\\\"add vim cmd\\\" -a=\\\"xiaobear\\\" c7f841ff540c centos1:1.0 sha256:08f93c12227128eef1e61d0c064ad1b74f903e4102071528a798389980e69608 \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"[root@docker /]## docker run -it centos1:1.0 /bin/bash [root@80c4d3b1ea3b /]## vim xiaobear.txt [root@80c4d3b1ea3b /]## \",\"官网是默认下载的centos没有vim命令 我们自己commit构建的镜像，新增加了vim功能，可以成功使用。\"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[\"Docker中的镜像分层，支持通过扩展现有镜像，创建新的镜像。类似Java继承于一个Base基础类，自己再按需扩展。 新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层\"]}]},\"/study-tutorial/distributed/docker/install.html\":{\"title\":\"2、Docker安装\",\"contents\":[{\"header\":\"1、前提说明\",\"slug\":\"_1、前提说明\",\"contents\":[]},{\"header\":\"1、前提条件\",\"slug\":\"_1、前提条件\",\"contents\":[\"目前，CentOS 仅发行版本中的内核支持 Docker。Docker 运行在CentOS 7 (64-bit)上，要求系统为64位、Linux系统内核版本为 3.8以上，这里选用Centos7.x\"]},{\"header\":\"2、查看自己内核\",\"slug\":\"_2、查看自己内核\",\"contents\":[\"uname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等）。\",\"[root@docker ~]## cat /etc/redhat-release CentOS Linux release 7.8.2003 (Core) [root@docker ~]## uname -r 3.10.0-1127.el7.x86_64 \"]},{\"header\":\"2、Docker的组成\",\"slug\":\"_2、docker的组成\",\"contents\":[]},{\"header\":\"1、镜像 image\",\"slug\":\"_1、镜像image\",\"contents\":[\"Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。它也相当于是一个root文件系统。\",\"比如官方镜像 centos:7 就包含了完整的一套 centos:7 最小系统的 root 文件系统。\",\"相当于容器的“源代码”，docker镜像文件类似于Java的类模板，而docker容器实例类似于java中new出来的实例对象。\"]},{\"header\":\"2、容器 container\",\"slug\":\"_2、容器container\",\"contents\":[]},{\"header\":\"1、从面向对象角度看\",\"slug\":\"_1、从面向对象角度看\",\"contents\":[\"Docker 利用容器（Container）独立运行的一个或一组应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境，容器是用镜像创建的运行实例。就像是Java中的类和实例对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器为镜像提供了一个标准的和隔离的运行环境，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台\"]},{\"header\":\"2、从容器角度看\",\"slug\":\"_2、从容器角度看\",\"contents\":[\"可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\"]},{\"header\":\"3、仓库 repository\",\"slug\":\"_3、仓库repository\",\"contents\":[\"仓库（Repository）是集中存放镜像文件的场所。\",\"类似于 Maven仓库，存放各种jar包的地方； github仓库，存放各种git项目的地方； Docker公司提供的官方registry被称为Docker Hub，存放各种镜像模板的地方。\",\"仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub(https://hub.docker.com/)， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云等\"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[\"需要正确的理解仓库/镜像/容器这几个概念:\",\"Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是image镜像文件。只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。\",\"image文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。\",\"镜像文件\",\"image 文件生成的容器实例，本身也是一个文件，称为镜像文件。\",\"容器实例\",\"一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器\",\"仓库\",\"就是放一堆镜像的地方，我们可以把镜像发布到仓库中，需要的时候再从仓库中拉下来就可以了。\"]},{\"header\":\"3、整体架构及底层原理\",\"slug\":\"_3、整体架构及底层原理\",\"contents\":[\"Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。\"]},{\"header\":\"4、Docker安装\",\"slug\":\"_4、docker安装\",\"contents\":[\"官方安装地址：https://docs.docker.com/engine/install/centos/\",\"个人总结安装地址：http://xiao_bear.gitee.io/javaxiaobear/install/linux.html#_1%E3%80%81linux%E5%AE%89%E8%A3%85docker\"]},{\"header\":\"1、卸载系统之前的docker\",\"slug\":\"_1、卸载系统之前的docker\",\"contents\":[\"sudo yum remove docker \\\\ docker-client \\\\ docker-client-latest \\\\ docker-common \\\\ docker-latest \\\\ docker-latest-logrotate \\\\ docker-logrotate \\\\ docker-engine \"]},{\"header\":\"2、yum安装gcc相关\",\"slug\":\"_2、yum安装gcc相关\",\"contents\":[\"yum install -y gcc gcc-c++ \"]},{\"header\":\"3、安装需要的软件包\",\"slug\":\"_3、安装需要的软件包\",\"contents\":[\"yum install -y yum-utils \"]},{\"header\":\"4、设置stable镜像仓库\",\"slug\":\"_4、设置stable镜像仓库\",\"contents\":[\"最好使用阿里云镜像,国外下载速度慢 官方镜像：yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 阿里云镜像：yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo \",\"[root@docker ~]## yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 已加载插件：fastestmirror, langpacks adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo grabbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo \"]},{\"header\":\"5、更新yum软件包索引\",\"slug\":\"_5、更新yum软件包索引\",\"contents\":[\"yum makecache fast \"]},{\"header\":\"6、安装DOCKER CE\",\"slug\":\"_6、安装docker-ce\",\"contents\":[\"yum -y install docker-ce docker-ce-cli containerd.io \"]},{\"header\":\"7、启动docker\",\"slug\":\"_7、启动docker\",\"contents\":[\"systemctl start docker \"]},{\"header\":\"8、测试\",\"slug\":\"_8、测试\",\"contents\":[\"查看docker版本\",\"docker version \",\"永远的hello-world：没有找到镜像，会自动拉去镜像下载\",\"docker run hello-world \",\"[root@docker ~]## docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:bfea6278a0a267fad2634554f4f0c6f31981eea41c553fdf5a83e95a41d40c38 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \\\"hello-world\\\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share ../../../images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ \"]},{\"header\":\"9、卸载\",\"slug\":\"_9、卸载\",\"contents\":[\"停止docker systemctl stop docker 移除docker组件 yum remove docker-ce docker-ce-cli containerd.io rm -rf /var/lib/docker rm -rf /var/lib/containerd \"]},{\"header\":\"5、阿里云加速镜像\",\"slug\":\"_5、阿里云加速镜像\",\"contents\":[]},{\"header\":\"1、阿里云镜像地址\",\"slug\":\"_1、阿里云镜像地址\",\"contents\":[\"https://promotion.aliyun.com/ntms/act/kubernetes.html\"]},{\"header\":\"2、获得加速器地址连接\",\"slug\":\"_2、获得加速器地址连接\",\"contents\":[\"登录\",\"控制台\",\"选择容器镜像服务\",\"获取加速器地址\",\"运行命名\",\"#分步骤执行 mkdir -p /etc/docker vim /etc/docker/daemon.json #阿里云 { \\\"registry-mirrors\\\": [\\\"https://｛自已的编码｝.mirror.aliyuncs.com\\\"] } \",\"重启服务器\",\"sudo systemctl daemon-reload sudo systemctl restart docker \"]},{\"header\":\"6、Hello World\",\"slug\":\"_6、hello-world\",\"contents\":[\"启动Docker后台容器(测试运行 hello-world)\",\"PS：输出这段提示以后，hello world就会停止运行，容器自动终止。\"]},{\"header\":\"run的流程\",\"slug\":\"run的流程\",\"contents\":[]},{\"header\":\"7、底层原理\",\"slug\":\"_7、底层原理\",\"contents\":[]},{\"header\":\"1、为什么Docker会比虚拟机快\",\"slug\":\"_1、为什么docker会比虚拟机快\",\"contents\":[\"docker有着比虚拟机更少的抽象层\",\"由于docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。\",\"docker利用的是宿主机的内核,而不需要加载操作系统OS内核\",\"当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。进而避免引寻、加载操作系统内核返回等比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载OS,返回新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返回过程,因此新建一个docker容器只需要几秒钟。\"]}]},\"/study-tutorial/distributed/docker/introduction.html\":{\"title\":\"1、Docker简介\",\"contents\":[{\"header\":\"1、Docker为何会出现\",\"slug\":\"_1、docker为何会出现\",\"contents\":[]},{\"header\":\"1、引出Docker\",\"slug\":\"_1、引出docker\",\"contents\":[\"假定您在开发一个尚硅谷的谷粒商城，您使用的是一台笔记本电脑而且您的开发环境具有特定的配置。其他开发人员身处的环境配置也各有不同。您正在开发的应用依赖于您当前的配置且还要依赖于某些配置文件。此外，您的企业还拥有标准化的测试和生产环境，且具有自身的配置和一系列支持文件。您希望尽可能多在本地模拟这些环境而不产生重新创建服务器环境的开销。\",\"请问？ 您要如何确保应用能够在这些环境中运行和通过质量检测？并且在部署过程中不出现令人头疼的版本、配置问题，也无需重新编写代码和进行故障修复？\",\"答案：使用容器。Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案-----系统平滑移植，容器虚拟化技术。\",\"环境配置相当麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。\",\"之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说，Java/RabbitMQ/MySQL/JDBC驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在 Windows 上安装的这些环境，到了 Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。\",\"传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等(java为例)。而为了让这些程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker的出现使得Docker得以打破过去「程序即应用」的观念。透过镜像(../../../images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。\"]},{\"header\":\"2、什么是Docker\",\"slug\":\"_2、什么是docker\",\"contents\":[\"Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到“一次镜像，处处运行”。\",\"Linux容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用打成镜像，通过镜像成为运行在Docker容器上面的实例，而 Docker容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。\",\"只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。\"]},{\"header\":\"2、容器与虚拟机的比较\",\"slug\":\"_2、容器与虚拟机的比较\",\"contents\":[]},{\"header\":\"1、容器的发展史\",\"slug\":\"_1、容器的发展史\",\"contents\":[\"具体资讯参考：https://developer.aliyun.com/article/775778\"]},{\"header\":\"2、传统虚拟机技术\",\"slug\":\"_2、传统虚拟机技术\",\"contents\":[\"虚拟机（virtual machine）就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。\",\"虚拟机的缺点：\",\"占用资源多\",\"冗余步骤多\",\"启动速度慢\"]},{\"header\":\"3、容器虚拟化技术\",\"slug\":\"_3、容器虚拟化技术\",\"contents\":[\"由于前面虚拟机存在某些缺点，Linux发展出了另一种虚拟化技术：\"]},{\"header\":\"Linux容器(Linux Containers，缩写为 LXC)\",\"slug\":\"linux容器-linux-containers-缩写为-lxc\",\"contents\":[\"Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。\",\"Linux 容器不是模拟一个完整的操作系统而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。\",\"Docker容器则是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统虚拟机则是在硬件层面上实现虚拟化，与传统的虚拟机相比，Docker优势体现为启动速度快，占用内存小。\"]},{\"header\":\"4、区别\",\"slug\":\"_4、区别\",\"contents\":[\"Docker 和传统虚拟化方式的不同之处：\",\"传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；\",\"容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。\",\"每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。\"]},{\"header\":\"3、学习的用处\",\"slug\":\"_3、学习的用处\",\"contents\":[]},{\"header\":\"1、职场的变化\",\"slug\":\"_1、职场的变化\",\"contents\":[\"coder -> programmer -> software engineer -> DevOps engineer\"]},{\"header\":\"2、一次构建，随处运行\",\"slug\":\"_2、一次构建-随处运行\",\"contents\":[\"更快速的应用交付和部署\",\"传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间。\",\"更便捷的升级和扩缩容\",\"随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成一块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。\",\"更简单的系统运维\",\"应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度一致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。\",\"更高效的计算资源利用\",\"Docker是内核级虚拟化，其不像传统的虚拟化技术一样需要额外的Hypervisor支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。\"]},{\"header\":\"3、Docker应用场景\",\"slug\":\"_3、docker应用场景\",\"contents\":[]},{\"header\":\"4、官网地址\",\"slug\":\"_4、官网地址\",\"contents\":[\"官网：docker官网：http://www.docker.com\",\"仓库：Docker Hub官网: https://hub.docker.com/\"]}]},\"/study-tutorial/distributed/docker/microservice.html\":{\"title\":\"11、Docker微服务实战\",\"contents\":[{\"header\":\"1、建立测试微服务模块\",\"slug\":\"_1、建立测试微服务模块\",\"contents\":[]},{\"header\":\"2、通过dockerfile发布微服务部署到docker容器\",\"slug\":\"_2、通过dockerfile发布微服务部署到docker容器\",\"contents\":[]},{\"header\":\"1、IDEA 进行打包\",\"slug\":\"_1、idea-进行打包\",\"contents\":[]},{\"header\":\"2、编写Dockerfile\",\"slug\":\"_2、编写dockerfile\",\"contents\":[\"## 基础镜像使用java FROM java:8 ## 作者 MAINTAINER xiaobear ## VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp ## 将jar包添加到容器中并更名为xiaobear_docker.jar ADD Docker-Boot-0.0.1-SNAPSHOT.jar xiaobear_docker.jar ## 运行jar包 RUN bash -c 'touch /xiaobear_docker.jar' ENTRYPOINT [\\\"java\\\",\\\"-jar\\\",\\\"/xiaobear_docker.jar\\\"] #暴露6688端口作为微服务 EXPOSE 6688 \",\"PS：将微服务jar包和Dockerfile文件上传到同一个目录下\"]},{\"header\":\"3、构建镜像\",\"slug\":\"_3、构建镜像\",\"contents\":[\"docker build -t docker_boot:1.0 . \"]},{\"header\":\"4、运行镜像\",\"slug\":\"_4、运行镜像\",\"contents\":[\" docker run -d -p 6688:6688 docker_boot:1.0 \"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"curl 127.0.0.1:6688/hello \"]}]},\"/study-tutorial/distributed/docker/network.html\":{\"title\":\"12、Docker网络\",\"contents\":[{\"header\":\"1、什么是Docker网络\",\"slug\":\"_1、什么是docker网络\",\"contents\":[\"没有开启Docker网络时的网络状态：\",\"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9e:80:10 brd ff:ff:ff:ff:ff:ff inet 192.168.130.132/24 brd 192.168.130.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::987d:a504:45a9:2011/64 scope link noprefixroute valid_lft forever preferred_lft forever 39: veth33b55fa@if38: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 2a:e3:37:b2:68:47 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::28e3:37ff:feb2:6847/64 scope link valid_lft forever preferred_lft forever \",\"开启docker后\",\"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9e:80:10 brd ff:ff:ff:ff:ff:ff inet 192.168.130.132/24 brd 192.168.130.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::987d:a504:45a9:2011/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:8a:3f:98:14 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:8aff:fe3f:9814/64 scope link valid_lft forever preferred_lft forever 39: veth33b55fa@if38: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 2a:e3:37:b2:68:47 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::28e3:37ff:feb2:6847/64 scope link valid_lft forever preferred_lft forever \",\"会产生一个名为docker0的虚拟网桥\",\"查看Docker网络的情况\",\"docker network ls \",\"[root@docker docker-boot]## docker network ls NETWORK ID NAME DRIVER SCOPE 012ef5850c2b bridge bridge local 15bd0ffe79b0 host host local 28741f9b037c none null local [root@docker docker-boot]## \"]},{\"header\":\"2、基本命令\",\"slug\":\"_2、基本命令\",\"contents\":[]},{\"header\":\"1、所有命令\",\"slug\":\"_1、所有命令\",\"contents\":[\"[root@docker docker-boot]## docker network --help Usage: docker network COMMAND Manage networks Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. [root@docker docker-boot]## \"]},{\"header\":\"2、查看网络命令\",\"slug\":\"_2、查看网络命令\",\"contents\":[\"docker network ls \"]},{\"header\":\"3、查看网络源\",\"slug\":\"_3、查看网络源\",\"contents\":[\"docker network inspect XXX网络名字 \"]},{\"header\":\"4、删除网络\",\"slug\":\"_4、删除网络\",\"contents\":[\"docker network rm 网络名字 \",\"全程案例过程：\",\"[root@docker docker-boot]## docker network create test_network 473cef3a3e4ca58d62898caa93ae97a3772ab0d90325fc7a107082ef8c5a5bce [root@docker docker-boot]## docker network ls NETWORK ID NAME DRIVER SCOPE 012ef5850c2b bridge bridge local 15bd0ffe79b0 host host local 28741f9b037c none null local 473cef3a3e4c test_network bridge local [root@docker docker-boot]## docker network rm test_network test_network [root@docker docker-boot]## docker network ls NETWORK ID NAME DRIVER SCOPE 012ef5850c2b bridge bridge local 15bd0ffe79b0 host host local 28741f9b037c none null local [root@docker docker-boot]## \"]},{\"header\":\"3、作用\",\"slug\":\"_3、作用\",\"contents\":[\"容器间的互联和通信以及端口映射\",\"容器IP变动时候可以通过服务名直接网络通信而不受到影响\"]},{\"header\":\"4、网络模式\",\"slug\":\"_4、网络模式\",\"contents\":[]},{\"header\":\"1、模式类型\",\"slug\":\"_1、模式类型\",\"contents\":[\"bridge模式：使用--network bridge指定，默认使用docker0\",\"host模式：使用--network host指定\",\"none模式：使用--network none指定\",\"container模式：使用--network container:NAME或者容器ID指定\"]},{\"header\":\"2、容器实例内默认网络IP生产规则\",\"slug\":\"_2、容器实例内默认网络ip生产规则\",\"contents\":[]},{\"header\":\"1、先启动两个centos容器实例\",\"slug\":\"_1、先启动两个centos容器实例\",\"contents\":[\"docker run -it -d --name c1 centos bash docker run -it -d --name c2 centos bash \"]},{\"header\":\"2、2 docker inspect 容器ID or 容器名字\",\"slug\":\"_2、2-docker-inspect-容器id-or-容器名字\",\"contents\":[\"[root@docker docker-boot]## docker inspect c1 | tail -n 20 \\\"Networks\\\": { \\\"bridge\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"012ef5850c2b3e380a1cb58d9365d60a6ee8ee4b943d8d6ab90aaebd22e1bf62\\\", \\\"EndpointID\\\": \\\"257f45d018046dcfa90a5d201c436db20dea41cc9ec2bb58d88576c25143c62f\\\", \\\"Gateway\\\": \\\"172.17.0.1\\\", \\\"IPAddress\\\": \\\"172.17.0.3\\\", \\\"IPPrefixLen\\\": 16, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"02:42:ac:11:00:03\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker docker-boot]## docker inspect c2 | tail -n 20 \\\"Networks\\\": { \\\"bridge\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"012ef5850c2b3e380a1cb58d9365d60a6ee8ee4b943d8d6ab90aaebd22e1bf62\\\", \\\"EndpointID\\\": \\\"3b2688b510738178d64344d97e245b38442fbe91e71d486da2fd1ccf1953ec92\\\", \\\"Gateway\\\": \\\"172.17.0.1\\\", \\\"IPAddress\\\": \\\"172.17.0.4\\\", \\\"IPPrefixLen\\\": 16, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"02:42:ac:11:00:04\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker docker-boot]## \",\"两个示例对应的ip地址分别是：172.17.0.4和172.17.0.3\"]},{\"header\":\"3、关闭其中一个实例\",\"slug\":\"_3、关闭其中一个实例\",\"contents\":[\"关闭c1，再新建一个c3实例\",\"[root@docker docker-boot]## docker stop c1 c1 [root@docker docker-boot]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 010441413dc6 centos \\\"bash\\\" 12 minutes ago Up 12 minutes c2 f1141e4f51b9 90c07658a4b1 \\\"java -jar /xiaobear…\\\" 2 hours ago Up 2 hours 0.0.0.0:6688->6688/tcp, :::6688->6688/tcp zealous_shockley [root@docker docker-boot]## docker run -it -d --name c3 centos bash 70066a4f0735eea5a9d4f175bb4d4af4efe516f7d98e919bfbfe9a4699925515 [root@docker docker-boot]## docker inspect c3 | tail -n 20 \\\"Networks\\\": { \\\"bridge\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"012ef5850c2b3e380a1cb58d9365d60a6ee8ee4b943d8d6ab90aaebd22e1bf62\\\", \\\"EndpointID\\\": \\\"f1f8b9195c706b7e618953746be882c68a11bb36cd689519fc1a0b3ef580b6c9\\\", \\\"Gateway\\\": \\\"172.17.0.1\\\", \\\"IPAddress\\\": \\\"172.17.0.3\\\", \\\"IPPrefixLen\\\": 16, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"02:42:ac:11:00:03\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker docker-boot]## docker inspect c2 | tail -n 20 \\\"Networks\\\": { \\\"bridge\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"012ef5850c2b3e380a1cb58d9365d60a6ee8ee4b943d8d6ab90aaebd22e1bf62\\\", \\\"EndpointID\\\": \\\"3b2688b510738178d64344d97e245b38442fbe91e71d486da2fd1ccf1953ec92\\\", \\\"Gateway\\\": \\\"172.17.0.1\\\", \\\"IPAddress\\\": \\\"172.17.0.4\\\", \\\"IPPrefixLen\\\": 16, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"02:42:ac:11:00:04\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker docker-boot]## \",\"总结：docker容器内部的ip是有可能会发生改变的\"]},{\"header\":\"3、模式案例\",\"slug\":\"_3、模式案例\",\"contents\":[]},{\"header\":\"1、 bridge\",\"slug\":\"_1、bridge\",\"contents\":[\"Docker 服务默认会创建一个 docker0 网桥（其上有一个 docker0 内部接口），该桥接网络的名称为docker0，它在内核层连通了其他的物理或虚拟网卡，这就将所有容器和本地主机都放到同一个物理网络。Docker 默认指定了 docker0 接口 的 IP 地址和子网掩码，让主机和容器之间可以通过网桥相互通信。\",\"查看 bridge 网络的详细信息，并通过 grep 获取名称项\",\"docker network inspect bridge | grep name \",\"[root@docker ~]## docker network inspect bridge | grep name \\\"com.docker.network.bridge.name\\\": \\\"docker0\\\", [root@docker ~]## [root@docker ~]## ifconfig | grep docker docker0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500 [root@docker ~]## \"]},{\"header\":\"说明\",\"slug\":\"说明\",\"contents\":[\"Docker使用Linux桥接，在宿主机虚拟一个Docker容器网桥(docker0)，Docker启动一个容器时会根据Docker网桥的网段分配给容器一个IP地址，称为Container-IP，同时Docker网桥是每个容器的默认网关。因为在同一宿主机内的容器都接入同一个网桥，这样容器之间就能够通过容器的Container-IP直接通信。\",\"docker run 的时候，没有指定network的话默认使用的网桥模式就是bridge，使用的就是docker0。在宿主机ifconfig,就可以看到docker0和自己create的network(后面讲)eth0，eth1，eth2……代表网卡一，网卡二，网卡三……，lo代表127.0.0.1，即localhost，inet addr用来表示网卡的IP地址\",\"网桥docker0创建一对对等虚拟设备接口一个叫veth，另一个叫eth0，成对匹配。\",\"整个宿主机的网桥模式都是docker0，类似一个交换机有一堆接口，每个接口叫veth，在本地主机和容器内分别创建一个虚拟接口，并让他们彼此联通（这样一对接口叫veth pair）；\",\"每个容器实例内部也有一块网卡，每个接口叫eth0；\",\"docker0上面的每个veth匹配某个容器实例内部的eth0，两两配对，一一匹配。\",\"通过上述，将宿主机上的所有容器都连接到这个内部网络上，两个容器在同一个网络下,会从这个网关下各自拿到分配的ip，此时两个容器的网络是互通的。\"]},{\"header\":\"案例\",\"slug\":\"案例\",\"contents\":[\"docker run -d -p 8081:8080 --name tomcat81 billygoo/tomcat8-jdk8 docker run -d -p 8082:8080 --name tomcat81 billygoo/tomcat8-jdk8 \",\"主机IP，新增了5: veth2dc5f8a@if4 和 7: vethad18b38@if6\",\"[root@docker ~]## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9e:80:10 brd ff:ff:ff:ff:ff:ff inet 192.168.130.132/24 brd 192.168.130.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::987d:a504:45a9:2011/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:df:0e:5d:5b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:dfff:fe0e:5d5b/64 scope link valid_lft forever preferred_lft forever 9: vethe7bf9cb@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 6a:23:3d:79:f7:ea brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::6823:3dff:fe79:f7ea/64 scope link valid_lft forever preferred_lft forever 11: veth7e028d8@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default link/ether be:01:87:68:c7:c1 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::bc01:87ff:fe68:c7c1/64 scope link valid_lft forever preferred_lft forever \",\"进入容器内部，发现没有ip addr 和ifconfig命令\",\"可以拉取免修改版docker pull billygoo/tomcat8-jdk8\",\"验证结果：\"]},{\"header\":\"2、 host\",\"slug\":\"_2、host\",\"contents\":[\"直接使用宿主机的 IP 地址与外界进行通信，不再需要额外进行NAT 转换。\"]},{\"header\":\"说明\",\"slug\":\"说明-1\",\"contents\":[\"容器将不会获得一个独立的Network Namespace， 而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡而是使用宿主机的IP和端口。\"]},{\"header\":\"案例\",\"slug\":\"案例-1\",\"contents\":[\"docker run -d -p 8083:8080 --network host --name tomcat83 billygoo/tomcat8-jdk8 \",\"出现了警告，但是并不影响\",\"[root@docker ~]## docker run -d -p 8083:8080 --network host --name tomcat83 billygoo/tomcat8-jdk8 WARNING: Published ports are discarded when using host network mode 3ef26b95ddc8a507fce7dd83becf360d1a3fe150b436a23246d68cf584b7e026 [root@docker ~]## \",\"问题： docke启动时总是遇见标题中的警告 原因： docker启动时指定--network=host或-net=host，如果还指定了-p映射端口，那这个时候就会有此警告， 并且通过-p设置的参数将不会起到任何作用，端口号会以主机端口号为主，重复时则递增。 解决: 解决的办法就是使用docker的其他网络模式，例如--network=bridge，这样就可以解决问题，或者直接无视\",\"若想不出现警告，正确的命令如下：\",\"docker run -d --network host --name tomcat83 billygoo/tomcat8-jdk8 \",\"[root@docker ~]## docker run -d --network host --name tomcat83 billygoo/tomcat8-jdk8 a4dd488fe6958bfe58c0064c7d9082bb86e37f6549592e6d0d7e26b30c222947 \",\"进入容器内部\",\"[root@docker ~]## docker inspect tomcat83 | tail -n 20 \\\"Networks\\\": { \\\"host\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"15bd0ffe79b02f0d6afc16e32b9c4246befedde12b2ea733f1f87f02ec24ad83\\\", \\\"EndpointID\\\": \\\"4f72c5950edc766e80d6dad4cd9c6798e0760e5eccd53dbab473e33718e3f765\\\", \\\"Gateway\\\": \\\"\\\", \\\"IPAddress\\\": \\\"\\\", \\\"IPPrefixLen\\\": 0, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker ~]## \",\"发现ip地址是空的，则是访问的宿主机的ip\"]},{\"header\":\"3、 none\",\"slug\":\"_3、none\",\"contents\":[\"在none模式下，并不为Docker容器进行任何网络配置。 也就是说，这个Docker容器没有网卡、IP、路由等信息，只有一个lo 需要我们自己为Docker容器添加网卡、配置IP等。\",\"禁用网络功能，只有lo标识(就是127.0.0.1表示本地回环)\",\"docker run -d -p 8084:8080 --network none --name tomcat84 billygoo/tomcat8-jdk8 \",\"[root@docker ~]## docker run -d -p 8084:8080 --network none --name tomcat84 billygoo/tomcat8-jdk8 944253c3400d267e524f4235d0fc424e4ec9e37790e28cabf66a87014f1ff76d [root@docker ~]## docker exec -it tomcat84 bash root@944253c3400d:/usr/local/tomcat## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever \",\"在外部容器查看\",\"[root@docker ~]## docker inspect tomcat84 | tail -n 20 \\\"Networks\\\": { \\\"none\\\": { \\\"IPAMConfig\\\": null, \\\"Links\\\": null, \\\"Aliases\\\": null, \\\"NetworkID\\\": \\\"28741f9b037cace317ec9b6dfad0609fb39b7dcc17e88e617298b6185d25301c\\\", \\\"EndpointID\\\": \\\"af0d382eee640ec36b20bd6109a9c9a5e46d4652904fd17253963751d4b41bd1\\\", \\\"Gateway\\\": \\\"\\\", \\\"IPAddress\\\": \\\"\\\", \\\"IPPrefixLen\\\": 0, \\\"IPv6Gateway\\\": \\\"\\\", \\\"GlobalIPv6Address\\\": \\\"\\\", \\\"GlobalIPv6PrefixLen\\\": 0, \\\"MacAddress\\\": \\\"\\\", \\\"DriverOpts\\\": null } } } } ] [root@docker ~]## \"]},{\"header\":\"4、 container\",\"slug\":\"_4、container\",\"contents\":[\"新建的容器和已经存在的一个容器共享一个网络ip配置而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。\"]},{\"header\":\"错误案例\",\"slug\":\"错误案例\",\"contents\":[\"[root@docker ~]## docker run -d -p 8085:8080 --name tomcat85 billygoo/tomcat8-jdk8 fb05b8d4dc8af6f4da53c755a599a5184c80f5ae26944280c6b880698832ed6a [root@docker ~]## docker run -d -p 8086:8080 --network container:tomcat85 --name tomcat86 billygoo/tomcat8-jdk8 docker: Error response from daemon: conflicting options: port publishing and the container type network mode. See 'docker run --help'. [root@docker ~]## \",\"相当于tomcat86和tomcat85公用同一个ip同一个端口，导致端口冲突 本案例用tomcat演示不合适。。。演示坑。。。。。。o(╥﹏╥)o\",\"换一个镜像给大家演示，\"]},{\"header\":\"正确案例\",\"slug\":\"正确案例\",\"contents\":[\"Alpine Linux 是一款独立的、非商业的通用 Linux 发行版，专为追求安全性、简单性和资源效率的用户而设计。 可能很多人没听说过这个 Linux 发行版本，但是经常用 Docker 的朋友可能都用过，因为他小，简单，安全而著称，所以作为基础镜像是非常好的一个选择，可谓是麻雀虽小但五脏俱全，镜像非常小巧，不到 6M的大小，所以特别适合容器打包。\",\"[root@docker ~]## docker run -it --name alpine1 alpine /bin/sh Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine 59bf1c3509f3: Pull complete Digest: sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300 Status: Downloaded newer image for alpine:latest / ## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 14: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / ## \",\"[root@docker ~]## docker run -it --network container:alpine1 --name alpine2 alpine /bin/sh / ## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 14: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / ## \",\"关闭alpine1，再看看alpine2\",\"[root@docker ~]## docker exec -it alpine2 /bin/sh / ## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever / ## \"]},{\"header\":\"5、 自定义网络\",\"slug\":\"_5、自定义网络\",\"contents\":[]},{\"header\":\"1、过时的 link\",\"slug\":\"_1、过时的link\",\"contents\":[\"官方文档：https://docs.docker.com/network/links/\"]},{\"header\":\"2、自定义网络之前\",\"slug\":\"_2、自定义网络之前\",\"contents\":[\"启动两个实例，然后互ping 对方的ip，发现可用ping通\",\"[root@docker ~]## docker exec -it tomcat81 bash root@91a8a3567d11:/usr/local/tomcat## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 8: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever root@91a8a3567d11:/usr/local/tomcat## ping 172.17.0.3 PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data. 64 bytes from 172.17.0.3: icmp_seq=1 ttl=64 time=76.2 ms 64 bytes from 172.17.0.3: icmp_seq=2 ttl=64 time=0.096 ms ^C --- 172.17.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 0.096/38.150/76.204/38.054 ms root@91a8a3567d11:/usr/local/tomcat## \",\"\\\\\",\"[root@docker ~]## docker exec -it tomcat82 bash root@1cf3a8ebb134:/usr/local/tomcat## ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 10: eth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever root@1cf3a8ebb134:/usr/local/tomcat## ping 172.17.0.2 PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data. 64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=9.62 ms 64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.139 ms 64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.139 ms 64 bytes from 172.17.0.2: icmp_seq=4 ttl=64 time=0.139 ms 64 bytes from 172.17.0.2: icmp_seq=5 ttl=64 time=0.145 ms ^Z [1]+ Stopped ping 172.17.0.2 root@1cf3a8ebb134:/usr/local/tomcat## \",\"两个实例互ping服务名\",\"root@91a8a3567d11:/usr/local/tomcat## ping tomcat82 ping: tomcat82: Name or service not known root@91a8a3567d11:/usr/local/tomcat## \"]},{\"header\":\"3、自定义网络之后\",\"slug\":\"_3、自定义网络之后\",\"contents\":[\"自定义桥接网络,自定义网络默认使用的是桥接网络bridge\",\"[root@docker ~]## docker network ls NETWORK ID NAME DRIVER SCOPE 718aa0f569ac bridge bridge local 15bd0ffe79b0 host host local 28741f9b037c none null local [root@docker ~]## docker network create xiaobear-network 490b2d2ac003088758a7b0fa6cb8840ffe4c34b935d0e5b0d683ee55b42256b5 [root@docker ~]## docker network ls NETWORK ID NAME DRIVER SCOPE 718aa0f569ac bridge bridge local 15bd0ffe79b0 host host local 28741f9b037c none null local 490b2d2ac003 xiaobear-network bridge local \",\"新建容器加入上一步新建的自定义网络\",\"docker run -d -p 8081:8080 --network xiaobear-network --name tomcat81 billygoo/tomcat8-jdk8 docker run -d -p 8082:8080 --network xiaobear-network --name tomcat82 billygoo/tomcat8-jdk8 \",\"互相ping测试\",\"[root@docker ~]## docker run -d -p 8081:8080 --network xiaobear-network --name tomcat81 billygoo/tomcat8-jdk8 d39d6f0c02b99566a12dbbe05f3f94e4bd63de4103e19b676fa81001bd33e378 [root@docker ~]## docker run -d -p 8082:8080 --network xiaobear-network --name tomcat82 billygoo/tomcat8-jdk8 8a84293be606ff235d9c24372703dc48db3da4e84903d0e85b7120e855082b5f [root@docker ~]## docker exec -it tomcat81 bash root@d39d6f0c02b9:/usr/local/tomcat## ping tomcat82 PING tomcat82 (172.18.0.3) 56(84) bytes of data. 64 bytes from tomcat82.xiaobear-network (172.18.0.3): icmp_seq=1 ttl=64 time=0.699 ms 64 bytes from tomcat82.xiaobear-network (172.18.0.3): icmp_seq=2 ttl=64 time=0.173 ms 64 bytes from tomcat82.xiaobear-network (172.18.0.3): icmp_seq=3 ttl=64 time=0.140 ms ^C --- tomcat82 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 0.140/0.337/0.699/0.256 ms root@d39d6f0c02b9:/usr/local/tomcat## \",\"总结：自定义网络本身就维护好了主机名和ip的对应关系（ip和域名都能通）\"]},{\"header\":\"5、Docker平台架构图解\",\"slug\":\"_5、docker平台架构图解\",\"contents\":[]},{\"header\":\"1、整体说明\",\"slug\":\"_1、整体说明\",\"contents\":[\"从其架构和运行流程来看，Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。\",\"Docker 运行的基本流程为：\",\"用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。\",\"Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。\",\"Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。\",\"Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graph driver将下载镜像以Graph的形式存储。\",\"当需要为 Docker 创建网络环境时，通过网络管理驱动 Network driver 创建并配置 Docker 容器网络环境。\",\"当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成。\",\"Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体对容器进行的操作。\"]}]},\"/study-tutorial/distributed/docker/portainer.html\":{\"title\":\"14、轻量级可视化工具Portainer\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Portainer 是一款轻量级的应用，它提供了图形化界面，用于方便地管理Docker环境，包括单机环境和集群环境。\",\"官网地址：https://www.portainer.io/\"]},{\"header\":\"1、安装\",\"slug\":\"_1、安装\",\"contents\":[]},{\"header\":\"1、docker命令安装\",\"slug\":\"_1、docker命令安装\",\"contents\":[\"首次没有安装，系统会自己下载镜像并运行\",\"docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer \"]},{\"header\":\"2、访问\",\"slug\":\"_2、访问\",\"contents\":[\"第一次登录需创建admin，访问地址：xxx.xxx.xxx.xxx:9000，然后设置用户名和密码（密码需8位）登录\"]},{\"header\":\"3、图形化界面，一顿操作\",\"slug\":\"_3、图形化界面-一顿操作\",\"contents\":[]}]},\"/study-tutorial/distributed/docker/publish_private_library.html\":{\"title\":\"6、本地镜像发布到私有库\",\"contents\":[{\"header\":\"1、本地镜像发布到私有库流程\",\"slug\":\"_1、本地镜像发布到私有库流程\",\"contents\":[]},{\"header\":\"2、什么是 Docker Registry\",\"slug\":\"_2、什么是docker-registry\",\"contents\":[\"官方Docker Hub地址：https://hub.docker.com/，中国大陆访问太慢了且准备被阿里云取代的趋势，不太主流。\",\"Dockerhub、阿里云这样的公共镜像仓库可能不太方便，涉及机密的公司不可能提供镜像给公网，所以需要创建一个本地私人仓库供给团队使用，基于公司内部项目构建镜像。\",\"Docker Registry是官方提供的工具，可以用于构建私有镜像仓库\"]},{\"header\":\"3、将本地镜像推送到私有库\",\"slug\":\"_3、将本地镜像推送到私有库\",\"contents\":[]},{\"header\":\"1、下载镜像Docker Registry\",\"slug\":\"_1、下载镜像docker-registry\",\"contents\":[\"docker pull registry \",\"[root@docker ~]## docker pull registry Using default tag: latest latest: Pulling from library/registry 79e9f2f55bf5: Pull complete 0d96da54f60b: Pull complete 5b27040df4a2: Pull complete e2ead8259a04: Pull complete 3790aef225b9: Pull complete Digest: sha256:169211e20e2f2d5d115674681eb79d21a217b296b43374b8e39f97fcf866b375 Status: Downloaded newer image for registry:latest docker.io/library/registry:latest [root@docker ~]## docker ../../images REPOSITORY TAG IMAGE ID CREATED SIZE centos1 1.0 08f93c122271 5 days ago 298MB registry.cn-hangzhou.aliyuncs.com/xiaobear/xiaobear-dcoker 1.0 12e7a58fc36a 5 days ago 231MB redis latest 7614ae9453d1 3 months ago 113MB registry latest b8604a3fe854 5 months ago 26.2MB hello-world latest feb5d9fea6a5 6 months ago 13.3kB centos latest 5d0da3dc9764 7 months ago 231MB [root@docker ~]## \"]},{\"header\":\"2、运行私有库Registry，相当于本地有个私有Docker hub\",\"slug\":\"_2、运行私有库registry-相当于本地有个私有docker-hub\",\"contents\":[\"默认情况，仓库被创建在容器的/var/lib/registry目录下，建议自行用容器卷映射，方便于宿主机联调\",\"docker run -d -p 5000:5000 -v /zzyyuse/myregistry/:/tmp/registry --privileged=true registry \",\"-p：指定端口\",\"-P：随机端口\"]},{\"header\":\"3、案例演示创建一个新镜像，centos安装ifconfig命令\",\"slug\":\"_3、案例演示创建一个新镜像-centos安装ifconfig命令\",\"contents\":[]},{\"header\":\"1、从Hub上下载ubuntu镜像到本地并成功运行\",\"slug\":\"_1、从hub上下载ubuntu镜像到本地并成功运行\",\"contents\":[\"docker pull centos docker run -it centos /bin/bash [root@docker ~]## docker run -it centos /bin/bash [root@76d96b17087a /]## ifconfig bash: ifconfig: command not found \"]},{\"header\":\"2、外网连通，安装redis\",\"slug\":\"_2、外网连通-安装redis\",\"contents\":[\"yum install -y redis \"]},{\"header\":\"3、安装完成，提交镜像\",\"slug\":\"_3、安装完成-提交镜像\",\"contents\":[\"docker commit -m=\\\"提交的描述信息\\\" -a=\\\"作者\\\" 容器ID 要创建的目标镜像名:[标签名] 命令：在容器外执行，记得\",\"docker commit -m=\\\"add test\\\" -a=\\\"xiaobear\\\" 846cde73b2fd centos1:1.0 \"]},{\"header\":\"4、验证私服上镜像\",\"slug\":\"_4、验证私服上镜像\",\"contents\":[\"curl -XGET http://[ip]:5000/v2/_catalog \",\"[root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a35006a5eb6e registry \\\"/entrypoint.sh /etc…\\\" 13 seconds ago Up 7 seconds 0.0.0.0:5000->5000/tcp, :::5000->5000/tcp tender_buck 846cde73b2fd centos \\\"/bin/bash\\\" 7 minutes ago Up 7 minutes stupefied_wescoff [root@docker ~]## curl -XGET http://192.168.130.132:5000/v2/_catalog {\\\"repositories\\\":[]} [root@docker ~]## \"]},{\"header\":\"5、将新镜像centos1:1.0修改符合私服规范的Tag\",\"slug\":\"_5、将新镜像centos1-1-0修改符合私服规范的tag\",\"contents\":[\"公式： docker tag 镜像:Tag Host:Port/Repository:Tag\",\"[root@docker ~]## docker tag centos1:1.0 192.168.130.132:5000/centos1:1.0 [root@docker ~]## docker ../../images REPOSITORY TAG IMAGE ID CREATED SIZE 192.168.130.132:5000/centos1 1.0 39cad16c0525 32 minutes ago 231MB centos1 1.0 39cad16c0525 32 minutes ago 231MB registry.cn-hangzhou.aliyuncs.com/xiaobear/xiaobear-dcoker 1.0 12e7a58fc36a 6 days ago 231MB redis latest 7614ae9453d1 3 months ago 113MB registry latest b8604a3fe854 5 months ago 26.2MB hello-world latest feb5d9fea6a5 6 months ago 13.3kB centos latest 5d0da3dc9764 7 months ago 231MB [root@docker ~]## \"]},{\"header\":\"6、修改配置文件，支持http\",\"slug\":\"_6、修改配置文件-支持http\",\"contents\":[\"docker默认不允许http方式推送镜像，通过配置选项来取消这个限制。\"]},{\"header\":\"1、查看配置文件\",\"slug\":\"_1、查看配置文件\",\"contents\":[\"cat /etc/docker/daemon.json \"]},{\"header\":\"2、修改配置文件\",\"slug\":\"_2、修改配置文件\",\"contents\":[\"vim /etc/docker/daemon.json \",\"{ \\\"registry-mirrors\\\": [\\\"https://cmquykjm.mirror.aliyuncs.com\\\"], \\\"insecure-registries\\\": [\\\"192.168.140.132:5000\\\"] } \",\"[\\\"ip:端口\\\"]:\",\"ip：自己启动服务器的ip\",\"端口：映射的端口\"]},{\"header\":\"3、修改之后查看配置并重新启动\",\"slug\":\"_3、修改之后查看配置并重新启动\",\"contents\":[\"PS：重新启动端口之后，需启动私有仓库\"]},{\"header\":\"7、push到私有仓库\",\"slug\":\"_7、push到私有仓库\",\"contents\":[\"[root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e36d4066ba00 registry \\\"/entrypoint.sh /etc…\\\" 5 seconds ago Up 5 seconds 0.0.0.0:5000->5000/tcp, :::5000->5000/tcp ecstatic_goldstine [root@docker ~]## docker push 192.168.130.132:5000/centos1:1.0 The push refers to repository [192.168.130.132:5000/centos1] 74ddd0ec08fa: Pushed 1.0: digest: sha256:df7b98170728e2ae419520239adf7862c15a246a58115ba91ee5ac10dfb7fdb0 size: 529 [root@docker ~]## \"]},{\"header\":\"8、验证私服镜像\",\"slug\":\"_8、验证私服镜像\",\"contents\":[\"curl -XGET http://192.168.130.132:5000/v2/_catalog \",\"[root@docker ~]## curl -XGET http://192.168.130.132:5000/v2/_catalog {\\\"repositories\\\":[\\\"centos1\\\"]} [root@docker ~]## \"]},{\"header\":\"9、pull到本地并运行\",\"slug\":\"_9、pull到本地并运行\",\"contents\":[\"[root@docker ~]## docker pull 192.168.130.132:5000/centos1:1.0 1.0: Pulling from centos1 a1d0c7532777: Already exists Digest: sha256:df7b98170728e2ae419520239adf7862c15a246a58115ba91ee5ac10dfb7fdb0 Status: Downloaded newer image for 192.168.130.132:5000/centos1:1.0 192.168.130.132:5000/centos1:1.0 [root@docker ~]## \"]}]},\"/study-tutorial/distributed/docker/regular_install.html\":{\"title\":\"8、Docker常规安装\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"步骤：\",\"搜索镜像\",\"拉取镜像\",\"查看镜像\",\"启动镜像\",\"停止容器\",\"移除容器\"]},{\"header\":\"1、安装tomcat\",\"slug\":\"_1、安装tomcat\",\"contents\":[]},{\"header\":\"1、搜索镜像\",\"slug\":\"_1、搜索镜像\",\"contents\":[\"docker search tomcat \"]},{\"header\":\"2、拉取镜像\",\"slug\":\"_2、拉取镜像\",\"contents\":[\"docker pull tomcat \"]},{\"header\":\"3、查看镜像\",\"slug\":\"_3、查看镜像\",\"contents\":[\"docker ../../images \"]},{\"header\":\"4、创建容器示例\",\"slug\":\"_4、创建容器示例\",\"contents\":[\"docker run -it -p 8080:8080 tomcat \",\"-p 小写，主机端口:docker容器端口\",\"-P 大写，随机分配端口\",\"i:交互\",\"t:终端\",\"d:后台\",\"PS：宿主机8080映射到容器内的8080端口\"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"访问http://[ip]:8080/\"]},{\"header\":\"解决\",\"slug\":\"解决\",\"contents\":[\"可能没有映射端口或者没有关闭防火墙\",\"查看tomcat文件，把webapps.dist目录换成webapps\",\"先后台启动tomcat\",\"docker run -d -p 8080:8080 tomcat \",\"进入容器内部\",\"docker exec -it tomcat /bin/bash \",\"查看文件目录，webapps.dist目录换成webapps\",\"root@90c2f8c553b0:/usr/local/tomcat## ls BUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.dist CONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work root@90c2f8c553b0:/usr/local/tomcat## rm -f webapps rm: cannot remove 'webapps': Is a directory root@90c2f8c553b0:/usr/local/tomcat## rm -r webapps root@90c2f8c553b0:/usr/local/tomcat## mv webapps.dist webapps root@90c2f8c553b0:/usr/local/tomcat## ls BUILDING.txt LICENSE README.md RUNNING.txt conf logs temp work CONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps root@90c2f8c553b0:/usr/local/tomcat## \",\"重新访问：http://[ip]:8080/\"]},{\"header\":\"5、免修改版\",\"slug\":\"_5、免修改版\",\"contents\":[]},{\"header\":\"拉取镜像\",\"slug\":\"拉取镜像\",\"contents\":[\"docker pull billygoo/tomcat8-jdk8 \"]},{\"header\":\"创建容器\",\"slug\":\"创建容器\",\"contents\":[\"docker run -d -p 8080:8080 --name mytomcat8 billygoo/tomcat8-jdk8 \"]},{\"header\":\"2、安装MySQL\",\"slug\":\"_2、安装mysql\",\"contents\":[]},{\"header\":\"1、搜索镜像\",\"slug\":\"_1、搜索镜像-1\",\"contents\":[\"docker search mysql \"]},{\"header\":\"2、拉取指定版本镜像\",\"slug\":\"_2、拉取指定版本镜像\",\"contents\":[\"docker pull mysql:5.7 \"]},{\"header\":\"3、创建容器\",\"slug\":\"_3、创建容器\",\"contents\":[\"查看镜像省略\",\"docker run -itd --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 \",\"-p 3306:3306 ：映射容器服务的 3306 端口到宿主机的 3306 端口，外部主机可以直接通过 宿主机ip:3306 访问到 MySQL 的服务。\",\"MYSQL_ROOT_PASSWORD=123456：设置 MySQL 服务 root 用户的密码。\"]},{\"header\":\"4、使用\",\"slug\":\"_4、使用\",\"contents\":[]},{\"header\":\"1、简单版\",\"slug\":\"_1、简单版\",\"contents\":[]},{\"header\":\"1、使用MySQL镜像\",\"slug\":\"_1、使用mysql镜像\",\"contents\":[\"docker exec -it a6b1fc904bad /bin/bash \",\"[root@docker myHostData]## docker exec -it a6b1fc904bad /bin/bash root@a6b1fc904bad:/## mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\\\g. Your MySQL connection id is 2 Server version: 5.7.36 MySQL Community Server (GPL) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\\\h' for help. Type '\\\\c' to clear the current input statement. mysql> \"]},{\"header\":\"2、建库建表\",\"slug\":\"_2、建库建表\",\"contents\":[\"mysql> create database db1; Query OK, 1 row affected (0.01 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) mysql> use db1; Database changed mysql> create table test01(id int, name varchar(20)); Query OK, 0 rows affected (0.05 sec) mysql> insert into test01 values(1, 'xiaobear'); Query OK, 1 row affected (0.45 sec) mysql> select * from test01; +------+----------+ | id | name | +------+----------+ | 1 | xiaobear | +------+----------+ 1 row in set (0.00 sec) mysql> \"]},{\"header\":\"3、远程连接docker中的MySQL\",\"slug\":\"_3、远程连接docker中的mysql\",\"contents\":[]},{\"header\":\"4、插入中文数据\",\"slug\":\"_4、插入中文数据\",\"contents\":[\"**原因：**docker上默认字符集编码隐患\",\"mysql> show variables like 'character%'; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | latin1 | | character_set_connection | latin1 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | latin1 | | character_set_server | latin1 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 8 rows in set (0.01 sec) mysql> \"]},{\"header\":\"2、实战版\",\"slug\":\"_2、实战版\",\"contents\":[]},{\"header\":\"1、创建容器\",\"slug\":\"_1、创建容器\",\"contents\":[\"docker run -d -p 3306:3306 --privileged=true -v /xiaobear/mysql/log:/var/log/mysql -v /xiaobear/mysql/data:/var/lib/mysql -v /xiaobear/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7 \"]},{\"header\":\"2、创建配置文件\",\"slug\":\"_2、创建配置文件\",\"contents\":[\"[root@docker ~]## docker run -d -p 3306:3306 --privileged=true -v /xiaobear/mysql/log:/var/log/mysql -v /xiaobear/mysql/data:/var/lib/mysql -v /xiaobear/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7 9b08a40ccc0668416dbe8abc744cf2878940203f0e4fa3cb3a46c0179ac2244b [root@docker ~]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9b08a40ccc06 mysql:5.7 \\\"docker-entrypoint.s…\\\" 7 seconds ago Up 7 seconds 0.0.0.0:3306->3306/tcp, :::3306->3306/tcp, 33060/tcp mysql [root@docker ~]## cd /xiaobear/mysql/conf/ [root@docker conf]## ls [root@docker conf]## vim my.cnf [root@docker conf]## \",\"配置文件内容\",\"[client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake skip-name-resolve \"]},{\"header\":\"3、重新启动查看编码\",\"slug\":\"_3、重新启动查看编码\",\"contents\":[\"docker restart mysql docker exec -it mysql /bin/bash mysql -u root -p #show variables like 'character%'; \"]},{\"header\":\"4、中文测试\",\"slug\":\"_4、中文测试\",\"contents\":[]},{\"header\":\"5、总结\",\"slug\":\"_5、总结\",\"contents\":[\"docker安装完MySQL并run出容器后，建议请先修改完字符集编码后再新建mysql库-表-插数据\"]},{\"header\":\"3、安装redis\",\"slug\":\"_3、安装redis\",\"contents\":[]},{\"header\":\"1、拉取镜像\",\"slug\":\"_1、拉取镜像\",\"contents\":[\"docker pull redis \"]},{\"header\":\"2、创建容器\",\"slug\":\"_2、创建容器\",\"contents\":[\"docker run -d -p 6739:6739 redis \"]},{\"header\":\"3、进入容器\",\"slug\":\"_3、进入容器\",\"contents\":[\"docker exec -it 3f85f4351992 /bin/bash \"]},{\"header\":\"4、在宿主机创建目录\",\"slug\":\"_4、在宿主机创建目录\",\"contents\":[\"mkdir -p /app/redis/ \"]},{\"header\":\"5、复制一份redis.conf到该目录下,修改\",\"slug\":\"_5、复制一份redis-conf到该目录下-修改\",\"contents\":[\"/app/redis目录下修改redis.conf文件 3.1 开启redis验证 可选 requirepass 123\",\"3.2 允许redis外地连接 必须 注释掉 ## bind 127.0.0.1\",\"3.3 daemonize no 将daemonize yes注释起来或者 daemonize no设置，因为该配置和docker run中-d参数冲突，会导致容器一直启动失败\",\"3.4 开启redis数据持久化 appendonly yes 可选\"]},{\"header\":\"6、使用redis镜像创建容器(也叫运行镜像)\",\"slug\":\"_6、使用redis镜像创建容器-也叫运行镜像\",\"contents\":[\"docker run -p 6379:6379 --name myredis --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis redis-server /etc/redis/redis.conf \",\"[root@docker redis]## docker run -p 6379:6379 --name myredis --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis redis-server /etc/redis/redis.conf 50c5ce6ccb609cb8d4e3da1d40f51e5148c57b44f821ab55483ffe1aae689a31 [root@docker redis]## docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 50c5ce6ccb60 redis \\\"docker-entrypoint.s…\\\" 8 seconds ago Up 6 seconds 0.0.0.0:6379->6379/tcp, :::6379->6379/tcp myredis 9b08a40ccc06 mysql:5.7 \\\"docker-entrypoint.s…\\\" 2 days ago Up 2 days 0.0.0.0:3306->3306/tcp, :::3306->3306/tcp, 33060/tcp mysql [root@docker redis]## \"]},{\"header\":\"7、测试连接\",\"slug\":\"_7、测试连接\",\"contents\":[\"[root@docker redis]## docker exec -it myredis /bin/bash root@50c5ce6ccb60:/data## redis-cli 127.0.0.1:6379> set k hello OK 127.0.0.1:6379> get k \\\"hello\\\" 127.0.0.1:6379> \",\"docker exec -it 运行着Rediis服务的容器ID redis-cli\"]}]},\"/study-tutorial/distributed/docker/upload.html\":{\"title\":\"5、本地镜像上传至阿里云\",\"contents\":[{\"header\":\"1、镜像的生成方法\",\"slug\":\"_1、镜像的生成方法\",\"contents\":[\"基于当前容器创建一个新的镜像，新功能增强\",\"docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] \",\"DockerFile\"]},{\"header\":\"2、本地推送到阿里云\",\"slug\":\"_2、本地推送到阿里云\",\"contents\":[]},{\"header\":\"1、阿里云开发者平台\",\"slug\":\"_1、阿里云开发者平台\",\"contents\":[\"https://promotion.aliyun.com/ntms/act/kubernetes.html\"]},{\"header\":\"2、创建仓库镜像\",\"slug\":\"_2、创建仓库镜像\",\"contents\":[]},{\"header\":\"1、选择控制台，进入容器镜像服务\",\"slug\":\"_1、选择控制台-进入容器镜像服务\",\"contents\":[]},{\"header\":\"2、选择个人实例\",\"slug\":\"_2、选择个人实例\",\"contents\":[]},{\"header\":\"3、命名空间\",\"slug\":\"_3、命名空间\",\"contents\":[\"创建命名空间\"]},{\"header\":\"4、仓库名称\",\"slug\":\"_4、仓库名称\",\"contents\":[]},{\"header\":\"5、进入管理界面获得脚本\",\"slug\":\"_5、进入管理界面获得脚本\",\"contents\":[]},{\"header\":\"3、将镜像推送到阿里云\",\"slug\":\"_3、将镜像推送到阿里云\",\"contents\":[\"将镜像推送到阿里云registry\",\"管理界面脚本\",\"将镜像推送到Registry $ docker login --username=[阿里云账号] registry.cn-hangzhou.aliyuncs.com $ docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/xiaobear/xiaobear-dcoker:[镜像版本号] $ docker push registry.cn-hangzhou.aliyuncs.com/xiaobear/xiaobear-dcoker:[镜像版本号] \"]},{\"header\":\"3、将阿里云上的镜像下载到本地\",\"slug\":\"_3、将阿里云上的镜像下载到本地\",\"contents\":[]},{\"header\":\"从Registry中拉取镜像\",\"slug\":\"从registry中拉取镜像\",\"contents\":[\" docker pull registry.cn-hangzhou.aliyuncs.com/xiaobear/xiaobear-dcoker:[镜像版本号] \"]}]},\"/study-tutorial/distributed/rabbitmq/advanced.html\":{\"title\":\"8、发布确认高级\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在生产环境中由于一些不明原因，导致 rabbitmq 重启，在 RabbitMQ 重启期间生产者消息投递失败，导致消息丢失，需要手动处理和恢复。于是，我们开始思考，如何才能进行 RabbitMQ 的消息可靠投递呢？特别是在这样比较极端的情况，RabbitMQ 集群不可用的时候，无法投递的消息该如何处理呢:\"]},{\"header\":\"1、发布确认 springboot 版本\",\"slug\":\"_1、发布确认-springboot-版本\",\"contents\":[]},{\"header\":\"1、确认机制方案\",\"slug\":\"_1、确认机制方案\",\"contents\":[\"回顾之前学习的发布确认，只是简单的确认消息发出接收，并没有考虑交换机和队列出现问题的情况，如下图所示，若交换机或队列宕机/rabbitmq重启，导致消息传输过程中未被消费者消费，但是又不知道谁未被消费\"]},{\"header\":\"2、代码架构图\",\"slug\":\"_2、代码架构图\",\"contents\":[\"新建一个确认的直接交换机和确认队列，然后进行绑定，进行案例测试\"]},{\"header\":\"3、修改rabbitMq的配置文件\",\"slug\":\"_3、修改rabbitmq的配置文件\",\"contents\":[\"在配置文件中添加spring.rabbitmq.publisher-confirm-type=correlated\",\"spring.rabbitmq.publisher-confirm-type有三种选择：\",\"NONE：禁用发布确认模式，是默认值\",\"CORRELATED：发布消息成功到交换器后会触发回调方法\",\"SIMPLE：存在两种效果 \",\"一是和 CORRELATED 值一样会触发回调方法\",\"二在发布消息成功后使用 rabbitTemplate 调用 waitForConfirms 或 waitForConfirmsOrDie 方法等待 broker 节点返回发送结果，根据返回结果来判定下一步的逻辑，要注意的点是waitForConfirmsOrDie 方法如果返回 false 则会关闭 channel，则接下来无法发送消息到 broker\",\"配置文件内容如下：\",\"spring: rabbitmq: host: 192.168.130.134 port: 5672 username: admin password: admin123 #发布消息成功到交换器后会触发回调方法 publisher-confirm-type: correlated \"]},{\"header\":\"4、添加配置类\",\"slug\":\"_4、添加配置类\",\"contents\":[\"@Configuration public class ConfirmConfig { public static final String CONFIRM_EXCHANGE_NAME = \\\"confirm.exchange\\\"; public static final String CONFIRM_QUEUE_NAME = \\\"confirm.queue\\\"; public static final String ROUTING_KEY = \\\"key1\\\"; /** * 声明队列 * @return */ @Bean public Queue queue(){ return QueueBuilder.durable(CONFIRM_QUEUE_NAME).build(); } /** * 声明交换机 * @return */ @Bean public DirectExchange directExchange(){ return new DirectExchange(CONFIRM_EXCHANGE_NAME); } /** * 绑定交换机与队列 * @param queue * @param exchange * @return */ @Bean public Binding queueBinding(@Qualifier(\\\"queue\\\") Queue queue, @Qualifier(\\\"directExchange\\\") DirectExchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(ROUTING_KEY); } } \"]},{\"header\":\"5、消息生产者\",\"slug\":\"_5、消息生产者\",\"contents\":[\"@Slf4j @RestController @RequestMapping(\\\"/confirm\\\") public class ProducerController { @Resource private RabbitTemplate rabbitTemplate; @GetMapping(\\\"/send/{msg}\\\") public void send(@PathVariable String msg){ CorrelationData correlationData = new CorrelationData(\\\"key\\\"); rabbitTemplate.convertAndSend(ConfirmConfig.CONFIRM_EXCHANGE_NAME, ConfirmConfig.ROUTING_KEY, msg, correlationData); log.info(\\\"发送消息内容:{}\\\",msg); } } \"]},{\"header\":\"6、消息消费者\",\"slug\":\"_6、消息消费者\",\"contents\":[\"public class ConfirmConsumer { /** logger */ private static final Logger log = LoggerFactory.getLogger(ConfirmConsumer.class); @RabbitListener(queues = \\\"confirm.queue\\\") public void receive(Message message){ String msg=new String(message.getBody()); log.info(\\\"接受到队列 confirm.queue 消息:{}\\\",msg); } } \"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"访问： http://localhost:8080/confirm/send/xixi\"]},{\"header\":\"7、加入确认消息的回调接口\",\"slug\":\"_7、加入确认消息的回调接口\",\"contents\":[\"@Slf4j @Component public class MyConfirmCallBack implements RabbitTemplate.ConfirmCallback { @Resource private RabbitTemplate rabbitTemplate; @PostConstruct public void init(){ //将MyConfirmCallBack 注入到 rabbitTemplate rabbitTemplate.setConfirmCallback(this); } /** * 交换机不管是否收到消息的一个回调方法 * CorrelationData 消息相关数据 * ack 交换机是否收到消息 * */ @Override public void confirm(CorrelationData correlationData, boolean b, String s) { //是否接受到消息 String id = correlationData != null ? correlationData.getId():\\\"\\\"; if(b){ log.info(\\\"交换机已经收到 id 为:{}的消息\\\",id); }else{ log.info(\\\"交换机还未收到 id 为:{}消息,由于原因:{}\\\", id, s); } } } \",\"效果：http://localhost:8080/confirm/send/xixi\"]},{\"header\":\"8、模拟场景，消息发送，消费者未接收到消息\",\"slug\":\"_8、模拟场景-消息发送-消费者未接收到消息\",\"contents\":[]},{\"header\":\"1、更改生产者\",\"slug\":\"_1、更改生产者\",\"contents\":[\"消费者绑定队列的ROUTING_KEY为key1，生产者通过key2发送的，消费者是接收不到的\",\"@Slf4j @RestController @RequestMapping(\\\"/confirm\\\") public class ProducerController { @Resource private RabbitTemplate rabbitTemplate; @GetMapping(\\\"/send/{msg}\\\") public void send(@PathVariable String msg){ CorrelationData correlationData1 = new CorrelationData(\\\"key1\\\"); rabbitTemplate.convertAndSend(ConfirmConfig.CONFIRM_EXCHANGE_NAME, ConfirmConfig.ROUTING_KEY, msg + \\\"key1\\\", correlationData1); log.info(\\\"发送消息内容:{}\\\",msg + \\\"key1\\\"); CorrelationData correlationData2 = new CorrelationData(\\\"key2\\\"); rabbitTemplate.convertAndSend(ConfirmConfig.CONFIRM_EXCHANGE_NAME, \\\"key2\\\", msg + \\\"key2\\\", correlationData2); log.info(\\\"发送消息内容:{}\\\",msg + \\\"key2\\\"); } } \",\"可以看到，发送了两条消息，第一条消息的 RoutingKey 为 \\\"key1\\\"，第二条消息的 RoutingKey 为\\\"key2\\\"，两条消息都成功被交换机接收，也收到了交换机的确认回调，但消费者只收到了一条消息，因为第二条消息的 RoutingKey 与队列的 BindingKey 不一致，也没有其它队列能接收这个消息，所有第二条消息被直接丢弃了。\"]},{\"header\":\"2、回退消息\",\"slug\":\"_2、回退消息\",\"contents\":[]},{\"header\":\"1、Mandatory 参数\",\"slug\":\"_1、mandatory-参数\",\"contents\":[\"在仅开启了生产者确认机制的情况下，交换机接收到消息后，会直接给消息生产者发送确认消息，如果发现该消息不可路由，那么消息会被直接丢弃，此时生产者是不知道消息被丢弃这个事件的。那么如何让无法被路由的消息帮我想办法处理一下？最起码通知我一声，我好自己处理啊。通过设置 mandatory 参数可以在当消息传递过程中不可达目的地时将消息返回给生产者。\"]},{\"header\":\"2、修改回调函数\",\"slug\":\"_2、修改回调函数\",\"contents\":[\"@Slf4j @Component public class MyConfirmCallBack implements RabbitTemplate.ConfirmCallback, RabbitTemplate.ReturnsCallback { @Resource private RabbitTemplate rabbitTemplate; @PostConstruct public void init(){ //将MyConfirmCallBack 注入到 rabbitTemplate rabbitTemplate.setConfirmCallback(this); /** * true:交换机无法将消息进行路由时，会将该消息返回给生产者 * false：如果发现消息无法进行路由，则直接丢弃 */ rabbitTemplate.setMandatory(true); rabbitTemplate.setReturnsCallback(this); } /** * 交换机不管是否收到消息的一个回调方法 * CorrelationData 消息相关数据 * ack 交换机是否收到消息 * */ @Override public void confirm(CorrelationData correlationData, boolean b, String s) { //是否接受到消息 String id = correlationData != null ? correlationData.getId():\\\"\\\"; if(b){ log.info(\\\"交换机已经收到 id 为:{}的消息\\\",id); }else{ log.info(\\\"交换机还未收到 id 为:{}消息,由于原因:{}\\\", id, s); } } /** * 回退消息回调函数 * @param returnedMessage */ @Override public void returnedMessage(ReturnedMessage returnedMessage) { log.info(\\\"消息:{}被服务器退回，退回原因:{}, 交换机是:{}, 路由 key:{}\\\", new String(returnedMessage.getMessage().getBody()), returnedMessage.getReplyText(), returnedMessage.getExchange(), returnedMessage.getRoutingKey()); } } \",\"测试结果：http://localhost:8080/confirm/send/xixi\"]},{\"header\":\"3、备份交换机\",\"slug\":\"_3、备份交换机\",\"contents\":[\"有了 mandatory 参数和回退消息，我们获得了对无法投递消息的感知能力，有机会在生产者的消息无法被投递时发现并处理。但有时候，我们并不知道该如何处理这些无法路由的消息，最多打个日志，然后触发报警，再来手动处理。而通过日志来处理这些无法路由的消息是很不优雅的做法，特别是当生产者所在的服务有多台机器的时候，手动复制日志会更加麻烦而且容易出错。而且设置 mandatory 参数会增加生产者的复杂性，需要添加处理这些被退回的消息的逻辑。如果既不想丢失消息，又不想增加生产者的复杂性，该怎么做呢？\",\"前面在设置死信队列的文章中，我们提到，可以为队列设置死信交换机来存储那些处理失败的消息，可是这些不可路由消息根本没有机会进入到队列，因此无法使用死信队列来保存消息。在 RabbitMQ 中，有一种备份交换机的机制存在，可以很好的应对这个问题。\",\"什么是备份交换机呢？备份交换机可以理解为 RabbitMQ 中交换机的“备胎”，当我们为某一个交换机声明一个对应的备份交换机时，\",\"就是为它创建一个备胎，当交换机接收到一条不可路由消息时，将会把这条消息转发到备份交换机中，由备份交换机来进行转发和处理，通常备份交换机的类型为 Fanout ，这样就能把所有消息都投递到与其绑定的队列中，然后我们在备份交换机下绑定一个队列，这样所有那些原交换机无法被路由的消息，就会都进入这个队列了。当然，我们还可以建立一个报警队列，用独立的消费者来进行监测和报警。\"]},{\"header\":\"1、代码架构图\",\"slug\":\"_1、代码架构图\",\"contents\":[\"将无法接收的消息发送给备份交换机\"]},{\"header\":\"2、修改配置类\",\"slug\":\"_2、修改配置类\",\"contents\":[\"@Configuration public class ConfirmConfig { public static final String CONFIRM_EXCHANGE_NAME = \\\"confirm.exchange\\\"; public static final String CONFIRM_QUEUE_NAME = \\\"confirm.queue\\\"; public static final String ROUTING_KEY = \\\"key1\\\"; public static final String BACKUP_EXCHANGE_NAME = \\\"backup.exchange\\\"; public static final String BACKUP_QUEUE_NAME = \\\"backup.queue\\\"; public static final String WARNING_QUEUE_NAME = \\\"warning.queue\\\"; /** * 声明队列 * @return */ @Bean public Queue queue(){ return QueueBuilder.durable(CONFIRM_QUEUE_NAME).build(); } /** * 声明交换机 * @return */ @Bean public DirectExchange directExchange(){ ExchangeBuilder exchangeBuilder = ExchangeBuilder.directExchange(CONFIRM_EXCHANGE_NAME) .durable(true) //设置该交换机的备份交换机 .withArgument(\\\"alternate-exchange\\\", BACKUP_EXCHANGE_NAME); return exchangeBuilder.build(); } /** * 绑定交换机与队列 * @param queue * @param exchange * @return */ @Bean public Binding queueBinding(@Qualifier(\\\"queue\\\") Queue queue, @Qualifier(\\\"directExchange\\\") DirectExchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(ROUTING_KEY); } /** * 备份队列 * @return */ @Bean public Queue backupQueue(){ return QueueBuilder.durable(BACKUP_QUEUE_NAME).build(); } /** * 告警队列 * @return */ @Bean public Queue warningQueue(){ return QueueBuilder.durable(WARNING_QUEUE_NAME).build(); } /** * 声明备份交换机 * @return */ @Bean public FanoutExchange backupExchange(){ return new FanoutExchange(BACKUP_EXCHANGE_NAME); } /** * 绑定备份交换机和备份队列 * @param queue * @param fanoutExchange * @return */ @Bean public Binding bindingBackUp(@Qualifier(\\\"backupQueue\\\") Queue queue, @Qualifier(\\\"backupExchange\\\") FanoutExchange fanoutExchange){ return BindingBuilder.bind(queue).to(fanoutExchange); } /** * 绑定备份交换机和备份队列 * @param queue * @param fanoutExchange * @return */ @Bean public Binding bindingWarning(@Qualifier(\\\"warningQueue\\\") Queue queue, @Qualifier(\\\"backupExchange\\\") FanoutExchange fanoutExchange){ return BindingBuilder.bind(queue).to(fanoutExchange); } } \"]},{\"header\":\"3、告警消费者\",\"slug\":\"_3、告警消费者\",\"contents\":[\"@Slf4j @Component public class WarningConsumer { public static final String WARNING_QUEUE_NAME = \\\"warning.queue\\\"; @RabbitListener(queues = WARNING_QUEUE_NAME) public void receiveWarningMsg(Message message){ String msg = new String(message.getBody()); log.error(\\\"报警发现不可路由消息：{}\\\", msg); } } \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"PS：重新启动项目的时候需要把原来的 confirm.exchange 删除因为我们修改了其绑定属性，不然会报错\",\"浏览器访问：http://localhost:8080/confirm/send/xixi\"]}]},\"/study-tutorial/distributed/rabbitmq/dead_queue.html\":{\"title\":\"6、死信队列\",\"contents\":[{\"header\":\"1、概念\",\"slug\":\"_1、概念\",\"contents\":[\"先从概念解释上搞清楚这个定义，死信，顾名思义就是无法被消费的消息，字面意思可以这样理解，一般来说，producer 将消息投递到 broker 或者直接到 queue 里了，consumer 从 queue 取出消息进行消费，但某些时候由于特定的原因导致queue中的某些消息无法被消费，这样的消息如果没有后续的处理，就变成了死信，有死信自然就有了死信队列。\",\"**应用场景：**为了保证订单业务的消息数据不丢失，需要使用到 RabbitMQ 的死信队列机制，当消息消费发生异常时，将消息投入死信队列中；还有比如说: 用户在商城下单成功并点击去支付后在指定时间未支付时自动失效\"]},{\"header\":\"2、死信的来源\",\"slug\":\"_2、死信的来源\",\"contents\":[\"消息 TTL 过期\",\"队列达到最大长度(队列满了，无法再添加数据到 mq 中)\",\"消息被拒绝(basic.reject 或 basic.nack)并且 requeue=false.\"]},{\"header\":\"3、实战\",\"slug\":\"_3、实战\",\"contents\":[]},{\"header\":\"1、代码架构图\",\"slug\":\"_1、代码架构图\",\"contents\":[\"流程说明：\",\"一个生产者，两个消费者，交换机为直接类型\",\"正常流程：生产者发送消息，经过正常队列后，由 C1 进行消费\",\"产生三大问题后的流程：生产者发送消息，经过正常队列，由于出现了三大问题，C1没有接收到消息，不能进行消费，需转发到死信交换机中的队列里，由C2进行消费。\",\"实现说明：\",\"由于C1未进行消费，需转发到C2，所以C1消费者需创建两个交换机和队列\",\"C2只需进行正常消费死信队列的消息即可\"]},{\"header\":\"2、消息 TTL 过期\",\"slug\":\"_2、消息-ttl-过期\",\"contents\":[]},{\"header\":\"1、场景模拟\",\"slug\":\"_1、场景模拟\",\"contents\":[\"先启动消费者C1，创建好交换机和队列\",\"关闭C1，然后启动生产者和消费者C2\",\"因C1被关闭，所以生产者发送的消息无法被消费，过了时间后，会进入死信队列，被C2进行消费\"]},{\"header\":\"2、消费者01\",\"slug\":\"_2、消费者01\",\"contents\":[\"public class Consumer01 { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //正常队列 public static final String NORMAL_QUEUE = \\\"normal_queue\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT); channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //正常队列绑定死信队列信息 Map<String, Object> params = new HashMap<>(); //设置过期时间 // params.put(\\\"x-message-ttl\\\", 10000); //正常队列设置死信交换机 参数 key 是固定值 params.put(\\\"x-dead-letter-exchange\\\", DEAD_EXCHANGE); //正常队列设置死信 routing-key 参数 key 是固定值 params.put(\\\"x-dead-letter-routing-key\\\", \\\"dead\\\"); channel.queueDeclare(NORMAL_QUEUE, false, false, false, params); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, \\\"normal\\\"); channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); System.out.println(\\\"Consumer01接收到的消息为：\\\" + message); }; channel.basicConsume(NORMAL_QUEUE, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"3、消费者02\",\"slug\":\"_3、消费者02\",\"contents\":[\"public class Consumer02 { /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); System.out.println(\\\"Consumer02接收到的消息为：\\\" + message); }; channel.basicConsume(DEAD_QUEUE, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"4、生产者\",\"slug\":\"_4、生产者\",\"contents\":[\"public class Product { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; //正常队列 public static final String NORMAL_QUEUE = \\\"normal_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //设置消息的 TTL 时间 AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().expiration(\\\"10000\\\").build(); for (int i = 1; i < 11; i++) { String msg = \\\"消息\\\" + i; channel.basicPublish(NORMAL_EXCHANGE, \\\"normal\\\", properties, msg.getBytes(StandardCharsets.UTF_8)); } } } \"]},{\"header\":\"5、监控图对比\",\"slug\":\"_5、监控图对比\",\"contents\":[\"启动生产者，关闭消费者C1，未启动消费者C2\",\"启动生产者，关闭消费者C1，未启动消费者C2，时间经过10s后\",\"启动生产者，关闭消费者C1，启动消费者C2\"]},{\"header\":\"6、设置过期时间\",\"slug\":\"_6、设置过期时间\",\"contents\":[]},{\"header\":\"1、生产者设置\",\"slug\":\"_1、生产者设置\",\"contents\":[\"生产者设置具有灵活性，第一条消息可设置10s，第二条设置8s\",\"//设置消息的 TTL 时间 AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().expiration(\\\"10000\\\").build(); \"]},{\"header\":\"2、消费者设置\",\"slug\":\"_2、消费者设置\",\"contents\":[\"消费者设置，所有消息的ttl都是一样的，没有灵活性\",\"//正常队列绑定死信队列信息 Map<String, Object> params = new HashMap<>(); //设置过期时间 params.put(\\\"x-message-ttl\\\", 10000); //正常队列设置死信交换机 参数 key 是固定值 params.put(\\\"x-dead-letter-exchange\\\", DEAD_EXCHANGE); //正常队列设置死信 routing-key 参数 key 是固定值 params.put(\\\"x-dead-letter-routing-key\\\", \\\"dead\\\"); channel.queueDeclare(NORMAL_QUEUE, false, false, false, params); \"]},{\"header\":\"3、队列达到最大长度\",\"slug\":\"_3、队列达到最大长度\",\"contents\":[\"消息达到设置的最大长度后，多与的消息会进入死信队列\"]},{\"header\":\"1、场景模拟\",\"slug\":\"_1、场景模拟-1\",\"contents\":[\"先启动消费者C1，创建好交换机和队列\",\"关闭C1，然后启动生产者和消费者C2\",\"C1设置的队列最大长度为6，多与的消息会进入死信队列，被C2消费\",\"再启动C1，C1消费6条\"]},{\"header\":\"2、生产者\",\"slug\":\"_2、生产者\",\"contents\":[\"public class Product { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); for (int i = 1; i < 11; i++) { String msg = \\\"消息\\\" + i; channel.basicPublish(NORMAL_EXCHANGE, \\\"normal\\\", properties, msg.getBytes(StandardCharsets.UTF_8)); } } } \"]},{\"header\":\"3、消费者01\",\"slug\":\"_3、消费者01\",\"contents\":[\"public class Consumer01 { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //正常队列 public static final String NORMAL_QUEUE = \\\"normal_queue\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT); channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //正常队列绑定死信队列信息 Map<String, Object> params = new HashMap<>(); //设置过期时间 // params.put(\\\"x-message-ttl\\\", 10000); //设置队列长度的限制 params.put(\\\"x-max-length\\\", 6); //正常队列设置死信交换机 参数 key 是固定值 params.put(\\\"x-dead-letter-exchange\\\", DEAD_EXCHANGE); //正常队列设置死信 routing-key 参数 key 是固定值 params.put(\\\"x-dead-letter-routing-key\\\", \\\"dead\\\"); channel.queueDeclare(NORMAL_QUEUE, false, false, false, params); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, \\\"normal\\\"); channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); System.out.println(\\\"Consumer01接收到的消息为：\\\" + message); }; channel.basicConsume(NORMAL_QUEUE, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"4、消费者02\",\"slug\":\"_4、消费者02\",\"contents\":[\"public class Consumer02 { /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); System.out.println(\\\"Consumer02接收到的消息为：\\\" + message); }; channel.basicConsume(DEAD_QUEUE, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"5、监控图对比\",\"slug\":\"_5、监控图对比-1\",\"contents\":[\"C1启动时，其他未启动\",\"启动生产者，关闭消费者，未启动C2\",\"启动C2\",\"启动C1\"]},{\"header\":\"6、设置队列最大长度\",\"slug\":\"_6、设置队列最大长度\",\"contents\":[\"//正常队列绑定死信队列信息 Map<String, Object> params = new HashMap<>(); //设置队列长度的限制 params.put(\\\"x-max-length\\\", 6); //正常队列设置死信交换机 参数 key 是固定值 params.put(\\\"x-dead-letter-exchange\\\", DEAD_EXCHANGE); //正常队列设置死信 routing-key 参数 key 是固定值 params.put(\\\"x-dead-letter-routing-key\\\", \\\"dead\\\"); channel.queueDeclare(NORMAL_QUEUE, false, false, false, params); \"]},{\"header\":\"4、消息被拒绝\",\"slug\":\"_4、消息被拒绝\",\"contents\":[]},{\"header\":\"1、场景模拟\",\"slug\":\"_1、场景模拟-2\",\"contents\":[\"先启动消费者C1，创建好交换机和队列\",\"然后启动生产者和消费者C2\",\"C1设置的队列拒绝接收消息“消息6”\",\"“消息6”被C2消费\"]},{\"header\":\"2、生产者\",\"slug\":\"_2、生产者-1\",\"contents\":[\"public class Product { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); for (int i = 1; i < 11; i++) { String msg = \\\"消息\\\" + i; channel.basicPublish(NORMAL_EXCHANGE, \\\"normal\\\", properties, msg.getBytes(StandardCharsets.UTF_8)); } } } \"]},{\"header\":\"3、消费者01\",\"slug\":\"_3、消费者01-1\",\"contents\":[\"public class Consumer01 { /** * 正常交换机 */ public static final String NORMAL_EXCHANGE = \\\"normal_exchange\\\"; /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //正常队列 public static final String NORMAL_QUEUE = \\\"normal_queue\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT); channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //正常队列绑定死信队列信息 Map<String, Object> params = new HashMap<>(); //正常队列设置死信交换机 参数 key 是固定值 params.put(\\\"x-dead-letter-exchange\\\", DEAD_EXCHANGE); //正常队列设置死信 routing-key 参数 key 是固定值 params.put(\\\"x-dead-letter-routing-key\\\", \\\"dead\\\"); channel.queueDeclare(NORMAL_QUEUE, false, false, false, params); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, \\\"normal\\\"); channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); if (\\\"消息6\\\".equals(message)){ System.out.println(\\\"Consumer01 接收到消息\\\" + message + \\\"并拒绝签收该消息\\\"); //requeue 设置为 false 代表拒绝重新入队 该队列如果配置了死信交换机将发送到死信队列中 channel.basicReject(deliver.getEnvelope().getDeliveryTag(), false); }else { System.out.println(\\\"Consumer01 接收到消息\\\"+message); channel.basicAck(deliver.getEnvelope().getDeliveryTag(), false); } }; // channel.basicConsume(NORMAL_QUEUE, false, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"4、消费者02\",\"slug\":\"_4、消费者02-1\",\"contents\":[\"public class Consumer02 { /** * 死信交换机 */ public static final String DEAD_EXCHANGE = \\\"dead_exchange\\\"; //死信队列 public static final String DEAD_QUEUE = \\\"dead_queue\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机 channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT); //声明队列 不共享 不持久化 不删除 channel.queueDeclare(DEAD_QUEUE, false, false, false, null); //绑定交换机和队列 channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, \\\"dead\\\"); System.out.println(\\\"等待接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); System.out.println(\\\"Consumer02接收到的消息为：\\\" + message); }; channel.basicConsume(DEAD_QUEUE, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"5、监控图对比\",\"slug\":\"_5、监控图对比-2\",\"contents\":[\"其他均启动，C2未启动时\"]},{\"header\":\"6、设置消息被拒绝\",\"slug\":\"_6、设置消息被拒绝\",\"contents\":[\"//获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); if (\\\"消息6\\\".equals(message)){ System.out.println(\\\"Consumer01 接收到消息\\\" + message + \\\"并拒绝签收该消息\\\"); //requeue 设置为 false 代表拒绝重新入队 该队列如果配置了死信交换机将发送到死信队列中 channel.basicReject(deliver.getEnvelope().getDeliveryTag(), false); }else { System.out.println(\\\"Consumer01 接收到消息\\\"+message); channel.basicAck(deliver.getEnvelope().getDeliveryTag(), false); } //消费成功之后是否要自动应答 true 代表自动应答 false 手动应答 channel.basicConsume(NORMAL_QUEUE, false, deliverCallback, cancelCallback -> {}); \"]}]},\"/study-tutorial/distributed/rabbitmq/delayed_queue.html\":{\"title\":\"7、延迟队列\",\"contents\":[{\"header\":\"1、概念\",\"slug\":\"_1、概念\",\"contents\":[\"延时队列,队列内部是有序的，最重要的特性就体现在它的延时属性上，延时队列中的元素是希望在指定时间到了以后或之前取出和处理，简单来说，延时队列就是用来存放需要在指定时间被处理的元素的队列。\"]},{\"header\":\"2、延迟队列使用场景\",\"slug\":\"_2、延迟队列使用场景\",\"contents\":[\"订单在十分钟之内未支付则自动取消\",\"新创建的店铺，如果在十天内都没有上传过商品，则自动发送消息提醒\",\"用户注册成功后，如果三天内没有登陆则进行短信提醒\",\"用户发起退款，如果三天内没有得到处理则通知相关运营人员\",\"预定会议后，需要在预定的时间点前十分钟通知各个与会人员参加会议\",\"这些场景都有一个特点，需要在某个事件发生之后或者之前的指定时间点完成某一项任务，如：发生订单生成事件，在十分钟之后检查该\",\"订单支付状态，然后将未支付的订单进行关闭；看起来似乎使用定时任务，一直轮询数据，每秒查一次，取出需要被处理的数据，然后处\",\"理不就完事了吗？如果数据量比较少，确实可以这样做，比如：对于“如果账单一周内未支付则进行自动结算”这样的需求，如果对于时间\",\"不是严格限制，而是宽松意义上的一周，那么每天晚上跑个定时任务检查一下所有未支付的账单，确实也是一个可行的方案。但对于数据\",\"量比较大，并且时效性较强的场景，如：“订单十分钟内未支付则关闭“，短期内未支付的订单数据可能会有很多，活动期间甚至会达到百\",\"万甚至千万级别，对这么庞大的数据量仍旧使用轮询的方式显然是不可取的，很可能在一秒内无法完成所有订单的检查，同时会给数据库\",\"带来很大压力，无法满足业务要求而且性能低下。\"]},{\"header\":\"3、RabbitMQ 中的TTL\",\"slug\":\"_3、rabbitmq-中的ttl\",\"contents\":[\"TTL ：RabbitMQ 中一个消息或者队列的属性，表明一条消息或者该队列中的所有消息的最大存活时间\",\"单位是毫秒。换句话说，如果一条消息设置了 TTL 属性或者进入了设置 TTL 属性的队列，那么这条消息如果在 TTL 设置的时间内没有被消费，则会成为\\\"死信\\\"。如果同时配置了队列的 TTL 和消息的TTL，那么较小的那个值将会被使用，有两种方式设置 TTL。\"]},{\"header\":\"1、消息设置 TTL\",\"slug\":\"_1、消息设置-ttl\",\"contents\":[\"rabbitTemplate.convertAndSend(\\\"X\\\", \\\"XC\\\", message, correlationData ->{ //设置ttl correlationData.getMessageProperties().setExpiration(ttl); return correlationData; }); \"]},{\"header\":\"2、队列设置TTL\",\"slug\":\"_2、队列设置ttl\",\"contents\":[\"第一种是在创建队列的时候设置队列的x-message-ttl属性\",\"args.put(\\\"x-message-ttl\\\", 5000); return QueueBuilder.durable(QUEUE_A).withArguments(args).build(); \"]},{\"header\":\"3、区别\",\"slug\":\"_3、区别\",\"contents\":[\"如果设置了队列的 TTL 属性，那么一旦消息过期，就会被队列丢弃(如果配置了死信队列被丢到死信队列中)，而第二种方式，消息即使过期，也不一定会被马上丢弃，因为消息是否过期是在即将投递到消费者之前判定的，如果当前队列有严重的消息积压情况，则已过期的消息也许还能存活较长时间；另外，还需要注意的一点是，如果不设置 TTL，表示消息永远不会过期，如果将 TTL 设置为 0，则表示除非此时可以直接投递该消息到消费者，否则该消息将会被丢弃。\",\"前一节我们介绍了死信队列，刚刚又介绍了 TTL，至此利用 RabbitMQ 实现延时队列的两大要素已经集齐，接下来只需要将它们进行融合，再加入一点点调味料，延时队列就可以新鲜出炉了。想想看，延时队列，不就是想要消息延迟多久被处理吗，TTL 则刚好能让消息在延迟多久之后成为死信，另一方面，成为死信的消息都会被投递到死信队列里，这样只需要消费者一直消费死信队列里的消息就完事了，因为里面的消息都是希望被立即处理的消息。\"]},{\"header\":\"4、整合Spring Boot\",\"slug\":\"_4、整合spring-boot\",\"contents\":[]},{\"header\":\"5、队列TTL\",\"slug\":\"_5、队列ttl\",\"contents\":[]},{\"header\":\"1、代码架构图\",\"slug\":\"_1、代码架构图\",\"contents\":[\"**场景：**创建两个队列 QA 和 QB，两者队列 TTL 分别设置为 10S 和 40S，然后在创建一个交换机 X 和死信交换机 Y，它们的类型都是 direct，创建一个死信队列 QD，它们的绑定关系如下：\"]},{\"header\":\"2、队列配置类\",\"slug\":\"_2、队列配置类\",\"contents\":[\"声明队列和交换机，并进行绑定\",\"@Configuration public class TtlQueueConfig { //普通交换机 public static final String X_EXCHANGE = \\\"X\\\"; /** * 死信交换机 */ public static final String Y_DEAD_LETTER_EXCHANGE = \\\"Y\\\"; public static final String QUEUE_A = \\\"QA\\\"; public static final String QUEUE_B = \\\"QB\\\"; public static final String DEAD_LETTER_QUEUE = \\\"QD\\\"; /** * 声明 xExchange * @return DirectExchange */ @Bean(\\\"xExchange\\\") public DirectExchange xExchange(){ return new DirectExchange(X_EXCHANGE); } /** * 声明死信交换机 y * @return */ @Bean(\\\"yExchange\\\") public DirectExchange yExchange(){ return new DirectExchange(Y_DEAD_LETTER_EXCHANGE); } @Bean public Queue queueA(){ Map<String, Object> args = new HashMap<>(3); //声明当前队列绑定的死信交换机 args.put(\\\"x-dead-letter-exchange\\\", Y_DEAD_LETTER_EXCHANGE); //声明当前队列的死信路由 key args.put(\\\"x-dead-letter-routing-key\\\", \\\"YD\\\"); //声明队列的 TTL args.put(\\\"x-message-ttl\\\", 10000); return QueueBuilder.durable(QUEUE_A).withArguments(args).build(); } /** * 绑定队列A与交换机X * @param queueA * @param xExchange * @return */ @Bean public Binding queueABindingX(@Qualifier(\\\"queueA\\\") Queue queueA, @Qualifier(\\\"xExchange\\\") DirectExchange xExchange){ return BindingBuilder.bind(queueA).to(xExchange).with(\\\"XA\\\"); } @Bean public Queue queueB(){ Map<String, Object> args = new HashMap<>(3); //声明当前队列绑定的死信交换机 args.put(\\\"x-dead-letter-exchange\\\", Y_DEAD_LETTER_EXCHANGE); //声明当前队列的死信路由 key args.put(\\\"x-dead-letter-routing-key\\\", \\\"YD\\\"); //声明队列的 TTL args.put(\\\"x-message-ttl\\\", 40000); return QueueBuilder.durable(QUEUE_B).withArguments(args).build(); } /** * 绑定队列B与交换机X * @param queueB * @param xExchange * @return */ @Bean public Binding queueBBindingX(@Qualifier(\\\"queueB\\\") Queue queueB, @Qualifier(\\\"xExchange\\\") DirectExchange xExchange){ return BindingBuilder.bind(queueB).to(xExchange).with(\\\"XB\\\"); } /** * 死信队列 * @return */ @Bean public Queue queueD(){ return QueueBuilder.durable(DEAD_LETTER_QUEUE).build(); } @Bean public Binding queueDBindingY(@Qualifier(\\\"queueD\\\") Queue queueD, @Qualifier(\\\"yExchange\\\") DirectExchange yExchange){ return BindingBuilder.bind(queueD).to(yExchange).with(\\\"YD\\\"); } } \"]},{\"header\":\"3、生产者\",\"slug\":\"_3、生产者\",\"contents\":[\"通过controller发送消息，分别发送到队列A和B中\",\"@RestController @RequestMapping(\\\"ttl\\\") public class SendMsgController { /** logger */ private static final Logger log = LoggerFactory.getLogger(SendMsgController.class); @Resource private RabbitTemplate rabbitTemplate; @GetMapping(\\\"/send/{message}\\\") public void sendMsg(@PathVariable(\\\"message\\\") String message){ log.info(\\\"当前时间：{},发送一条信息给两个 TTL 队列:{}\\\", new Date(), message); rabbitTemplate.convertAndSend(\\\"X\\\",\\\"XA\\\", \\\"消息来自 ttl 为 10S 的队列: \\\" + message); rabbitTemplate.convertAndSend(\\\"X\\\",\\\"XB\\\", \\\"消息来自 ttl 为 40S 的队列: \\\" + message); } } \"]},{\"header\":\"4、消费者代码\",\"slug\":\"_4、消费者代码\",\"contents\":[\"打印消费的消息\",\"@Component public class DeadLetterQueueConsumer { /** logger */ private static final Logger log = LoggerFactory.getLogger(DeadLetterQueueConsumer.class); @RabbitListener(queues = \\\"QD\\\") public void receiveD(Message message, Channel channel) throws Exception{ String msg = new String(message.getBody()); log.info(\\\"当前时间：{},收到死信队列信息{}\\\", new Date().toString(), msg); } } \"]},{\"header\":\"5、结果\",\"slug\":\"_5、结果\",\"contents\":[\"发起一个请求 http://localhost:8080/ttl/send/hahaha\",\"2022-08-05 10:49:14.864 INFO 11296 --- [nio-8080-exec-1] c.x.r.controller.SendMsgController : 当前时间：Fri Aug 05 10:49:14 CST 2022,发送一条信息给两个 TTL 队列:hahaha 2022-08-05 10:49:24.877 INFO 11296 --- [ntContainer#0-2] c.x.r.consumer.DeadLetterQueueConsumer : 当前时间：Fri Aug 05 10:49:24 CST 2022,收到死信队列信息消息来自 ttl 为 10S 的队列: hahaha 2022-08-05 10:49:54.874 INFO 11296 --- [ntContainer#0-2] c.x.r.consumer.DeadLetterQueueConsumer : 当前时间：Fri Aug 05 10:49:54 CST 2022,收到死信队列信息消息来自 ttl 为 40S 的队列: hahaha \",\"第一条消息在 10S 后变成了死信消息，然后被消费者消费掉，第二条消息在 40S 之后变成了死信消息，然后被消费掉，这样一个延时队列就打造完成了。\",\"不过，如果这样使用的话，岂不是每增加一个新的时间需求，就要新增一个队列，这里只有 10S 和 40S两个时间选项，如果需要一个小时后处理，那么就需要增加 TTL 为一个小时的队列，如果是预定会议室然后提前通知这样的场景，岂不是要增加无数个队列才能满足需求？\"]},{\"header\":\"6、延时队列优化\",\"slug\":\"_6、延时队列优化\",\"contents\":[]},{\"header\":\"1、代码架构图\",\"slug\":\"_1、代码架构图-1\",\"contents\":[\"场景：在这里新增了一个队列 QC,绑定关系如下,该队列不设置 TTL 时间，由用户传参设置ttl\"]},{\"header\":\"2、队列配置类增加\",\"slug\":\"_2、队列配置类增加\",\"contents\":[\"添加对 队列C的配置与绑定\",\"@Bean public Queue queueC(){ Map<String, Object> args = new HashMap<>(3); //声明当前队列绑定的死信交换机 args.put(\\\"x-dead-letter-exchange\\\", Y_DEAD_LETTER_EXCHANGE); //声明当前队列的死信路由 key args.put(\\\"x-dead-letter-routing-key\\\", \\\"YD\\\"); return QueueBuilder.durable(QUEUE_C).withArguments(args).build(); } /** * 绑定队列C与交换机X * @param queueC * @param xExchange * @return */ @Bean public Binding queueCBindingX(@Qualifier(\\\"queueC\\\") Queue queueC, @Qualifier(\\\"xExchange\\\") DirectExchange xExchange){ return BindingBuilder.bind(queueC).to(xExchange).with(\\\"XC\\\"); } \"]},{\"header\":\"3、生产者发送消息\",\"slug\":\"_3、生产者发送消息\",\"contents\":[\"@GetMapping(\\\"/send/{message}/{ttl}\\\") public void sendMsg(@PathVariable(\\\"message\\\") String message, @PathVariable String ttl){ rabbitTemplate.convertAndSend(\\\"X\\\", \\\"XC\\\", message, correlationData ->{ correlationData.getMessageProperties().setExpiration(ttl); return correlationData; }); log.info(\\\"当前时间：{},发送一条时长{}毫秒 TTL 信息给队列 C:{}\\\", new Date(),ttl, message); } \"]},{\"header\":\"4、结果\",\"slug\":\"_4、结果\",\"contents\":[\"看起来似乎没什么问题，但是在最开始的时候，就介绍过如果使用在消息属性上设置 TTL 的方式，消息可能并不会按时“死亡“，因为 RabbitMQ只会检查第一个消息是否过期，如果过期则丢到死信队列，如果第一个消息的延时时长很长，而第二个消息的延时时长很短，第二个消息并不会优先得到执行。\"]},{\"header\":\"7、RabbitMQ 插件实现延迟队列\",\"slug\":\"_7、rabbitmq-插件实现延迟队列\",\"contents\":[\"上文中提到的问题，确实是一个问题，如果不能实现在消息粒度上的 TTL，并使其在设置的 TTL 时间及时死亡，就无法设计成一个通用的延时队列。那如何解决呢，接下来我们就去解决该问题。\",\"使用插件实现延迟队列\"]},{\"header\":\"1、安装插件\",\"slug\":\"_1、安装插件\",\"contents\":[\"下载地址：https://www.rabbitmq.com/community-plugins.html\",\"找到延迟队列插件：rabbitmq_delayed_message_exchange\",\"PS：下载的插件是：rabbitmq_delayed_message_exchange-3.10.2.ez，而不是zip等其他格式\",\"进入rabbitmq的插件目录\",\"/usr/lib/rabbitmq/lib/rabbitmq_server-3.10.5/plugins \",\"启动延迟插件\",\"[root@xiaobear plugins]## rabbitmq-plugins enable rabbitmq_delayed_message_exchange Enabling plugins on node rabbit@xiaobear: rabbitmq_delayed_message_exchange The following plugins have been configured: rabbitmq_delayed_message_exchange rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatch Applying plugin configuration to rabbit@xiaobear... Plugin configuration unchanged. [root@xiaobear plugins]## \",\"添加插件后\"]},{\"header\":\"2、代码架构图\",\"slug\":\"_2、代码架构图\",\"contents\":[\"新增了一个队列 delayed.queue,一个自定义交换机 delayed.exchange，绑定关系如下:\"]},{\"header\":\"3、配置类代码\",\"slug\":\"_3、配置类代码\",\"contents\":[\"声明一个延迟队列和交换机，并进行绑定\",\"在我们自定义的交换机中，这是一种新的交换类型，该类型消息支持延迟投递机制 消息传递后并不会立即投递到目标队列中，而是存储在 mnesia(一个分布式数据系统)表中，当达到投递时间时，才投递到目标队列中。\",\"@Configuration public class DelayedQueueConfig { public static final String DELAYED_QUEUE_NAME = \\\"delayed.queue\\\"; public static final String DELAYED_EXCHANGE_NAME = \\\"delayed.exchange\\\"; public static final String DELAYED_ROUTING_KEY = \\\"delayed.routingkey\\\"; @Bean public Queue delayedQueue(){ return new Queue(DELAYED_QUEUE_NAME); } /** * 自定义交换机 定义延迟交换机 * @return */ @Bean public CustomExchange delayedExchange(){ Map<String, Object> map = new HashMap<>(1); map.put(\\\"x-delayed-type\\\",\\\"direct\\\"); return new CustomExchange(DELAYED_EXCHANGE_NAME, \\\"x-delayed-message\\\", true, false, map); } /** * 绑定队列与交换机 * @return */ @Bean public Binding bindingDelayedQueue(@Qualifier(\\\"delayedQueue\\\")Queue queue, @Qualifier(\\\"delayedExchange\\\") CustomExchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(DELAYED_ROUTING_KEY).noargs(); } } \"]},{\"header\":\"4、生产者代码\",\"slug\":\"_4、生产者代码\",\"contents\":[\"@RestController @RequestMapping(\\\"/delay\\\") public class DelayedMsgController { /** logger */ private static final Logger log = LoggerFactory.getLogger(DelayedMsgController.class); @Resource private RabbitTemplate rabbitTemplate; public static final String DELAYED_EXCHANGE_NAME = \\\"delayed.exchange\\\"; public static final String DELAYED_ROUTING_KEY = \\\"delayed.routingkey\\\"; @GetMapping(\\\"/send/{message}/{delayTime}\\\") public void sendMsg(@PathVariable String message, @PathVariable Integer delayTime){ rabbitTemplate.convertAndSend(DELAYED_EXCHANGE_NAME, DELAYED_ROUTING_KEY, message, correlationData ->{ //设置延迟消息 correlationData.getMessageProperties().setDelay(delayTime); return correlationData; }); log.info(\\\" 当 前 时 间 ： {}, 发送一条延迟 {} 毫秒的信息给队列 delayed.queue:{}\\\", new Date(),delayTime, message); } } \"]},{\"header\":\"5、消费者代码\",\"slug\":\"_5、消费者代码\",\"contents\":[\"通过监听器消费 延迟队列中的消息\",\"@RabbitListener(queues = \\\"delayed.queue\\\") public void receiveDelayMsg(Message message){ String msg = new String(message.getBody()); log.info(\\\"当前时间：{},收到延迟队列信息{}\\\", new Date(), msg); } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"发送请求：http://localhost:8080/delay/send/xixi/20000 和 http://localhost:8080/delay/send/hahha/2000\",\"结果显示：第二个消息被先消费掉了，符合预期\"]}]},\"/study-tutorial/distributed/rabbitmq/exchanges.html\":{\"title\":\"5、交换机\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在上一节中，我们创建了一个工作队列。我们假设的是工作队列背后，每个任务都恰好交付给一个消费者(工作进程)。在这一部分中，我们将做一些完全不同的事情-我们将消息传达给多个消费者。这种模式称为 ”发布/订阅”.\",\"为了说明这种模式，我们将构建一个简单的日志系统。它将由两个程序组成:第一个程序将发出日志消息，第二个程序是消费者。其中我们会启动两个消费者，其中一个消费者接收到消息后把日志存储在磁盘，另外一个消费者接收到消息后把消息打印在屏幕上，事实上第一个程序发出的日志消息将广播给所有消费者\"]},{\"header\":\"1、Exchanges\",\"slug\":\"_1、exchanges\",\"contents\":[]},{\"header\":\"1、Exchanges概念\",\"slug\":\"_1、exchanges概念\",\"contents\":[\"RabbitMQ 消息传递模型的核心思想是: 生产者生产的消息从不会直接发送到队列。实际上，通常生产者甚至都不知道这些消息传递传递到了哪些队列中。\",\"相反，生产者只能将消息发送到交换机(exchange)，交换机工作的内容非常简单，一方面它接收来自生产者的消息，另一方面将它们推入队列。交换机必须确切知道如何处理收到的消息。是应该把这些消息放到特定队列还是说把他们到许多队列中还是说应该丢弃它们。这就的由交换机的类型来决定。\"]},{\"header\":\"2、类型\",\"slug\":\"_2、类型\",\"contents\":[\"direct（直接）\",\"topic（主题）：路由\",\"headers （标题）\",\"fanout（扇出）：发布订阅\"]},{\"header\":\"3、无名Exchange\",\"slug\":\"_3、无名exchange\",\"contents\":[\"在前面部分我们对 exchange 一无所知，但仍然能够将消息发送到队列。之前能实现的原因是因为我们使用的是默认交换，我们通过空字符串(“”)进行标识。\",\"/** * 发送一个消息 * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */ channel.basicPublish(\\\"\\\", \\\"hello\\\", null, message.getBytes()); \",\"第一个参数是交换机的名称。空字符串表示默认或无名称交换机：消息能路由发送到队列中其实是由 routingKey(bindingkey)绑定 key 指定的，如果它存在的话\"]},{\"header\":\"2、临时队列\",\"slug\":\"_2、临时队列\",\"contents\":[\"前面的学习我们使用的是具有特定名称的队列(还记得 hello 和 ack_queue 吗？)。队列的名称我们来说至关重要-我们需要指定我们的消费者去消费哪个队列的消息。\",\"每当我们连接到 Rabbit 时，我们都需要一个全新的空队列，为此我们可以创建一个具有随机名称的队列，或者能让服务器为我们选择一个随机队列名称那就更好了。其次一旦我们断开了消费者的连接，队列将被自动删除。\",\"创建临时队列的方式如下:\",\"String queueName = channel.queueDeclare().getQueue(); \"]},{\"header\":\"3、绑定(bindings)\",\"slug\":\"_3、绑定-bindings\",\"contents\":[\"什么是 bingding 呢，binding 其实是 exchange 和 queue 之间的桥梁，它告诉我们 exchange 和那个队列进行了绑定关系。比如说下面这张图告诉我们的就是 X 与 Q1 和 Q2 进行了绑定。\",\"//String queue, String exchange, String routingKey channel.queueBind(queueName, \\\"logs\\\", \\\"\\\"); \"]},{\"header\":\"4、Fanout\",\"slug\":\"_4、fanout\",\"contents\":[]},{\"header\":\"1、Fanout 概念\",\"slug\":\"_1、fanout-概念\",\"contents\":[\"Fanout 这种类型非常简单。正如从名称中猜到的那样，它是将接收到的所有消息广播到它知道的所有队列中。系统中默认有些 exchange 类型\"]},{\"header\":\"2、Fanout 实战\",\"slug\":\"_2、fanout-实战\",\"contents\":[\"场景：一个生产者，两个消费者，绑定空routingKey，实现一次发送，同时消费\",\"消费者1\",\"public class ReceiveLog01 { public static final String EXCHANGE_NAME = \\\"logs\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明一个交换机 类型为：扇出 channel.exchangeDeclare(EXCHANGE_NAME, \\\"fanout\\\"); //声明工作队列 String queueName = channel.queueDeclare().getQueue(); //将临时队列绑定到交换机上 String queue, String exchange, String routingKey channel.queueBind( queueName, EXCHANGE_NAME, \\\"\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ System.out.println(\\\"ReceiveLog01接收到的消息为：\\\" + new String(deliver.getBody(), \\\"UTF-8\\\")); }; //String queue, boolean autoAck, DeliverCallback deliverCallback, CancelCallback cancelCallback //取消接收消息的回调为空 channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \",\"消费者2\",\"public class ReceiveLog02 { public static final String EXCHANGE_NAME = \\\"logs\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明一个交换机 类型为：扇出 channel.exchangeDeclare(EXCHANGE_NAME, \\\"fanout\\\"); //声明工作队列 String queueName = channel.queueDeclare().getQueue(); //将临时队列绑定到交换机上 String queue, String exchange, String routingKey channel.queueBind( queueName, EXCHANGE_NAME, \\\"\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ System.out.println(\\\"ReceiveLog02接收到的消息为：\\\" + new String(deliver.getBody(), \\\"UTF-8\\\")); }; //String queue, boolean autoAck, DeliverCallback deliverCallback, CancelCallback cancelCallback //取消接收消息的回调为空 channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \",\"生产者\",\"public class EmitLog { public static final String EXCHANGE_NAME = \\\"logs\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //声明交换机，因消费者已经声明，故不需要重复声明 Scanner sc = new Scanner(System.in); while (sc.hasNext()){ String msg = sc.nextLine(); //String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish(EXCHANGE_NAME, \\\"\\\", null, msg.getBytes(StandardCharsets.UTF_8)); System.out.println(\\\"生产者发送的消息为:\\\" + msg); } } } \"]},{\"header\":\"5、Direct exchange\",\"slug\":\"_5、direct-exchange\",\"contents\":[\"在上一节中，我们构建了一个简单的日志记录系统。我们能够向许多接收者广播日志消息。在本节我们将向其中添加一些特别的功能-比方说我们只让某个消费者订阅发布的部分消息。例如我们只把严重错误消息定向存储到日志文件(以节省磁盘空间)，同时仍然能够在控制台上打印所有日志消息。\",\"我们再次来回顾一下什么是 bindings，绑定是交换机和队列之间的桥梁关系。也可以这么理解：队列只对它绑定的交换机的消息感兴趣。绑定用参数：routingKey 来表示也可称该参数为 binding key，创建绑定我们用代码:\",\"channel.queueBind(queueName, EXCHANGE_NAME, \\\"routingKey\\\"); \",\"绑定之后的意义由其交换类型决定。\"]},{\"header\":\"1、Direct exchange 介绍\",\"slug\":\"_1、direct-exchange-介绍\",\"contents\":[\"上一节中的我们的日志系统将所有消息广播给所有消费者，对此我们想做一些改变，例如我们希望将日志消息写入磁盘的程序仅接收严重错误(errros)，而不存储哪些警告(warning)或信息(info)日志消息避免浪费磁盘空间。Fanout 这种交换类型并不能给我们带来很大的灵活性-它只能进行无意识的广播，在这里我们将使用 direct 这种类型来进行替换，这种类型的工作方式是，消息只去到它绑定的routingKey 队列中去。\",\"在上面这张图中，我们可以看到 X 绑定了两个队列，绑定类型是 direct。队列 Q1 绑定键为 orange，队列 Q2 绑定键有两个:一个绑定键为 black，另一个绑定键为 green.\",\"在这种绑定情况下，生产者发布消息到 exchange 上，绑定键为 orange 的消息会被发布到队列Q1。绑定键为 black、green 和的消息会被发布到队列 Q2，其他消息类型的消息将被丢弃。\"]},{\"header\":\"2、多重绑定\",\"slug\":\"_2、多重绑定\",\"contents\":[\"当然如果 exchange 的绑定类型是 direct，但是它绑定的多个队列的key如果都相同，在这种情况下虽然绑定类型是 direct 但是它表现的就和fanout有点类似了，就跟广播差不多，如上图所示。\"]},{\"header\":\"3、实战\",\"slug\":\"_3、实战\",\"contents\":[\"引入依赖，因用到导出文件，用到了commons-io\",\"<!-- https://mvnrepository.com/artifact/commons-io/commons-io --> <dependency> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> <version>2.5</version> </dependency> \"]},{\"header\":\"1、消费者01\",\"slug\":\"_1、消费者01\",\"contents\":[\"消费者01绑定的routingKey为error，只接收绑定的消息，然后写入文件中\",\"public class ReceiveDirectLog01 { public static final String EXCHANGE_DIRECT_NAME = \\\"direct_log\\\"; public static void main(String[] args) throws Exception{ //获取信道 Channel channel = RabbitMQConfig.getChannel(); //创建交换机，类型为direct channel.exchangeDeclare(EXCHANGE_DIRECT_NAME, BuiltinExchangeType.DIRECT); //创建临时队列 String queueName = channel.queueDeclare().getQueue(); //String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments // channel.queueDeclare(queueName, false, false, false, null); //绑定交换机与队列 String queue, String exchange, String routingKey channel.queueBind(queueName, EXCHANGE_DIRECT_NAME, \\\"error\\\"); System.out.println(\\\"等待接收消息\\\"); //接收消息的回调函数 DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); message = \\\"接收绑定键:\\\" + deliver.getEnvelope().getRoutingKey() + \\\",消息:\\\" + message; File file = new File(\\\"C:\\\\\\\\Users\\\\\\\\xiaobear\\\\\\\\Desktop\\\\\\\\rabbitmq_log.txt\\\"); FileUtils.writeStringToFile(file, message, \\\"UTF-8\\\"); System.out.println(\\\"错误日志已经接收\\\"); }; channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"2、消费者02\",\"slug\":\"_2、消费者02\",\"contents\":[\"消费者01绑定的routingKey为info和warning，只接收绑定的消息，然后打印到控制台\",\"public class ReceiveDirectLog02 { public static final String EXCHANGE_DIRECT_NAME = \\\"direct_log\\\"; public static void main(String[] args) throws Exception{ //获取信道 Channel channel = RabbitMQConfig.getChannel(); //创建交换机，类型为direct channel.exchangeDeclare(EXCHANGE_DIRECT_NAME, BuiltinExchangeType.DIRECT); //创建临时队列 String queueName = channel.queueDeclare().getQueue(); //String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments // channel.queueDeclare(queueName, false, false, false, null); //绑定交换机与队列 String queue, String exchange, String routingKey channel.queueBind(queueName, EXCHANGE_DIRECT_NAME, \\\"info\\\"); channel.queueBind(queueName, EXCHANGE_DIRECT_NAME, \\\"warning\\\"); System.out.println(\\\"等待接收消息\\\"); //接收消息的回调函数 DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); message = \\\"接收绑定键:\\\" + deliver.getEnvelope().getRoutingKey() + \\\",消息:\\\" + message; System.out.println(message); }; channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \"]},{\"header\":\"3、生产者\",\"slug\":\"_3、生产者\",\"contents\":[\"public class EmitDirectLog { public static final String EXCHANGE_DIRECT_NAME = \\\"direct_log\\\"; public static void main(String[] args) throws Exception{ try(Channel channel = RabbitMQConfig.getChannel()){ channel.exchangeDeclare(EXCHANGE_DIRECT_NAME, BuiltinExchangeType.DIRECT); //声明交换机，因消费者已经声明，故不需要重复声明 //创建多个 bindingKey Map<String, String> bindingKeyMap = new HashMap<>(4); bindingKeyMap.put(\\\"error\\\",\\\"这是错误消息\\\"); bindingKeyMap.put(\\\"info\\\",\\\"这是info消息\\\"); bindingKeyMap.put(\\\"debug\\\",\\\"这是debug消息\\\"); bindingKeyMap.put(\\\"warning\\\",\\\"这是警告消息\\\"); bindingKeyMap.forEach((k,v) -> { try { channel.basicPublish(EXCHANGE_DIRECT_NAME, k, null, v.getBytes(StandardCharsets.UTF_8)); } catch (IOException e) { e.printStackTrace(); } System.out.println(\\\"生产者发出的消息为：\\\" + v); }); } } } \"]},{\"header\":\"6、Topics\",\"slug\":\"_6、topics\",\"contents\":[\"前面存在的问题：\",\"在上一个小节中，我们改进了日志记录系统。我们没有使用只能进行随意广播的 fanout 交换机，而是使用了 direct 交换机，从而有能实现有选择性地接收日志。\",\"尽管使用 direct 交换机改进了我们的系统，但是它仍然存在局限性-比方说我们想接收的日志类型有info.base 和 info.advantage，某个队列只想 info.base 的消息，那这个时候 direct 就办不到了。这个时候就只能使用 topic 类型\"]},{\"header\":\"1、要求\",\"slug\":\"_1、要求\",\"contents\":[\"发送到类型是 topic 交换机的消息的 routing_key 不能随意写，必须满足一定的要求，它必须是一个单词列表，以点号分隔开。这些单词可以是任意单词，比如说：\\\"stock.usd.nyse\\\", \\\"nyse.vmw\\\",\",\"​ \\\"quick.orange.rabbit\\\".这种类型的。当然这个单词列表最多不能超过 255 个字节。\",\"有两个替换符是大家需要注意的：\",\"*(星号)可以代替一个单词\",\"#(井号)可以替代零个或多个单词\"]},{\"header\":\"2、匹配案例\",\"slug\":\"_2、匹配案例\",\"contents\":[\"Q1：中间带 orange 带 3 个单词的字符串(.orange.)\",\"Q2：最后一个单词是 rabbit 的 3 个单词(..rabbit)、第一个单词是 lazy 的多个单词(lazy.#)\",\"quick.orange.rabbit 被队列 Q1Q2 接收到 lazy.orange.elephant 被队列 Q1Q2 接收到 quick.orange.fox 被队列 Q1 接收到 lazy.brown.fox 被队列 Q2 接收到 lazy.pink.rabbit 虽然满足两个绑定但只被队列 Q2 接收一次 quick.brown.fox 不匹配任何绑定不会被任何队列接收到会被丢弃 quick.orange.male.rabbit 是四个单词不匹配任何绑定会被丢弃 lazy.orange.male.rabbit 是四个单词但匹配 Q2 \",\"当队列绑定关系是下列这种情况时需要引起注意\",\"当一个队列绑定键是#，那么这个队列将接收所有数据，就有点像fanout 了\",\"如果队列绑定键当中没有#和*出现，那么该队列绑定类型就是direct了\"]},{\"header\":\"3、实战\",\"slug\":\"_3、实战-1\",\"contents\":[]},{\"header\":\"1、生产者\",\"slug\":\"_1、生产者\",\"contents\":[\"public class EmitTopicLog { public static final String EXCHANGE_DIRECT_NAME = \\\"topic_log\\\"; public static void main(String[] args) throws Exception{ try(Channel channel = RabbitMQConfig.getChannel()){ channel.exchangeDeclare(EXCHANGE_DIRECT_NAME, BuiltinExchangeType.TOPIC); //声明交换机，因消费者已经声明，故不需要重复声明 //创建多个 bindingKey Map<String, String> bindingKeyMap = new HashMap<>(4); bindingKeyMap.put(\\\"quick.orange.rabbit\\\",\\\"被队列 Q1Q2 接收到\\\"); bindingKeyMap.put(\\\"lazy.orange.elephant\\\",\\\"被队列 Q1Q2 接收到\\\"); bindingKeyMap.put(\\\"quick.orange.fox\\\",\\\"被队列 Q1 接收到\\\"); bindingKeyMap.put(\\\"lazy.brown.fox\\\",\\\"被队列 Q2 接收到\\\"); bindingKeyMap.put(\\\"lazy.pink.rabbit\\\",\\\"虽然满足两个绑定但只被队列 Q2 接收一次\\\"); bindingKeyMap.put(\\\"quick.brown.fox\\\",\\\"不匹配任何绑定不会被任何队列接收到会被丢弃\\\"); bindingKeyMap.put(\\\"quick.orange.male.rabbit\\\",\\\"是四个单词不匹配任何绑定会被丢弃\\\"); bindingKeyMap.put(\\\"lazy.orange.male.rabbit\\\",\\\"是四个单词但匹配 Q2\\\"); bindingKeyMap.forEach((k,v) -> { try { channel.basicPublish(EXCHANGE_DIRECT_NAME, k, null, v.getBytes(StandardCharsets.UTF_8)); } catch (IOException e) { e.printStackTrace(); } System.out.println(\\\"生产者发出的消息为：\\\" + v); }); } } } \",\"生产者发出的消息为：是四个单词不匹配任何绑定会被丢弃 生产者发出的消息为：不匹配任何绑定不会被任何队列接收到会被丢弃 生产者发出的消息为：被队列 Q1Q2 接收到 生产者发出的消息为：被队列 Q2 接收到 生产者发出的消息为：被队列 Q1Q2 接收到 生产者发出的消息为：被队列 Q1 接收到 生产者发出的消息为：虽然满足两个绑定但只被队列 Q2 接收一次 生产者发出的消息为：是四个单词但匹配 Q2 \"]},{\"header\":\"2、消费者01\",\"slug\":\"_2、消费者01\",\"contents\":[\"public class ReceiveTopic01 { public static final String EXCHANGE_NAME_TOPIC = \\\"topic_log\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //交换机为主题模式 channel.exchangeDeclare(EXCHANGE_NAME_TOPIC, BuiltinExchangeType.TOPIC); //创建临时队列 String queueName = \\\"Q1\\\"; //绑定队列 channel.queueDeclare(queueName, false, false, false, null); channel.queueBind(queueName, EXCHANGE_NAME_TOPIC, \\\"*.orange.*\\\"); System.out.println(\\\"等待接收消息\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); message = \\\"接收绑定键:\\\" + deliver.getEnvelope().getRoutingKey() + \\\",消息:\\\" + message; System.out.println(message); }; channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \",\"等待接收消息 接收绑定键:lazy.orange.elephant,消息:被队列 Q1Q2 接收到 接收绑定键:quick.orange.rabbit,消息:被队列 Q1Q2 接收到 接收绑定键:quick.orange.fox,消息:被队列 Q1 接收到 \"]},{\"header\":\"3、消费者02\",\"slug\":\"_3、消费者02\",\"contents\":[\"public class ReceiveTopic02 { public static final String EXCHANGE_NAME_TOPIC = \\\"topic_log\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); //交换机为主题模式 channel.exchangeDeclare(EXCHANGE_NAME_TOPIC, BuiltinExchangeType.TOPIC); //创建临时队列 String queueName = \\\"Q2\\\"; //绑定队列 channel.queueDeclare(queueName, false, false, false, null); channel.queueBind(queueName, EXCHANGE_NAME_TOPIC, \\\"*.*.rabbit\\\"); channel.queueBind(queueName, EXCHANGE_NAME_TOPIC, \\\"lazy.#\\\"); System.out.println(\\\"等待接收消息\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ //获取消息 String message = new String(deliver.getBody(), \\\"UTF-8\\\"); message = \\\"接收绑定键:\\\" + deliver.getEnvelope().getRoutingKey() + \\\",消息:\\\" + message; System.out.println(message); }; channel.basicConsume(queueName, true, deliverCallback, cancelCallback -> {}); } } \",\"等待接收消息 接收绑定键:lazy.orange.elephant,消息:被队列 Q1Q2 接收到 接收绑定键:lazy.brown.fox,消息:被队列 Q2 接收到 接收绑定键:quick.orange.rabbit,消息:被队列 Q1Q2 接收到 接收绑定键:lazy.pink.rabbit,消息:虽然满足两个绑定但只被队列 Q2 接收一次 接收绑定键:lazy.orange.male.rabbit,消息:是四个单词但匹配 Q2 \"]}]},\"/study-tutorial/distributed/rabbitmq/hello_word.html\":{\"title\":\"2、Hello World\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"官网Hello World地址：https://www.rabbitmq.com/tutorials/tutorial-one-java.html\"]},{\"header\":\"1、新建maven项目\",\"slug\":\"_1、新建maven项目\",\"contents\":[\"添加依赖\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <modelVersion>4.0.0</modelVersion> <groupId>org.example</groupId> <artifactId>xiaobear-RabbitMQ</artifactId> <version>1.0-SNAPSHOT</version> <properties> <maven.compiler.source>8</maven.compiler.source> <maven.compiler.target>8</maven.compiler.target> </properties> <dependencies> <dependency> <groupId>com.rabbitmq</groupId> <artifactId>amqp-client</artifactId> <version>5.13.1</version> </dependency> </dependencies> </project> \"]},{\"header\":\"2、建生产者类\",\"slug\":\"_2、建生产者类\",\"contents\":[\"public class Product { private final static String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { //创建连接 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\\\"192.168.130.134\\\"); factory.setUsername(\\\"admin\\\"); factory.setPassword(\\\"admin123\\\"); //channel 实现了自动 close 接口 自动关闭 不需要显示关闭 try (Connection connection = factory.newConnection(); Channel channel = connection.createChannel()) { /** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 true 可以多个消费者消费 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); String message = \\\"Hello World!\\\"; /** * 发送一个消息 * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */ channel.basicPublish(\\\"\\\", QUEUE_NAME, null, message.getBytes()); System.out.println(\\\" [x] Sent '\\\" + message + \\\"'\\\"); } } } \",\"执行后，打开管理面板，会发现，存在一条消息待消费\"]},{\"header\":\"3、建消费者类\",\"slug\":\"_3、建消费者类\",\"contents\":[\"public class RabbitMQConfig { /** * rabbitmq连接信息 * @return */ public static ConnectionFactory connectRabbitMq(){ //创建连接 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\\\"192.168.130.134\\\"); factory.setUsername(\\\"admin\\\"); factory.setPassword(\\\"admin123\\\"); return factory; } } \",\"public class Consumer { private final static String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { //创建连接诶信息 ConnectionFactory factory = RabbitMQConfig.connectRabbitMq(); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); System.out.println(\\\" [*] Waiting for messages. To exit press CTRL+C\\\"); //推送的消息如何进行消费的接口回调 DeliverCallback deliverCallback = (consumerTag, delivery) -> { String message = new String(delivery.getBody(), \\\"UTF-8\\\"); System.out.println(\\\" [x] Received '\\\" + message + \\\"'\\\"); }; /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 false 手动应答 * 3.消费者未成功消费的回调 */ channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -> { }); } } \",\"执行消费者后，控制台打印出消费的消息，此时管理面板待消费的消息为0\"]},{\"header\":\"4、遇到的问题\",\"slug\":\"_4、遇到的问题\",\"contents\":[]},{\"header\":\"1、Connection timed out: connect\",\"slug\":\"_1、connection-timed-out-connect\",\"contents\":[\"查看5672端口是否开放\"]},{\"header\":\"2、connection error; protocol method: #method<connection.close>(reply-code=530, reply-text=NOT_ALLOWED\",\"slug\":\"_2、connection-error-protocol-method-method-connection-close-reply-code-530-reply-text-not-allowed\",\"contents\":[\"rabbitmqctl set_permissions -p \\\"/\\\" admin \\\".*\\\" \\\".*\\\" \\\".*\\\" \",\"把Virtual Host为【/】的set permission给用户\"]}]},\"/study-tutorial/distributed/rabbitmq/other.html\":{\"title\":\"9、RabbitMQ其他知识点\",\"contents\":[{\"header\":\"1、幂等性\",\"slug\":\"_1、幂等性\",\"contents\":[]},{\"header\":\"1、概念\",\"slug\":\"_1、概念\",\"contents\":[\"用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单的例子，那就是支付，用户购买商品后支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额发现多扣钱了，流水记录也变成了两条。在以前的单应用系统中，我们只需要把数据操作放入事务中即可，发生错误立即回滚，但是再响应客户端的时候也有可能出现网络中断或者异常等等。\"]},{\"header\":\"2、消息重复消费\",\"slug\":\"_2、消息重复消费\",\"contents\":[\"消费者在消费 MQ 中的消息时，MQ 已把消息发送给消费者，消费者在给 MQ 返回 ack 时网络中断，故 MQ 未收到确认信息，该条消息会重新发给其他的消费者，或者在网络重连后再次发送给该消费者，但实际上该消费者已成功消费了该条消息，造成消费者消费了重复的消息。\"]},{\"header\":\"3、解决思路\",\"slug\":\"_3、解决思路\",\"contents\":[\"MQ 消费者的幂等性的解决一般使用全局 ID 或者写个唯一标识比如时间戳 或者 UUID 或者订单消费者消费 MQ 中的消息也可利用 MQ 的该 id 来判断，或者可按自己的规则生成一个全局唯一 id，每次消费消息时用该 id 先判断该消息是否已消费过。\"]},{\"header\":\"4、消费端的幂等性保障\",\"slug\":\"_4、消费端的幂等性保障\",\"contents\":[\"在海量订单生成的业务高峰期，生产端有可能就会重复发生了消息，这时候消费端就要实现幂等性，这就意味着我们的消息永远不会被消费多次，即使我们收到了一样的消息。业界主流的幂等性有两种操作:\",\"唯一 ID+指纹码机制,利用数据库主键去重,\",\"利用 redis 的原子性去实现\"]},{\"header\":\"1、唯一 ID+指纹码机制\",\"slug\":\"_1、唯一-id-指纹码机制\",\"contents\":[\"指纹码:我们的一些规则或者时间戳加别的服务给到的唯一信息码,它并不一定是我们系统生成的，基本都是由我们的业务规则拼接而来，但是一定要保证唯一性，然后就利用查询语句进行判断这个 id 是否存在数据库中,优势就是实现简单就一个拼接，然后查询判断是否重复；劣势就是在高并发时，如果是单个数据库就会有写入性能瓶颈当然也可以采用分库分表提升性能，但也不是我们最推荐的方式。\"]},{\"header\":\"2、Redis 原子性\",\"slug\":\"_2、redis-原子性\",\"contents\":[\"利用 redis 执行 setnx 命令，天然具有幂等性。从而实现不重复消费\"]},{\"header\":\"2、优先级队列\",\"slug\":\"_2、优先级队列\",\"contents\":[]},{\"header\":\"1、使用场景\",\"slug\":\"_1、使用场景\",\"contents\":[\"在我们系统中有一个订单催付的场景，我们的客户在天猫下的订单,淘宝会及时将订单推送给我们，如果在用户设定的时间内未付款那么就会给用户推送一条短信提醒，很简单的一个功能对吧，但是，tmall商家对我们来说，肯定是要分大客户和小客户的对吧，比如像苹果，小米这样大商家一年起码能给我们创造很大的利润，所以理应当然，他们的订单必须得到优先处理，而曾经我们的后端系统是使用 redis 来存放的定时轮询，大家都知道 redis 只能用 List 做一个简简单单的消息队列，并不能实现一个优先级的场景，所以订单量大了后采用 RabbitMQ 进行改造和优化,如果发现是大客户的订单给一个相对比较高的优先级，否则就是默认优先级。\"]},{\"header\":\"2、如何添加\",\"slug\":\"_2、如何添加\",\"contents\":[]},{\"header\":\"1、控制台页面\",\"slug\":\"_1、控制台页面\",\"contents\":[\"添加一个x-max-priority\"]},{\"header\":\"2、队列中代码添加优先级\",\"slug\":\"_2、队列中代码添加优先级\",\"contents\":[\"Map<String, Object> params = new HashMap(); params.put(\\\"x-max-priority\\\", 10); channel.queueDeclare(\\\"hello\\\", true, false, false, params); \"]},{\"header\":\"3、消息中代码添加优先级\",\"slug\":\"_3、消息中代码添加优先级\",\"contents\":[\"AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().priority(5).build(); \",\"PS：要让队列实现优先级需要做的事情有如下事情:队列需要设置为优先级队列，消息需要设置消息的优先级，消费者需要等待消息已经发送到队列中才去消费因为，这样才有机会对消息进行排序\"]},{\"header\":\"3、模拟场景\",\"slug\":\"_3、模拟场景\",\"contents\":[]},{\"header\":\"1、消息生产者\",\"slug\":\"_1、消息生产者\",\"contents\":[\"public class Product { private static final String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { try(Channel channel = RabbitMQConfig.getChannel()){ //给消息赋予一个 priority 属性 AMQP.BasicProperties basicProperties = new AMQP.BasicProperties().builder().priority(5).build(); for (int i = 1; i < 11; i++) { String msg = \\\"info\\\" + i; if(i==5){ channel.basicPublish(\\\"\\\", QUEUE_NAME, basicProperties, msg.getBytes()); }else{ channel.basicPublish(\\\"\\\", QUEUE_NAME, null, msg.getBytes()); } System.out.println(\\\"发送消息完成:\\\" + msg); } } } } \"]},{\"header\":\"2、消费消费者\",\"slug\":\"_2、消费消费者\",\"contents\":[\"public class Consumer { private static final String QUEUE_NAME = \\\"hello\\\"; public static void main(String[] args) throws Exception { Channel channel = RabbitMQConfig.getChannel(); //设置队列的最大优先级 最大可以设置到 255 官网推荐 1-10 如果设置太高比较吃内存和 CPU Map<String, Object> map = new HashMap<>(); map.put(\\\"x-max-priority\\\", 10); channel.queueDeclare(QUEUE_NAME, true, false, false, map); System.out.println(\\\"消费者等待启动接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, delivery) ->{ String receivedMessage = new String(delivery.getBody()); System.out.println(\\\"接收到消息:\\\"+receivedMessage); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, (consumerTag) ->{ System.out.println(\\\"消费者无法消费消息时调用，如队列被删除\\\"); }); } } \"]},{\"header\":\"3、结果分析\",\"slug\":\"_3、结果分析\",\"contents\":[\"发送消息完成:info1 发送消息完成:info2 发送消息完成:info3 发送消息完成:info4 发送消息完成:info5 发送消息完成:info6 发送消息完成:info7 发送消息完成:info8消费者 发送消息完成:info9 发送消息完成:info10 -------------------------------------消费者接收消息----------------------------------------- 消费者等待启动接收消息...... 接收到消息:info5 接收到消息:info1 接收到消息:info2 接收到消息:info3 接收到消息:info4 接收到消息:info6 接收到消息:info7 接收到消息:info8 接收到消息:info9 接收到消息:info10 \"]},{\"header\":\"3、惰性队列\",\"slug\":\"_3、惰性队列\",\"contents\":[]},{\"header\":\"1、使用场景\",\"slug\":\"_1、使用场景-1\",\"contents\":[\"RabbitMQ 从 3.6.0 版本开始引入了惰性队列的概念。惰性队列会尽可能的将消息存入磁盘中，而在消费者消费到相应的消息时才会被加载到内存中，它的一个重要的设计目标是能够支持更长的队列，即支持更多的消息存储。当消费者由于各种各样的原因(比如消费者下线、宕机亦或者是由于维护而关闭等)而致使长时间内不能消费消息造成堆积时，惰性队列就很有必要了。\",\"默认情况下，当生产者将消息发送到 RabbitMQ 的时候，队列中的消息会尽可能的存储在内存之中，这样可以更加快速的将消息发送给消费者。即使是持久化的消息，在被写入磁盘的同时也会在内存中驻留一份备份。当 RabbitMQ 需要释放内存的时候，会将内存中的消息换页至磁盘中，这个操作会耗费较长的时间，也会阻塞队列的操作，进而无法接收新的消息。虽然 RabbitMQ 的开发者们一直在升级相关的算法，但是效果始终不太理想，尤其是在消息量特别大的时候。\"]},{\"header\":\"2、两种模式\",\"slug\":\"_2、两种模式\",\"contents\":[\"队列具备两种模式：\",\"default：默认的为 default 模式，在 3.6.0 之前的版本无需做任何变更\",\"lazy：lazy模式即为惰性队列的模式，可以通过调用 channel.queueDeclare 方法的时候在参数中设置，也可以通过Policy 的方式设置，如果一个队列同时使用这两种方式设置的话，那么 Policy 的方式具备更高的优先级。如果要通过声明的方式改变已有队列的模式的话，那么只能先删除队列，然后再重新声明一个新的\",\"在队列声明的时候可以通过“x-queue-mode”参数来设置队列的模式，取值为“default”和“lazy”。下面示例中演示了一个惰性队列的声明细节：\",\"Map<String, Object> args = new HashMap<String, Object>(); args.put(\\\"x-queue-mode\\\", \\\"lazy\\\"); channel.queueDeclare(\\\"myqueue\\\", false, false, false, args); \"]},{\"header\":\"3、场景模拟\",\"slug\":\"_3、场景模拟\",\"contents\":[]},{\"header\":\"1、消息生产者\",\"slug\":\"_1、消息生产者-1\",\"contents\":[\"public class Product { private static final String QUEUE_NAME = \\\"lazy\\\"; public static void main(String[] args) throws Exception { try(Channel channel = RabbitMQConfig.getChannel()){ for (int i = 1; i < 1000001; i++) { String msg = \\\"info\\\" + i; channel.basicPublish(\\\"\\\", QUEUE_NAME, null, msg.getBytes()); } System.out.println(\\\"发送消息完成:\\\"); } } } \"]},{\"header\":\"2、消息消费者\",\"slug\":\"_2、消息消费者\",\"contents\":[\"public class Consumer { private static final String QUEUE_NAME = \\\"lazy\\\"; public static void main(String[] args) throws Exception { Channel channel = RabbitMQConfig.getChannel(); //设置队列模式 Map<String, Object> map = new HashMap<>(); map.put(\\\"x-queue-mode\\\", \\\"lazy\\\"); channel.queueDeclare(QUEUE_NAME, true, false, false, null); System.out.println(\\\"消费者等待启动接收消息......\\\"); DeliverCallback deliverCallback = (consumerTag, delivery) ->{ String receivedMessage = new String(delivery.getBody()); System.out.println(\\\"接收到消息:\\\"+receivedMessage); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, (consumerTag) ->{ System.out.println(\\\"消费者无法消费消息时调用，如队列被删除\\\"); }); } } \"]},{\"header\":\"3、结果分析\",\"slug\":\"_3、结果分析-1\",\"contents\":[\"内存开销对比\",\"在发送 1 百万条消息，每条消息大概占 1KB 的情况下，普通队列占用内存是 1.2GB，而惰性队列仅仅占用 1.5MB\"]}]},\"/study-tutorial/distributed/rabbitmq/overview.html\":{\"title\":\"1、消息队列\",\"contents\":[{\"header\":\"1、 MQ 的相关概念\",\"slug\":\"_1、mq的相关概念\",\"contents\":[]},{\"header\":\"1、什么是MQ\",\"slug\":\"_1、什么是mq\",\"contents\":[\"MQ(message queue)，从字面意思上看，本质是个队列，FIFO 先入先出，只不过队列中存放的内容是message 而已，还是一种跨进程的通信机制，用于上下游传递消息。\",\"在互联网架构中，MQ 是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。使用了MQ之后，消息发送上游只需要依赖 MQ，不用依赖其他服务。\"]},{\"header\":\"2、为什么要使用MQ\",\"slug\":\"_2、为什么要使用mq\",\"contents\":[]},{\"header\":\"1、流量消峰\",\"slug\":\"_1、流量消峰\",\"contents\":[\"举个例子：如果订单系统最多能处理一万次订单，这个处理能力应付正常时段的下单时绰绰有余，正常时段我们下单一秒后就能返回结果。但是在高峰期，如果有两万次下单操作系统是处理不了的，只能限制订单超过一万后不允许用户下单。使用消息队列做缓冲，我们可以取消这个限制，把一秒内下的订单分散成一段时间来处理，这时有些用户可能在下单十几秒后才能收到下单成功的操作，但是比不能下单的体验要好。\"]},{\"header\":\"2、应用解耦\",\"slug\":\"_2、应用解耦\",\"contents\":[\"以电商应用为例，应用中有订单系统、库存系统、物流系统、支付系统。用户创建订单后，如果耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障，都会造成下单操作异常。当转变成基于消息队列的方式后，系统间调用的问题会减少很多，比如物流系统因为发生故障，需要几分钟来修复。在这几分钟的时间里，物流系统要处理的内存被缓存在消息队列中，用户的下单操作可以正常完成。当物流系统恢复后，继续处理订单信息即可，中单用户感受不到物流系统的故障，提升系统的可用性。\"]},{\"header\":\"3、异步处理\",\"slug\":\"_3、异步处理\",\"contents\":[\"有些服务间调用是异步的，例如 A 调用 B，B 需要花费很长时间执行，但是 A 需要知道 B 什么时候可以执行完，以前一般有两种方式：\",\"A 过一段时间去调用 B 的查询 api 查询\",\"A 提供一个 callback api， B 执行完之后调用 api 通知 A 服务。\",\"这两种方式都不是很优雅，使用消息总线，可以很方便解决这个问题，A 调用 B 服务后，只需要监听 B 处理完成的消息，当 B 处理完成后，会发送一条消息给 MQ，MQ 会将此消息转发给 A 服务。这样 A 服务既不用循环调用 B 的查询 api，也不用提供 callback api。同样 B 服务也不用做这些操作。A 服务还能及时的得到异步处理成功的消息。\"]},{\"header\":\"3、MQ的分类\",\"slug\":\"_3、mq的分类\",\"contents\":[]},{\"header\":\"1、 ActiveMQ\",\"slug\":\"_1、activemq\",\"contents\":[\"优点：单机吞吐量万级，时效性 ms 级，可用性高，基于主从架构实现高可用性，消息可靠性较低的概率丢失数据\",\"缺点：官方社区现在对 ActiveMQ 5.x 维护越来越少，高吞吐量场景较少使用。\"]},{\"header\":\"2、 Kafka\",\"slug\":\"_2、kafka\",\"contents\":[\"大数据的杀手锏，谈到大数据领域内的消息传输，则绕不开 Kafka，这款为大数据而生的消息中间件，以其百万级 TPS 的吞吐量名声大噪，迅速成为大数据领域的宠儿，在数据采集、传输、存储的过程中发挥着举足轻重的作用。目前已经被 LinkedIn，Uber, Twitter, Netflix 等大公司所采纳。\",\"优点：性能卓越，单机写入 TPS 约在百万条/秒，最大的优点，就是吞吐量高。时效性 ms 级可用性非常高，kafka 是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用,消费者采用 Pull 方式获取消息, 消息有序, 通过控制能够保证所有消息被消费且仅被消费一次;有优秀的第三方Kafka Web 管理界面 Kafka-Manager；在日志领域比较成熟，被多家公司和多个开源项目使用；功能支持：功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用.\",\"缺点：Kafka 单机超过 64 个队列/分区，Load 会发生明显的飙高现象，队列越多，load 越高，发送消息响应时间变长，使用短轮询方式，实时性取决于轮询间隔时间，消费失败不支持重试；支持消息顺序，但是一台代理宕机后，就会产生消息乱序，社区更新较慢；\"]},{\"header\":\"3、 RocketMQ\",\"slug\":\"_3、rocketmq\",\"contents\":[\"RocketMQ 出自阿里巴巴的开源产品，用 Java 语言实现，在设计时参考了 Kafka，并做出了自己的一些改进。被阿里巴巴广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理，binglog 分发等场景。\",\"优点:单机吞吐量十万级,可用性非常高，分布式架构,消息可以做到 0 丢失,MQ 功能较为完善，还是分布式的，扩展性好,支持 10 亿级别的消息堆积，不会因为堆积导致性能下降,源码是 java 我们可以自己阅读源码，定制自己公司的 MQ\",\"缺点：支持的客户端语言不多，目前是 java 及 c++，其中 c++不成熟；社区活跃度一般,没有在 MQ核心中去实现 JMS 等接口,有些系统要迁移需要修改大量代码\"]},{\"header\":\"4、 RabbitMQ\",\"slug\":\"_4、rabbitmq\",\"contents\":[\"2007 年发布，是一个在 AMQP(高级消息队列协议)基础上完成的，可复用的企业消息系统，是当前最主流的消息中间件之一。\",\"优点：由于 erlang 语言的高并发特性，性能较好；吞吐量到万级，MQ 功能比较完备,健壮、稳定、易用、跨平台、支持多种语言 如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持 AJAX 文档齐全；开源提供的管理界面非常棒，用起来很好用,社区活跃度高；更新频率相当高\",\"缺点：商业版需要收费,学习成本较高\"]},{\"header\":\"对比\",\"slug\":\"对比\",\"contents\":[\"特性\",\"ActiveMQ\",\"Kafka\",\"RocketMQ\",\"RabbitMQ\",\"单机吞吐量\",\"单机吞吐量万级，比 RocketMQ、Kafka 低一个数量级\",\"10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景\",\"10 万级，支撑高吞吐\",\"吞吐量到万级\",\"topic 数量对吞吐量的影响\",\"topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源\",\"topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic\",\"时效性\",\"ms级\",\"延迟在 ms 级以内\",\"ms 级\",\"微秒级，这是 RabbitMQ 的一大特点，延迟最低\",\"可用性\",\"高，基于主从架构实现高可用\",\"非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用\",\"非常高，分布式架构\",\"高，基于主从架构实现高可用\",\"消息可靠性\",\"有较低的概率丢失数据\",\"经过参数优化配置，可以做到 0 丢失\",\"经过参数优化配置，可以做到 0 丢失\",\"基本不丢\",\"功能支持\",\"MQ 领域的功能极其完备\",\"功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用\",\"MQ 功能较为完善，还是分布式的，扩展性好\",\"基于 erlang 开发，并发能力很强，性能极好，延时很低\"]},{\"header\":\"4、MQ的选择\",\"slug\":\"_4、mq的选择\",\"contents\":[\"对消息队列进行技术选型时，需要通过以下指标衡量你所选择的消息队列，是否可以满足你的需求：\",\"消息顺序：发送到队列的消息，消费时是否可以保证消费的顺序，比如A先下单，B后下单，应该是A先去扣库存，B再去扣，顺序不能反。\",\"消息路由：根据路由规则，只订阅匹配路由规则的消息，比如有A/B两者规则的消息，消费者可以只订阅A消息，B消息不会消费。\",\"消息可靠性：是否会存在丢消息的情况，比如有A/B两个消息，最后只有B消息能消费，A消息丢失。\",\"消息时序：主要包括“消息存活时间”和“延迟/预定的消息”，“消息存活时间”表示生产者可以对消息设置TTL，如果超过该TTL，消息会自动消失；“延迟/预定的消息”指的是可以延迟或者预订消费消息，比如延时5分钟，那么消息会5分钟后才能让消费者消费，时间未到的话，是不能消费的。\",\"消息留存：消息消费成功后，是否还会继续保留在消息队列。\",\"容错性：当一条消息消费失败后，是否有一些机制，保证这条消息是一种能成功，比如异步第三方退款消息，需要保证这条消息消费掉，才能确定给用户退款成功，所以必须保证这条消息消费成功的准确性。\",\"伸缩：当消息队列性能有问题，比如消费太慢，是否可以快速支持库容；当消费队列过多，浪费系统资源，是否可以支持缩容。\",\"吞吐量：支持的最高并发数\"]},{\"header\":\"1、 kafka\",\"slug\":\"_1、kafka\",\"contents\":[\"Kafka主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务。大型公司建议可以选用，如果有日志采集、实时计算功能，肯定是首选kafka。\"]},{\"header\":\"2、 RocketMQ\",\"slug\":\"_2、rocketmq\",\"contents\":[\"天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况。RoketMQ在稳定性上可能更值得信赖，这些业务场景在阿里双11已经经历了多次考验，如果你的业务有上述并发场景，建议可以选择RocketMQ。\"]},{\"header\":\"3、 RabbitMQ\",\"slug\":\"_3、rabbitmq\",\"contents\":[\"结合erlang语言本身的并发优势，性能好时效性微秒级，社区活跃度也比较高，管理界面用起来十分方便，如果你的数据量没有那么大，中小型公司优先选择功能比较完备的RabbitMQ。\"]},{\"header\":\"2、消息队列模式\",\"slug\":\"_2、消息队列模式\",\"contents\":[]},{\"header\":\"1、点对点模式\",\"slug\":\"_1、点对点模式\",\"contents\":[\"一个具体的消息只能由一个消费者消费，多个生产者可以向同一个消息队列发送消息，但是一个消息在被一个消息者处理的时候，这个消息在队列上会被锁住或者被移除并且其他消费者无法处理该消息。\",\"需要额外注意的是，如果消费者处理一个消息失败了，消息系统一般会把这个消息放回队列，这样其他消费者可以继续处理。\"]},{\"header\":\"2、发布/订阅模式\",\"slug\":\"_2、发布-订阅模式\",\"contents\":[\"单个消息可以被多个订阅者并发的获取和处理。一般来说，订阅有两种类型：\",\"临时（ephemeral）订阅：这种订阅只有在消费者启动并且运行的时候才存在。一旦消费者退出，相应的订阅以及尚未处理的消息就会丢失。\",\"持久（durable）订阅：这种订阅会一直存在，除非主动去删除。消费者退出后，消息系统会继续维护该订阅，并且后续消息可以被继续处理。\"]},{\"header\":\"3、 RabbitMQ\",\"slug\":\"_3、rabbitmq-1\",\"contents\":[]},{\"header\":\"1、什么是 RabbitMQ\",\"slug\":\"_1、什么是rabbitmq\",\"contents\":[\"RabbitMQ是一个消息中间件：它接受并转发消息。你可以把它当做一个快递站点，当你要发送一个包裹时，你把你的包裹放到快递站，快递员最终会把你的快递送到收件人那里，按照这种逻辑RabbitMQ是一个快递站，一个快递员帮你传递快件。RabbitMQ与快递站的主要区别在于，它不处理快件而是接收，存储和转发消息数据。\"]},{\"header\":\"2、四大核心概念\",\"slug\":\"_2、四大核心概念\",\"contents\":[]},{\"header\":\"1、生产者\",\"slug\":\"_1、生产者\",\"contents\":[\"产生数据发送消息的程序是生产者\"]},{\"header\":\"2、交换机\",\"slug\":\"_2、交换机\",\"contents\":[\"交换机是RabbitMQ非常重要的一个部件，一方面它接收来自生产者的消息，另一方面它将消息推送到队列中。交换机必须确切知道如何处理它接收到的消息，是将这些消息推送到特定队列还是推送到多个队列，亦或者是把消息丢弃，这个得有交换机类型决定。\"]},{\"header\":\"3、队列\",\"slug\":\"_3、队列\",\"contents\":[\"队列是RabbitMQ内部使用的一种数据结构，尽管消息流经RabbitMQ和应用程序，但它们只能存储在队列中。队列仅受主机的内存和磁盘限制的约束，本质上是一个大的消息缓冲区。许多生产者可以将消息发送到一个队列，许多消费者可以尝试从一个队列接收数据。这就是我们使用队列的方式\"]},{\"header\":\"4、消费者\",\"slug\":\"_4、消费者\",\"contents\":[\"消费与接收具有相似的含义。消费者大多时候是一个等待接收消息的程序。请注意生产者，消费者和消息中间件很多时候并不在同一机器上。同一个应用程序既可以是生产者又是可以是消费者。\"]},{\"header\":\"3、基本概念\",\"slug\":\"_3、基本概念\",\"contents\":[\"提到RabbitMQ，就不得不提AMQP协议。AMQP协议是具有现代特征的二进制协议。是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。\",\"先了解一下AMQP协议中间的几个重要概念：\",\"Server：接收客户端的连接，实现AMQP实体服务。\",\"Connection：连接，应用程序与Server的网络连接，TCP连接。\",\"Channel：信道，消息读写等操作在信道中进行。客户端可以建立多个信道，每个信道代表一个会话任务。如果每一次访问 RabbitMQ 都建立一个 Connection，在消息量大的时候建立 TCP Connection 的开销将是巨大的，效率也较低。Channel 是在 connection 内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread 创建单独的 channel 进行通讯，AMQP method 包含了 channel id 帮助客户端和 message broker 识别 channel，所以 channel 之间是完全隔离的。Channel 作为轻量级的\",\"Connection极大减少了操作系统建立TCP connection的开销\",\"Message：消息，应用程序和服务器之间传送的数据，消息可以非常简单，也可以很复杂。由Properties和Body组成。Properties为外包装，可以对消息进行修饰，比如消息的优先级、延迟等高级特性；Body就是消息体内容。\",\"Virtual Host：虚拟主机，用于逻辑隔离。一个虚拟主机里面可以有若干个Exchange和Queue，同一个虚拟主机里面不能有相同名称的Exchange或Queue。\",\"Exchange：交换器，接收消息，按照路由规则将消息路由到一个或者多个队列。如果路由不到，或者返回给生产者，或者直接丢弃。RabbitMQ常用的交换器常用类型有direct、topic、fanout、headers四种，后面详细介绍。\",\"Binding：绑定，交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个RoutingKey，Binding 信息被保存到 exchange 中的查询表中，用于 message 的分发依据\",\"RoutingKey：路由键，生产者将消息发送给交换器的时候，会发送一个RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。路由键通常为一个“.”分割的字符串，例如“com.rabbitmq”。\",\"Queue：消息队列，用来保存消息，供消费者消费。\"]},{\"header\":\"4、工作原理\",\"slug\":\"_4、工作原理\",\"contents\":[\"不得不看一下经典的图了，如下👇\",\"AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下：\",\"生产者是连接到 Server，建立一个连接，开启一个信道。\",\"生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。\",\"消费者也需要进行建立连接，开启信道等操作，便于接收消息。\",\"生产者发送消息，发送到服务端中的虚拟主机。\",\"虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。\",\"订阅了消息队列的消费者就可以获取到消息，进行消费。\"]},{\"header\":\"5、环境搭建\",\"slug\":\"_5、环境搭建\",\"contents\":[\"我使用的Linux搭建，搭建流程可参考官网：https://www.rabbitmq.com/install-rpm.html\",\"查看系统版本号，Rabbitmq 对 Erlang 有版本要求，不能使用太旧的Erlang版本：https://www.rabbitmq.com/which-erlang.html\",\"Erlang下载地址：https://github.com/rabbitmq/erlang-rpm/releases\"]},{\"header\":\"1、下载rpm包\",\"slug\":\"_1、下载rpm包\",\"contents\":[\"erlang-23.3.4.8-1.el7.x86_64.rpm\",\"rabbitmq-server-3.10.5-1.el8.noarch.rpm\"]},{\"header\":\"2、上传至服务器\",\"slug\":\"_2、上传至服务器\",\"contents\":[\"#创建存放目录 mkdir -p /usr/rabbitmq \"]},{\"header\":\"3、安装文件\",\"slug\":\"_3、安装文件\",\"contents\":[\"[root@xiaobear rabbitmq]## rpm -Uvih erlang-25.0.2-1.el8.x86_64.rpm 警告：erlang-25.0.2-1.el8.x86_64.rpm: 头V4 RSA/SHA256 Signature, 密钥 ID cc4bbe5b: NOKEY 错误：依赖检测失败： libcrypto.so.1.1()(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libcrypto.so.1.1(OPENSSL_1_1_0)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libcrypto.so.1.1(OPENSSL_1_1_1)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libstdc++.so.6(CXXABI_1.3.9)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libtinfo.so.6()(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 libz.so.1(ZLIB_1.2.7.1)(64bit) 被 erlang-25.0.2-1.el8.x86_64 需要 \",\"PS：这是因为下载版本的问题，el8下载的是8的版本，改回下载7的就可以了\",\"我下载的版本：erlang-23.3.4.8-1.el7.x86_64.rpm\",\"[root@xiaobear rabbitmq]## rpm -Uivh erlang-23.3.4.8-1.el7.x86_64.rpm 警告：erlang-23.3.4.8-1.el7.x86_64.rpm: 头V4 RSA/SHA256 Signature, 密钥 ID cc4bbe5b: NOKEY 准备中... ################################## [100%] 正在升级/安装... 1:erlang-23.3.4.8-1.el7 ################################## [100%] [root@xiaobear rabbitmq]## yum install socat -y \",\"查看版本\",\"## 测试 erl -version \",\"rabbitmq 在安装过程中需要依赖socat这个插件，需要先安装\",\"[root@xiaobear rabbitmq]## rpm -ivh rabbitmq-server-3.10.5-1.el8.noarch.rpm 警告：rabbitmq-server-3.10.5-1.el8.noarch.rpm: 头V4 RSA/SHA512 Signature, 密钥 ID 6026dfca: NOKEY 准备中... ################################## [100%] 正在升级/安装... 1:rabbitmq-server-3.10.5-1.el8 ################################## [100%] [root@xiaobear rabbitmq]## \"]},{\"header\":\"4、常用命令\",\"slug\":\"_4、常用命令\",\"contents\":[\"#开机启动 chkconfig rabbitmq-server on会转发到下面命令 systemctl enable rabbitmq-server.service ## 启动服务 systemctl start rabbitmq-server ## 查看服务状态，running表示启动成功 systemctl status rabbitmq-server.service ## 开机自启动 systemctl enable rabbitmq-server ## 停止服务 systemctl stop rabbitmq-server \"]},{\"header\":\"5、安装Web管理插件\",\"slug\":\"_5、安装web管理插件\",\"contents\":[\"rabbitmq-plugins enable rabbitmq_management #安装完成后，重启服务 systemctl restart rabbitmq-server \"]},{\"header\":\"6、访问web页面\",\"slug\":\"_6、访问web页面\",\"contents\":[\"访问地址：服务器 IP+端口号（默认15672）,若没有反应，请开放端口，执行下面命令\",\"## 防火墙开放15672端口 firewall-cmd --zone=public --add-port=15672/tcp --permanent firewall-cmd --reload \",\"注意：\",\"在对应服务器（阿里云，腾讯云等）的安全组中开放15672端口（rabbitmq默认端口号），5672端口后续程序需要使用也要开放\",\"rabbitmq有一个默认账号和密码都是：guest默认情况只能在 localhost本计下访问，所以需要添加一个远程登录的用户\"]},{\"header\":\"7、添加用户\",\"slug\":\"_7、添加用户\",\"contents\":[\"创建账号\",\"rabbitmqctl add_user admin admin123 \",\"分配角色\",\"rabbitmqctl set_user_tags admin administrator \",\"用户操作权限分四种级别：\",\"administrator：可以登录控制台、查看所有信息、可以对 rabbitmq进行管理\",\"monitoring：监控者 登录控制台，查看所有信息\",\"policymaker：策略制定者 登录控制台，指定策略\",\"managment 普通管理员 登录控制台\",\"设置权限\",\"set_permissions [-p <vhostpath>] <user> <conf> <write> <read> \",\"用户 user_admin 具有/vhost1 这个 virtual host 中所有资源的配置、写、读权限当前用户和角色\",\"rabbitmqctl set_permissions -p \\\"/\\\" admin \\\".*\\\" \\\".*\\\" \\\".*\\\" \",\"再次访问登录，即可成功\"]},{\"header\":\"8、重置命令\",\"slug\":\"_8、重置命令\",\"contents\":[\"#关闭应用的命令 rabbitmqctl stop_app #清除的命令 rabbitmqctl reset #重新启动命令 rabbitmqctl start_app \"]},{\"header\":\"9、其他命令\",\"slug\":\"_9、其他命令\",\"contents\":[\"## 添加账号、密码 rabbitmqctl add_user ## 设置账号为管理员 rabbitmqctl set_user_tags 账号 administrator ## 修改账号密码 rabbitmqctl change_password Username Newpassword ## 查看用户清单 rabbitmqctl list_users ## 添加账号查看资源的权限 rabbitmqctl set_permissions -p / 用户名 \\\".*\\\"\\\".*\\\"\\\".*\\\" \"]}]},\"/study-tutorial/distributed/rabbitmq/publish_confirmation.html\":{\"title\":\"4、发布确认\",\"contents\":[{\"header\":\"1、原理\",\"slug\":\"_1、原理\",\"contents\":[\"生产者将信道设置成 confirm 模式，一旦信道进入 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，一旦消息被投递到所有匹配的队列之后，broker就会发送一个确认给生产者(包含消息的唯一 ID)，这就使得生产者知道消息已经正确到达目的队列了，如果消息和队列是可持久化的，那么确认消息会在将消息写入磁盘之后发出，broker 回传给生产者的确认消息中 delivery-tag 域包含了确认消息的序列号，此外 broker 也可以设置basic.ack 的 multiple 域，表示到这个序列号之前的所有消息都已经得到了处理。\",\"confirm 模式最大的好处在于它是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果 RabbitMQ 因为自身内部错误导致消息丢失，就会发送一条 nack 消息，生产者应用程序同样可以在回调方法中处理该 nack 消息。\"]},{\"header\":\"2、发布确认策略\",\"slug\":\"_2、发布确认策略\",\"contents\":[]},{\"header\":\"1、开启发布确认\",\"slug\":\"_1、开启发布确认\",\"contents\":[\"发布者确认是 AMQP 0.9.1 协议的RabbitMQ扩展，因此默认情况下不启用它们。使用confirmSelect方法在通道级别启用发布者确认\",\"Channel channel = connection.createChannel(); channel.confirmSelect(); \"]},{\"header\":\"2、单独发布消息(Publishing Messages Individually)\",\"slug\":\"_2、单独发布消息-publishing-messages-individually\",\"contents\":[\"这是一种简单的确认方式，它是一种同步确认发布的方式，也就是发布一个消息之后只有它被确认发布，后续的消息才能继续发布，waitForConfirmsOrDie(long)这个方法只有在消息被确认的时候才返回，如果在指定时间范围内这个消息没有被确认那么它将抛出异常。\",\"这种确认方式有一个最大的缺点就是：**发布速度特别的慢，**因为如果没有确认发布的消息就会阻塞所有后续消息的发布，这种方式最多提供每秒不超过数百条发布消息的吞吐量。当然对于某些应用程序来说这可能已经足够了。\",\"public class PublishingMessagesIndividually { public static void main(String[] args) { try(Channel channel = RabbitMQConfig.getChannel()) { //随机生成队列 String queryName = UUID.randomUUID().toString(); //生成队列 不持久化 不共享 不删除 参数为空 channel.queueDeclare(queryName, false, false, false, null); //开启发布确认 channel.confirmSelect(); long startTime = System.nanoTime(); //消息数量为10 for (int i = 0; i < 10; i++) { String message = i + \\\"\\\"; channel.basicPublish(\\\"\\\", queryName, null, message.getBytes()); if (channel.waitForConfirms()) { System.out.println(\\\"消息发送成功\\\"); } } long endTime = System.nanoTime(); System.out.println(\\\"发布10\\\" + \\\"个单独确认消息,耗时\\\" + (endTime - startTime) + \\\"ms\\\"); } catch (Exception e) { e.printStackTrace(); } } } \",\"消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 消息发送成功 发布10个单独确认消息,耗时18288200ms \"]},{\"header\":\"3、批量确认发布(Publishing Messages in Batches)\",\"slug\":\"_3、批量确认发布-publishing-messages-in-batches\",\"contents\":[\"上面那种方式非常慢，与单个等待确认消息相比，先发布一批消息然后一起确认可以极大地提高吞吐量，当然这种方式的缺点就是:当发生故障导致发布出现问题时，不知道是哪个消息出现问题了，我们必须将整个批处理保存在内存中，以记录重要的信息而后重新发布消息。当然这种方案仍然是同步的，也一样阻塞消息的发布。\",\"public class BatchesPublishingMessages { public static void main(String[] args) { try(Channel channel = RabbitMQConfig.getChannel()) { String queueName = UUID.randomUUID().toString(); channel.queueDeclare(queueName, false, false, false, null); //开启发布确认 channel.confirmSelect(); //批量确认消息大小 int msgSize = 100; //初始化未确认消息个数 int outstandingMessageCount = 0; long start = System.currentTimeMillis(); for (int i = 0; i < 10; i++) { String message = i + \\\"\\\"; channel.basicPublish(\\\"\\\", queueName, null, message.getBytes()); outstandingMessageCount++; if (outstandingMessageCount == msgSize) { channel.waitForConfirms(); outstandingMessageCount = 0; } //为了确保还有剩余没有确认消息 再次确认 if (outstandingMessageCount > 0) { channel.waitForConfirms(); } } long end = System.currentTimeMillis(); System.out.println(\\\"发布10\\\" + \\\"个批量确认消息,耗时\\\" + (end - start) + \\\"ms\\\"); } catch (Exception e) { e.printStackTrace(); } } } \",\"发布10个批量确认消息,耗时6ms \"]},{\"header\":\"4、异步发布确认(Handling Publisher Confirms Asynchronously)\",\"slug\":\"_4、异步发布确认-handling-publisher-confirms-asynchronously\",\"contents\":[\"异步确认虽然编程逻辑比上两个要复杂，但是性价比最高，无论是可靠性还是效率都没得说，他是利用回调函数来达到消息可靠性传递的，这个中间件也是通过函数回调来保证是否投递成功，下面就让我们来详细讲解异步确认是怎么实现的？\",\"public class AsyncHandlingPublisherConfirms { public static void main(String[] args) throws Exception { asyncPublishMessage(); } public static void asyncPublishMessage() throws Exception{ try(Channel channel = RabbitMQConfig.getChannel()) { String queueName = UUID.randomUUID().toString(); channel.queueDeclare(queueName, false, false, false, null); //开启发布确认 channel.confirmSelect(); /** * 线程安全有序的一个哈希表，适用于高并发的情况 * 1.轻松的将序号与消息进行关联 * 2.轻松批量删除条目 只要给到序列号 * 3.支持并发访问 */ ConcurrentSkipListMap<Long, String> outstandingConfirms = new ConcurrentSkipListMap<>(); ConfirmCallback callback = (sequenceNumber, multiple) ->{ if (multiple){ //返回的是小于等于当前序列号的未确认消息 是一个 map ConcurrentNavigableMap<Long, String> confirmed = outstandingConfirms.headMap( sequenceNumber, true ); //清除该部分未确认消息 confirmed.clear(); }else { //只清除当前序列号的消息 outstandingConfirms.remove(sequenceNumber); } }; ConfirmCallback nackCallback = (sequenceNumber, multiple) -> { String message = outstandingConfirms.get(sequenceNumber); System.out.println(\\\"发布的消息\\\"+message+\\\"未被确认，序列号\\\"+sequenceNumber); }; //接收消息回调 未接收消息回调 channel.addConfirmListener(callback, null); long begin = System.currentTimeMillis(); for (int i = 0; i < 10; i++) { String message = \\\"消息\\\" + i; /** * channel.getNextPublishSeqNo()获取下一个消息的序列号 * 通过序列号与消息体进行一个关联 * 全部都是未确认的消息体 */ outstandingConfirms.put(channel.getNextPublishSeqNo(), message); channel.basicPublish(\\\"\\\", queueName, null, message.getBytes()); } long end = System.currentTimeMillis(); System.out.println(\\\"发布10\\\" + \\\"个异步确认消息,耗时\\\" + (end - begin) + \\\"ms\\\"); } } } \",\"发布10个异步确认消息,耗时4ms \"]},{\"header\":\"5、如何处理异步未确认消息\",\"slug\":\"_5、如何处理异步未确认消息\",\"contents\":[\"最好的解决的解决方案就是把未确认的消息放到一个基于内存的能被发布线程访问的队列，比如说用 ConcurrentLinkedQueue 这个队列在 confirm callbacks 与发布线程之间进行消息的传递。\"]},{\"header\":\"6、对比\",\"slug\":\"_6、对比\",\"contents\":[\"类型\",\"优点\",\"缺点\",\"单独发布确认\",\"同步等待确认，简单\",\"吞吐量有限，100左右\",\"批量发布确认\",\"批量同步等待确认，简单，合理的吞吐量\",\"一旦出现问题但很难推断出是那条消息出现了问题\",\"异步发布确认\",\"最佳性能和资源使用，在出现错误的情况下可以很好地控制\",\"实现起来稍微难些\",\"1000个消息的执行时间对比 发布1000个单独确认消息,耗时561ms 发布1000个批量确认消息,耗时558ms 发布1000个异步确认消息,耗时42ms \"]}]},\"/study-tutorial/distributed/rabbitmq/work_queues.html\":{\"title\":\"3、Work Queues(工作队列)\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"The main idea behind Work Queues (aka: Task Queues) is to avoid doing a resource-intensive task immediately and having to wait for it to complete. Instead we schedule the task to be done later. We encapsulate a task as a message and send it to a queue. A worker process running in the background will pop the tasks and eventually execute the job. When you run many workers the tasks will be shared between them.\",\"工作队列(又称任务队列)的主要思想是避免立即执行资源密集型任务，而不得不等待它完成。相反我们安排任务在之后执行。我们把任务封装为消息并将其发送到队列。在后台运行的工作进程将弹出任务并最终执行作业。当有多个工作线程时，这些工作线程将一起处理这些任务。\"]},{\"header\":\"1、轮训分发消息\",\"slug\":\"_1、轮训分发消息\",\"contents\":[\"工作线程接收消息，采用轮询接收，三个线程中只有一个能接收到\",\"案例：启动两个线程，一个线程发送消息，看看他们是如何工作的？\"]},{\"header\":\"1、抽取工具类\",\"slug\":\"_1、抽取工具类\",\"contents\":[\"public class RabbitMQConfig { /** * rabbitmq连接信息 * @return */ public static ConnectionFactory connectRabbitMq(){ //创建连接 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\\\"192.168.130.134\\\"); factory.setUsername(\\\"admin\\\"); factory.setPassword(\\\"admin123\\\"); return factory; } /** * 得到一个连接的 channel * @return */ public static Channel getChannel() throws Exception { //创建连接 ConnectionFactory factory = connectRabbitMq(); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); return channel; } } \"]},{\"header\":\"2、启动两个线程\",\"slug\":\"_2、启动两个线程\",\"contents\":[]},{\"header\":\"1、消费者\",\"slug\":\"_1、消费者\",\"contents\":[\"public class Work01 { private static final String QUEUE_NAME = \\\"workQueues\\\"; public static void main(String[] args) throws Exception { //获取信道 Channel channel = RabbitMQConfig.getChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); //消息的接收 DeliverCallback deliverCallback = (custom, deliver) ->{ System.out.println(\\\"接收的消息：\\\" + new String(deliver.getBody())); }; //消息被取消的回调 CancelCallback callback = (item) ->{ System.out.println(\\\"取消的消息为：\\\" + item); }; /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 false 手动应答 * 3.消费者未成功消费的回调 */ System.out.println(\\\"C2_等待接收消息......\\\"); channel.basicConsume(QUEUE_NAME, true, deliverCallback, callback); } } \"]},{\"header\":\"2、生产者\",\"slug\":\"_2、生产者\",\"contents\":[\"public class Task01 { private static final String QUEUE_NAME = \\\"workQueues\\\"; public static void main(String[] args) throws Exception{ try(Channel channel = RabbitMQConfig.getChannel()){ /** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 true 可以多个消费者消费 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); Scanner sc = new Scanner(System.in); while (sc.hasNext()){ String next = sc.next(); /** * 发送一个消息 * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */ channel.basicPublish(\\\"\\\", QUEUE_NAME, null, next.getBytes()); System.out.println(\\\"发送消息完成->\\\" + next); } } } } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"启动消费者，然后勾选all ... instance，允许多个实例\"]},{\"header\":\"4、测试结果\",\"slug\":\"_4、测试结果\",\"contents\":[\"通过程序执行发现生产者总共发送 4 个消息，消费者 1 和消费者 2 分别分得两个消息，并且是按照有序的一个接收一次消息\"]},{\"header\":\"2、消息应答\",\"slug\":\"_2、消息应答\",\"contents\":[]},{\"header\":\"1、概念\",\"slug\":\"_1、概念\",\"contents\":[\"消费者完成一个任务可能需要一段时间，如果其中一个消费者处理一个长的任务并且只完成了部分突然它挂掉了，会发生什么情况？\",\"RabbitMQ 一旦向消费者传递了一条消息，便立即将该消息标记为删除。在这种情况下，突然有个消费者挂掉了，我们将丢失正在处理的消息。以及后续发送给该消费这的消息，因为它无法接收到。\",\"为了保证消息在发送过程中不丢失，rabbitmq 引入消息应答机制，消息应答就是:消费者在接收到消息并且处理该消息之后，告诉 rabbitmq 它已经处理了，rabbitmq 可以把该消息删除了。\"]},{\"header\":\"2、自动应答\",\"slug\":\"_2、自动应答\",\"contents\":[\"消息发送后立即被认为已经传送成功，这种模式需要在高吞吐量和数据传输安全性方面做权衡,因为这种模式如果消息在接收到之\",\"前，消费者那边出现连接或者 channel 关闭，那么消息就丢失了，当然另一方面这种模式消费者那边可以传递过载的消息，没有对\",\"传递的消息数量进行限制，当然这样有可能使得消费者这边由于接收太多还来不及处理的消息，导致这些消息的积压，最终使\",\"得内存耗尽，最终这些消费者线程被操作系统杀死，所以这种模式仅适用在消费者可以高效并以某种速率能够处理这些消息的情况下使用。\"]},{\"header\":\"3、消息应答的方法\",\"slug\":\"_3、消息应答的方法\",\"contents\":[\"Channel.basicAck (用于肯定确认)\",\"RabbitMQ 已知道该消息并且成功的处理消息，可以将其丢弃了 \",\"Channel.basicNack(用于否定确认)\",\"Channel.basicReject (用于否定确认)\",\"与 Channel.basicNack 相比少一个参数 不处理该消息了直接拒绝，可以将其丢弃了 \"]},{\"header\":\"4、 Multiple\",\"slug\":\"_4、multiple\",\"contents\":[\"手动应答的好处是可以批量应答并且减少网络拥堵\",\"//源码 public void basicAck(long deliveryTag, boolean multiple) throws IOException { this.delegate.basicAck(deliveryTag, multiple); } \",\"multiple 的 true 和 false 代表不同意思\",\"true 代表批量应答 channel 上未应答的消息\",\"比如说 channel 上有传送 tag 的消息 5,6,7,8 当前 tag 是 8 那么此时5-8 的这些还未应答的消息都会被确认收到消息应答\",\"false 同上面相比\",\"只会应答 tag=8 的消息 5,6,7 这三个消息依然不会被确认收到消息应答\"]},{\"header\":\"5、消息自动重新入队\",\"slug\":\"_5、消息自动重新入队\",\"contents\":[\"如果消费者由于某些原因失去连接(其通道已关闭，连接已关闭或 TCP 连接丢失)，导致消息未发送 ACK 确认，RabbitMQ 将了解到消息未完全处理，并将对其重新排队。如果此时其他消费者可以处理，它将很快将其重新分发给另一个消费者。这样，即使某个消费者偶尔死亡，也可以确保不会丢失任何消息。\"]},{\"header\":\"6、手动应答实现\",\"slug\":\"_6、手动应答实现\",\"contents\":[\"默认消息采用的是自动应答，所以我们要想实现消息消费过程中不丢失，需要把自动应答改为手动应答\",\"//最主要的变化为如下 channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); //将自定应答关闭 boolean autoAck = false; channel.basicConsume(TASK_QUEUE_NAME, autoAck, deliverCallback, consumerTag -> { }); \",\"//官网案例 channel.basicQos(1); // accept only one unack-ed message at a time (see below) DeliverCallback deliverCallback = (consumerTag, delivery) -> { String message = new String(delivery.getBody(), \\\"UTF-8\\\"); System.out.println(\\\" [x] Received '\\\" + message + \\\"'\\\"); try { doWork(message); } finally { System.out.println(\\\" [x] Done\\\"); channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); } }; boolean autoAck = false; channel.basicConsume(TASK_QUEUE_NAME, autoAck, deliverCallback, consumerTag -> { }); \"]},{\"header\":\"案例实现\",\"slug\":\"案例实现\",\"contents\":[\"采用一个生产者和两个消费者，消费者利用线程睡眠，来查看效果\",\"生产者\",\"public class Task02 { /** 手动应答队列 */ public static final String ANSWER_MANUALLY = \\\"answer manually\\\"; public static void main(String[] args) throws Exception{ //获取信道 try(Channel channel = RabbitMQConfig.getChannel()){ //生成队列 不持久化 不共享 不删除 参数为空 channel.queueDeclare(ANSWER_MANUALLY, false, false, false, null); Scanner scanner = new Scanner(System.in); while (scanner.hasNext()){ String message = scanner.nextLine(); channel.basicPublish(\\\"\\\", ANSWER_MANUALLY, null, message.getBytes(StandardCharsets.UTF_8)); System.out.println(\\\"生产者发出消息为：\\\" + message); } } } } \",\"消费者\",\"public class Work03 { /** 手动应答队列 */ public static final String ANSWER_MANUALLY = \\\"answer manually\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); System.out.println(\\\"Work03等待的时间为1000ms\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ String message = new String(deliver.getBody()); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\\\"接收到的消息为：\\\" + message); //消息标记Tag 不批量显示应答消息 channel.basicAck(deliver.getEnvelope().getDeliveryTag(), false); }; //采用手动应答 boolean autoAck = false; channel.basicConsume(ANSWER_MANUALLY, autoAck, deliverCallback, (consumerTag) ->{ System.out.println(consumerTag + \\\"消费者取消接收消息回调逻辑\\\"); }); } } \",\"public class Work04 { /** 手动应答队列 */ public static final String ANSWER_MANUALLY = \\\"answer manually\\\"; public static void main(String[] args) throws Exception{ Channel channel = RabbitMQConfig.getChannel(); System.out.println(\\\"Work04等待的时间较长\\\"); DeliverCallback deliverCallback = (consumerTag, deliver) ->{ String message = new String(deliver.getBody()); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\\\"接收到的消息为：\\\" + message); //消息标记Tag 不批量显示应答消息 channel.basicAck(deliver.getEnvelope().getDeliveryTag(), false); }; //采用手动应答 boolean autoAck = false; channel.basicConsume(ANSWER_MANUALLY, autoAck, deliverCallback, (consumerTag) ->{ System.out.println(consumerTag + \\\"消费者取消接收消息回调逻辑\\\"); }); } } \"]},{\"header\":\"结果显示\",\"slug\":\"结果显示\",\"contents\":[\"消息发送方发送两个消息 work03 和 work04 分别接收到消息并进行处理\"]},{\"header\":\"3、持久化\",\"slug\":\"_3、持久化\",\"contents\":[]},{\"header\":\"1、概念\",\"slug\":\"_1、概念-1\",\"contents\":[\"刚刚我们已经看到了如何处理任务不丢失的情况，但是如何保障当 RabbitMQ 服务停掉以后消息生产者发送过来的消息不丢失。默认情况下 RabbitMQ 退出或由于某种原因崩溃时，它忽视队列和消息，除非告知它不要这样做。确保消息不会丢失需要做两件事：我们需要将队列和消息都标记为持久化。\"]},{\"header\":\"2、队列如何持久化\",\"slug\":\"_2、队列如何持久化\",\"contents\":[\"之前我们创建的队列都是非持久化的，rabbitmq 如果重启的化，该队列就会被删除掉，如果要队列实现持久化 需要在声明队列的时候把 durable 参数设置为持久化。\",\"翻出之前的设置，第二个参数标记是否为持久化\",\"/** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 true 可以多个消费者消费 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); \",\"将上面的Task02改为true之后，启动报错了\",\"Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for queue 'answer manually' in vhost '/': received 'true' but current is 'false', class-id=50, method-id=10) \",\"PS：需要注意的就是如果之前声明的队列不是持久化的，需要把原先队列先删除，或者重新创建一个持久化的队列，不然就会出现错误\"]},{\"header\":\"3、消息持久化\",\"slug\":\"_3、消息持久化\",\"contents\":[\"消息持久化需要增加属性MessageProperties.PERSISTENT_TEXT_PLAIN\",\"import com.rabbitmq.client.MessageProperties; channel.basicPublish(\\\"\\\", \\\"task_queue\\\", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes()); \",\"将消息标记为持久化并不能完全保证不会丢失消息。尽管它告诉 RabbitMQ 将消息保存到磁盘，但是这里依然存在当消息刚准备存储在磁\",\"盘的时候 但是还没有存储完，消息还在缓存的一个间隔点。此时并没有真正写入磁盘。持久性保证并不强，但是对于我们的简单任务队\",\"列而言，这已经绰绰有余了。\"]},{\"header\":\"4、不公平分发\",\"slug\":\"_4、不公平分发\",\"contents\":[\"在最开始的时候我们学习到 RabbitMQ 分发消息采用的轮训分发，但是在某种场景下这种策略并不是很好，比方说有两个消费者在处理任务，其中有个消费者 1 处理任务的速度非常快，而另外一个消费者 2 处理速度却很慢，这个时候我们还是采用轮训分发的化就会到这处理速度快的这个消费者很大一部分时间处于空闲状态，而处理慢的那个消费者一直在干活，这种分配方式在这种情况下其实就不太好，但是RabbitMQ 并不知道这种情况它依然很公平的进行分发。\",\"为了避免这种情况，我们可以设置参数 channel.basicQos(1);\",\"int prefetchCount = 1 ; channel.basicQos(prefetchCount) \",\"意思就是如果这个任务我还没有处理完或者我还没有应答你，你先别分配给我，我目前只能处理一个任务，然后 rabbitmq 就会把该任务分配给没有那么忙的那个空闲消费者，当然如果所有的消费者都没有完成手上任务，队列还在不停的添加新任务，队列有可能就会遇到队列被撑满的情况，这个时候就只能添加新的 worker 或者改变其他存储任务的策略。\"]},{\"header\":\"5、预取值\",\"slug\":\"_5、预取值\",\"contents\":[\"本身消息的发送就是异步发送的，所以在任何时候，channel 上肯定不止只有一个消息另外来自消费者的手动确认本质上也是异步的。因此这里就存在一个未确认的消息缓冲区，因此希望开发人员能限制此缓冲区的大小，以避免缓冲区里面无限制的未确认消息问题。\",\"这个时候就可以通过使用 basic.qos 方法设置“预取计数”值来完成的。该值定义通道上允许的未确认消息的最大数量。一旦数量达到配置的数量，RabbitMQ 将停止在通道上传递更多消息，除非至少有一个未处理的消息被确认，例如，假设在通道上有未确认的消息 5、6、7，8，并且通道的预取计数设置为 4，此时 RabbitMQ 将不会在该通道上再传递任何消息，除非至少有一个未应答的消息被 ack。比方说 tag=6 这个消息刚刚被确认 ACK，RabbitMQ 将会感知这个情况到并再发送一条消息。消息应答和 QoS 预取值对用户吞吐量有重大影响。\",\"通常，增加预取将提高向消费者传递消息的速度。**虽然自动应答传输消息速率是最佳的，但是，在这种情况下已传递但尚未处理****的消息的数量也会增加，从而增加了消费者的 RAM消耗(随机存取存储器)应该小心使用具有无限预处理的自动确认模式或手动确认模式，消费者消费了大量的消息如果没有确认的话，会导致消费者连接节点的内存消耗变大，所以找到合适的预取值是一个反复试验的过程，不同的负载该值取值也不同 100 到 300 范围内的值通常可提供最佳的吞吐量，并且不会给消费者带来太大的风险。预取值为 1 是最保守的。当然这将使吞吐量变得很低，特别是消费者连接延迟很严重的情况下，特别是在消费者连接等待时间较长的环境\",\"中。对于大多数应用来说，稍微高一点的值将是最佳的。\"]}]},\"/study-tutorial/distributed/redis/get-started.html\":{\"title\":\"1、Redis入门\",\"contents\":[{\"header\":\"1、redis简介\",\"slug\":\"_1、redis简介\",\"contents\":[\"Redis是一个使用ANSI C编写的开源、支持网络、基于内存、可选持久性的[键值对存储数据库]\",\"它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。\"]},{\"header\":\"2、linux安装\",\"slug\":\"_2、linux安装\",\"contents\":[]},{\"header\":\"1、下载\",\"slug\":\"_1、下载\",\"contents\":[\"在线下载\",\"$ wget http://download.redis.io/releases/redis-5.0.7.tar.gz $ tar xzf redis-5.0.7.tar.gz -C /root/tool //解压到指定目录 $ cd redis-5.0.7.tar.gz $ make \",\"利用ftp等软件上传至Linux桌面\",\"$ tar xzf redis-5.0.7.tar.gz $ cd redis-5.0.7.tar.gz $ make \",\"**注：**我们需要将源码编译后再安装，因此需要安装 c 语言的编译环境！不能直接 make\",\"yum install gcc-c++ -y \",\"make时遇到的错误：\",\"In file included from adlist.c:34:0: zmalloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory #include <jemalloc/jemalloc.h> ^ compilation terminated. \",\"解决办法：make MALLOC=libc\"]},{\"header\":\"2、安装\",\"slug\":\"_2、安装\",\"contents\":[\"make PREFIX=/usr/local/redis instal \",\"安装到指定目录，若没有，则创建该目录；PREFIX必须大写\"]},{\"header\":\"3、启动服务端跟客户端\",\"slug\":\"_3、启动服务端跟客户端\",\"contents\":[\"./redis-server ./redis-cli \",\"Redis服务器默认会使用6379端口1) , 通过－－port参数可以自定义端口号：\",\"redis-server --port 6380 \"]},{\"header\":\"4、关闭\",\"slug\":\"_4、关闭\",\"contents\":[\"shutdown \"]},{\"header\":\"5、docker安装redis\",\"slug\":\"_5、docker安装redis\",\"contents\":[\"docker search redis //命令来查看可用版本 docker pull redis:latest //拉取官方的最新版本的镜像 docker images //查看镜像 docker run -d --name redis-test -p 6379:6379 redis --requirepass \\\"密码\\\"//创建并运行容器 // --requirepass 密码 -d: 后台运行容器，并返回容器ID； docker exec -it redis redis-cli -a 密码 //运行客户端 --it 交互 /*docker常用命令 1、docker ps 查看进程 2、docker container rm 删除容器 https://www.runoob.com/docker/docker-command-manual.html */ \"]},{\"header\":\"3、redis配置\",\"slug\":\"_3、redis配置\",\"contents\":[]},{\"header\":\"1、redis.conf\",\"slug\":\"_1、redis-conf\",\"contents\":[]},{\"header\":\"2、内存维护策略（缓存清理策略）\",\"slug\":\"_2、内存维护策略-缓存清理策略\",\"contents\":[\"属性\",\"含义\",\"备注\",\"bind\",\"限定访问的主机地\",\"如果没有bind，就是任意ip 地址都可以访问。生产环境下，需要写自己应用服务器的ip 地址。\",\"protected-mode\",\"安全防护模式\",\"如果没有指定bind 指令，也没有配置密码，那么保护模式就开启，只允许本机访问。\",\"port\",\"端口号\",\"默认是6379\",\"timeout\",\"超时时间\",\"默认永不超时\",\"daemonize\",\"是否为守护进程模式运行\",\"守护进程模式可以在后台运行，默认是no\",\"pidfile\",\"进程id 文件保存的路径\",\"配置PID 文件路径，当redis 作为守护进程运行的时候，它会把pid默认写到/var/redis/run/redis_6379.pid 文件里面\",\"logfile\",\"日志文件的位置\",\"当指定为空字符串时，为标准输出，如果redis 以守护进程模式运行，那么日志将会输出到/dev/null\",\"databases\",\"设置数据库数量\",\"默认是0\",\"requirepass\",\"设置密码\",\"默认没有，但远程连接可能会连接不上\",\"maxclients\",\"最大连接数\",\"maxmemory\",\"最大占用多少内存\",\"一旦占用内存超限，就开始根据缓存清理策略移除数据如果Redis无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么Redis 则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH 等。\",\"maxmemory-policy noeviction\",\"缓存清理策略\",\"（1）volatile-lru：使用LRU 算法移除key，只对设置了过期时间的键（2）allkeys-lru：使用LRU 算法移除key（3）volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键（4）allkeys-random：移除随机的key（5）volatile-ttl：移除那些TTL 值最小的key，即那些最近要过期的key（6）noeviction：不进行移除。针对写操作，只是返回错误信息\"]},{\"header\":\"4、Redis基本操作\",\"slug\":\"_4、redis基本操作\",\"contents\":[]},{\"header\":\"1、数据库\",\"slug\":\"_1、数据库\",\"contents\":[\"select <dbid> //切换数据库 select 1 切换到1号数据库，默认为0 flushdb //清空当前库 dbsize //查看数据库个数 flushall //通杀全部库 \"]},{\"header\":\"2、Key\",\"slug\":\"_2、key\",\"contents\":[\"Redis 中的数据以键值对（key-value）为基本存储方式，其中key 都是字符串。\",\"KEYS pattern //查询符合指定表达式的所有key，支持*，？等 TYPE key //查看key 对应值的类型 EXISTS key //指定的key 是否存在，0 代表不存在，1 代表存在 DEL key //删除指定key RANDOMKEY //在现有的KEY 中随机返回一个 EXPIRE key seconds //为键值设置过期时间，单位是秒，过期后key 会被redis 移除 TTL key //查看key 还有多少秒过期，-1 表示永不过期，-2 表示已过期 RENAME key newkey //重命名一个key，NEWKEY 不管是否是已经存在的都会执行，如果NEWKEY 已经存在则会被覆盖 RENAMENX key newkey //只有在NEWKEY 不存在时能够执行成功，否则失败 \"]},{\"header\":\"3、String\",\"slug\":\"_3、string\",\"contents\":[\"String 类型是Redis 中最基本的类型，它是key 对应的一个单一值。\",\"二进制安全，不必担心由于编码等问题导致二进制数据变化。所以redis 的string 可以包含任何数据，比如jpg\",\"图片或者序列化的对象。Redis 中一个字符串值的最大容量是512M。\",\"SET key value //添加键值对 GET key //查询指定key 的值 APPEND key value //将给定的value 追加到原值的末尾 STRLEN key //获取值的长度 SETNX key value //只有在key 不存在时设置key 的值 INCR key //指定key 的值自增1，只对数字有效 DECR key //指定key 的值自减1，只对数字有效 INCRBY key num //自增num DECRBY key num //自减num MSET key1 value1 key2 value2… //同时设置多个key-value 对 MGET key1 key2 //同时获取一个或多个value MSETNX key1 value1 key2 value2 //当key 不存在时，设置多个key-value 对 GETRANGE key 起始索引 结束索 //获取指定范围的值，都是闭区间 SETRANGE key 起始索引value //从起始位置开始覆写指定的值 GETSET key value //以新换旧，同时获取旧值 SETEX key 过期时间value //设置键值的同时，设置过期时间，单位秒 \"]},{\"header\":\"4、List\",\"slug\":\"_4、list\",\"contents\":[\"Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)\",\"常见操作：\",\"遍历：遍历的时候，是从左往右取值；\",\"删除：弹栈，POP；\",\"添加：压栈，PUSH ；\",\"它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。\",\"LPUSH/RPUSH key value1 value2… //从左边/右边压入一个或多个值,头尾效率高，中间效率低 LPOP/RPOP key //从左边/右边弹出一个值,值在键在，值光键亡,弹出=返回+删除 LRANGE key start stop //查看指定区间的元素,正着数：0,1,2,3,...倒着数：-1,-2,-3,... LINDEX key index //按照索引下标获取元素（从左到右） LLEN key //获取列表长度 LINSERT key BEFORE|AFTER value newvalue //在指定value 的前后插入newvalue LREM key n value //从左边删除n 个value LSET key index value //把指定索引位置的元素替换为另一个值 LTRIM key start stop //仅保留指定区间的数据 RPOPLPUSH key1 key2 //从key1 右边弹出一个值，左侧压入到key2 \"]},{\"header\":\"5、Set\",\"slug\":\"_5、set\",\"contents\":[\"####### set 是无序的，且是不可重复的。\",\"SADD key member [member ...] //将一个或多个member 元素加入到集合key 当中，已经存在于集合的member 元素将被忽略。 SMEMBERS key //取出该集合的所有值 SISMEMBER key value //判断集合<key>是否为含有该<value>值，有返回1，没有返回0 SCARD key //返回集合中元素的数量 SREM key member [member ...] //从集合中删除元素 SPOP key [count] //从集合中随机弹出count 个数量的元素，count 不指定就弹出1 个 SRANDMEMBER key [count] //从集合中随机返回count 个数量的元素，count 不指定就返回1 个 SINTER key [key ...] //将指定的集合进行“交集”操作 SINTERSTORE dest key [key ...] //取交集，另存为一个set SUNION key [key ...] //将指定的集合执行“并集”操作 SUNIONSTORE dest key [key ...] //取并集，另存为set SDIFF key [key ...] //将指定的集合执行“差集”操作 SDIFFSTORE dest key [key ...] //取差集，另存为set \"]},{\"header\":\"6、Hash\",\"slug\":\"_6、hash\",\"contents\":[\"Hash 数据类型的键值对中的值是“单列”的，不支持进一步的层次结构。hash 特别适合用于存储对象。\",\"Redis 中每个 hash 可以存储 232 - 1 键值对（40多亿）。\",\"//从前到后的数据对应关系 @Data public class User implements Serializable { private String id; private String name; private Integer age; } User user = new User(); user.setId(\\\"1\\\"); user.setName(\\\"yhx\\\"); user.setAge(18); \",\"HSET key field value //为key 中的field 赋值value HMSET key field value [field value ...] //为指定key 批量设置field-value HSETNX key field value //当指定key 的field 不存在时，设置其value HGETALL key //获取指定key 的所有信息（field 和value） HKEYS key //获取指定key 的所有field HVALS key //获取指定key 的所有value HLEN key //指定key 的field 个数 HGET key field //从key 中根据field 取出value HMGET key field [field ...] //为指定key 获取多个filed 的值 HEXISTS key field //指定key 是否有field HINCRBY key field increment //为指定key 的field 加上增量increment \"]},{\"header\":\"7、Zset\",\"slug\":\"_7、zset\",\"contents\":[\"zset 是一种特殊的set（sorted set），在保存value 的时候，为每个value 多保存了一个score 信息。根据score 信息，可以进行排序。\",\"这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可 以是重复了\",\"ZADD key [score member ...] //添加 ZSCORE key member //返回指定值的分数 ZRANGE key start stop [WITHSCORES] //返回指定区间的值，可选择是否一起返回scores ZRANGEBYSCORE key min max [WITHSCORES][LIMIT offset count] //在分数的指定区间内返回数据，从小到大排列 ZREVRANGEBYSCORE key max min[WITHSCORES] [LIMIT offset count] //在分数的指定区间内返回数据，从大到小排列 ZCARD key //返回集合中所有的元素的数量 ZCOUNT key min max //统计分数区间内的元素个数 ZREM key member //删除该集合下，指定值的元素 ZRANK key member //返回该值在集合中的排名，从0 开始 ZINCRBY key increment value //为元素的score 加上增量 \"]},{\"header\":\"8、HyperLogLog\",\"slug\":\"_8、hyperloglog\",\"contents\":[\"Redis 在 2.8.9 版本添加了 HyperLogLog 结构。\",\"Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。\",\"在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\",\"但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\"]},{\"header\":\"什么是基数?\",\"slug\":\"什么是基数\",\"contents\":[\"比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是\",\"在误差可接受的范围内，快速计算基数。\",\"PFADD key element [element ...] //添加指定元素到 HyperLogLog 中。 PFCOUNT key [key ...] //返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey ...] //将多个 HyperLogLog 合并为一个 HyperLogLog \"]},{\"header\":\"应用场景\",\"slug\":\"应用场景\",\"contents\":[\"统计注册ip数\",\"统计每日访问IP数\",\"统计页面实时uv数\",\"统计在线用户数\",\"统计用户每天搜索不同词条的个数\",\"统计真实文章阅读数\"]},{\"header\":\"9、redis发布订阅\",\"slug\":\"_9、redis发布订阅\",\"contents\":[\"Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。\",\"Redis 客户端可以订阅任意数量的频道。\",\"SUBSCRIBE channel [channel ...] //订阅给定的一个或多个频道的信息。 PUBLISH channel message //将信息发送到指定的频道。 PUBSUB subcommand [argument [argument ...]]//查看订阅与发布系统状态。 PSUBSCRIBE pattern [pattern ...] //订阅一个或多个符合给定模式的频道。 UNSUBSCRIBE [channel [channel ...]] //指退订给定的频道。 \"]},{\"header\":\"5、持久化\",\"slug\":\"_5、持久化\",\"contents\":[\"Redis 主要是工作在内存中。内存本身就不是一个持久化设备，断电后数据会清空。所以Redis 在工作过程中，如果发生了意外停电事故，如何尽可能减少数据丢失。\"]},{\"header\":\"1、RDB\",\"slug\":\"_1、rdb\",\"contents\":[\"在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是Snapshot 快照，它恢复时是将快照文件直接读到内存里。 ==工作机制：每隔一段时间，就把内存中的数据保存到硬盘上的指定文件中。==RDB 是默认开启的！\",\"Redis 会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO 操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB 方式要比AOF 方式更加的高效。 RDB 的缺点是最后一次持久化后的数据可能丢失。\"]},{\"header\":\"1、RDB保存策略\",\"slug\":\"_1、rdb保存策略\",\"contents\":[\"save 900 1 //900 秒内如果至少有1 个key 的值变化，则保存 save 300 10 //300 秒内如果至少有10 个key 的值变化，则保存 save 60 10000 //60 秒内如果至少有10000 个key 的值变化，则保存 save “” //就是禁用RDB 模式； \"]},{\"header\":\"2、RDB 常用属性配置\",\"slug\":\"_2、rdb-常用属性配置\",\"contents\":[\"save //保存策略 dbfilename RDB //快照文件名 dir RDB //快照保存的目录必须是一个目录，不能是文件名。最好改为固定目录。默认为./代表执行redis-server 命令时的当前目录！ stop-writes-on-bgsave-error //是否在备份出错时，继续接受写操作如果用户开启了RDB 快照功能，那么在redis 持久化数据到磁盘时如果出现失败，默认情况下，redis 会停止接受所有的写请求 rdbcompression //对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis 会采用LZF 算法进行压缩。如果你不想消耗CPU 来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。 rdbchecksum //是否进行数据校验在存储快照后，我们还可以让redis 使用CRC64 算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。 \"]},{\"header\":\"3、RDB 数据丢失的情况\",\"slug\":\"_3、rdb-数据丢失的情况\",\"contents\":[\"两次保存的时间间隔内，服务器宕机，或者发生断电问题。\"]},{\"header\":\"4、RDB 的触发\",\"slug\":\"_4、rdb-的触发\",\"contents\":[\"​ ① 基于自动保存的策略\",\"​ ② 执行save，或者bgsave 命令！执行时，是阻塞状态。\",\"​ ③ 执行flushdb 命令，也会产生dump.rdb，但里面是空的，没有意义。\",\"​ ④ 当执行shutdown 命令时，也会主动地备份数据。\"]},{\"header\":\"5、RDB的优缺点\",\"slug\":\"_5、rdb的优缺点\",\"contents\":[\"RDB优点：\",\"（1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备。 （2）RDB对redis对外提供读写服务的时候，影像非常小，因为redis 主进程只需要fork一个子进程出来，让子进程对磁盘io来进行rdb持久化 （3）RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快\",\"（4）RDB 适合用于灾难恢复，因为它只有一个文件，而且体积小，方便拷贝。\",\"RDB缺点：\",\"（1）如果redis要故障时要尽可能少的丢失数据，RDB没有AOF好，例如1:00进行的快照，在1:10又要进行快照的时候宕机了，这个时候就会丢失10分钟的数据。 （2）RDB每次fork出子进程来执行RDB快照生成文件时，如果文件特别大，可能会导致客户端提供服务暂停数毫秒或者几秒\"]},{\"header\":\"2、AOF\",\"slug\":\"_2、aof\",\"contents\":[\"AOF 是 以日志的形式来记录每个写操作，将每一次对数据进行修改，都把新建、修改数据的命令保存到指 定文件中。Redis 重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据。\",\"默认不开启，需要手动开启\",\"AOF 文件的保存路径，同RDB 的路径一致。\",\"AOF 在保存命令的时候，只会保存对数据有修改的命令，也就是写操作！\",\"当RDB 和AOF 存的不一致的情况下，按照AOF 来恢复。因为AOF 是对RDB 的补充。备份周期更短，也就更 可靠。\"]},{\"header\":\"1、AOF 保存策略\",\"slug\":\"_1、aof-保存策略\",\"contents\":[\"appendfsync always：每次产生一条新的修改数据的命令都执行保存操作；效率低，但是安全！ appendfsync everysec：每秒执行一次保存操作。如果在未保存当前秒内操作时发生了断电，仍然会导致一部分数据丢失（即1 秒钟的数据）。 appendfsync no：从不保存，将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒fsync 一次， 这种fsync 策略可以兼顾速度和安全性。 \"]},{\"header\":\"2、AOF 常用属性\",\"slug\":\"_2、aof-常用属性\",\"contents\":[\"appendonly //是否开启AOF 功能默认是关闭的 appendfilename //AOF文件名称 appendfsync AOF //保存策略官方建议everysec no-appendfsync-on-rewrite //在重写时，是否执行保存策略执行重写，可以节省AOF 文件的体积；而且在恢复的时候效率也更高。 auto-aof-rewrite-percentage //重写的触发条件当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写 auto-aof-rewrite-min-size //设置允许重写的最小aof文件大小,避免了达到约定百分比但尺寸仍然很小的情况还要重写 aof-load-truncated //截断设置如果选择的是yes，当截断的aof 文件被导入的时候，会自动发布一个log 给客户端然后load \"]},{\"header\":\"3、AOF 文件的修复\",\"slug\":\"_3、aof-文件的修复\",\"contents\":[\"如果AOF 文件中出现了残余命令，会导致服务器无法重启。此时需要借助redis-check-aof 工具来修复！\",\"redis-check-aof –fix 文件 \"]},{\"header\":\"4、AOF 的优缺点\",\"slug\":\"_4、aof-的优缺点\",\"contents\":[\"AOF的优点：\",\"（1）AOF可以更好的保护数据不丢失，一般AOF会以每隔1秒，通过后台的一个线程去执行一次fsync操作，如果\",\"redis进程挂掉，最多丢失1秒的数据。\",\"（2）AOF以appen-only的模式写入，所以没有任何磁盘寻址的开销，写入性能非常高。9\",\"（3）AOF日志文件的命令通过非常可读的方式进行记录，这个非常适合做灾难性的误删除紧急恢复，如果某人不\",\"小心用flushall命令清空了所有数据，只要这个时候还没有执行rewrite，那么就可以将日志文件中的flushall删除，\",\"进行恢复。\",\"- 备份机制更稳健，丢失数据概率更低 - 可读的日志文本，通过操作AOF 稳健，可以处理误操作 \",\"AOF的缺点：\",\"（1）对于同一份文件AOF文件比RDB数据快照要大。\",\"（2）AOF开启后支持写的QPS会比RDB支持的写的QPS低，因为AOF一般会配置成每秒fsync操作，每秒的fsync\",\"操作还是很高的\",\"（3）数据恢复比较慢，不适合做冷备。\",\"- 比起RDB 占用更多的磁盘空间 - 恢复备份速度要慢 - 每次读写都同步的话，有一定的性能压力 - 存在个别Bug，造成恢复不能 \"]},{\"header\":\"3、备份建议\",\"slug\":\"_3、备份建议\",\"contents\":[\"如何看待数据“绝对”安全？\",\"Redis 作为内存数据库从本质上来说，如果不想牺牲性能，就不可能做到数据的“绝对”安全。 RDB 和AOF 都只是尽可能在兼顾性能的前提下降低数据丢失的风险，如果真的发生数据丢失问题，尽可能 减少损失。 在整个项目的架构体系中，Redis 大部分情况是扮演“二级缓存”角色。\",\"二级缓存适合保存的数据\",\"经常要查询，很少被修改的数据。\",\"不是非常重要，允许出现偶尔的并发问题。\",\"不会被其他应用程序修改。\",\"如果Redis 是作为缓存服务器，那么说明数据在MySQL 这样的传统关系型数据库中是有正式版本的。数据最终以MySQL 中的为准。\",\"RDB和AOF到底如何选择?\",\"（1）不要仅仅使用RDB这样会丢失很多数据。\",\"（2）也不要仅仅使用AOF，因为这会有两个问题，第一通过AOF做冷备没有RDB做冷备恢复的速度快；第二\",\"RDB每次简单粗暴生成数据快照，更加健壮。\",\"（3）综合AOF和RDB两种持久化方式，用AOF来保证数据不丢失，作为恢复数据的第一选择；用RDB来做不同程\",\"度的冷备，在AOF文件都丢失或损坏不可用的时候，可以使用RDB进行快速的数据恢复。\",\"官方推荐两个都用：如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是 做纯内存缓存，可以都不用\"]},{\"header\":\"6、事务\",\"slug\":\"_6、事务\",\"contents\":[\"Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证：\",\"批量操作在发送 EXEC 命令前被放入队列缓存。\",\"收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行。\",\"在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。\",\"一个事务从开始到执行会经历以下三个阶段：\",\"开始事务。\",\"命令入队。\",\"执行事务。\"]},{\"header\":\"1、事务简介\",\"slug\":\"_1、事务简介\",\"contents\":[\"Redis 中事务，不同于传统的关系型数据库中的事务。\",\"Redis 中的事务指的是一个单独的隔离操作。\",\"Redis 的事务中的所有命令都会序列化、按顺序地执行且不会被其他客户端发送来的命令请求所打\",\"Redis 事务的主要作用是串联多个命令防止别的命令插队\"]},{\"header\":\"2、常用命令\",\"slug\":\"_2、常用命令\",\"contents\":[\"MULTI //标记一个事务块的开始 EXEC //执行所有事务块内的命令。执行事务中所有在排队等待的指令并将链接状态恢复到正常当使用WATCH时，只有当被监视的键没有被修改，且允许检查设定机制时，EXEC会被执行 WATCH //标记所有指定的key 被监视起来，在事务中有条件的执行（乐观锁） UNWATCH //取消 WATCH 命令对所有 key 的监视。 DISCARD //刷新一个事务中所有在排队等待的指令，并且将连接状态恢复到正常。如果已使用WATCH，DISCARD将释放所有被WATCH 的key。 \",\"MULTI 开启组队，EXEC 依次执行队列中的命令。\",\"DISCARD 中途取消组队\"]},{\"header\":\"3、错误命令处理\",\"slug\":\"_3、错误命令处理\",\"contents\":[\"1、此种情况，语法符合规范，Redis 只有在执行中，才可以发现错误。而在Redis 中，并没有回滚机制，因此错误的命令，无法执行，正确的命令会全部执行！\",\"2、在编译的过程中，Redis 检测出来了错误的语法命令，因此它认为这条组队，一定会发生错误，因此全体取消；\"]},{\"header\":\"4、锁\",\"slug\":\"_4、锁\",\"contents\":[]},{\"header\":\"1、悲观锁\",\"slug\":\"_1、悲观锁\",\"contents\":[\"执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关\",\"资源锁定，不允许自己执行期间有其他操作干扰。\",\"Redis 不支持悲观锁。Redis 作为缓存服务器使用时，以读操作为主，很少写操作，相应的操作被打断的几率较\",\"少。不采用悲观锁是为了防止降低性能。\"]},{\"header\":\"2、乐观锁\",\"slug\":\"_2、乐观锁\",\"contents\":[\"执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操\",\"作的干扰，那么本次操作将被放弃。\"]},{\"header\":\"5、Redis 中的锁策略\",\"slug\":\"_5、redis-中的锁策略\",\"contents\":[\"Redis 采用了乐观锁策略**（通过watch 操作）**。乐观锁支持读操作，适用于多读少写的情况！\",\"在事务中，可以通过watch 命令来加锁；使用UNWATCH 可以取消加锁；\",\"如果在事务之前，执行了WATCH（加锁），那么执行EXEC 命令或DISCARD 命令后，锁对自动释放，即不需\",\"要再执行UNWATCH 了\"]},{\"header\":\"7、Redis Cluste集群\",\"slug\":\"_7、redis-cluste集群\",\"contents\":[\"什么是集群： Redis 集群实现了对Redis 的水平扩容，即启动N 个redis 节点，将整个数据库分布存储在这N 个节点中，每个节点存储总数据的1/N。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。\"]},{\"header\":\"1、搭建集群环境\",\"slug\":\"_1、搭建集群环境\",\"contents\":[]},{\"header\":\"1、创建目录并修改集群配置文件（redis.conf）\",\"slug\":\"_1、创建目录并修改集群配置文件-redis-conf\",\"contents\":[\"mkdir cluster-test cd cluster-test mkdir 7000 7001 7002 7003 7004 7005 \",\"port 7000 //开启集群模式 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 appendonly yes //依次修改6个配置文件 cp ./7000/redis.conf ./7000/ :%s/7000/7001/g 替换 \"]},{\"header\":\"2、启动节点\",\"slug\":\"_2、启动节点\",\"contents\":[\"把安装Redis目录下的src复制到redis_cluster下，方便启动服务\",\"//进入安装目录下 cd /root/tool/redis-5.0.8/（我的为例） cp -r ./src/ /usr/local/redis_cluster ./src/redis-server ./7000/redis.conf ./src/redis-server ./7001/redis.conf ./src/redis-server ./7002/redis.conf ./src/redis-server ./7003/redis.conf ./src/redis-server ./7004/redis.conf ./src/redis-server ./7005/redis.conf \",\"ps -ef |grep -i redis //查看进程 \"]},{\"header\":\"3、创建集群\",\"slug\":\"_3、创建集群\",\"contents\":[\"//redis5之后 ./src/redis-cli --cluster create -a 密码 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1 \"]},{\"header\":\"2、集群验证\",\"slug\":\"_2、集群验证\",\"contents\":[\"./src/redis-cli -p 7000 -a 密码 \",\"127.0.0.1:7000> CLUSTER nodes b7959193eb80fa0abc8cf9d296d08de76cf47f4d 127.0.0.1:7002@17002 master - 0 1586346968000 3 connected 10923-16383 74dc8b2155478d22d147d86dd052f84824bcc021 127.0.0.1:7001@17001 master - 0 1586346969041 2 connected 5461-10922 26764a2d2dc63b0ba3216de62ac0a171329e6f43 127.0.0.1:7005@17005 slave 1af6831ee1e7f9448fed04fab40b68f30b001b59 0 1586346968033 6 connected 1af6831ee1e7f9448fed04fab40b68f30b001b59 127.0.0.1:7000@17000 myself,master - 0 1586346968000 1 connected 0-5460 6373481e4202f55a6d3a0f46a0da00a395573861 127.0.0.1:7003@17003 slave 74dc8b2155478d22d147d86dd052f84824bcc021 0 1586346968537 4 connected 20c5257ce6ea6a2aa666f283e068e865152c52a6 127.0.0.1:7004@17004 slave b7959193eb80fa0abc8cf9d296d08de76cf47f4d 0 1586346969544 5 connected \"]},{\"header\":\"3、集群开启与关闭\",\"slug\":\"_3、集群开启与关闭\",\"contents\":[]},{\"header\":\"1、开启\",\"slug\":\"_1、开启\",\"contents\":[\"#redisstart.sh –c 参数实现自动重定向 /usr/local/redis_cluster/src/redis-server ./7000/redis.conf /usr/local/redis_cluster/src/redis-server ./7001/redis.conf /usr/local/redis_cluster/src/redis-server ./7002/redis.conf /usr/local/redis_cluster/src/redis-server ./7003/redis.conf /usr/local/redis_cluster/src/redis-server ./7004/redis.conf /usr/local/redis_cluster/src/redis-server ./7005/redis.conf \",\"chmod u+x redisall.sh //变成可执行文件 ./redisall.sh //在当前目录下启动 \",\"#startall.sh /usr/local/redis_cluster/src/redis-cli --cluster create -a 密码 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1 \",\"chmod u+x startall.sh ./startall.sh \"]},{\"header\":\"2、关闭\",\"slug\":\"_2、关闭\",\"contents\":[\"#shutdown.sh /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7000 -a 密码 shutdown /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7001 -a 密码 shutdown /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7002 -a 密码 shutdown /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7003 -a 密码 shutdown /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7004 -a 密码 shutdown /usr/local/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7005 -a 密码 shutdown \",\"chmod u+x shutdown.sh ./shutdown.sh \"]},{\"header\":\"4、集群中故障恢复\",\"slug\":\"_4、集群中故障恢复\",\"contents\":[\"问题1：如果主节点下线？从节点能否自动升为主节点？\",\"答：主节点下线，从节点自动升为主节点。\",\"问题2：主节点恢复后，主从关系会如何？\",\"主节点恢复后，主节点变为从节点！\",\"问题3：如果所有某一段插槽的主从节点都宕掉，redis 服务是否还能继续?\",\"答：服务是否继续，可以通过redis.conf 中的cluster-require-full-coverage 参数进行控制。\",\"主从都宕掉，意味着有一片数据，会变成真空，没法再访问了！ 如果无法访问的数据，是连续的业务数据，我们需要停止集群，避免缺少此部分数据，造成整个业务的异常。此时可以通过配置cluster-require-full-coverage 为yes. 如果无法访问的数据，是相对独立的，对于其他业务的访问，并不影响，那么可以继续开启集群体提供服务。此时，可以配置cluster-require-full-coverage 为no。\"]},{\"header\":\"5、集群的优缺点\",\"slug\":\"_5、集群的优缺点\",\"contents\":[\"优点：\",\"实现扩容\",\"分摊压力\",\"无中心配置相对简单\",\"缺点：\",\"多键操作是不被支持的\",\"多键的Redis 事务是不被支持的。lua 脚本不被支持。\",\"由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁\\n移至redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。\"]}]},\"/study-tutorial/frame/javaweb/concept.html\":{\"title\":\"1、JavaWeb的概念\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"JavaWeb 是指，所有通过Java 语言编写可以通过浏览器访问的程序的总称，叫JavaWeb。 JavaWeb 是基于请求和响应来开发的。\",\"什么是请求?\",\"请求是指客户端给服务器发送数 据，叫请求Request。\",\"什么是响应?\",\"响应是指服务器给客户端回传数据，叫响应Response。\",\"请求和响应是成对出现的，有请求就有响应。\"]},{\"header\":\"1、Web 资源的分类\",\"slug\":\"_1、web-资源的分类\",\"contents\":[\"静态资源： html、css、js、txt、mp4 视频, jpg 图片\",\"动态资源： jsp 页面、Servlet 程序\"]},{\"header\":\"2、常用的Web 服务器\",\"slug\":\"_2、常用的web-服务器\",\"contents\":[\"Tomcat：由Apache 组织提供的一种Web 服务器，提供对jsp 和Servlet 的支持。它是一种轻量级的javaWeb 容器（服务器），也是当前应用最广的JavaWeb 服务器（免费）。\",\"Jboss：是一个遵从JavaEE 规范的、开放源代码的、纯Java 的EJB 服务器，它支持所有的JavaEE 规范（免\",\"费）。\",\"GlassFish： 由Oracle 公司开发的一款JavaWeb 服务器，是一款强健的商业服务器，达到产品级质量（应用很\",\"少）。\",\"Resin：是CAUCHO 公司的产品，是一个非常流行的服务器，对servlet 和JSP 提供了良好的支持，\",\"性能也比较优良，resin 自身采用JAVA 语言开发（收费，应用比较多）。\",\"WebLogic：是Oracle 公司的产品，是目前应用最广泛的Web 服务器，支持JavaEE 规范，而且不断的完善以适应新的开发要求，适合大型项目（收费，用的不多，适合大公司）。\"]},{\"header\":\"1、Tomcat 服务器和Servlet 版本的对应关系\",\"slug\":\"_1、tomcat-服务器和servlet-版本的对应关系\",\"contents\":[\"Tomcat版本\",\"Servlet/Jsp版本\",\"JavaEE版本\",\"运行环境\",\"7.0\",\"3.0 / 2.2\",\"6.0\",\"JDK6.0\",\"8.0\",\"3.1/ 2.3\",\"7.0\",\"JDK7.0\",\"Servlet 程序从2.5 版本是现在世面使用最多的版本（xml 配置），Servlet3.0 之后。就是注解版本的Servlet 使用。\"]}]},\"/study-tutorial/frame/javaweb/cookie_session.html\":{\"title\":\"9、Cookie & Session\",\"contents\":[{\"header\":\"1、cookie\",\"slug\":\"_1、cookie\",\"contents\":[\"1、Cookie 翻译过来是饼干的意思。 2、Cookie 是服务器通知客户端保存键值对的一种技术。 3、客户端有了Cookie 后，每次请求都发送给服务器。 4、每个Cookie 的大小不能超过4kb\",\"准备页面cookie.html\",\"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Cookie</title> <base href=\\\"http://localhost:8080/JavaWeb_05_Cookie_Session/\\\"> <style type=\\\"text/css\\\"> ul li { list-style: none; } </style> </head> <body> <iframe name=\\\"target\\\" width=\\\"500\\\" height=\\\"500\\\" style=\\\"float: left;\\\"></iframe> <div style=\\\"float: left;\\\"> <ul> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie的创建</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie的获取</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie值的修改</a></li> <li>Cookie的存活周期</li> <li> <ul> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie的默认存活时间（会话）</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie立即删除</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie存活3600秒（1小时）</a></li> </ul> </li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie的路径设置</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Cookie的用户免登录练习</a></li> </ul> </div> </body> </html> \",\"BaseServlet\",\"public abstract class BaseServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 解决post请求中文乱码问题 // 一定要在获取请求参数之前调用才有效 req.setCharacterEncoding(\\\"UTF-8\\\"); // 解决响应中文乱码问题 resp.setContentType(\\\"text/html; charset=UTF-8\\\"); String action = req.getParameter(\\\"action\\\"); try { // 获取action业务鉴别字符串，获取相应的业务 方法反射对象 Method method = this.getClass().getDeclaredMethod(action, HttpServletRequest.class, HttpServletResponse.class); // System.out.println(method); // 调用目标业务 方法 method.invoke(this, req, resp); } catch (Exception e) { e.printStackTrace(); } } } \"]},{\"header\":\"1、Cookie的创建\",\"slug\":\"_1、cookie的创建\",\"contents\":[\"Servlet程序\",\"public class CookieServlet extends BaseServlet{ @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 创建Cookie 对象 Cookie cookie = new Cookie(\\\"key4\\\", \\\"value4\\\"); // 通知客户端保存Cookie resp.addCookie(cookie); resp.getWriter().write(\\\"Cookie 创建成功\\\"); } } \"]},{\"header\":\"2、服务器如何获取Cookie\",\"slug\":\"_2、服务器如何获取cookie\",\"contents\":[\"/*Cookie的获取*/ protected void getCookie(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { Cookie[] cookies = req.getCookies(); for (Cookie cookie : cookies) { //返回Cookie的名称 resp.getWriter().write(\\\"Cookie[\\\"+cookie.getName()+\\\"=\\\"+cookie.getValue()+\\\"]\\\"); } } \"]},{\"header\":\"3、Cookie 值的修改\",\"slug\":\"_3、cookie-值的修改\",\"contents\":[\"方案一：\",\"先创建一个要修改的同名（指的就是key）的Cookie 对象\",\"在构造器，同时赋于新的Cookie 值。\",\"调用response.addCookie( Cookie );\",\" /* *Cookie值修改 */ protected void updateCookie(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // - 先创建一个要修改的同名（指的就是key）的Cookie 对象 Cookie cookie = new Cookie(\\\"key4\\\",\\\"newValue4\\\"); // - 在构造器，同时赋于新的Cookie 值。 resp.addCookie(cookie); // - 调用response.addCookie( Cookie ); resp.getWriter().write(\\\"Cookie修改成功\\\"); } \",\"方案二：\",\"先查找到需要修改的Cookie 对象\",\"调用setValue()方法赋于新的Cookie 值。\",\"调用response.addCookie()通知客户端保存修改\",\"public class CookieUtils { public static Cookie findCookie(String name , Cookie[] cookies){ if (name == null || cookies == null || cookies.length == 0) { return null; } for (Cookie cookie : cookies) { if (name.equals(cookie.getName())) { return cookie; } } return null; } } \",\"Cookie cookie1 = CookieUtils.findCookie(\\\"key5\\\", req.getCookies()); if (cookie1 != null) { cookie1.setValue(\\\"newValue5\\\"); } resp.addCookie(cookie1); resp.getWriter().write(\\\"Cookie修改成功\\\"); \"]},{\"header\":\"4、Cookie 生命控制\",\"slug\":\"_4、cookie-生命控制\",\"contents\":[\"Cookie 的生命控制指的是如何管理Cookie 什么时候被销毁（删除）\",\"setMaxAge()\",\"正数，表示在指定的秒数后过期\",\"负数，表示浏览器一关，Cookie 就会被删除（默认值是-1）\",\"零，表示马上删除Cookie\",\" protected void defaultCookie(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { Cookie cookie = new Cookie(\\\"default\\\",\\\"default\\\"); cookie.setMaxAge(-1); //cookie.setMaxAge(60 * 60); // 设置Cookie 一小时之后被删除。无效 resp.addCookie(cookie); } protected void deleteNow(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 先找到你要删除的Cookie 对象 Cookie cookie = CookieUtils.findCookie(\\\"key4\\\", req.getCookies()); if (cookie != null) { // 调用setMaxAge(0); cookie.setMaxAge(0); // 表示马上删除，都不需要等待浏览器关闭 // 调用response.addCookie(cookie); resp.addCookie(cookie); resp.getWriter().write(\\\"key4 的Cookie 已经被删除\\\"); } } \"]},{\"header\":\"5、Cookie 有效路径Path 的设置\",\"slug\":\"_5、cookie-有效路径path-的设置\",\"contents\":[\"Cookie 的path 属性可以有效的过滤哪些Cookie 可以发送给服务器。哪些不发。 path 属性是通过请求的地址来进行有效的过滤。\",\"protected void pathCookie(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { Cookie cookie = new Cookie(\\\"path\\\",\\\"path\\\"); cookie.setPath(req.getContextPath()+\\\"/abc\\\"); resp.addCookie(cookie); resp.getWriter().write(\\\"创建了一个带有path的Cookie\\\"); } \"]},{\"header\":\"6、练习--免密登录\",\"slug\":\"_6、练习-免密登录\",\"contents\":[\"<%--login.jsp--%> <form action=\\\"http://localhost:8080/JavaWeb_05_Cookie_Session/loginServlet\\\" method=\\\"get\\\"> 用户名：<input type=\\\"text\\\" name=\\\"username\\\" value=\\\"${cookie.username.value}\\\"><br> 密码：<input type=\\\"password\\\" name=\\\"password\\\" ><br> <input type=\\\"submit\\\" value=\\\"登录\\\"> </form> \",\"//LoginServlet public class LoginServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String username = req.getParameter(\\\"username\\\"); String password = req.getParameter(\\\"password\\\"); if (\\\"xiaobear\\\".equals(username) && \\\"123456\\\".equals(password)) { //登录成功 Cookie cookie = new Cookie(\\\"username\\\", username); cookie.setMaxAge(60 * 60 * 24 * 7); //当前Cookie 一周内有效 resp.addCookie(cookie); System.out.println(\\\"登录成功\\\"); } else { // 登录失败 System.out.println(\\\"登录失败\\\"); } } } \"]},{\"header\":\"2、Session\",\"slug\":\"_2、session\",\"contents\":[\"1、Session 就一个接口（HttpSession）。 2、Session 就是会话。它是用来维护一个客户端和服务器之间关联的一种技术。 3、每个客户端都有自己的一个Session 会话。 4、Session 会话中，我们经常用来保存用户登录之后的信息。\"]},{\"header\":\"1、创建Session 和获取(id 号,是否为新)\",\"slug\":\"_1、创建session-和获取-id-号-是否为新\",\"contents\":[\"session.html页面\",\"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Session</title> <base href=\\\"http://localhost:8080/JavaWeb_05_Cookie_Session/\\\"> <style type=\\\"text/css\\\"> base ul li { list-style: none; } </style> </head> <body> <iframe name=\\\"target\\\" width=\\\"500\\\" height=\\\"500\\\" style=\\\"float: left;\\\"></iframe> <div style=\\\"float: left;\\\"> <ul> <li><a href=\\\"sessionServlet?action=creatOrGetSession\\\" target=\\\"target\\\">Session的创建和获取（id号、是否为新创建）</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Session域数据的存储</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Session域数据的获取</a></li> <li>Session的存活</li> <li> <ul> <li><a href=\\\"\\\" target=\\\"target\\\">Session的默认超时及配置</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Session3秒超时销毁</a></li> <li><a href=\\\"\\\" target=\\\"target\\\">Session马上销毁</a></li> </ul> </li> <li><a href=\\\"\\\" target=\\\"target\\\">浏览器和Session绑定的原理</a></li> </ul> </div> </body> </html> \",\"如何创建和获取Session。它们的API 是一样的。 request.getSession() 第一次调用是：创建Session 会话 之后调用都是：获取前面创建好的Session 会话对象。 isNew(); 判断到底是不是刚创建出来的（新的） true 表示刚创建 false 表示获取之前创建 每个会话都有一个身份证号。也就是ID 值。而且这个ID 是唯一的。 getId() 得到Session 的会话id 值。 \",\"protected void creatOrGetSession(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { HttpSession session = req.getSession(); boolean aNew = session.isNew(); String id = session.getId(); resp.getWriter().write(\\\"Session的ID:\\\"+id+\\\" ;\\\"); resp.getWriter().write(\\\"Session是否是新创建：\\\"+aNew+\\\" ;\\\"); } \"]},{\"header\":\"2、Session 域数据的存取\",\"slug\":\"_2、session-域数据的存取\",\"contents\":[\"protected void getSession(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { Object attribute = req.getSession().getAttribute(\\\"key1\\\"); resp.getWriter().write(\\\"从Session 中获取出key1 的数据是：\\\" + attribute); } protected void setSession(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { req.getSession().setAttribute(\\\"key1\\\", \\\"value1\\\"); resp.getWriter().write(\\\"已经往Session 中保存了数据\\\"); } \"]},{\"header\":\"3、Session 生命周期控制\",\"slug\":\"_3、session-生命周期控制\",\"contents\":[\"public void setMaxInactiveInterval(int interval) 设置Session 的超时时间（以秒为单位），超过指定的时长，Session 就会被销毁。 值为正数的时候，设定Session 的超时时长。 负数表示永不超时（极少使用） public int getMaxInactiveInterval()获取Session 的超时时间 public void invalidate() 让当前Session 会话马上超时无效。 Session 默认的超时时长是多少！ Session 默认的超时时间长为30 分钟。 因为在Tomcat 服务器的配置文件web.xml 中默认有以下的配置，它就表示配置了当前Tomcat 服务器下所有的Session 超时配置默认时长为：30 分钟。 <session-config> <session-timeout>30</session-timeout> </session-config> 如果你想只修改个别Session 的超时时长。就可以使用上面的API。setMaxInactiveInterval(int interval)来进行单独的设置。 session.setMaxInactiveInterval(int interval)单独设置超时时长。 \",\"protected void setTime(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { HttpSession session = req.getSession(); session.setMaxInactiveInterval(10); resp.getWriter().write(\\\"10s后超时！！\\\"); } \",\"protected void deleteNow(HttpServletRequest req, HttpServletResponse resp) throws ServletException,IOException { // 先获取Session 对象 HttpSession session = req.getSession(); // 让Session 会话马上超时 session.invalidate(); resp.getWriter().write(\\\"Session 已经设置为超时（无效）\\\"); } \"]},{\"header\":\"4、浏览器和Session\",\"slug\":\"_4、浏览器和session\",\"contents\":[\"Session 技术，底层其实是基于Cookie 技术来实现的。\"]}]},\"/study-tutorial/frame/javaweb/el-and-jstl.html\":{\"title\":\"7、EL表达式 & JSTL 标签库\",\"contents\":[{\"header\":\"1、EL 表达式\",\"slug\":\"_1、el-表达式\",\"contents\":[]},{\"header\":\"1、什么是EL 表达式，EL 表达式的作用?\",\"slug\":\"_1、什么是el-表达式-el-表达式的作用\",\"contents\":[\"EL 表达式的全称是：Expression Language。是表达式语言。\",\"EL 表达式的什么作用：EL 表达式主要是代替jsp 页面中的表达式脚本在jsp 页面中进行数据的输出。 因为EL 表达式在输出数据的时候，要比jsp 的表达式脚本要简洁很多。\",\"<% request.setAttribute(\\\"key\\\",\\\"鄢汉雄\\\"); %> 表达式脚本输出的值是：<%=request.getAttribute(\\\"key\\\")%></br> EL表达式输出的值是：${key} \",\"EL 表达式的格式是：${表达式}\",\"EL 表达式在输出null 值的时候，输出的是空串。\",\"jsp 表达式脚本输出null 值的时候，输出的是null 字符串\"]},{\"header\":\"2、EL 表达式搜索域数据的顺序\",\"slug\":\"_2、el-表达式搜索域数据的顺序\",\"contents\":[\"EL 表达式主要是在jsp 页面中输出数据。主要是输出域对象中的数据。\",\"当四个域中都有相同的key 的数据的时候，EL 表达式会按照四个域的从小到大的顺序去进行搜索，找到就输出。\",\"pageContext ----> request ----> session ----> application\",\"<% request.setAttribute(\\\"key\\\",\\\"request\\\"); session.setAttribute(\\\"key\\\",\\\"session\\\"); application.setAttribute(\\\"key\\\",\\\"application\\\"); pageContext.setAttribute(\\\"key\\\",\\\"pageContext\\\"); %> <%----%> 表达式脚本输出的值是：<%=request.getAttribute(\\\"key\\\")%></br> EL表达式输出的值是：${key} \"]},{\"header\":\"3、EL 表达式输出Bean 的普通属性，数组属性。List 集合属性，map 集合属性\",\"slug\":\"_3、el-表达式输出bean-的普通属性-数组属性。list-集合属性-map-集合属性\",\"contents\":[\"@Data @AllArgsConstructor @NoArgsConstructor public class Person { private Integer id; private String[] phone; private List<String> city; private Map<String,Object> map; } \",\"<% Person person = new Person(); person.setId(12); person.setPhone(new String[]{\\\"18391242239\\\",\\\"1234523121\\\"}); ArrayList<String> objects = new ArrayList<>(); objects.add(\\\"湖南\\\"); objects.add(\\\"长沙\\\"); objects.add(\\\"上海\\\"); person.setCity(objects); Map<String, Object> map = new HashMap<>(); map.put(\\\"Key1\\\",\\\"value1\\\"); map.put(\\\"Key2\\\",\\\"value2\\\"); map.put(\\\"Key3\\\",\\\"value3\\\"); person.setMap(map); pageContext.setAttribute(\\\"p\\\", person); %> 输出person:${p}</br> 输出person id的属性：${p.id} 输出Person 的pnones 数组属性值：${p.phones[2]} <br> 输出Person 的cities 集合中的元素值：${p.city} <br> 输出Person 的List 集合中个别元素值：${p.city[2]} <br> 输出Person 的Map 集合: ${p.map} <br> 输出Person 的Map 集合中某个key 的值: ${p.map.key3} <br> \"]},{\"header\":\"4、EL 表达式——运算\",\"slug\":\"_4、el-表达式——运算\",\"contents\":[\"语法：$\"]},{\"header\":\"1、关系运算\",\"slug\":\"_1、关系运算\",\"contents\":[\"关系运算符\",\"说明\",\"范例\",\"结果\",\"== 或eq\",\"等于\",\"${1001 == 1001} or ${1001 eq 1001}\",\"true or true\",\"!= 或ne\",\"不等于\",\"${1001 != 1001} or ${1001 ne 1001}\",\"false or false\",\"<或lt\",\"小于\",\"${1001 < 1004} or ${1001 lt 1001}\",\"true or false\",\"> 或者gt\",\"大于\",\"${1001 > 1004} or ${1001 gt 1001} \",\"false or false\",\"<= 或le\",\"小于等于\",\"${1001 <= 1004} or ${1001 le 1001}\",\"true or true\",\">= 或ge\",\"大于等于\",\"${1001 >= 1004} or ${1001 ge 1001}\",\"false or true\"]},{\"header\":\"2、逻辑运算\",\"slug\":\"_2、逻辑运算\",\"contents\":[\"逻辑运算符\",\"说明\",\"范例\",\"结果\",\"&& 或and\",\"与运算\",\"${14 == 10 && 10 < 14} or ${14 == 10 and 10 < 14}\",\"false or false\",\"||或or\",\"或运算\",\"${14 == 10 || 10 < 14} or ${14 == 10 or 10 < 14}\",\"true or true\",\"! 或not\",\"取反运算\",\"${!false} or ${not true}\",\"true or false\"]},{\"header\":\"3、算数运算\",\"slug\":\"_3、算数运算\",\"contents\":[\"逻辑运算符\",\"说明\",\"范例\",\"结果\",\"+\",\"加\",\"${10 + 14}\",\"24\",\"-\",\"减\",\"${520 - 250}\",\"270\",\"*\",\"乘\",\"${25 * 25} \",\"625\",\"/或div\",\"除\",\"${99 / 3} or ${99 div 3}\",\"33.0 or 33.0\",\"%或mod\",\"取余\",\"${100 % 3} or ${100 mod 3}\",\"1 or 1\"]},{\"header\":\"4、empty运算\",\"slug\":\"_4、empty运算\",\"contents\":[\"empty 运算可以判断一个数据是否为空，如果为空，则输出true,不为空输出false\",\"为空的情况：\",\"值为null 值的时候，为空\",\"值为空串的时候，为空\",\"值是Object 类型数组，长度为零的时候\",\"list 集合，元素个数为零\",\"map 集合，元素个数为零\",\"<% //值为null 值的时候，为空 request.setAttribute(\\\"emptyNull\\\",null); //值为空串的时候，为空 request.setAttribute(\\\"emptyStr\\\",\\\"\\\"); // 值是Object 类型数组，长度为零的时候 request.setAttribute(\\\"emptyArr\\\",new Object[]{}); //list 集合，元素个数为零 request.setAttribute(\\\"emptyList\\\",new ArrayList<>()); //map 集合，元素个数为零 request.setAttribute(\\\"emptyMap\\\",new HashMap<>()); %> ${ empty emptyNull } <br/> ${ empty emptyStr } <br/> ${ empty emptyArr } <br/> ${ empty emptyList } <br/> ${ empty emptyMap } <br/> \"]},{\"header\":\"5、三元运算\",\"slug\":\"_5、三元运算\",\"contents\":[\"表达式1？表达式2：表达式3\",\"${100>120?\\\"Xiaobear很帅！！\\\":\\\"非常帅！！！\\\"} \"]},{\"header\":\"6、.点运算和[] 中括号运算符\",\"slug\":\"_6、-点运算和-中括号运算符\",\"contents\":[\".点运算，可以输出Bean 对象中某个属性的值。 []中括号运算，可以输出有序集合中某个元素的值。 并且[]中括号运算，还可以输出map 集合中key 里含有特殊字符的key 的值。\",\"<% HashMap<String, Object> map = new HashMap<String,Object>(); map.put(\\\"a.b.c\\\",\\\"abc\\\"); map.put(\\\"a-b-c\\\",\\\"a-b-c\\\"); request.setAttribute(\\\"map\\\",map); %> ${map['a.b.c']} ${map['a-b-c']} \"]},{\"header\":\"5、EL 表达式的11 个隐含对象\",\"slug\":\"_5、el-表达式的11-个隐含对象\",\"contents\":[\"变量\",\"类型\",\"作用\",\"pageContext\",\"pageContextImpl\",\"它可以获取jsp 中的九大内置对象\",\"pageScope\",\"Map<String,Object>\",\"它可以获取pageContext 域中的数据\",\"requestScope\",\"Map<String,Object>\",\"它可以获取Request 域中的数据\",\"sessionScope\",\"Map<String,Object>\",\"它可以获取Session 域中的数据\",\"applicationScope\",\"Map<String,Object>\",\"它可以获取ServletContext 域中的数据\",\"param\",\"Map<String,String>\",\"它可以获取请求参数的值\",\"paramValues\",\"Map<String,String[]>\",\"它也可以获取请求参数的值，获取多个值的时候使用。\",\"header\",\"Map<String,String>\",\"它可以获取请求头的信息\",\"headerValues\",\"Map<String,String[]>\",\"它可以获取请求头的信息，它可以获取多个值的情况\",\"cookie\",\"Map<String,Cookie>\",\"它可以获取当前请求的Cookie 信息\",\"initParam\",\"Map<String,String>\",\"它可以获取在web.xml 中配置的<context-param>上下文参数\"]},{\"header\":\"1、EL 获取四个特定域中的属性\",\"slug\":\"_1、el-获取四个特定域中的属性\",\"contents\":[\"pageScope ====== pageContext 域 requestScope ====== Request 域 sessionScope ====== Session 域 applicationScope ====== ServletContext 域\",\"<% pageContext.setAttribute(\\\"key1\\\",\\\"value1\\\"); pageContext.setAttribute(\\\"key2\\\",\\\"value2\\\"); request.setAttribute(\\\"key2\\\",\\\"value2\\\"); session.setAttribute(\\\"key2\\\",\\\"value2\\\"); application.setAttribute(\\\"key2\\\",\\\"value2\\\"); %> ${applicationScope.key2} \"]},{\"header\":\"2、pageContext 对象的使用\",\"slug\":\"_2、pagecontext-对象的使用\",\"contents\":[\"协议：\",\"服务器ip：\",\"服务器端口：\",\"获取工程路径：\",\"获取请求方法：\",\"获取客户端ip 地址：\",\"获取会话的id 编号：\",\"<% request.setAttribute(\\\"req\\\",request); %> <%=request.getScheme() %> <br> 1.协议： ${ req.scheme }<br> 2.服务器ip：${ pageContext.request.serverName }<br> 3.服务器端口：${ pageContext.request.serverPort }<br> 4.获取工程路径：${ pageContext.request.contextPath }<br> 5.获取请求方法：${ pageContext.request.method }<br> 6.获取客户端ip 地址：${ pageContext.request.remoteHost }<br> 7.获取会话的id 编号：${ pageContext.session.id }<br> \"]},{\"header\":\"2、JSTL 标签库\",\"slug\":\"_2、jstl-标签库\",\"contents\":[\"JSTL 标签库全称是指JSP Standard Tag Library JSP 标准标签库。是一个不断完善的开放源代码的JSP 标签库。 EL 表达式主要是为了替换jsp 中的表达式脚本，而标签库则是为了替换代码脚本。这样使得整个jsp 页面变得更佳简洁。\",\"JSTL 标签库的使用步骤：\",\"先导入jstl 标签库的jar 包。http://archive.apache.org/dist/jakarta/taglibs/standard/binaries/\",\"使用taglib 指令引入标签库\",\"<%@ taglib prefix=\\\"c\\\" uri=\\\"http://java.sun.com/jsp/jstl/core\\\" %> \"]},{\"header\":\"1、core 核心库使用\",\"slug\":\"_1、core-核心库使用\",\"contents\":[]},{\"header\":\"1、<c:set />（使用很少）\",\"slug\":\"_1、-c-set-使用很少\",\"contents\":[\"set 标签可以往域中保存数据\",\"保存之前：${requestScope.abc} <c:set scope=\\\"session\\\" var=\\\"abc\\\" value=\\\"abcValue\\\"></c:set> 保存之后：${sessionScope.abc} \"]},{\"header\":\"2、<c:if />\",\"slug\":\"_2、-c-if\",\"contents\":[\"if 标签用来做if 判断。不支持if-else\",\"<%-- ii.<c:if /> if 标签用来做if 判断。 test 属性表示判断的条件（使用EL 表达式输出） --%> <c:if test=\\\"${ 12 == 12 }\\\"> <h1>12 等于12</h1> </c:if> <c:if test=\\\"${ 12 != 12 }\\\"> <h1>12 不等于12</h1> </c:if> \"]},{\"header\":\"3、<c:choose> <c:when> <c:otherwise>标签\",\"slug\":\"_3、-c-choose-c-when-c-otherwise-标签\",\"contents\":[\"多路判断。跟switch ... case .... default 非常接近\",\"<%--<c:choose> <c:when> <c:otherwise>标签 1、标签里不能使用html 注释，要使用jsp 注释 2、when 标签的父标签一定要是choose 标签 --%> <% request.setAttribute(\\\"height\\\",175); %> <c:choose> <c:when test=\\\"${requestScope.height} > 180\\\"> <h2>是巨人</h2> </c:when> <c:when test=\\\"${requestScope.height} < 170\\\"> <h2>还可以</h2> </c:when> <c:when test=\\\"${requestScope.height} > 185\\\"> <h2>超巨人</h2> </c:when><c:when test=\\\"${requestScope.height} > 180\\\"> <h2>可以</h2> </c:when> <c:otherwise> qita </c:otherwise> </c:choose> \"]},{\"header\":\"4、<c:forEach />\",\"slug\":\"_4、-c-foreach\",\"contents\":[\"遍历输出使用。\"]},{\"header\":\"1. 遍历1 到10\",\"slug\":\"_1-遍历1-到10\",\"contents\":[\"<%--1.遍历1 到10，输出 begin 属性设置开始的索引 end 属性设置结束的索引 var 属性表示循环的变量(也是当前正在遍历到的数据) for (int i = 1; i < 10; i++) --%> <c:forEach begin=\\\"1\\\" end=\\\"10\\\" var=\\\"i\\\"> ${i} </c:forEach> \"]},{\"header\":\"2、遍历Object 数组\",\"slug\":\"_2、遍历object-数组\",\"contents\":[\"<%-- 2.遍历Object 数组 for (Object item: arr) items 表示遍历的数据源（遍历的集合） var 表示当前遍历到的数据 --%> <% request.setAttribute(\\\"hello\\\",new String[]{\\\"123\\\",\\\"1234\\\",\\\"124\\\"}); %> <c:forEach items=\\\"${requestScope.hello}\\\" var=\\\"item\\\"> ${item} </c:forEach> \"]},{\"header\":\"3、遍历Map 集合\",\"slug\":\"_3、遍历map-集合\",\"contents\":[\"<% Map<String,Object> map = new HashMap<String, Object>(); map.put(\\\"key1\\\", \\\"value1\\\"); map.put(\\\"key2\\\", \\\"value2\\\"); map.put(\\\"key3\\\", \\\"value3\\\"); // for ( Map.Entry<String,Object> entry : map.entrySet()) { // } request.setAttribute(\\\"map\\\", map); %> <c:forEach items=\\\"${ requestScope.map }\\\" var=\\\"entry\\\"> <h1>${entry.key} = ${entry.value}</h1> </c:forEach> \"]},{\"header\":\"4、遍历List 集合\",\"slug\":\"_4、遍历list-集合\",\"contents\":[\"@Data @AllArgsConstructor @NoArgsConstructor public class Student { private Integer id; private String username; private String password; private Integer age; private String phone; } \",\"<% ArrayList<Student> objects = new ArrayList<Student>(); for (int i = 0; i <= 10; i++) { objects.add(new Student(1,\\\"username\\\"+i,\\\"pass\\\"+i,18+i,\\\"phone\\\"+i)); } request.setAttribute(\\\"stu\\\",objects); %> <table> <tr> <th>编号</th> <th>用户名</th> <th>密码</th> <th>年龄</th> <th>电话</th> <th>操作</th> </tr> <%-- items 表示遍历的集合 var 表示遍历到的数据 begin 表示遍历的开始索引值 end 表示结束的索引值 step 属性表示遍历的步长值 varStatus 属性表示当前遍历到的数据的状态 for（int i = 1; i < 10; i+=2） --%> <c:forEach begin=\\\"2\\\" end=\\\"7\\\" varStatus=\\\"status\\\" items=\\\"${requestScope.stu}\\\" var=\\\"stu\\\"> <tr> <td>${stu.id}</td> <td>${stu.username}</td> <td>${stu.password}</td> <td>${stu.age}</td> <td>${stu.phone}</td> </tr> </c:forEach> \"]}]},\"/study-tutorial/frame/javaweb/file-upload-or-download.html\":{\"title\":\"8、文件上传与下载\",\"contents\":[{\"header\":\"1、文件的上传\",\"slug\":\"_1、文件的上传\",\"contents\":[\"需要的jar包：commons-fileupload-1.2.1.jar、commons-io-1.4.jar\",\"要有一个form 标签，method=post 请求\",\"form 标签的encType 属性值必须为multipart/form-data 值\",\"在form 标签中使用input type=file 添加上传的文件\",\"编写服务器代码（Servlet 程序）接收，处理上传的数据。\"]},{\"header\":\"1、 commons-fileupload.jar\",\"slug\":\"_1、commons-fileupload-jar\",\"contents\":[\"ServletFileUpload 类，用于解析上传的数据。 FileItem 类，表示每一个表单项。 boolean ServletFileUpload.isMultipartContent(HttpServletRequest request); 判断当前上传的数据格式是否是多段的格式。 public List<FileItem> parseRequest(HttpServletRequest request) 解析上传的数据 boolean FileItem.isFormField() 判断当前这个表单项，是否是普通的表单项。还是上传的文件类型。 true 表示普通类型的表单项 false 表示上传的文件类型 String FileItem.getFieldName() 获取表单项的name 属性值 String FileItem.getString() 获取当前表单项的值。 String FileItem.getName(); 获取上传的文件名 void FileItem.write( file ); 将上传的文件写到参数file 所指向抽硬盘位置。 \",\"form表单\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>upload</title> </head> <body> <form action=\\\"http://localhost:8080/JavaWeb_04_war_exploded/upload\\\" method=\\\"post\\\" enctype=\\\"multipart/form-data\\\"> 用户名:<input type=\\\"text\\\" name=\\\"username\\\"><br> 头像:<input type=\\\"file\\\" name=\\\"photo\\\"><br> <input type=\\\"submit\\\" value=\\\"上传\\\"> </form> </body> </html> \",\"Servlet，web.xml配置省略\",\"public class UploadServlet extends HttpServlet { @SneakyThrows @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { /* System.out.println(\\\"文件已上传\\\"); ServletInputStream inputStream = req.getInputStream(); byte[] buffer = new byte[10240000]; int read = inputStream.read(buffer); System.out.println(new String(buffer,0,read)); */ // 先判断上传的数据是否多段数据（只有是多段的数据，才是文件上传的） if(ServletFileUpload.isMultipartContent(req)){ //创建FileItemFactory 工厂实现类 FileItemFactory factory = new DiskFileItemFactory(); // 创建用于解析上传数据的工具类ServletFileUpload 类 ServletFileUpload upload = new ServletFileUpload(factory); try{ // 解析上传的数据，得到每一个表单项FileItem List<FileItem> list = upload.parseRequest(req); // 循环判断，每一个表单项，是普通类型，还是上传的文件 for (FileItem fileItem : list) { if (fileItem.isFormField()) { // 普通表单项 System.out.println(\\\"表单项的name 属性值：\\\" + fileItem.getFieldName()); // 参数UTF-8.解决乱码问题 System.out.println(\\\"表单项的value 属性值：\\\" + fileItem.getString(\\\"UTF-8\\\")); } else { // 上传的文件 System.out.println(\\\"表单项的name 属性值：\\\" + fileItem.getFieldName()); System.out.println(\\\"上传的文件名：\\\" + fileItem.getName()); fileItem.write(new File(\\\"e:\\\\\\\\\\\" + fileItem.getName())); } } }catch (Exception e){ e.printStackTrace(); } } } } \"]},{\"header\":\"2、文件的下载\",\"slug\":\"_2、文件的下载\",\"contents\":[\"下载的常用API 说明： response.getOutputStream(); servletContext.getResourceAsStream(); servletContext.getMimeType(); response.setContentType();\",\"public class Download extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { //获取下载的文件名 String downFile = \\\"t011c6cdb031350fcb6.jpg\\\"; //读取下载的文件内容 ServletContext context = getServletContext(); //获取下载的文件类型 String mimeType = context.getMimeType(\\\"/file/\\\"+downFile); System.out.println(\\\"下载的文件类型：\\\"+mimeType); //在回传前，通过响应头告诉客户端返回的数据类型 resp.setContentType(mimeType); resp.setHeader(\\\"Content-Disposition\\\", \\\"attachment; fileName=t011c6cdb031350fcb6.jpg\\\"); //告诉客户端收到的数据类型是用于下载 InputStream stream = context.getResourceAsStream(\\\"/file/\\\"+downFile); //获取输出流 ServletOutputStream outputStream = resp.getOutputStream(); //读取全部数据，复制给输出流，输出给客户端 IOUtils.copy(stream,outputStream); //把下载的内容回传给客户端 } } \",\"中文名乱码问题:\",\"URLEncoder 解决IE 和谷歌浏览器的附件中文名问题\",\"// 把中文名进行UTF-8 编码操作。 String str = \\\"attachment; fileName=\\\" + URLEncoder.encode(\\\"中文.jpg\\\", \\\"UTF-8\\\"); // 然后把编码后的字符串设置到响应头中 response.setHeader(\\\"Content-Disposition\\\", str); \",\"BASE64 编解码解决火狐浏览器的附件中文名问题\",\"//因为火狐使用的是BASE64 的编解码方式还原响应中的汉字。所以需要使用BASE64Encoder 类进行编码操作。 // 使用下面的格式进行BASE64 编码后 String str = \\\"attachment; fileName=\\\" + \\\"=?utf-8?B?\\\" + new BASE64Encoder().encode(\\\"中文.jpg\\\".getBytes(\\\"utf-8\\\")) + \\\"?=\\\"; // 设置到响应头中 response.setHeader(\\\"Content-Disposition\\\", str); ---------------------------------------------------------------------------------- //通过判断请求头中User-Agent 这个请求头携带过来的浏览器信息即可判断出是什么浏览器。如下： String ua = request.getHeader(\\\"User-Agent\\\"); // 判断是否是火狐浏览器 if (ua.contains(\\\"Firefox\\\")) { // 使用下面的格式进行BASE64 编码后 String str = \\\"attachment; fileName=\\\" + \\\"=?utf-8?B?\\\" + new BASE64Encoder().encode(\\\"中文.jpg\\\".getBytes(\\\"utf-8\\\")) + \\\"?=\\\"; // 设置到响应头中 response.setHeader(\\\"Content-Disposition\\\", str); } else { // 把中文名进行UTF-8 编码操作。 String str = \\\"attachment; fileName=\\\" + URLEncoder.encode(\\\"中文.jpg\\\", \\\"UTF-8\\\"); // 然后把编码后的字符串设置到响应头中 response.setHeader(\\\"Content-Disposition\\\", str); } \"]}]},\"/study-tutorial/frame/javaweb/filter.html\":{\"title\":\"10、Filter 过滤器\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Filter 过滤器它的作用是：拦截请求，过滤响应。\",\"拦截请求常见的应用场景有：\",\"权限检查\",\"日记操作\",\"事务管理\",\"......\"]},{\"header\":\"1、案例：用户登录\",\"slug\":\"_1、案例-用户登录\",\"contents\":[\"<form action=\\\"http://localhost:8080/JavaWeb_06_war_exploded/loginServlet\\\" method=\\\"get\\\"> 用户名：<input type=\\\"text\\\" name=\\\"username\\\" ><br> 密码：<input type=\\\"password\\\" name=\\\"password\\\" ><br> <input type=\\\"submit\\\" value=\\\"登录\\\"> </form> \",\"//LoginServlet public class LoginServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String username = req.getParameter(\\\"username\\\"); String password = req.getParameter(\\\"password\\\"); if (\\\"xiaobear\\\".equals(username) && \\\"123456\\\".equals(password)) { //登录成功 Cookie cookie = new Cookie(\\\"username\\\", username); cookie.setMaxAge(60 * 60 * 24 * 7); //当前Cookie 一周内有效 resp.addCookie(cookie); System.out.println(\\\"登录成功\\\"); } else { // 登录失败 System.out.println(\\\"登录失败\\\"); } } } \",\"//AdminFilter public class AdminFilter implements Filter { public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; HttpSession session = request.getSession(); Object user = session.getAttribute(\\\"user\\\"); if (user == null) { servletRequest.getRequestDispatcher(\\\"/login.jsp\\\").forward(servletRequest,servletResponse); }else { //让程序继续访问目标资源 filterChain.doFilter(servletRequest,servletResponse); } } } \",\"<!--Web.xml--> <filter> <filter-name>AdminFilter</filter-name> <filter-class>com.xiaobear.filter.AdminFilter</filter-class> </filter> <filter-mapping> <!--filter-name 表示当前的拦截路径给哪个filter 使用--> <filter-name>AdminFilter</filter-name> <!--url-pattern 配置拦截路径 / 表示请求地址为：http://ip:port/工程路径/ 映射到IDEA 的web 目录 /admin/* 表示请求地址为：http://ip:port/工程路径/admin/* --> <url-pattern>/admin/*</url-pattern> </filter-mapping> <servlet> <servlet-name>LoginServlet</servlet-name> <servlet-class>com.xiaobear.servlet.LoginServlet</servlet-class> </servlet> <servlet-mapping> <servlet-name>LoginServlet</servlet-name> <url-pattern>/loginServlet</url-pattern> </servlet-mapping> \"]},{\"header\":\"2、Filter生命周期\",\"slug\":\"_2、filter生命周期\",\"contents\":[\"Filter 的生命周期包含几个方法 1、构造器方法 2、init 初始化方法 第1，2 步，在web 工程启动的时候执行（Filter 已经创建） 3、doFilter 过滤方法 第3 步，每次拦截到请求，就会执行 4、destroy 销毁 第4 步，停止web 工程的时候，就会执行（停止web 工程，也会销毁Filter 过滤器） \",\"public class AdminFilter implements Filter { public AdminFilter() { System.out.println(\\\"1、构造器方法\\\"); } public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\\\"2、初始化方法\\\"); } public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\\\"3、doFilter过滤方法\\\"); HttpServletRequest request = (HttpServletRequest) servletRequest; HttpSession session = request.getSession(); Object user = session.getAttribute(\\\"user\\\"); if (user == null) { servletRequest.getRequestDispatcher(\\\"/login.jsp\\\").forward(servletRequest,servletResponse); }else { //让程序继续访问目标资源 filterChain.doFilter(servletRequest,servletResponse); } } public void destroy() { System.out.println(\\\"4、destory方法\\\"); } } \"]},{\"header\":\"3、FilterConfig 类\",\"slug\":\"_3、filterconfig-类\",\"contents\":[\"Tomcat 每次创建Filter 的时候，也会同时创建一个FilterConfig 类，这里包含了Filter 配置文件的配置信息。\",\"FilterConfig 类的作用：获取filter 过滤器的配置内容\",\"获取Filter 的名称filter-name 的内容\",\"获取在Filter 中配置的init-param 初始化参数\",\"获取ServletContext 对象\",\"public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\\\"2、初始化方法\\\"); System.out.println(\\\"filter-name的值是：\\\"+filterConfig.getFilterName()); System.out.println(\\\"init-param中username的值 是：\\\"+filterConfig.getInitParameter(\\\"username\\\")); System.out.println(filterConfig.getServletContext()); } \"]},{\"header\":\"4、FilterChain过滤器链\",\"slug\":\"_4、filterchain过滤器链\",\"contents\":[]},{\"header\":\"5、Filter拦截路径\",\"slug\":\"_5、filter拦截路径\",\"contents\":[\"精准匹配\",\"<url-pattern>/target.jsp</url-pattern> 以上配置的路径，表示请求地址必须为：http://ip:port/工程路径/target.jsp \",\"目录匹配\",\"<url-pattern>/admin/*</url-pattern> 以上配置的路径，表示请求地址必须为：http://ip:port/工程路径/admin/* \",\"后缀名匹配\",\"<url-pattern>*.html</url-pattern> 以上配置的路径，表示请求地址必须以.html 结尾才会拦截到 <url-pattern>*.do</url-pattern> 以上配置的路径，表示请求地址必须以.do 结尾才会拦截到 <url-pattern>*.action</url-pattern> 以上配置的路径，表示请求地址必须以.action 结尾才会拦截到 Filter 过滤器它只关心请求的地址是否匹配，不关心请求的资源是否存在！！！ \"]}]},\"/study-tutorial/frame/javaweb/http.html\":{\"title\":\"3、HTTP协议\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"什么是协议 ? 协议是指双方，或多方，相互约定好，大家都需要遵守的规则，叫协议。\",\"客户端和服务器之间通信时，发送的数据，需要遵守的规则，叫HTTP 协议。HTTP 协议中的数据又叫报文\"]},{\"header\":\"1、请求的HTTP 协议格式\",\"slug\":\"_1、请求的http-协议格式\",\"contents\":[\"请求分为GET 请求，和POST 请求\",\"哪些是GET 请求，哪些是POST 请求\",\"GET 请求有哪些：\",\"1、form 标签method=get 2、a 标签 3、link 标签引入css 4、Script 标签引入js 文件 5、img 标签引入图片 6、iframe 引入html 页面 7、在浏览器地址栏中输入地址后敲回车\",\"POST 请求有哪些：\",\"form 标签method=post\"]},{\"header\":\"2、常用的响应码说明\",\"slug\":\"_2、常用的响应码说明\",\"contents\":[\"200 表示请求成功\",\"302 表示请求重定向（明天讲）\",\"404 表示请求服务器已经收到了，但是你要的数据不存在（请求地址错误）\",\"500 表示服务器已经收到请求，但是服务器内部错误（代码错误）\"]}]},\"/study-tutorial/frame/javaweb/httpServlet.html\":{\"title\":\"4、HttpServlet\",\"contents\":[{\"header\":\"1、HttpServletRequest\",\"slug\":\"_1、httpservletrequest\",\"contents\":[]},{\"header\":\"1、HttpServletRequest 类有什么作用。\",\"slug\":\"_1、httpservletrequest-类有什么作用。\",\"contents\":[\"每次只要有请求进入Tomcat 服务器，Tomcat 服务器就会把请求过来的HTTP 协议信息解析好封装到Request 对象中。然后传递到service 方法（doGet 和doPost）中给我们使用。我们可以通过HttpServletRequest 对象，获取到所有请求的信息。\"]},{\"header\":\"2、HttpServletRequest 类的常用方法\",\"slug\":\"_2、httpservletrequest-类的常用方法\",\"contents\":[\"getRequestURI() 获取请求的资源路径\",\"getRequestURL() 获取请求的统一资源定位符（绝对路径）\",\"getRemoteHost() 获取客户端的ip 地址\",\"getHeader() 获取请求头\",\"getParameter() 获取请求的参数\",\"getParameterValues() 获取请求的参数（多个值的时候使用）\",\"getMethod() 获取请求的方式GET 或POST\",\"setAttribute(key, value); 设置域数据\",\"getAttribute(key); 获取域数据\",\"getRequestDispatcher() 获取请求转发对象\",\"public class RequestApiServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(\\\"获取请求的路径=>\\\"+ req.getRequestURI()); System.out.println(\\\"获取统一资源定位符=>\\\"+ req.getRequestURI()); System.out.println(\\\"获取ip=>\\\"+req.getRemoteHost()); System.out.println(\\\"获取请求头=>\\\"+req.getHeader(\\\"User-Agent\\\")); System.out.println(\\\"请求方式=>\\\"+req.getMethod()); } } \",\"<!--index.html--> <!DOCTYPE html> <html lang=\\\"en\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>Title</title> </head> <body> <form action=\\\"http://localhost:8080/JavaWeb_02_war_exploded/parameterServlet\\\" method=\\\"get\\\"> 用户名：<input type=\\\"text\\\" name=\\\"username\\\"><br/> 密码：<input type=\\\"password\\\" name=\\\"password\\\"><br/> 兴趣爱好：<input type=\\\"checkbox\\\" name=\\\"hobby\\\" value=\\\"cpp\\\">C++ <input type=\\\"checkbox\\\" name=\\\"hobby\\\" value=\\\"java\\\">Java <input type=\\\"checkbox\\\" name=\\\"hobby\\\" value=\\\"js\\\">JavaScript<br/> <input type=\\\"submit\\\"> </form> </body> </html> \",\"public class ParameterServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String username = req.getParameter(\\\"username\\\"); String password = req.getParameter(\\\"password\\\"); String[] hobby = req.getParameterValues(\\\"hobby\\\"); System.out.println(\\\"用户名：\\\"+username); System.out.println(\\\"密码：\\\"+password); System.out.println(\\\"兴趣爱好：\\\"+ Arrays.asList(hobby)); } } \"]},{\"header\":\"3、doGet 请求的中文乱码解决：\",\"slug\":\"_3、doget-请求的中文乱码解决\",\"contents\":[\"// 获取请求参数 String username = req.getParameter(\\\"username\\\"); //1 先以iso8859-1 进行编码 //2 再以utf-8 进行解码 username = new String(username.getBytes(\\\"iso-8859-1\\\"), \\\"UTF-8\\\"); \"]},{\"header\":\"4、POST 请求的中文乱码\",\"slug\":\"_4、post-请求的中文乱码\",\"contents\":[\"@Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 设置请求体的字符集为UTF-8，从而解决post 请求的中文乱码问题 req.setCharacterEncoding(\\\"UTF-8\\\"); System.out.println(\\\"-------------doPost------------\\\"); // 获取请求参数 String username = req.getParameter(\\\"username\\\"); String password = req.getParameter(\\\"password\\\"); String[] hobby = req.getParameterValues(\\\"hobby\\\"); System.out.println(\\\"用户名：\\\" + username); System.out.println(\\\"密码：\\\" + password); System.out.println(\\\"兴趣爱好：\\\" + Arrays.asList(hobby)); } \"]},{\"header\":\"5、Web路径\",\"slug\":\"_5、web路径\",\"contents\":[\"相对路径 \",\". 表示当前目录\",\".. 表示上一级目录\",\"资源名 表示当前目录/资源名\",\"绝对路径 \",\"http://ip:port/工程路径/资源路径\"]},{\"header\":\"6、/ 斜杠的不同意义\",\"slug\":\"_6、-斜杠的不同意义\",\"contents\":[\"在web 中**/ 斜杠是一种绝对路径**。\",\"/ 斜杠如果被浏览器解析，得到的地址是：http://ip:port/\",\"<a href=\\\"/\\\">斜杠</a> \",\"/ 斜杠如果被服务器解析，得到的地址是：http://ip:port/工程路径\",\"<url-pattern>/servlet1</url-pattern> \",\"servletContext.getRealPath(“/”); request.getRequestDispatcher(“/”); \",\"特殊情况：response.sendRediect(“/”);把斜杠发送给浏览器解析。得到http://ip:port/\"]},{\"header\":\"2、HttpServletResponse\",\"slug\":\"_2、httpservletresponse\",\"contents\":[]},{\"header\":\"1、HttpServletResponse 类的作用\",\"slug\":\"_1、httpservletresponse-类的作用\",\"contents\":[\"HttpServletResponse 类和HttpServletRequest 类一样。每次请求进来，Tomcat 服务器都会创建一个Response 对象传递给Servlet 程序去使用。HttpServletRequest 表示请求过来的信息，HttpServletResponse 表示所有响应的信息，我们如果需要设置返回给客户端的信息，都可以通过HttpServletResponse 对象来进行设置\"]},{\"header\":\"2、相关的输出流\",\"slug\":\"_2、相关的输出流\",\"contents\":[\"字节流 getOutputStream(); 常用于下载（传递二进制数据） 字符流 getWriter(); 常用于回传字符串（常用）\",\"两个流同时只能使用一个。\"]},{\"header\":\"3、往客户端回传数据\",\"slug\":\"_3、往客户端回传数据\",\"contents\":[\"public class ReponseIOServlet extends HttpServlet { @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { PrintWriter writer = response.getWriter(); writer.write(\\\"xiaobear,you are nice!!!\\\"); } } \"]},{\"header\":\"4、响应的乱码解决\",\"slug\":\"_4、响应的乱码解决\",\"contents\":[\"/*-----------------------------------方式一（不推荐）--------------------------------------*/ // 设置服务器字符集为UTF-8 resp.setCharacterEncoding(\\\"UTF-8\\\"); // 通过响应头，设置浏览器也使用UTF-8 字符集 resp.setHeader(\\\"Content-Type\\\", \\\"text/html; charset=UTF-8\\\"); /*-----------------------------------方式二（推荐）--------------------------------------*/ // 它会同时设置服务器和客户端都使用UTF-8 字符集，还设置了响应头 // 此方法一定要在获取流对象之前调用才有效 resp.setContentType(\\\"text/html; charset=UTF-8\\\"); PrintWriter writer = response.getWriter(); writer.write(\\\"鄢汉雄最棒！！！\\\"); \"]},{\"header\":\"5、请求重定向\",\"slug\":\"_5、请求重定向\",\"contents\":[\"请求重定向，是指客户端给服务器发请求，然后服务器告诉客户端说。我给你一些地址。你去新地址访问。叫请求重定向（因为之前的地址可能已经被废弃）。\",\"请求重定向方式一：\",\"// 设置响应状态码302 ，表示重定向，（已搬迁） resp.setStatus(302); // 设置响应头，说明新的地址在哪里 resp.setHeader(\\\"Location\\\", \\\"http://localhost:8080\\\"); \",\"请求重定向方式二（推荐）：\",\"resp.sendRedirect(\\\"http://localhost:8080\\\"); \"]}]},\"/study-tutorial/frame/javaweb/json_ajax_i18n.html\":{\"title\":\"11、JSON & Ajax & i18n\",\"contents\":[{\"header\":\"1、JSON\",\"slug\":\"_1、json\",\"contents\":[\"JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。JSON采用完全独立于语言的文本格式，而且很多语言都提供了对json 的支持（包括C, C++, C#, Java, JavaScript, Perl, Python等）。这样就使得JSON 成为理想的数据交换格式。\",\"轻量级指的是跟xml 做比较\",\"数据交换指的是客户端和服务器之间业务数据的传递格式\"]},{\"header\":\"1、json 的定义\",\"slug\":\"_1、json-的定义\",\"contents\":[\"json 是由键值对组成，并且由花括号（大括号）包围。每个键由引号引起来，键和值之间使用冒号进行分隔，多\",\"组键值对之间进行逗号进行分隔。\",\"// json的定义 var jsonObj = { \\\"key1\\\":12, \\\"key2\\\":\\\"yhx\\\", \\\"key3\\\":true, \\\"key4\\\":[11,\\\"arr\\\",false], \\\"key5\\\":{ \\\"key6\\\":1001, \\\"key7\\\":\\\"102\\\" }, \\\"key8\\\":[{ \\\"key9\\\":1001, \\\"key10\\\":\\\"102\\\" }, { \\\"key9\\\":1001, \\\"key10\\\":\\\"102\\\" }] } \"]},{\"header\":\"2、json的访问\",\"slug\":\"_2、json的访问\",\"contents\":[\"json就是一个对象\",\"json 中的key 我们可以理解为是对象中的一个属性\",\"json 中的key 访问就跟访问对象的属性一样： json 对象.key\",\"// json的访问 alert(typeof(jsonObj)); alert(jsonObj.key1); alert(jsonObj.key2); alert(jsonObj.key3); alert(jsonObj.key4); //遍历数组 for (var i = 0; i < jsonObj.key4.length ; i++) { alert(jsonObj.key4[i]); } alert(jsonObj.key5.key6); alert(jsonObj.key5.key7); alert( jsonObj.key8 );// 得到json 数组 // 取出来每一个元素都是json 对象 var jsonItem = jsonObj.key6[0]; alert( jsonItem.key6_1_1 ); alert( jsonItem.key9 ); \"]},{\"header\":\"3、json的常用方法\",\"slug\":\"_3、json的常用方法\",\"contents\":[\"json 的存在有两种形式。\",\"对象的形式存在，我们叫它json 对象，一般我们要操作json 中的数据的时候，需要json 对象的格式。\",\"字符串的形式存在，我们叫它json 字符串，一般我们要在客户端和服务器之间进行数据交换的时候，使用json 字符串。\",\"JSON.stringify() 把json 对象转换成为json 字符串\",\"JSON.parse() 把json 字符串转换成为json 对象\",\"// json对象转字符串 像Java中的toString var Stringobj = JSON.stringify(jsonObj); alert(Stringobj); // json字符串转json对象 var jsonobj2 = JSON.parse(jsonObj); alert(jsonobj2.key1); alert(jsonobj2.key4); \"]},{\"header\":\"4、JSON 在java 中的使用\",\"slug\":\"_4、json-在java-中的使用\",\"contents\":[]},{\"header\":\"1、JavaBean 与 json的互转\",\"slug\":\"_1、javabean-与-json的互转\",\"contents\":[\"@Test public void test1(){ Person person = new Person(); Gson gson = new Gson(); // toJson 方法可以把java 对象转换成为json 字符串 String json = gson.toJson(person); System.out.println(json); // fromJson 把json 字符串转换回Java 对象 // 第一个参数是json 字符串 // 第二个参数是转换回去的Java 对象类型 Person person1 = gson.fromJson(json, Person.class); System.out.println(person1); } \"]},{\"header\":\"2、List 和json 的互转\",\"slug\":\"_2、list-和json-的互转\",\"contents\":[\"@Test public void test2(){ List<Person> list = new ArrayList<Person>(); list.add(new Person(1,\\\"yhx\\\")); list.add(new Person(2,\\\"lwh\\\")); Gson gson = new Gson(); String json = gson.toJson(list); System.out.println(json); } \"]},{\"header\":\"3、Map和json的互转\",\"slug\":\"_3、map和json的互转\",\"contents\":[\"@Test public void test3(){ Map<Integer, Person> map = new HashMap<Integer, Person>(); map.put(1,new Person(3,\\\"333\\\")); map.put(2,new Person(4,\\\"444\\\")); Gson gson = new Gson(); // 把map 集合转换成为json 字符串 String s = gson.toJson(map); System.out.println(s); } \"]},{\"header\":\"2、Ajax\",\"slug\":\"_2、ajax\",\"contents\":[\"AJAX 即==“Asynchronous Javascript And XML”（异步JavaScript 和XML==），是指一种创建交互式网页应用的网页开发技术。\",\"Ajax 是一种浏览器通过js 异步发起请求，局部更新页面的技术。\",\"Ajax 请求的局部更新，浏览器地址栏不会发生变化\",\"局部更新不会舍弃原来页面的内容\"]},{\"header\":\"1、原生Ajax请求\",\"slug\":\"_1、原生ajax请求\",\"contents\":[\"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Insert title here</title> <script type=\\\"text/javascript\\\"> function ajaxRequest() { // 1、我们首先要创建XMLHttpRequest var request = new XMLHttpRequest(); // 2、调用open方法设置请求参数 request.open(\\\"GET\\\",\\\"http://localhost:8080/JavaWeb_07_war_exploded/ajaxServlet?action=javaScriptAjax\\\",true) // request.onreadystatechange = function () { if(request.readyState == 4 && request.status == 200){ alert(request.responseText); } } // 3、调用send方法发送请求 request.send(); // 4、在send方法前绑定onreadystatechange事件，处理请求完成后的操作。 } </script> </head> <body> <button onclick=\\\"ajaxRequest()\\\">ajax request</button> <div id=\\\"div01\\\"> </div> </body> </html> \",\"BaseServlet代码：\",\"public abstract class BaseServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 解决post请求中文乱码问题 // 一定要在获取请求参数之前调用才有效 req.setCharacterEncoding(\\\"UTF-8\\\"); // 解决响应中文乱码问题 resp.setContentType(\\\"text/html; charset=UTF-8\\\"); String action = req.getParameter(\\\"action\\\"); try { // 获取action业务鉴别字符串，获取相应的业务 方法反射对象 Method method = this.getClass().getDeclaredMethod(action, HttpServletRequest.class, HttpServletResponse.class); // System.out.println(method); // 调用目标业务 方法 method.invoke(this, req, resp); } catch (Exception e) { e.printStackTrace(); } } } \",\"AjaxServlet代码：\",\"public class AjaxServelt extends BaseServlet{ protected void javaScriptAjax(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(\\\"Ajax请求过来了！！！\\\"); } } \"]},{\"header\":\"2、jQuery 中的AJAX 请求\",\"slug\":\"_2、jquery-中的ajax-请求\",\"contents\":[\"Jquery_Ajax_request.html页面\",\"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Insert title here</title> <script type=\\\"text/javascript\\\" src=\\\"script/jquery-1.7.2.js\\\"></script> <script type=\\\"text/javascript\\\"> $(function(){ // ajax请求 $(\\\"#ajaxBtn\\\").click(function(){ $.ajax({ url:\\\"http://localhost:8080/JavaWeb_07_war_exploded/ajaxServlet\\\" , data:\\\"action=jQueryAjax\\\" , type:\\\"GET\\\" , success: function (data) { alert(\\\"服务器返回的类型是：\\\"+data) }, dataType:\\\"json\\\", }); }) alert(\\\"ajax btn\\\"); }); // ajax--get请求 $(\\\"#getBtn\\\").click(function(){ alert(\\\" get btn \\\"); }); // ajax--post请求 $(\\\"#postBtn\\\").click(function(){ // post请求 alert(\\\"post btn\\\"); }); // ajax--getJson请求 $(\\\"#getJSONBtn\\\").click(function(){ // 调用 alert(\\\"getJSON btn\\\"); }); // ajax请求 $(\\\"#submit\\\").click(function(){ // 把参数序列化 alert(\\\"serialize()\\\"); }); }); </script> </head> <body> <div> <button id=\\\"ajaxBtn\\\">$.ajax请求</button> <button id=\\\"getBtn\\\">$.get请求</button> <button id=\\\"postBtn\\\">$.post请求</button> <button id=\\\"getJSONBtn\\\">$.getJSON请求</button> </div> <br/><br/> <form id=\\\"form01\\\" > 用户名：<input name=\\\"username\\\" type=\\\"text\\\" /><br/> 密码：<input name=\\\"password\\\" type=\\\"password\\\" /><br/> 下拉单选：<select name=\\\"single\\\"> <option value=\\\"Single\\\">Single</option> <option value=\\\"Single2\\\">Single2</option> </select><br/> 下拉多选： <select name=\\\"multiple\\\" multiple=\\\"multiple\\\"> <option selected=\\\"selected\\\" value=\\\"Multiple\\\">Multiple</option> <option value=\\\"Multiple2\\\">Multiple2</option> <option selected=\\\"selected\\\" value=\\\"Multiple3\\\">Multiple3</option> </select><br/> 复选： <input type=\\\"checkbox\\\" name=\\\"check\\\" value=\\\"check1\\\"/> check1 <input type=\\\"checkbox\\\" name=\\\"check\\\" value=\\\"check2\\\" checked=\\\"checked\\\"/> check2<br/> 单选： <input type=\\\"radio\\\" name=\\\"radio\\\" value=\\\"radio1\\\" checked=\\\"checked\\\"/> radio1 <input type=\\\"radio\\\" name=\\\"radio\\\" value=\\\"radio2\\\"/> radio2<br/> </form> <button id=\\\"submit\\\">提交--serialize()</button> </body> </html> \",\"$.ajax 方法\",\"url 表示请求的地址\",\"type 表示请求的类型GET 或POST 请求\",\"data 表示发送给服务器的数据\",\"格式有两种：\",\"name=value&name=value\",\"{key:value} success 请求成功，响应的回调函数 dataType 响应的数据类型 常用的数据类型有：\",\"text 表示纯文本\",\"xml 表示xml 数据\",\"json 表示json 对象\",\"$(\\\"#ajaxBtn\\\").click(function(){ $.ajax({ url:\\\"http://localhost:8080/16_json_ajax_i18n/ajaxServlet\\\", // data:\\\"action=jQueryAjax\\\", data:{action:\\\"jQueryAjax\\\"}, type:\\\"GET\\\", success:function (data) { // alert(\\\"服务器返回的数据是：\\\" + data); // var jsonObj = JSON.parse(data); $(\\\"#msg\\\").html(\\\"编号：\\\" + data.id + \\\" , 姓名：\\\" + data.name); }, dataType : \\\"json\\\" }); }); \",\".get方法和.post 方法 url 请求的url 地址 data 发送的数据 callback 成功的回调函数 type 返回的数据类型\",\"$(\\\"#getBtn\\\").click(function(){ $.get(\\\"http://localhost:8080/16_json_ajax_i18n/ajaxServlet\\\",\\\"action=jQueryGet\\\",function (data) { $(\\\"#msg\\\").html(\\\" get 编号：\\\" + data.id + \\\" , 姓名：\\\" + data.name); },\\\"json\\\"); }); // ajax--post 请求 $(\\\"#postBtn\\\").click(function(){ $.post(\\\"http://localhost:8080/16_json_ajax_i18n/ajaxServlet\\\",\\\"action=jQueryPost\\\",function (data) { $(\\\"#msg\\\").html(\\\" post 编号：\\\" + data.id + \\\" , 姓名：\\\" + data.name); },\\\"json\\\"); }); \",\"$.getJSON 方法 url 请求的url 地址 data 发送给服务器的数据 callback 成功的回调函数\",\"// ajax--getJson 请求 $(\\\"#getJSONBtn\\\").click(function(){ $.getJSON(\\\"http://localhost:8080/16_json_ajax_i18n/ajaxServlet\\\",\\\"action=jQueryGetJSON\\\",function (data) { $(\\\"#msg\\\").html(\\\" getJSON 编号：\\\" + data.id + \\\" , 姓名：\\\" + data.name); }); }); \",\"表单序列化serialize() serialize() 可以把表单中所有表单项的内容都获取到，并以name=value&name=value 的形式进行拼接。\",\"// ajax 请求 $(\\\"#submit\\\").click(function(){ // 把参数序列化 $.getJSON(\\\"http://localhost:8080/16_json_ajax_i18n/ajaxServlet\\\",\\\"action=jQuerySerialize&\\\" + $(\\\"#form01\\\").serialize(),function (data) { $(\\\"#msg\\\").html(\\\" Serialize 编号：\\\" + data.id + \\\" , 姓名：\\\" + data.name); }); }); \"]},{\"header\":\"3、i18n国际化\",\"slug\":\"_3、i18n国际化\",\"contents\":[\"国际化（Internationalization）指的是同一个网站可以支持多种不同的语言，以方便不同国家，不同语种的用户访问。\",\"关于国际化我们想到的最简单的方案就是为不同的国家创建不同的网站，比如苹果公司，他的英文官网是：http://www.apple.com 而中国官网是http://www.apple.com/cn\",\"苹果公司这种方案并不适合全部公司，而我们希望相同的一个网站，而不同人访问的时候可以根据用户所在的区域显示不同的语言文字，而网站的布局样式等不发生改变。\",\"于是就有了我们说的国际化，国际化总的来说就是同一个网站不同国家的人来访问可以显示出不同的语言。但实际上这种需求并不强烈，一般真的有国际化需求的公司，主流采用的依然是苹果公司的那种方案，为不同的国家创建不同的页面。所以国际化的内容我们了解一下即可。\",\"国际化的英文Internationalization，但是由于拼写过长，老外想了一个简单的写法叫做I18N，代表的是Internationalization这个单词，以I 开头，以N 结尾，而中间是18 个字母，所以简写为I18N。以后我们说I18N 和国际化是一个意思。\"]},{\"header\":\"1、i18n相关要素\",\"slug\":\"_1、i18n相关要素\",\"contents\":[\"国际化包名命名规则：baseName_locale.properties (locale表示不同的时区、位置、语言)\",\"中文的配置文件名：i18n_zh_CN.properties\",\"英文的配置文件名：i18n_en_US.properties\"]},{\"header\":\"2、国际化资源properties 测试\",\"slug\":\"_2、国际化资源properties-测试\",\"contents\":[\"i18n_en_US.properties 英文\",\"username=username password=password sex=sex age=age regist=regist boy=boy email=email girl=girl reset=reset submit=submit \",\"i18n_zh_CN.properties 中文\",\"username=用户名 password=密码 sex=性别 age=年龄 regist=注册 boy=男 girl=女 email=邮箱 reset=重置 submit=提交 \",\"测试代码：\",\"public class I18nTest { @Test public void LocaleTest(){ Locale locale = Locale.getDefault(); System.out.println(locale); //遍历不同国家的locale for (Locale availableLocale : Locale.getAvailableLocales()) { System.out.println(availableLocale); } // 获取中文，中文的常量的Locale 对象 System.out.println(Locale.CHINA); // 获取英文，英文的常量的Locale 对象 System.out.println(Locale.US); } @Test public void testI18n(){ // 得到我们需要的Locale 对象 Locale locale = Locale.CHINA; // 通过指定的basename 和Locale 对象，读取相应的配置文件 ResourceBundle resourceBundle = ResourceBundle.getBundle(\\\"i18n\\\", locale); System.out.println(\\\"username: \\\"+resourceBundle.getString(\\\"username\\\")); System.out.println(\\\"password: \\\"+resourceBundle.getString(\\\"password\\\")); System.out.println(\\\"girl: \\\"+resourceBundle.getString(\\\"girl\\\")); System.out.println(\\\"boy: \\\"+resourceBundle.getString(\\\"boy\\\")); } } \"]},{\"header\":\"3、请求头国际化页面\",\"slug\":\"_3、请求头国际化页面\",\"contents\":[\"<%@ page import=\\\"java.util.Locale\\\" %> <%@ page import=\\\"java.util.ResourceBundle\\\" %> <%@ page language=\\\"java\\\" contentType=\\\"text/html; charset=UTF-8\\\" pageEncoding=\\\"UTF-8\\\"%> <!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Insert title here</title> </head> <body> <% Locale locale = request.getLocale(); System.out.println(locale); ResourceBundle i18n = ResourceBundle.getBundle(\\\"i18n\\\", locale); %> <a href=\\\"\\\">中文</a>| <a href=\\\"\\\">english</a> <center> <h1><%=i18n.getString(\\\"regist\\\")%></h1> <table> <form> <tr> <td><%=i18n.getString(\\\"username\\\")%></td> <td><input name=\\\"username\\\" type=\\\"text\\\" /></td> </tr> <tr> <td><%=i18n.getString(\\\"password\\\")%></td> <td><input type=\\\"password\\\" /></td> </tr> <tr> <td><%=i18n.getString(\\\"sex\\\")%></td> <td><input type=\\\"radio\\\" /><%=i18n.getString(\\\"boy\\\")%><input type=\\\"radio\\\" /><%=i18n.getString(\\\"girl\\\")%></td> </tr> <tr> <td><%=i18n.getString(\\\"email\\\")%></td> <td><input type=\\\"text\\\" /></td> </tr> <tr> <td colspan=\\\"2\\\" align=\\\"center\\\"> <input type=\\\"reset\\\" value=\\\"<%=i18n.getString(\\\"reset\\\")%>\\\" />&nbsp;&nbsp; <input type=\\\"submit\\\" value=\\\"<%=i18n.getString(\\\"submit\\\")%>\\\" /></td> </tr> </form> </table> <br /> <br /> <br /> <br /> </center> 国际化测试： <br /> 1、访问页面，通过浏览器设置，请求头信息确定国际化语言。 <br /> 2、通过左上角，手动切换语言 </body> </html> \"]},{\"header\":\"4、通过显示的选择语言类型进行国际化\",\"slug\":\"_4、通过显示的选择语言类型进行国际化\",\"contents\":[\"<%@ page import=\\\"java.util.Locale\\\" %> <%@ page import=\\\"java.util.ResourceBundle\\\" %> <%@ page language=\\\"java\\\" contentType=\\\"text/html; charset=UTF-8\\\" pageEncoding=\\\"UTF-8\\\"%> <!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Insert title here</title> </head> <body> <% Locale locale = null; String country = request.getParameter(\\\"country\\\"); if (\\\"cn\\\".equals(country)){ locale = Locale.CHINA; }else if(\\\"usa\\\".equals(country)){ locale = Locale.US; }else { locale = Locale.getDefault(); } System.out.println(locale); ResourceBundle i18n = ResourceBundle.getBundle(\\\"i18n\\\", locale); %> <a href=\\\"i18n.jsp?country=cn\\\">中文</a>| <a href=\\\"i18n.jsp?country=usa\\\">english</a> <center> <h1><%=i18n.getString(\\\"regist\\\")%></h1> <table> <form> <tr> <td><%=i18n.getString(\\\"username\\\")%></td> <td><input name=\\\"username\\\" type=\\\"text\\\" /></td> </tr> <tr> <td><%=i18n.getString(\\\"password\\\")%></td> <td><input type=\\\"password\\\" /></td> </tr> <tr> <td><%=i18n.getString(\\\"sex\\\")%></td> <td><input type=\\\"radio\\\" /><%=i18n.getString(\\\"boy\\\")%><input type=\\\"radio\\\" /><%=i18n.getString(\\\"girl\\\")%></td> </tr> <tr> <td><%=i18n.getString(\\\"email\\\")%></td> <td><input type=\\\"text\\\" /></td> </tr> <tr> <td colspan=\\\"2\\\" align=\\\"center\\\"> <input type=\\\"reset\\\" value=\\\"<%=i18n.getString(\\\"reset\\\")%>\\\" />&nbsp;&nbsp; <input type=\\\"submit\\\" value=\\\"<%=i18n.getString(\\\"submit\\\")%>\\\" /></td> </tr> </form> </table> <br /> <br /> <br /> <br /> </center> 国际化测试： <br /> 1、访问页面，通过浏览器设置，请求头信息确定国际化语言。 <br /> 2、通过左上角，手动切换语言 </body> </html> \"]},{\"header\":\"5、JSTL 标签库实现国际化\",\"slug\":\"_5、jstl-标签库实现国际化\",\"contents\":[\"<%--1 使用标签设置Locale 信息--%> <fmt:setLocale value=\\\"\\\" /> <%--2 使用标签设置baseName--%> <fmt:setBundle basename=\\\"\\\"/> <%--3 输出指定key 的国际化信息--%> <fmt:message key=\\\"\\\" /> \",\"<%@ taglib prefix=\\\"fmt\\\" uri=\\\"http://java.sun.com/jstl/fmt\\\" %> <%@ page import=\\\"java.util.Locale\\\" %> <%@ page language=\\\"java\\\" contentType=\\\"text/html; charset=UTF-8\\\" pageEncoding=\\\"UTF-8\\\"%> <!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\" \\\"http://www.w3.org/TR/html4/loose.dtd\\\"> <html> <head> <meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"cache-control\\\" content=\\\"no-cache\\\" /> <meta http-equiv=\\\"Expires\\\" content=\\\"0\\\" /> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <title>Insert title here</title> </head> <body> <%--1 使用标签设置Locale 信息--%> <fmt:setLocale value=\\\"${param.locale}\\\" /> <%--2 使用标签设置baseName--%> <fmt:setBundle basename=\\\"i18n\\\"/> <a href=\\\"i18n_fmt.jsp?locale=zh_CN\\\">中文</a>| <a href=\\\"i18n_fmt.jsp?locale=en_US\\\">english</a> <center> <h1><fmt:message key=\\\"regist\\\" /></h1> <table> <form> <tr> <td><fmt:message key=\\\"username\\\" /></td> <td><input name=\\\"username\\\" type=\\\"text\\\" /></td> </tr> <tr> <td><fmt:message key=\\\"password\\\" /></td> <td><input type=\\\"password\\\" /></td> </tr> <tr> <td><fmt:message key=\\\"sex\\\" /></td> <td><input type=\\\"radio\\\" /><fmt:message key=\\\"boy\\\" /> <input type=\\\"radio\\\" /><fmt:message key=\\\"girl\\\" /></td> </tr> <tr> <td><fmt:message key=\\\"email\\\" /></td> <td><input type=\\\"text\\\" /></td> </tr> <tr> <td colspan=\\\"2\\\" align=\\\"center\\\"> <input type=\\\"reset\\\" value=\\\"<fmt:message key=\\\"reset\\\" />\\\" />&nbsp;&nbsp; <input type=\\\"submit\\\" value=\\\"<fmt:message key=\\\"submit\\\" />\\\" /></td> </tr> </form> </table> <br /> <br /> <br /> <br /> </center> </body> </html> \",\"错误总结：\",\"Exception org.apache.jasper.JasperException: /i18n_fmt.jsp (行.: [16], 列: [0]) According to TLD or attribute directive in tag file, attribute [value] does not accept any expressions org.apache.jasper.compiler.DefaultErrorHandler.jspError(DefaultErrorHandler.java:42) org.apache.jasper.compiler.ErrorDispatcher.dispatch(ErrorDispatcher.java:292) org.apache.jasper.compiler.ErrorDispatcher.jspError(ErrorDispatcher.java:115) org.apache.jasper.compiler.Validator$ValidateVisitor.checkXmlAttributes(Validator.java:1250) org.apache.jasper.compiler.Validator$ValidateVisitor.visit(Validator.java:888) org.apache.jasper.compiler.Node$CustomTag.accept(Node.java:1544) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2389) org.apache.jasper.compiler.Node$Visitor.visitBody(Node.java:2441) org.apache.jasper.compiler.Node$Visitor.visit(Node.java:2447) org.apache.jasper.compiler.Node$Root.accept(Node.java:470) org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2389) org.apache.jasper.compiler.Validator.validateExDirectives(Validator.java:1857) org.apache.jasper.compiler.Compiler.generateJava(Compiler.java:224) org.apache.jasper.compiler.Compiler.compile(Compiler.java:386) org.apache.jasper.compiler.Compiler.compile(Compiler.java:362) org.apache.jasper.compiler.Compiler.compile(Compiler.java:346) org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:605) org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:400) org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:385) org.apache.jasper.servlet.JspServlet.service(JspServlet.java:329) javax.servlet.http.HttpServlet.service(HttpServlet.java:741) org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) \",\"原因：版本不支持该标签\",\"解决：\",\"<%@ taglib prefix=\\\"fmt\\\" uri=\\\"http://java.sun.com/jstl/fmt\\\" %> 改为 <%@ taglib prefix=\\\"fmt\\\" uri=\\\"http://java.sun.com/jsp/jstl/fmt\\\" %> \"]}]},\"/study-tutorial/frame/javaweb/jsp.html\":{\"title\":\"5、jsp\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"jsp 的全换是java server pages。Java 的服务器页面。 jsp 的主要作用是代替Servlet 程序回传html 页面的数据。 因为Servlet 程序回传html 页面数据是一件非常繁锁的事情。开发成本和维护成本都极高。\",\"Servlet 回传html 页面数据的代码：\",\"public class JspDemo extends HttpServlet { @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\\\"text/html;charset=UTF-8\\\"); PrintWriter writer = response.getWriter(); writer.write(\\\"<!DOCTYPE html>\\\\r\\\\n\\\"); writer.write(\\\" <html lang=\\\\\\\"en\\\\\\\">\\\\r\\\\n\\\"); writer.write(\\\" <head>\\\\r\\\\n\\\"); writer.write(\\\" <meta charset=\\\\\\\"UTF-8\\\\\\\">\\\\r\\\\n\\\"); writer.write(\\\" <title>Title</title>\\\\r\\\\n\\\"); writer.write(\\\" </head>\\\\r\\\\n\\\"); writer.write(\\\" <body>\\\\r\\\\n\\\"); writer.write(\\\" 这是html 页面数据\\\\r\\\\n\\\"); writer.write(\\\" </body>\\\\r\\\\n\\\"); writer.write(\\\"</html>\\\\r\\\\n\\\"); writer.write(\\\"\\\\r\\\\n\\\"); } } \",\"<!DOCTYPE html> <html lang=\\\"en\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>Title</title> </head> <body> 这是HTML页面数据 </body> </html> \"]},{\"header\":\"1、jsp的本质\",\"slug\":\"_1、jsp的本质\",\"contents\":[\"jsp 页面本质上是一个Servlet 程序。\"]},{\"header\":\"2、jsp 语法\",\"slug\":\"_2、jsp-语法\",\"contents\":[]},{\"header\":\"1、jsp 头部的page 指令\",\"slug\":\"_1、jsp-头部的page-指令\",\"contents\":[\"jsp 的page 指令可以修改jsp 页面中一些重要的属性，或者行为。\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" import=\\\"java.applet.Applet\\\" autoFlush=\\\"true\\\" buffer=\\\"8kb\\\" errorPage=\\\"index.jsp\\\" isErrorPage=\\\"true\\\" session=\\\"true\\\" extends=\\\"java.io\\\" language=\\\"java\\\" %> <%-- language 属性表示jsp 翻译后是什么语言文件。暂时只支持java。 contentType 属性表示jsp 返回的数据类型是什么。也是源码中response.setContentType()参数值 pageEncoding 属性表示当前jsp 页面文件本身的字符集。 import 属性跟java 源代码中一样。用于导包，导类。 ========================两个属性是给out 输出流使用============================= autoFlush 属性设置当out 输出流缓冲区满了之后，是否自动刷新缓冲区。默认值是true。 buffer 属性设置out 缓冲区的大小。默认是8kb ========================两个属性是给out 输出流使用============================= errorPage 属性设置当jsp 页面运行时出错，自动跳转去的错误页面路径。 isErrorPage 属性设置当前jsp 页面是否是错误信息页面。默认是false。如果是true 可以获取异常信息。 session 属性设置访问当前jsp 页面，是否会创建HttpSession 对象。默认是true。 extends 属性设置jsp 翻译出来的java 类默认继承谁。 --%> \"]},{\"header\":\"2、jsp常用脚本\",\"slug\":\"_2、jsp常用脚本\",\"contents\":[\"声明脚本\",\"<%--声明脚本的格式是：--%> <%! 声明java 代码%> \",\"作用：可以给jsp 翻译出来的java 类定义属性和方法甚至是静态代码块。内部类等。\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> </head> <body> <%--1、声明类属性--%> <%! private Integer id; private String name; private static Map<String,Object> map; %> <%--2、声明static 静态代码块--%> <%! static { map = new HashMap<String,Object>(); map.put(\\\"key1\\\",\\\"value1\\\"); map.put(\\\"key2\\\",\\\"value2\\\"); map.put(\\\"key3\\\",\\\"value3\\\"); } %> <%--3、声明类方法--%> <%! public int sb(){ return 12; } %> <%--4、声明内部类--%> <%! public static class Yhx{ private Integer id = 10; private String abc = \\\"abc\\\"; } %> </body> </html> \",\"表达式脚本（常用）\",\"表达式脚本的格式是：<%=表达式%> 表达式脚本的作用是：jsp 页面上输出数据。\",\" <%--1. 输出整型--%> <%=12%> <%--2. 输出浮点型--%> <%=12.12%> <%--3. 输出字符串--%> <%=\\\"你是最棒的！\\\"%> <%--4. 输出对象--%> <%=request.getParameter(\\\"username\\\")%> \",\"表达式脚本的特点： \",\"所有的表达式脚本都会被翻译到jspService() 方法中\",\"表达式脚本都会被翻译成为out.print()输出到页面上\",\"由于表达式脚本翻译的内容都在jspService() 方法中,所以_jspService()方法中的对象都可以直接使用\",\"表达式脚本中的表达式不能以分号结束*。\",\"代码脚本\",\"<% java 语句 %> \",\"代码脚本的作用是：可以在jsp 页面中，编写我们自己需要的功能（写的是java 语句）。\",\" <%--代码脚本----if 语句--%> <% int i = 18; if (i == 10){ System.out.println(\\\"你很帅！！！\\\"); }else{ System.out.println(\\\"你是真的帅！！！\\\"); } %> <%--2. 代码脚本----for 循环语句--%> <% for (int j = 0; j < 100; j++) { System.out.println(j); } %> <%--3. 翻译后java 文件中_jspService 方法内的代码都可以写--%> <% String username = request.getParameter(\\\"username\\\"); System.out.println(username); %> \",\"代码脚本的特点是： \",\"代码脚本翻译之后都在_jspService 方法中\",\"代码脚本由于翻译到_jspService()方法中，所以在_jspService()方法中的现有对象都可以直接使用。\",\"还可以由多个代码脚本块组合完成一个完整的java 语句。\",\"代码脚本还可以和表达式脚本一起组合使用，在jsp 页面上输出数据\"]},{\"header\":\"3、jsp 中的三种注释\",\"slug\":\"_3、jsp-中的三种注释\",\"contents\":[\"html 注释\",\"html 注释会被翻译到java 源代码中。在_jspService 方法里，以out.writer 输出到客户端。\",\"java 注释\",\"<% // 单行java 注释 /* 多行java 注释*/ %> java 注释会被翻译到java 源代码中。\",\"jsp 注释\",\"<%-- 这是jsp 注释--%> jsp 注释可以注掉，jsp 页面中所有代码。\"]},{\"header\":\"4、jsp 九大内置对象\",\"slug\":\"_4、jsp-九大内置对象\",\"contents\":[\"jsp 中的内置对象，是指Tomcat 在翻译jsp 页面成为Servlet 源代码后，内部提供的九大对象，叫内置对象。\"]},{\"header\":\"5、jsp的四大域对象\",\"slug\":\"_5、jsp的四大域对象\",\"contents\":[\"域对象是可以像Map 一样存取数据的对象\",\"pageContext (PageContextImpl 类) 当前jsp 页面范围内有效\",\"request (HttpServletRequest 类) 一次请求内有效\",\"session (HttpSession 类) 一个会话范围内有效（打开浏览器访问服务器，直到关闭浏览器）\",\"application (ServletContext 类) 整个web 工程范围内都有效（只要web 工程不停止，数据都在）\",\"优先顺序：pageContext ----> request ----> session ----> application\",\"<%----------------------------------------socre.jsp--------------------------------------%> <body> <h2>score.jsp页面</h2> <% pageContext.setAttribute(\\\"key\\\",\\\"pageContext\\\"); request.setAttribute(\\\"key\\\",\\\"request\\\"); session.setAttribute(\\\"key\\\",\\\"session\\\"); application.setAttribute(\\\"key\\\",\\\"application\\\"); %> pageContext域是否有值：<%=pageContext.getAttribute(\\\"key\\\") %><br> request域是否有值：<%=request.getAttribute(\\\"key\\\") %><br> session域是否有值：<%=session.getAttribute(\\\"key\\\") %><br> application域是否有值：<%=application.getAttribute(\\\"key\\\") %><br> <% request.getRequestDispatcher(\\\"/score2.jsp\\\").forward(request,response); %> </body> </html> <%--------------------------------------socre2.jsp--------------------------------------%> <body> <h2>score2.jsp页面</h2> pageContext域是否有值：<%=pageContext.getAttribute(\\\"key\\\") %><br> request域是否有值：<%=request.getAttribute(\\\"key\\\") %><br> session域是否有值：<%=session.getAttribute(\\\"key\\\") %><br> application域是否有值：<%=application.getAttribute(\\\"key\\\") %><br> <% request.getRequestDispatcher(\\\"/score2.jsp\\\").forward(request,response); %> </body> </html> \"]},{\"header\":\"6、out 输出和response.getWriter 输出\",\"slug\":\"_6、out-输出和response-getwriter-输出\",\"contents\":[\"response 中表示响应，我们经常用于设置返回给客户端的内容（输出）。out 也是给用户做输出使用的。\",\"out.write() 输出字符串没有问题\",\"out.print() 输出任意数据都没有问题（都转换成为字符串后调用的write 输出）\",\"在jsp 页面中，可以统一使用out.print()来进行输出\"]},{\"header\":\"3、jsp 的常用标签\",\"slug\":\"_3、jsp-的常用标签\",\"contents\":[]},{\"header\":\"1、jsp 静态包含\",\"slug\":\"_1、jsp-静态包含\",\"contents\":[\"<%-- <%@ include file=\\\"\\\"%> 就是静态包含 file 属性指定你要包含的jsp 页面的路径 地址中第一个斜杠/ 表示为http://ip:port/工程路径/ 映射到代码的web 目录 静态包含的特点： 1、静态包含不会翻译被包含的jsp 页面。 2、静态包含其实是把被包含的jsp 页面的代码拷贝到包含的位置执行输出。 --%> <%@ include file=\\\"/include/footer.jsp\\\"%> \"]},{\"header\":\"2、jsp动态包含\",\"slug\":\"_2、jsp动态包含\",\"contents\":[\"<%-- <jsp:include page=\\\"\\\"></jsp:include> 这是动态包含 page 属性是指定你要包含的jsp 页面的路径 动态包含也可以像静态包含一样。把被包含的内容执行输出到包含位置 动态包含的特点： 1、动态包含会把包含的jsp 页面也翻译成为java 代码 2、动态包含底层代码使用如下代码去调用被包含的jsp 页面执行输出。 JspRuntimeLibrary.include(request, response, \\\"/include/footer.jsp\\\", out, false); 3、动态包含，还可以传递参数 --%> <jsp:include page=\\\"/include/footer.jsp\\\"> <jsp:param name=\\\"username\\\" value=\\\"bbj\\\"/> <jsp:param name=\\\"password\\\" value=\\\"root\\\"/> </jsp:include> \"]},{\"header\":\"3、jsp 标签-转发\",\"slug\":\"_3、jsp-标签-转发\",\"contents\":[\"<%-- <jsp:forward page=\\\"\\\"></jsp:forward> 是请求转发标签，它的功能就是请求转发 page 属性设置请求转发的路径 --%> <jsp:forward page=\\\"/scope2.jsp\\\"></jsp:forward> \"]},{\"header\":\"4、jsp练习\",\"slug\":\"_4、jsp练习\",\"contents\":[\"9*9乘法表\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>输出99乘法表</title> </head> <body> <h1 align=\\\"center\\\">乘法表</h1> <table align=\\\"center\\\"> <% for (int i = 0; i <= 9; i++) { %> <tr> <% for (int j = 1; j <= i; j++) { %> <td><%=j + \\\"x\\\" + i + \\\"=\\\" +(i*j) %></td> <% } %> </tr> <% } %> </table> </body> </html> \",\"jsp 输出一个表格，里面有10 个学生信息。\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> <style> table{ border: 1px blue solid; width: 600px; border-collapse: collapse; } td,th{ border: 1px blue solid; } </style> </head> <body> <%--jsp 输出一个表格，里面有10 个学生信息。--%> <% List<Student> studentList = new ArrayList<Student>(); for (int i = 0; i < 10; i++) { int t = i + 1; studentList.add(new Student(\\\"name\\\"+t,t,18+t,\\\"phone\\\"+t)); } %> <% for (Student student : studentList) { %> <table> <tr> <td>编号</td> <td>姓名</td> <td>年龄</td> <td>电话</td> <td>操作</td> </tr> <tr> <td><%=student.getId()%></td> <td><%=student.getName()%></td> <td><%=student.getAge()%></td> <td><%=student.getPhone()%></td> <td>删除、修改</td> </tr> <% } %> </table> </body> </html> \"]}]},\"/study-tutorial/frame/javaweb/listener.html\":{\"title\":\"6、Listener 监听器\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"1、Listener 监听器它是JavaWeb 的三大组件之一。JavaWeb 的三大组件分别是：Servlet 程序、Filter 过滤器、Listener 监听器。 2、Listener 它是JavaEE 的规范，就是接口 3、监听器的作用是，监听某种事物的变化。然后通过回调函数，反馈给客户（程序）去做一些相应的处理。\"]},{\"header\":\"ServletContextListener 监听器\",\"slug\":\"servletcontextlistener-监听器\",\"contents\":[\"ServletContextListener 它可以监听ServletContext 对象的创建和销毁。 **ServletContext 对象在web 工程启动的时候创建，在web 工程停止的时候销毁。**监听到创建和销毁之后都会分别调用ServletContextListener 监听器的方法反馈。\",\"public interface ServletContextListener extends EventListener { default void contextInitialized(ServletContextEvent sce) { //在ServletContext 对象创建之后马上调用，做初始化 } default void contextDestroyed(ServletContextEvent sce) { //在ServletContext 对象销毁之后调用 } } \",\"如何使用ServletContextListener 监听器监听ServletContext 对象\",\"编写一个类去实现ServletContextListener\",\"实现其两个回调方法\",\"到web.xml 中去配置监听器\",\"public class MyServletContextListener implements ServletContextListener { public void contextInitialized(ServletContextEvent sce) { System.out.println(\\\"ServletContext被创建了！！！\\\"); } public void contextDestroyed(ServletContextEvent sce) { System.out.println(\\\"ServletContext被销毁了！！！\\\"); } } \",\"<!--配置listener--> <listener> <listener-class>com.xiaobear.listener.MyServletContextListener</listener-class> </listener> \"]}]},\"/study-tutorial/frame/javaweb/servlet.html\":{\"title\":\"2、Servlet\",\"contents\":[{\"header\":\"1、什么是Servlet？\",\"slug\":\"_1、什么是servlet\",\"contents\":[\"Servlet 是JavaEE 规范之一。规范就是接口 Servlet 就JavaWeb 三大组件之一。三大组件分别是：Servlet 程序、Filter 过滤器、Listener 监听器。 Servlet 是运行在服务器上的一个java 小程序，它可以接收客户端发送过来的请求，并响应数据给客户端\"]},{\"header\":\"2、手动实现Servlet 程序\",\"slug\":\"_2、手动实现servlet-程序\",\"contents\":[\"1、编写一个类去实现Servlet 接口\",\"2、实现service 方法，处理请求，并响应数据\",\"3、到web.xml 中去配置servlet 程序的访问地址\",\"Servlet 程序的示例代码：\",\"public class HelloServlet implements Servlet { /** * service 方法是专门用来处理请求和响应的 */ @Override public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException { System.out.println(\\\"Hello Servlet 被访问了\\\"); } } \",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <web-app xmlns=\\\"http://xmlns.jcp.org/xml/ns/javaee\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\\\" version=\\\"4.0\\\"> <!-- servlet 标签给Tomcat 配置Servlet 程序--> <servlet> <!--servlet-name 标签Servlet 程序起一个别名（一般是类名） --> <servlet-name>HelloServlet</servlet-name> <!--servlet-class 是Servlet 程序的全类名--> <servlet-class>com.atguigu.servlet.HelloServlet</servlet-class> </servlet> <!--servlet-mapping 标签给servlet 程序配置访问地址--> <servlet-mapping> <!--servlet-name 标签的作用是告诉服务器，我当前配置的地址给哪个Servlet 程序使用--> <servlet-name>HelloServlet</servlet-name> <!--url-pattern 标签配置访问地址<br/> / 斜杠在服务器解析的时候，表示地址为：http://ip:port/工程路径<br/> /hello 表示地址为：http://ip:port/工程路径/hello <br/> --> <url-pattern>/hello</url-pattern> </servlet-mapping> </web-app> \",\"继承HttpServlet 实现Servlet 程序\",\"一般在实际项目开发中，都是使用继承HttpServlet 类的方式去实现Servlet 程序。\",\"1、编写一个类去继承HttpServlet 类\",\"2、根据业务需要重写doGet 或doPost 方法\",\"3、到web.xml 中的配置Servlet 程序的访问地址\",\"Servlet 类的代码：\",\"public class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(\\\"HelloServlet 调用doGet()被执行了！！！\\\"); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(\\\"HelloServlet 调用doPost()被执行了！！！\\\"); } } \",\"web.xml 中的配置：\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <web-app xmlns=\\\"http://xmlns.jcp.org/xml/ns/javaee\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\\\" version=\\\"4.0\\\"> <!-- servlet 标签给Tomcat 配置Servlet 程序--> <servlet> <!--servlet-name 标签Servlet 程序起一个别名（一般是类名） --> <servlet-name>HelloServlet</servlet-name> <!--servlet-class 是Servlet 程序的全类名--> <servlet-class>com.xiaobear.HelloServlet</servlet-class> </servlet> <!--servlet-mapping 标签给servlet 程序配置访问地址--> <servlet-mapping> <!--servlet-name 标签的作用是告诉服务器，我当前配置的地址给哪个Servlet 程序使用--> <servlet-name>HelloServlet</servlet-name> <!--url-pattern 标签配置访问地址<br/> / 斜杠在服务器解析的时候，表示地址为：http://ip:port/工程路径<br/> /hello 表示地址为：http://ip:port/工程路径/hello <br/> --> <url-pattern>/hello</url-pattern> </servlet-mapping> </web-app> \"]},{\"header\":\"3、Servlet 的生命周期\",\"slug\":\"_3、servlet-的生命周期\",\"contents\":[\"执行Servlet 构造器方法\",\"执行init 初始化方法 （第一、二步，是在第一次访问的时候创建Servlet 程序会调用）\",\"执行service 方法（第三步，每次访问都会调用）\",\"执行destroy 销毁方法（第四步，在web 工程停止的时候调用）\"]},{\"header\":\"4、Servlet继承体系\",\"slug\":\"_4、servlet继承体系\",\"contents\":[]},{\"header\":\"1、ServletConfig 类\",\"slug\":\"_1、servletconfig-类\",\"contents\":[\"public interface ServletConfig { String getServletName(); //可以获取Servlet 程序的别名servlet-name 的值 ServletContext getServletContext();//获取ServletContext 对象 String getInitParameter(String var1);//获取初始化参数init-param Enumeration<String> getInitParameterNames(); } \",\"ServletConfig 类从类名上来看，就知道是Servlet 程序的配置信息类。\",\"Servlet 程序和ServletConfig 对象都是由Tomcat 负责创建，我们负责使用。\",\"Servlet 程序默认是第一次访问的时候创建，ServletConfig 是每个Servlet 程序创建时，就创建一个对应的\",\"ServletConfig 对象。\"]},{\"header\":\"2、ServletConfig 类的三大作用\",\"slug\":\"_2、servletconfig-类的三大作用\",\"contents\":[\"可以获取Servlet 程序的别名servlet-name 的值\",\"获取ServletContext 对象\",\"获取初始化参数init-param\",\"web.xml\",\"<!-- servlet 标签给Tomcat 配置Servlet 程序--> <servlet> <!--servlet-name 标签Servlet 程序起一个别名（一般是类名） --> <servlet-name>HelloServlet</servlet-name> <!--servlet-class 是Servlet 程序的全类名--> <servlet-class>com.atguigu.servlet.HelloServlet</servlet-class> <!--init-param 是初始化参数--> <init-param> <!--是参数名--> <param-name>username</param-name> <!--是参数值--> <param-value>root</param-value> </init-param> <!--init-param 是初始化参数--> <init-param> <!--是参数名--> <param-name>url</param-name> <!--是参数值--> <param-value>jdbc:mysql://localhost:3306/test</param-value> </init-param> </servlet> <!--servlet-mapping 标签给servlet 程序配置访问地址--> <servlet-mapping> <!--servlet-name 标签的作用是告诉服务器，我当前配置的地址给哪个Servlet 程序使用--> <servlet-name>HelloServlet</servlet-name> <!-- url-pattern 标签配置访问地址<br/> / 斜杠在服务器解析的时候，表示地址为：http://ip:port/工程路径<br/> /hello 表示地址为：http://ip:port/工程路径/hello <br/> --> <url-pattern>/hello</url-pattern> </servlet-mapping> \",\"Servlet 中的代码\",\"@Override public void init(ServletConfig config) throws ServletException { System.out.println(\\\"init 初始化方法！！！\\\"); //可以获取Servlet 程序的别名servlet-name 的值 System.out.println(\\\"HelloServlet 程序的别名是:\\\" + config.getServletName()); //获取初始化参数init-param System.out.println(\\\"初始化参数username 的值是;\\\" + config.getInitParameter(\\\"username\\\")); System.out.println(\\\"初始化参数url 的值是;\\\" + config.getInitParameter(\\\"url\\\")); //获取ServletContext 对象 System.out.println(config.getServletContext()); super.init(config); } \"]},{\"header\":\"3、ServletContext 类\",\"slug\":\"_3、servletcontext-类\",\"contents\":[\"1、ServletContext 是一个接口，它表示Servlet 上下文对象\",\"2、一个web 工程，只有一个ServletContext 对象实例。\",\"3、ServletContext 对象是一个域对象。\",\"4、ServletContext 是在web 工程部署启动的时候创建。在web 工程停止的时候销毁。\",\"什么是域对象?域对象，是可以像Map 一样存取数据的对象，叫域对象。这里的域指的是存取数据的操作范围，整个web 工程。\",\"存数据\",\"取数据\",\"删除数据\",\"Map\",\"put()\",\"get()\",\"remove()\",\"域对象\",\"setAttribute()\",\"getAttribute()\",\"removeAttribute()\"]},{\"header\":\"4、ServletContext 类的四个作用\",\"slug\":\"_4、servletcontext-类的四个作用\",\"contents\":[\"1、获取web.xml 中配置的上下文参数context-param\",\"2、获取当前的工程路径，格式: /工程路径\",\"3、获取工程部署后在服务器硬盘上的绝对路径\",\"4、像Map 一样存取数据\",\"ServletContext的代码\",\" @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(\\\"HelloServlet 调用doGet()被执行了！！！\\\"); // 1、获取web.xml 中配置的上下文参数context-param ServletContext context = getServletConfig().getServletContext(); String username = context.getInitParameter(\\\"username\\\"); System.out.println(\\\"context-param 参数username 的值是:\\\" + username); System.out.println(\\\"context-param 参数password 的值是:\\\" + context.getInitParameter(\\\"password\\\")); // 2、获取当前的工程路径，格式: /工程路径 System.out.println( \\\"当前工程路径:\\\" + context.getContextPath() ); // 3、获取工程部署后在服务器硬盘上的绝对路径 /** * / 斜杠被服务器解析地址为:http://ip:port/工程名/ 映射到IDEA 代码的web 目录<br/> */ System.out.println(\\\"工程部署的路径是:\\\" + context.getRealPath(\\\"/\\\")); System.out.println(\\\"工程下css 目录的绝对路径是:\\\" + context.getRealPath(\\\"/css\\\")); System.out.println(\\\"工程下imgs 目录1.jpg 的绝对路径是:\\\" + context.getRealPath(\\\"/imgs/1.jpg\\\")); } \",\"<!--context-param 是上下文参数(它属于整个web 工程)--> <context-param> <param-name>username</param-name> <param-value>context</param-value> </context-param> <!--context-param 是上下文参数(它属于整个web 工程)--> <context-param> <param-name>password</param-name> <param-value>root</param-value> </context-param> \",\"ContextServlet1代码\",\"public class ContextServlet1 extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // 获取ServletContext 对象 ServletContext context = getServletContext(); System.out.println(context); System.out.println(\\\"保存之前: Context1 获取key1 的值是:\\\"+ context.getAttribute(\\\"key1\\\")); context.setAttribute(\\\"key1\\\", \\\"value1\\\"); System.out.println(\\\"Context1 中获取域数据key1 的值是:\\\"+ context.getAttribute(\\\"key1\\\")); } } //ContextServlet2 protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { ServletContext context = getServletContext(); System.out.println(context); System.out.println(\\\"Context2 中获取域数据key1 的值是:\\\"+ context.getAttribute(\\\"key1\\\")); } \"]}]},\"/study-tutorial/frame/mybatis/cache_mechanism.html\":{\"title\":\"10、缓存机制\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"MyBatis 包含一个非常强大的查询缓存特性,它可以非常方便地配置和定制。缓存可以极大的提升查询效率\",\"MyBatis 系统中默认定义了两级缓存：一级缓存 二级缓存\",\"默认情况下，只有一级缓存（SqlSession 级别的缓存，也称为本地缓存）开启。\",\"二级缓存需要手动开启和配置，他是基于namespace 级别的缓存。\",\"为了提高扩展性。MyBatis 定义了缓存接口Cache。我们可以通过实现Cache 接口来自定义二级缓存\"]},{\"header\":\"10.1、一级缓存\",\"slug\":\"_10-1、一级缓存\",\"contents\":[\"一级缓存失效的几种情况\",\"不同的SqlSession 对应不同的一级缓存\",\"同一个SqlSession 但是查询条件不同\",\"同一个SqlSession 两次查询期间执行了任何一次增删改操作\",\"同一个SqlSession 两次查询期间手动清空了缓存\"]},{\"header\":\"10.2、二级缓存\",\"slug\":\"_10-2、二级缓存\",\"contents\":[\"MyBatis 提供二级缓存的接口以及实现，缓存实现要求POJO 实现Serializable 接口\",\"二级缓存==在SqlSession 关闭或提交之后才会生效==\",\"二级缓存使用的步骤:\",\"1、全局配置文件中开启二级缓存\",\"<setting name=\\\"cacheEnabled\\\" value=\\\"true\\\"/> \",\"2、需要使用二级缓存的映射文件处使用cache 配置缓存\",\"<cache /> \",\"3、注意：POJO 需要实现Serializable 接口\",\"二级缓存相关的属性\",\"eviction=“FIFO”：缓存回收策略：\",\"​ LRU – 最近最少使用的：移除最长时间不被使用的对象。\",\"​ FIFO – 先进先出：按对象进入缓存的顺序来移除它们。\",\"​ SOFT – 软引用：移除基于垃圾回收器状态和软引用规则的对象。\",\"​ WEAK – 弱引用：更积极地移除基于垃圾收集器状态和弱引用规则的对象。默认的是LRU。\",\"flushInterval：刷新间隔，单位毫秒默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新\",\"size：引用数目，正整数 代表缓存最多可以存储多少个对象，太大容易导致内存溢出\",\"readOnly：只读，true/false\",\"true：只读缓存；会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要\",\"的性能优势。\",\"false：读写缓存；会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。\",\"<!--在当前mapper使用二级缓存--> <cache eviction=\\\"FIFO\\\" flushInterval=\\\"60000\\\" size=\\\"512\\\" readOnly=\\\"true\\\"/> \"]},{\"header\":\"10.3、缓存的相关属性设置\",\"slug\":\"_10-3、缓存的相关属性设置\",\"contents\":[\"全局setting 的cacheEnable：配置二级缓存的开关，一级缓存一直是打开的。\",\"select 标签的useCache 属性：配置这个select 是否使用二级缓存。一级缓存一直是使用的\",\"sql 标签的flushCache 属性：增删改默认flushCache=true。sql 执行以后，会同时清空一级和二级缓存。查\",\"询默认flushCache=false。\",\"sqlSession.clearCache()：只是用来清除一级缓存。\"]},{\"header\":\"10.4、自定义缓存--EhCache\",\"slug\":\"_10-4、自定义缓存-ehcache\",\"contents\":[\"EhCache 是一个纯Java 的进程内缓存框架，具有快速、精干等特点，是Hibernate 中默认的CacheProvider\",\"导入依赖\",\"<!-- https://mvnrepository.com/artifact/org.mybatis.caches/mybatis-ehcache --> <dependency> <groupId>org.mybatis.caches</groupId> <artifactId>mybatis-ehcache</artifactId> <version>1.2.0</version> </dependency> \",\"编写ehcache.xml 配置文件\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <ehcache xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:noNamespaceSchemaLocation=\\\"http://ehcache.org/ehcache.xsd\\\"> <!-- 磁盘保存路径--> <diskStore path=\\\"D:\\\\360MoveData\\\\Users\\\\Administrator\\\\Desktop\\\\ehcache\\\" /> <defaultCache maxElementsInMemory=\\\"1000\\\" maxElementsOnDisk=\\\"10000000\\\" eternal=\\\"false\\\" overflowToDisk=\\\"true\\\" timeToIdleSeconds=\\\"120\\\" timeToLiveSeconds=\\\"120\\\" diskExpiryThreadIntervalSeconds=\\\"120\\\" memoryStoreEvictionPolicy=\\\"LRU\\\"> </defaultCache> </ehcache> <!-- 属性说明： diskStore：指定数据在磁盘中的存储位置。 defaultCache：当借助CacheManager.add(\\\"demoCache\\\")创建Cache时，EhCache 便会采用<defalutCache/>指定的的管理策略 以下属性是必须的： maxElementsInMemory - 在内存中缓存的element的最大数目 maxElementsOnDisk - 在磁盘上缓存的element的最大数目，若是0表示无穷大 eternal - 设定缓存的elements是否永远不过期。如果为true，则缓存的数据始 终有效，如果为false那么还要根据timeToIdleSeconds，timeToLiveSeconds判断 overflowToDisk - 设定当内存缓存溢出的时候是否将过期的element缓存到磁 盘上 以下属性是可选的： timeToIdleSeconds - 当缓存在EhCache中的数据前后两次访问的时间超过 timeToIdleSeconds的属性取值时，这些数据便会删除，默认值是0,也就是可闲置 时间无穷大 timeToLiveSeconds - 缓存element的有效生命期，默认是0.,也就是element存活 时间无穷大 diskSpoolBufferSizeMB 这个参数设置DiskStore(磁盘缓存)的缓存区大小.默认 是30MB.每个Cache都应该有自己的一个缓冲区. diskPersistent - 在VM重启的时候是否启用磁盘保存EhCache中的数据，默认是 false。 diskExpiryThreadIntervalSeconds - 磁盘缓存的清理线程运行间隔，默认是120 秒。每个120s，相应的线程会进行一次EhCache中数据的清理工作 memoryStoreEvictionPolicy - 当内存缓存达到最大，有新的element加入的时 候， 移除缓存中element的策略。默认是LRU（最近最少使用），可选的有LFU （最不常使用）和FIFO（先进先出） --> \",\"配置cache 标签\",\" <cache type=\\\"org.mybatis.caches.ehcache.EhcacheCache\\\"/> \"]}]},\"/study-tutorial/frame/mybatis/crud.html\":{\"title\":\"3、CURD\",\"contents\":[{\"header\":\"3.1、namespace\",\"slug\":\"_3-1、namespace\",\"contents\":[\"namespace 中必须指定Mapper 接口的全类名\"]},{\"header\":\"3.2、select\",\"slug\":\"_3-2、select\",\"contents\":[]},{\"header\":\"3.2.1、编写接口\",\"slug\":\"_3-2-1、编写接口\",\"contents\":[\"//根据id查询 User getUserById(int id); \"]},{\"header\":\"3.2.2、编写对应的mapper中的sql语句（mapper映射文件）\",\"slug\":\"_3-2-2、编写对应的mapper中的sql语句-mapper映射文件\",\"contents\":[\"<select id=\\\"getUserById\\\" parameterType=\\\"int\\\" resultType=\\\"com.yan.bear.User\\\"> select * from mybatis.user where id = #{id} </select> <!-- id:namespace中的方法名 parameterType:参数类型 resultType:sql语句执行的返回值 --> \"]},{\"header\":\"3.2.3、测试\",\"slug\":\"_3-2-3、测试\",\"contents\":[\" @Test public void getUserId(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); User userById = mapper.getUserById(1); System.out.println(userById); sqlSession.close(); } \"]},{\"header\":\"3.3、insert、delete、update\",\"slug\":\"_3-3、insert、delete、update\",\"contents\":[\"编写接口\",\"//inset一个用户 int addUser(User user); //修改用户 int updateUser(User user); //删除用户 int deleteUser(int id); \",\"编写对应的mapper中的sql语句（mapper映射文件）\",\"<insert id=\\\"addUser\\\" parameterType=\\\"com.yan.bear.User\\\"> insert into mybatis.user (id, name, pwd) value (#{id},#{name},#{pwd}) </insert> <update id=\\\"updateUser\\\" parameterType=\\\"com.yan.bear.User\\\"> update mybatis.user set name = #{name},pwd = #{pwd} where id = #{id}; </update> <delete id=\\\"deleteUser\\\" parameterType=\\\"com.yan.bear.User\\\"> delete from mybatis.user where id = #{id} </delete> \",\"测试\",\"//增删改需要提交事务 @Test public void addUser(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); int res = mapper.addUser(new User(4, \\\"ly\\\", \\\"1111111\\\")); if (res > 0){ System.out.println(\\\"插入成功！\\\"); } sqlSession.commit(); //提交事务 sqlSession.close(); } @Test public void updateUser() { SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); int res = mapper.updateUser(new User(4,\\\"hehhe\\\",\\\"123123\\\")); if (res > 0){ System.out.println(\\\"修改成功！\\\"); } sqlSession.commit(); //提交事务 sqlSession.close(); } @Test public void deleteUser(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); int deleteUser = mapper.deleteUser(4); if (deleteUser > 0){ System.out.println(\\\"删除成功！\\\"); } sqlSession.commit();//提交事务 sqlSession.close(); } \"]},{\"header\":\"3.4、万能Map\",\"slug\":\"_3-4、万能map\",\"contents\":[\"如果实体类或者数据库中的表、字段或者参数很多，应当考虑Map\",\"int addUser2(Map<String,Object> map); \",\"<!--对象的属性，可以直接取出来 传递map的key --> <insert id=\\\"addUser\\\" parameterType=\\\"map\\\"> insert into mybatis.user (id, name, pwd) value (#{userid},#{username},#{password}) </insert> \",\"@Test public void addUser2(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); Map<String, Object> map = new HashMap<String, Object>(); map.put(\\\"userid\\\",5); map.put(\\\"username\\\",\\\"hh\\\"); map.put(\\\"password\\\",\\\"123456\\\"); mapper.addUser2(map); sqlSession.close(); } \"]},{\"header\":\"3.5、模糊查询\",\"slug\":\"_3-5、模糊查询\",\"contents\":[\"在Java执行代码中使用通配符% %\",\"List<User> userList = mapper.getUserLike(\\\"%李%\\\"); \",\"在sqlp拼接中使用通配符\",\"<select id=\\\"getUserLike\\\" resultType=\\\"com.yan.bear.User\\\"> select * from mybatis.user where name like \\\"%\\\"#{value}\\\"%\\\" </select> \",\" //模糊查询 List<User> getUserLike(String name); \",\"<select id=\\\"getUserLike\\\" resultType=\\\"com.yan.bear.User\\\"> select * from mybatis.user where name like \\\"\\\"#{value} </select> \",\"@Test public void getUserLike(){ SqlSession sqlSession =(SqlSession) MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); List<User> userList = mapper.getUserLike(\\\"%李%\\\"); for (User user : userList) { System.out.println(user); } sqlSession.close(); } \"]}]},\"/study-tutorial/frame/mybatis/dynamic_sql.html\":{\"title\":\"9、动态SQL\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"动态SQL 是MyBatis 强大特性之一。极大的简化我们==拼装SQL 的操作==\",\"动态SQL 元素和使用JSTL 或其他类似基于XML 的文本处理器相似\",\"if\",\"choose (when, otherwise)\",\"trim (where, set)\",\"foreach\"]},{\"header\":\"9.1、if where\",\"slug\":\"_9-1、if-where\",\"contents\":[\"if 用于完成简单的判断.\",\"Where 用于解决SQL 语句中where 关键字以及条件中第一个and 或者or 的问题\",\"<select id=\\\"queryBlogIF\\\" resultType=\\\"com.yan.pojo.Blog\\\"> select * from mybatis.blog where 1=1 <if test=\\\"title != null\\\"> and title = #{title} </if> <if test=\\\"author != null\\\"> and author = #{author} </if> </select> \"]},{\"header\":\"9.2、choose (when, otherwise)\",\"slug\":\"_9-2、choose-when-otherwise\",\"contents\":[\"choose 主要是用于分支判断，类似于java 中的switch case,只会满足所有分支中的一个\",\"<select id=\\\"queryBlogIF2\\\" resultType=\\\"com.yan.pojo.Blog\\\"> select * from mybatis.blog <where> <choose> <when test=\\\"titie != null\\\"> title = #{title} </when> <when test=\\\"author != null\\\"> AND author = #{author} </when> <otherwise>views = #{views}</otherwise> </choose> </where> </select> \"]},{\"header\":\"9.3、trim (where, set)\",\"slug\":\"_9-3、trim-where-set\",\"contents\":[\"Trim 可以在条件判断完的SQL 语句前后添加或者去掉指定的字符\",\"prefix: 添加前缀\",\"prefixOverrides: 去掉前缀\",\"suffix: 添加后缀\",\"suffixOverrides: 去掉后缀\",\" \",\"set 主要是用于解决修改操作中SQL 语句中可能多出逗号的问题\",\"<update id=\\\"queryBlogIF3\\\" parameterType=\\\"map\\\"> update mybatis.blog <set> <if test=\\\"title != null\\\"> and title = #{title}, </if> <if test=\\\"author != null\\\"> and author = #{author}, </if> </set> where id = #{id} </update> \"]},{\"header\":\"9.4、foreach\",\"slug\":\"_9-4、foreach\",\"contents\":[\"主要用户循环迭代\",\"collection: 要迭代的集合\",\"item: 当前从集合中迭代出的元素\",\"open: 开始字符\",\"close:结束字符\",\"separator: 元素与元素之间的分隔符\",\"index: 迭代的是List 集合: index 表示的当前元素的下标 迭代的Map 集合: index 表示的当前元素的key\",\"<!-- select * from mybatis.blog where 1=1 and(id=1 or id=2 or id=3)--> <select id=\\\"queryBlogForeach\\\" resultType=\\\"com.yan.pojo.Blog\\\"> <include refid=\\\"selectsql\\\"></include> <where> <foreach collection=\\\"ids\\\" item=\\\"id\\\" open=\\\"and (\\\" close=\\\")\\\" separator=\\\"or\\\"> id = #{id} </foreach> </where> </select> \"]},{\"header\":\"9.5、sql\",\"slug\":\"_9-5、sql\",\"contents\":[\"sql 标签是用于抽取可重用的sql 片段，将相同的，使用频繁的SQL 片段抽取出来，单独定义，方便多次引用.\",\"抽取sql:\",\"<sql id=\\\"selectsql\\\"> select * from mybatis.blog </sql> \",\"引用sql:\",\" <include refid=\\\"selectsql\\\"></include> \",\"注意：\",\" 最好基于单表定义sql片段\",\" 不要存在where标签\"]}]},\"/study-tutorial/frame/mybatis/first_program.html\":{\"title\":\"2、第一个程序\",\"contents\":[{\"header\":\"2.1、搭建数据库\",\"slug\":\"_2-1、搭建数据库\",\"contents\":[]},{\"header\":\"2.2、导入依赖\",\"slug\":\"_2-2、导入依赖\",\"contents\":[\"<!--导入依赖--> <dependencies> <!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>8.0.18</version> </dependency> <!-- https://mvnrepository.com/artifact/org.mybatis/mybatis --> <dependency> <groupId>org.mybatis</groupId> <artifactId>mybatis</artifactId> <version>3.5.2</version> </dependency> <!-- https://mvnrepository.com/artifact/junit/junit --> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"2.3、编写实体类\",\"slug\":\"_2-3、编写实体类\",\"contents\":[\"public class User { private int id; private String name; private String pwd; public User() { } public User(int id, String name, String pwd) { this.id = id; this.name = name; this.pwd = pwd; } public int getId() {return id } public void setId(int id) this.id = id;} public String getName() {return name;} public void setName(String name) {this.name = name;} public String getPwd() {return pwd;} public void setPwd(String pwd) {this.pwd = pwd;} @Override public String toString() { return \\\"User{\\\" + \\\"id=\\\" + id + \\\", name='\\\" + name + '\\\\'' + \\\", pwd='\\\" + pwd + '\\\\'' + '}'; } } \"]},{\"header\":\"2.4、创建核心配置文件mybatis-config.xml\",\"slug\":\"_2-4、创建核心配置文件mybatis-config-xml\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" ?> <!DOCTYPE configuration PUBLIC \\\"-//mybatis.org//DTD Config 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-config.dtd\\\"> <configuration> <!-- 数据库连接环境的配置--> <environments default=\\\"development\\\"> <environment id=\\\"development\\\"> <transactionManager type=\\\"JDBC\\\"/> <dataSource type=\\\"POOLED\\\"> <property name=\\\"driver\\\" value=\\\"com.mysql.cj.jdbc.Driver\\\"/> <property name=\\\"url\\\" value=\\\"jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=GMT\\\"/> <property name=\\\"username\\\" value=\\\"root\\\"/> <property name=\\\"password\\\" value=\\\"密码\\\"/> </dataSource> </environment> </environments> <!-- 每一个mapper.xml都需要在Mybatis核心配置文件中--> <!-- 引入SQL映射文件,Mapper映射文件--> <mappers> <mapper resource=\\\"UserMapper.xml\\\"/> </mappers> \"]},{\"header\":\"2.5、创建Mybatis 的sql 映射文件\",\"slug\":\"_2-5、创建mybatis-的sql-映射文件\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" ?> <!DOCTYPE mapper PUBLIC \\\"-//mybatis.org//DTD Mapper 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\\\"> <mapper namespace=\\\"com.yan.dao.UserMapper\\\"> <select id=\\\"getUserList\\\" resultType=\\\"com.yan.bear.User\\\"> select * from mybatis.user </select> </mapper> <!-- 1 Mapper 接口与Mapper映射文件的绑定 在Mppper 映射文件中的<mapper>标签中的namespace 中必须指定Mapper 接口 的全类名 2 Mapper 映射文件中的增删改查标签的id 必须指定成Mapper 接口中的方法名. 测试中遇到的问题 --> \"]},{\"header\":\"2.6、Mapper 接口\",\"slug\":\"_2-6、mapper-接口\",\"contents\":[\"public interface UserMapper { List<User> getUserList(); } \"]},{\"header\":\"2.7、工具类\",\"slug\":\"_2-7、工具类\",\"contents\":[\"public class MybatisUtils { public static SqlSessionFactory sqlSessionFactory; static { try { //获取sqlSessionFactory对象 String resource = \\\"mybatis-config.xml\\\"; InputStream inputStream = Resources.getResourceAsStream(resource); sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); } catch (IOException e) { e.printStackTrace(); } } public static SqlSession getSqlSession(){ return sqlSessionFactory.openSession(); } } \"]},{\"header\":\"2.8、测试类\",\"slug\":\"_2-8、测试类\",\"contents\":[\"public class UserDaoTest { @Test public void Test(){ SqlSession sqlSession = (SqlSession) MybatisUtils.getSqlSession(); try{ //方式一：Mapper接口:获取Mapper接口的代理实现类对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); List<User> userList = userMapper.getUserList(); //方式二 /* List<User> userList = sqlSession.selectList(\\\"com.yan.dao.UserMapper.getUserList\\\"); */ for (User user : userList) { System.out.println(user); } } finally { sqlSession.close(); } } } \"]},{\"header\":\"2.9、遇到的问题\",\"slug\":\"_2-9、遇到的问题\",\"contents\":[\"<!--资源过滤问题--> <build> <resources> <resource> <directory>src/main/resources</directory> <includes> <include>**/*.properties</include> <include>**/*.xml</include> </includes> <filtering>true</filtering> </resource> <resource> <directory>src/main/java</directory> <includes> <include>**/*.properties</include> <include>**/*.xml</include> </includes> <filtering>true</filtering> </resource> </resources> </build> 异常错误：java.sql.SQLException: The server time zone value '?й???????' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support. 显示新版本的数据库连接程序需要指定UTC时区，改正方法将配置文件中的“url”后面加上指定的时区，将其值改 <property name=\\\"url\\\" value=\\\"jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=GMT\\\"/> \"]}]},\"/study-tutorial/frame/mybatis/global_config.html\":{\"title\":\"4、全局配置文件\",\"contents\":[{\"header\":\"4.1、properties 属性\",\"slug\":\"_4-1、properties-属性\",\"contents\":[\"编写一个配置文件db.properties\",\"driver=com.mysql.cj.jdbc.Driver url=jdbc:mysql://localhost:3306/mybatis?useSSL=true&useUnicode=true&characterEncoding=UTF-8&serverTimezone=GMT username=root password=密码 \",\"<configuration> <properties resource=\\\"db.properties\\\"/> <!-- properties: 引入外部的属性文件 resource: 从类路径下引入属性文件 url: 引入网络路径或者是磁盘路径下的属性文件 --> \",\"读取配置优先级：外部配置db.properties > 内部配置.xml\"]},{\"header\":\"4.2、setting 设置\",\"slug\":\"_4-2、setting-设置\",\"contents\":[\"<settings> <setting name=\\\"cacheEnabled\\\" value=\\\"true\\\"/> <setting name=\\\"lazyLoadingEnabled\\\" value=\\\"true\\\"/> <setting name=\\\"multipleResultSetsEnabled\\\" value=\\\"true\\\"/> <setting name=\\\"useColumnLabel\\\" value=\\\"true\\\"/> <setting name=\\\"useGeneratedKeys\\\" value=\\\"false\\\"/> <setting name=\\\"autoMappingBehavior\\\" value=\\\"PARTIAL\\\"/> <setting name=\\\"autoMappingUnknownColumnBehavior\\\" value=\\\"WARNING\\\"/> <setting name=\\\"defaultExecutorType\\\" value=\\\"SIMPLE\\\"/> <setting name=\\\"defaultStatementTimeout\\\" value=\\\"25\\\"/> <setting name=\\\"defaultFetchSize\\\" value=\\\"100\\\"/> <setting name=\\\"safeRowBoundsEnabled\\\" value=\\\"false\\\"/> <setting name=\\\"mapUnderscoreToCamelCase\\\" value=\\\"false\\\"/> <setting name=\\\"localCacheScope\\\" value=\\\"SESSION\\\"/> <setting name=\\\"jdbcTypeForNull\\\" value=\\\"OTHER\\\"/> <setting name=\\\"lazyLoadTriggerMethods\\\" value=\\\"equals,clone,hashCode,toString\\\"/> </settings> \"]},{\"header\":\"4.3、typeAliases 别名处理\",\"slug\":\"_4-3、typealiases-别名处理\",\"contents\":[\"类型别名是为 Java 类型设置一个短的名字。 它只和 XML 配置有关，存在的意义仅在于用来减少类完全限定名的冗余\",\"<!--实体类别名--> <typeAliases> <typeAlias type=\\\"com.yan.pojo.User\\\" alias=\\\"user\\\"/> </typeAliases> \",\"可以指定一个包，默认就是这个类的类名，首字母小写\",\"<typeAliases> <package name=\\\"com.yan.pojo\\\"/> </typeAliases> ----------------------------------------------------------------------------------------- <select id=\\\"getUserList\\\" resultType=\\\"user\\\"> select * from mybatis.user </select> \",\"实体类较少的时候，用第一种；实体类十分多，用第二种；第一种可以DIY别名1，第二种不行\"]},{\"header\":\"4.4、environments 环境配置\",\"slug\":\"_4-4、environments-环境配置\",\"contents\":[\"MyBatis 可以配置多种环境，比如开发、测试和生产环境需要有不同的配置\",\"environment-指定具体环境 id：指定当前环境的唯一标识\",\"<!-- 数据库连接环境的配置--> <environments default=\\\"development\\\"> <environment id=\\\"development\\\"> <transactionManager type=\\\"JDBC\\\"/> <dataSource type=\\\"POOLED\\\"> <property name=\\\"driver\\\" value=\\\"${driver}\\\"/> <property name=\\\"url\\\" value=\\\"${url}\\\"/> <property name=\\\"username\\\" value=\\\"${username}\\\"/> <property name=\\\"password\\\" value=\\\"${password}\\\"/> </dataSource> </environment> <environment id=\\\"test\\\"> <transactionManager type=\\\"JDBC\\\"/> <dataSource type=\\\"POOLED\\\"> <property name=\\\"driver\\\" value=\\\"${driver}\\\"/> <property name=\\\"url\\\" value=\\\"${url}\\\"/> <property name=\\\"username\\\" value=\\\"${username}\\\"/> <property name=\\\"password\\\" value=\\\"${password}\\\"/> </dataSource> </environment> </environments> \",\"transactionManager\",\"type： JDBC | MANAGED | 自定义\",\"JDBC（默认）：使用了JDBC 的提交和回滚设置，依赖于从数据源得到的连接来管理事务\",\"范围。JdbcTransactionFactory\",\"MANAGED：不提交或回滚一个连接、让容器来管理事务的整个生命周期（比如JEE应用服务器的上下文）。\",\"ManagedTransactionFactory\",\"自定义：实现TransactionFactory 接口，type=全类名/别名\",\"dataSource\",\"type： UNPOOLED | POOLED | JNDI | 自定义\",\"UNPOOLED：不使用连接池， UnpooledDataSourceFactory\",\"POOLED（默认）：使用连接池， PooledDataSourceFactory\",\"JNDI： 在EJB 或应用服务器这类容器中查找指定的数据源\",\"自定义：实现DataSourceFactory 接口，定义数据源的获取方式。\"]},{\"header\":\"4.5、mapper映射器\",\"slug\":\"_4-5、mapper映射器\",\"contents\":[\"resource : 引入类路径下的文件\",\"url : 引入网络路径或者是磁盘路径下的文件\",\"class : 引入Mapper 接口.\",\"方法一：\",\"<mappers> <!-- <mapper resource=\\\"UserMapper.xml\\\"/>--> <mapper class=\\\"com.yan.dao.UserMapper\\\"/> <package name=\\\"com.yan.dao\\\"/> </mappers> \",\"使用批量注册，这种方式要求SQL 映射文件名必须和接口名相同并且在同一目录下\",\"<mappers> <!-- <mapper resource=\\\"UserMapper.xml\\\"/>--> <package name=\\\"com.yan.dao\\\"/> </mappers> \"]}]},\"/study-tutorial/frame/mybatis/introduction.html\":{\"title\":\"1、简介\",\"contents\":[{\"header\":\"1.1、MyBatis 历史\",\"slug\":\"_1-1、mybatis-历史\",\"contents\":[\"MyBatis是Apache 的一个开源项目iBatis, 2010 年6 月这个项目由Apache Software Foundation 迁移到了\",\"Google Code，随着开发团队转投Google Code 旗下， iBatis3.x正式更名为MyBatis ，代码于2013 年11 月\",\"迁移到Github\",\"iBatis 一词来源于“internet”和“abatis”的组合，是一个基于Java 的持久层框架。iBatis提供的持久层框架包括\",\"SQL Maps 和Data Access Objects（DAO）\"]},{\"header\":\"1.2、简介\",\"slug\":\"_1-2、简介\",\"contents\":[\"MyBatis 是支持定制化SQL、存储过程以及高级映射的优秀的持久层框架\",\"MyBatis 避免了几乎所有的JDBC 代码和手动设置参数以及获取结果集\",\"MyBatis 可以使用简单的XML 或注解用于配置和原始映射，将接口和Java 的POJO（Plain Old Java Objects，普通的Java 对象）映射成数据库中的记录\",\"Maven仓库 <!-- https://mvnrepository.com/artifact/org.mybatis/mybatis --> <dependency> <groupId>org.mybatis</groupId> <artifactId>mybatis</artifactId> <version>3.5.2</version> </dependency> \"]},{\"header\":\"1.3、为什么使用Mybatis---现有持久化技术的对比\",\"slug\":\"_1-3、为什么使用mybatis-现有持久化技术的对比\",\"contents\":[]},{\"header\":\"1.3.1、JDBC\",\"slug\":\"_1-3-1、jdbc\",\"contents\":[\"SQL 夹在Java 代码块里，耦合度高导致硬编码内伤\",\"维护不易且实际开发需求中sql 是有变化，频繁修改的情况多见\"]},{\"header\":\"1.3.2、Hibernate 和JPA\",\"slug\":\"_1-3-2、hibernate-和jpa\",\"contents\":[\"长难复杂SQL，对于Hibernate 而言处理也不容易\",\"内部自动生产的SQL，不容易做特殊优化\",\"基于全映射的全自动框架，大量字段的POJO 进行部分映射时比较困难。导致数据\",\"库性能下降\"]},{\"header\":\"1.3.3、MyBatis\",\"slug\":\"_1-3-3、mybatis\",\"contents\":[\"对开发人员而言，核心sql 还是需要自己优化\",\"sql 和java 编码分开，功能边界清晰，一个专注业务、一个专注数据\"]}]},\"/study-tutorial/frame/mybatis/log.html\":{\"title\":\"6、日志\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Mybatis 的内置日志工厂提供日志功能，内置日志工厂将日志交给以下其中一种工具作代理：\",\"SLF4J\",\"Apache Commons Logging\",\"Log4j 2\",\"Log4j\",\"JDK logging\",\"<!-- 标准的日志工厂实现--> <!--<settings> <setting name=\\\"logImpl\\\" value=\\\"STDOUT_LOGGING\\\"/> </settings>--> \"]},{\"header\":\"6.1、log4j\",\"slug\":\"_6-1、log4j\",\"contents\":[\"<dependencies> <!-- https://mvnrepository.com/artifact/log4j/log4j --> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>1.2.17</version> </dependency> </dependencies> \",\"public class UserMapperTest { static Logger log = Logger.getLogger(String.valueOf(UserMapperTest.class)); @Test public void Test() { SqlSession sqlSession = MybatisUtils.getSqlSession(); try { //方式一：Mapper接口:获取Mapper接口的代理实现类对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user = userMapper.getUserById(1); /*方式二 List<User> userList = sqlSession.selectList(\\\"com.yan.dao.UserMapper.getUserList\\\"); */ System.out.println(user); } finally { sqlSession.close(); } } @Test public void testLog4j(){ log.info(\\\"info:进入了log4j方法\\\"); log.fine(\\\"fine:进入了log4j方法\\\"); } \"]}]},\"/study-tutorial/frame/mybatis/page.html\":{\"title\":\"7、分页\",\"contents\":[{\"header\":\"7.1、limit分页\",\"slug\":\"_7-1、limit分页\",\"contents\":[\"sql语句分页\",\"接口\",\"//分页 List<User> getUserByLimit(Map<String, Object> map); \",\"mapper映射文件\",\"<!-- //分页--> <select id=\\\"getUserByLimit\\\" parameterType=\\\"map\\\" resultMap=\\\"UserMap\\\"> select * from mybatis.user limit #{startIndex},#{pageSize} </select> \",\"测试\",\"//分页 @Test public void getUserByLimit(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); HashMap<String, Object> map = new HashMap<String, Object>(); map.put(\\\"startIndex\\\",0); map.put(\\\"pageSize\\\",2); List<User> userList = mapper.getUserByLimit(map); for (User user : userList) { System.out.println(user); } sqlSession.close(); } \"]},{\"header\":\"7.2、RowBounds分页\",\"slug\":\"_7-2、rowbounds分页\",\"contents\":[\"//分页2 List<User> getUserByRowBounds(); \",\"<!--分页2--> <select id=\\\"getUserByRowBounds\\\" resultMap=\\\"UserMap\\\"> select * from mybatis.user </select> \",\"//分页2 @Test public void getUserByRowBounds(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); RowBounds rowBounds = new RowBounds(1, 2); List<User> userList = sqlSession.selectList(\\\"com.yan.dao.UserMapper.getUserByRowBounds\\\",null,rowBounds); for (User user : userList) { System.out.println(user); } sqlSession.close(); } \"]},{\"header\":\"7.3、分页插件-PageHelper\",\"slug\":\"_7-3、分页插件-pagehelper\",\"contents\":[\"官方文档：https://github.com/pagehelper/Mybatis-PageHelper/blob/master/README_zh.md\"]},{\"header\":\"7.4、使用注解开发\",\"slug\":\"_7-4、使用注解开发\",\"contents\":[\"注解在接口上实现\",\" @Select(\\\"select * from mybatis.user\\\") List<User> getUsers(); \",\"在核心配置文件中绑定接口\",\"<!-- 绑定接口--> <mappers> <mapper class=\\\"yan.dao.UserMapper\\\"/> </mappers> \",\"测试\",\"@Test public void test(){ SqlSession sqlSession = MybatisUtils.getSqlSession(); UserMapper mapper = sqlSession.getMapper(UserMapper.class); List<User> users = mapper.getUsers(); for (User user : users) { System.out.println(user); } sqlSession.close(); } \",\"本质：反射机制实现\",\"底层：动态代理\"]},{\"header\":\"CURD\",\"slug\":\"curd\",\"contents\":[\"开启自动提交事务\",\"public static SqlSession getSqlSession(){ //自动提交事务 return sqlSessionFactory.openSession(true); } \",\"注解在接口上\",\"@Select(\\\"select * from mybatis.user where id = #{id}\\\") User getUserById(@Param(\\\"id\\\") int id); @Insert(\\\"insert into user(id,name,pwd) value (#{id},#{name},#{password})\\\") int addUser(User user); @Update(\\\"update user set name=#{name},pwd=#{password} where id = #{id}\\\") int updateUser(User user); @Delete(\\\"delete from mybatis.user where id = #{id}\\\") int deleteUser(@Param(\\\"id\\\") int id); \"]}]},\"/study-tutorial/frame/mybatis/resultMap.html\":{\"title\":\"5、解决属性名跟字段名不一致的问题\",\"contents\":[{\"header\":\"5.1、resultMap 自定义映射\",\"slug\":\"_5-1、resultmap-自定义映射\",\"contents\":[\"ResultMap 的设计思想是，对于简单的语句根本不需要配置显式的结果映射，而对于复杂一点的语句只需要\",\"描述它们的关系就行了。\",\"、自定义resultMap，实现高级结果集映射\",\"、id ：用于完成主键值的映射\",\"、result ：用于完成普通列的映射\",\"、association ：一个复杂的类型关联;许多结果将包成这种类型\",\"、collection ： 复杂类型的集\",\"<resultMap id=\\\"UserMap\\\" type=\\\"User\\\"> <result property=\\\"id\\\" column=\\\"id\\\"/> <result property=\\\"name\\\" column=\\\"name\\\"/> <result property=\\\"password\\\" column=\\\"pwd\\\"/> </resultMap> <select id=\\\"getUserById\\\" parameterType=\\\"int\\\" resultMap=\\\"UserMap\\\"> select * from mybatis.user where id = #{id} </select> \"]}]},\"/study-tutorial/frame/mybatis/step_query.html\":{\"title\":\"8、分步查询\",\"contents\":[{\"header\":\"8.1、association 关联 【多对一】\",\"slug\":\"_8-1、association-关联-【多对一】\",\"contents\":[\"create table student ( id int not null primary key, name varchar(255) null, tid int not null, constraint student_ibfk_1 foreign key (tid) references teacher (id) ); create index tid on student (tid); create table teacher ( id int not null comment 'tid' primary key, name varchar(255) null ); \",\"List<Student> getStudent(); List<Student> getStudent2(); \",\"<!--==================方式一=================--> <select id=\\\"getStudent\\\" resultMap=\\\"TeacherStudent\\\"> select * from mybatis.student </select> <resultMap id=\\\"TeacherStudent\\\" type=\\\"Student\\\"> <result property=\\\"id\\\" column=\\\"id\\\"/> <result property=\\\"name\\\" column=\\\"name\\\"/> <association property=\\\"teacher\\\" column=\\\"tid\\\" javaType=\\\"com.yan.pojo.Teacher\\\" select=\\\"getTeacher\\\"/> </resultMap> <select id=\\\"getTeacher\\\" resultType=\\\"Teacher\\\"> select * from mybatis.teacher where id = #{id} </select> <!--====================方式二==================--> <resultMap id=\\\"TeacherStudent2\\\" type=\\\"com.yan.pojo.Student\\\"> <result property=\\\"id\\\" column=\\\"sid\\\"/> <result property=\\\"name\\\" column=\\\"sname\\\"/> <association property=\\\"teacher\\\" javaType=\\\"Teacher\\\"> <result property=\\\"name\\\" column=\\\"tname\\\"/> </association> </resultMap> <select id=\\\"getStudent2\\\" resultMap=\\\"TeacherStudent2\\\"> select s.id as sid, s.name as sname,t.name as tname from mybatis.student s, mybatis.teacher t where s.tid = t.id </select> \",\"association 分步查询使用延迟加载\",\"在分步查询的基础上，可以使用延迟加载来提升查询的效率，只需要在全局的Settings 中进行如下的配置:\",\"<!-- 开启延迟加载--> <setting name=\\\"lazyLoadingEnabled\\\" value=\\\"true\\\"/> <!-- 设置加载的数据是按需还是全部--> <setting name=\\\"aggressiveLazyLoading\\\" value=\\\"false\\\"/> \"]},{\"header\":\"8.2、collection 集合 【一对多】\",\"slug\":\"_8-2、collection-集合-【一对多】\",\"contents\":[\"@Data public class Teacher { private int id; private String name; private List<Student> students; } \",\"Teacher getTeacher(@Param(\\\"tid\\\") int id); \",\"按照结果嵌套处理\",\"<resultMap id=\\\"TeacherStudent\\\" type=\\\"com.yan.pojo.Teacher\\\"> <result property=\\\"id\\\" column=\\\"tid\\\"/> <result property=\\\"name\\\" column=\\\"tname\\\"/> <collection property=\\\"students\\\" ofType=\\\"Student\\\"> <!--property: 关联的属性名 ofType: 集合中元素的类型--> <result property=\\\"id\\\" column=\\\"sid\\\"/> <result property=\\\"name\\\" column=\\\"sname\\\"/> <result property=\\\"tid\\\" column=\\\"tid\\\"/> </collection> </resultMap> <select id=\\\"getTeacher\\\" resultMap=\\\"TeacherStudent\\\"> select s.id sid, s.name sname, t.name tname, t.id tid from mybatis.student s,mybatis.teacher t where s.tid = t.id and t.id = #{tid} </select> \",\"按照查询嵌套处理\",\"<resultMap id=\\\"TeacherStudent2\\\" type=\\\"com.yan.pojo.Teacher\\\"> <collection property=\\\"students\\\" ofType=\\\"com.yan.pojo.Student\\\" select=\\\"getStudentByTeacherId\\\" column=\\\"id\\\"> </collection> </resultMap> <select id=\\\"getStudentByTeacherId\\\" resultType=\\\"Student\\\"> select * from mybatis.student where tid = #{tid} </select> <select id=\\\"getTeacher2\\\" resultMap=\\\"TeacherStudent2\\\"> select * from mybatis.teacher where id = #{tid} </select> \",\"扩展: association 或collection 的fetchType 属性\",\"在<association> 和<collection>标签中都可以设置fetchType，指定本次查询是否要使用延迟加载。默\",\"认为fetchType=”lazy” ,如果本次的查询不想使用延迟加载，则可设置为fetchType=”eager”.\",\"fetchType 可以灵活的设置查询是否需要使用延迟加载，而不需要因为某个查询不想使用延迟加载将全局的延\",\"迟加载设置关闭.\"]}]},\"/study-tutorial/frame/spring-mvc/annotation_based.html\":{\"title\":\"4、基于注解开发Spring MVC\",\"contents\":[{\"header\":\"1、在 web.xml 中配置 DispatcherServlet\",\"slug\":\"_1、在-web-xml-中配置-dispatcherservlet\",\"contents\":[\"<!--注册DispatcherServlet--> <servlet> <servlet-name>springmvc</servlet-name> <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class> <init-param> <param-name>contextConfigLocation</param-name> <param-value>classpath:springmvc-servlet.xml</param-value> <!--这里有一个默认的规则，如果配置文件放在 webapp/WEB-INF/ 目录下，并且配置文件的名字等于 DispatcherServlet 的名字+ -servlet（即这里的配置文件路径是 webapp/WEB-INF/springmvc-servlet.xml），如果是这样的话，可以不用添加 init-param 参数，即不用手动配置 springmvc 的配置文件，框架会自动加载。--> </init-param> <load-on-startup>1</load-on-startup> </servlet> <servlet-mapping> <servlet-name>springmvc</servlet-name> <url-pattern>/</url-pattern> </servlet-mapping> \"]},{\"header\":\"2、新建spring配置文件： springmvc-servlet.xml\",\"slug\":\"_2、新建spring配置文件-springmvc-servlet-xml\",\"contents\":[\"<!--自动扫描包--> <context:component-scan base-package=\\\"com.xiaobear.controller\\\"/> <!--过滤--> <mvc:default-servlet-handler/> <!--自动配置 相当于HandleMapping和HandleAdapet--> <mvc:annotation-driven/> <!-- 配置映射解析器：如何将控制器返回的结果字符串，转换为一个物理的视图文件--> <bean id=\\\"InternalResourceViewResolver\\\" class=\\\"org.springframework.web.servlet.view.InternalResourceViewResolver\\\"> <property name=\\\"prefix\\\" value=\\\"/WEB-INF/jsp/\\\"/> <property name=\\\"suffix\\\" value=\\\".jsp\\\"/> </bean> \"]},{\"header\":\"3、Controller\",\"slug\":\"_3、controller\",\"contents\":[\"@Controller public class HelloController { @RequestMapping(\\\"/hello\\\") public String hello(Model model) { model.addAttribute(\\\"msg\\\",\\\"hello,spring mvc\\\"); return \\\"hello\\\"; } } \"]},{\"header\":\"4、在/WEB-INF/新建jsp目录，新建hello.jsp视图\",\"slug\":\"_4、在-web-inf-新建jsp目录-新建hello-jsp视图\",\"contents\":[\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> </head> <body> ${msg} </body> </html> \",\"总结：\",\"后续我们都是基于注解开发的，感觉那就是非常爽的\",\"springmvc必须配置的三大组件\",\"处理器映射器\",\"处理器适配器\",\"视图解析器\",\"<!--过滤--> <mvc:default-servlet-handler/> <!--自动配置 相当于HandleMapping和HandleAdapet--> <mvc:annotation-driven/> <!-- 配置映射解析器：如何将控制器返回的结果字符串，转换为一个物理的视图文件--> <bean id=\\\"InternalResourceViewResolver\\\" class=\\\"org.springframework.web.servlet.view.InternalResourceViewResolver\\\"> <property name=\\\"prefix\\\" value=\\\"/WEB-INF/jsp/\\\"/> <property name=\\\"suffix\\\" value=\\\".jsp\\\"/> </bean> \",\"Spring MVC万能模板(基于注解)\",\"注册DispatcherServlet\",\"<!--注册DispatcherServlet--> <servlet> <servlet-name>springmvc</servlet-name> <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class> <init-param> <param-name>contextConfigLocation</param-name> <param-value>classpath:springmvc-servlet.xml</param-value> </init-param> <load-on-startup>1</load-on-startup> </servlet> <servlet-mapping> <servlet-name>springmvc</servlet-name> <url-pattern>/</url-pattern> </servlet-mapping> \",\"spring配置文件模板\",\"注：导入mvc命令空间的时候记得先导入过滤，如果先导入处理器的命令空间，就会报错\",\"xmlns:mvc=\\\"http://www.springframework.org/schema/mvc\\\" <!--下面是错误的--> xmlns:mvc=\\\"http://www.springframework.org/schema/cache\\\" \",\"<!--自动扫描包--> <context:component-scan base-package=\\\"你自己的包\\\"/> <!--过滤--> <mvc:default-servlet-handler/> <!--自动配置 相当于HandleMapping和HandleAdapet--> <mvc:annotation-driven/> <!-- 配置映射解析器：如何将控制器返回的结果字符串，转换为一个物理的视图文件--> <bean id=\\\"InternalResourceViewResolver\\\" class=\\\"org.springframework.web.servlet.view.InternalResourceViewResolver\\\"> <property name=\\\"prefix\\\" value=\\\"/WEB-INF/jsp/\\\"/> <property name=\\\"suffix\\\" value=\\\".jsp\\\"/> </bean> \"]}]},\"/study-tutorial/frame/spring-mvc/dispatcherServlet.html\":{\"title\":\"5、`DispatcherServlet`与处理器\",\"contents\":[{\"header\":\"1、 DispatcherServlet\",\"slug\":\"_1、dispatcherservlet\",\"contents\":[\"DispatcherServlet 是前端控制器设计模式的实现，提供 Spring Web MVC 的集中访问点，而且负责职责的分派，而且与 Spring IoC 容器无缝集成，从而可以获得 Spring 的所有好处。DispatcherServlet 主要用作职责调度工作，本身主要用于控制流程，主要职责如下：\",\"文件上传解析，如果请求类型是 multipart 将通过 MultipartResolver 进行文件上传解析\",\"通过 HandlerMapping，将请求映射到处理器（返回一个 HandlerExecutionChain，它包括一个处理器、多个 HandlerInterceptor 拦截器）\",\"通过 HandlerAdapter 支持多种类型的处理器(HandlerExecutionChain 中的处理器)\",\"通过 ViewResolver 解析逻辑视图名到具体视图实现\",\"本地化解析\",\"渲染具体的视图等\",\"如果执行过程中遇到异常将交给 HandlerExceptionResolver 来解析\"]},{\"header\":\"1、 DispatcherServlet 配置详解\",\"slug\":\"_1、dispatcherservlet配置详解\",\"contents\":[\"<!--注册DispatcherServlet--> <servlet> <servlet-name>springmvc</servlet-name> <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class> <init-param> <param-name>contextConfigLocation</param-name> <param-value>classpath:springmvc-servlet.xml</param-value> </init-param> <load-on-startup>1</load-on-startup> </servlet> <servlet-mapping> <servlet-name>springmvc</servlet-name> <url-pattern>/</url-pattern> </servlet-mapping> \",\"• load-on-startup：表示启动容器时初始化该 Servlet\",\"• url-pattern：表示哪些请求交给Spring Web MVC处理， \\\"/\\\" 是用来定义默认servlet 映射的。也可以如\",\"*.html 表示拦截所有以 html 为扩展名的请求\",\"• contextConfigLocation：表示 SpringMVC 配置文件的路径\",\"DispatcherServlet其他初始化参数\",\"参数\",\"说明\",\"contextClass\",\"实现类，ConfigurableWebApplicationContext由该Servlet实例化并在本地配置。默认情况下，XmlWebApplicationContext使用。\",\"contextConfigLocation\",\"传递给上下文实例的字符串（由指定contextClass），以指示可以在哪里找到上下文。该字符串可能包含多个字符串（使用逗号作为分隔符）以支持多个上下文。对于具有两次定义的bean的多个上下文位置，以最新位置为准。\",\"namespace\",\"WebApplicationContext命名空间。默认为[servlet-name]-servlet。\",\"throwExceptionIfNoHandlerFound\",\"NoHandlerFoundException在找不到请求的处理程序时是否抛出。然后可以使用HandlerExceptionResolver（例如，通过使用 @ExceptionHandler控制器方法）捕获该异常并将其作为其他任何异常进行处理。默认情况下，它设置为false，在这种情况下，DispatcherServlet将响应状态设置为404（NOT_FOUND），而不会引发异常。请注意，如果还配置了默认servlet处理，则始终将未解决的请求转发到默认servlet，并且永远不会引发404。\"]},{\"header\":\"2、处理器\",\"slug\":\"_2、处理器\",\"contents\":[]},{\"header\":\"1、HandlerMapping\",\"slug\":\"_1、handlermapping\",\"contents\":[\"HandlerMapping ，中文译作处理器映射器，在 SpringMVC 中，系统提供了很多 HandlerMapping\",\"HandlerMapping 是负责根据 request 请求找到对应的 Handler 处理器及 Interceptor 拦截器，将它们封装在 HandlerExecutionChain 对象中返回给前端控制器。\",\"BeanNameUrlHandlerMapping\",\"BeanNameUrl 处理器映射器，根据请求的 url 与 Spring 容器中定义的 bean 的 name 进行匹配，从而从 Spring 容器中找到 bean 实例，就是说，请求的 Url 地址就是处理器 Bean 的名字。\",\"<bean class=\\\"org.springframework.web.servlet.handler.BeanNameUrlHand lerMapping\\\" id=\\\"handlerMapping\\\"> <property name=\\\"beanName\\\" value=\\\"/hello\\\"/> </bean> \",\"SimpleUrlHandlerMapping\",\"SimpleUrlHandlerMapping 是 BeanNameUrlHandlerMapping 的增强版本，它可以将 url 和处理器 bean 的 id 进行统一映射配置:\",\"<bean class=\\\"org.springframework.web.servlet.handler.SimpleUrlHandle rMapping\\\" id=\\\"handlerMapping\\\"> <property name=\\\"mappings\\\"> <props> <prop key=\\\"/hello\\\">myController</prop> <prop key=\\\"/hello2\\\">myController2</prop> </props> </property> </bean> <!--注意，在 props 中，可以配置多个请求路径和处理器实例的映射关系。--> \"]},{\"header\":\"2、 HandlerAdapter\",\"slug\":\"_2、handleradapter\",\"contents\":[\"HandlerAdapter 会根据适配器接口对后端控制器进行包装（适配），包装后即可对处理器进行执行，通过扩展处理器适配器可以执行多种类型的处理器，这里使用了适配器设计模式。\",\"SimpleControllerHandlerAdapter\",\"SimpleControllerHandlerAdapter 简单控制器处理器适配器，所有实现了 org.springframework.web.servlet.mvc.Controller 接口的 Bean 通过此适配器进行适配、执行，也就是说，如果我们开发的接口是通过实现 Controller 接口来完成的（不是通过注解开发的接口），那么 HandlerAdapter 必须是 SimpleControllerHandlerAdapter。\",\"<bean class=\\\"org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter\\\" /> \",\"HttpRequestHandlerAdapter\",\"HttpRequestHandlerAdapter，http 请求处理器适配器，所有实现了 org.springframework.web.HttpRequestHandler 接口的 Bean 通过此适配器进行适配、执行。\",\"@Controller public class MyController2 implements HttpRequestHandler { public void handleRequest(HttpServletRequest request, HttpServlet Response response) throws ServletException, IOException { System.out.println(\\\"-----MyController2-----\\\"); } } \",\"<bean class=\\\"org.springframework.web.servlet.handler.SimpleUrlHandle rMapping\\\" id=\\\"handlerMapping\\\"> <property name=\\\"mappings\\\"> <props> <prop key=\\\"/hello2\\\">myController2</prop> </props> </property> </bean> <bean class=\\\"org.springframework.web.servlet.mvc.HttpRequestHandlerA dapter\\\" id=\\\"handlerAdapter\\\"/> \"]}]},\"/study-tutorial/frame/spring-mvc/handle_data.html\":{\"title\":\"8、处理数据\",\"contents\":[{\"header\":\"1、处理请求数据\",\"slug\":\"_1、处理请求数据\",\"contents\":[\"Spring MVC 通过分析处理方法的签名，HTTP 请求信息绑定到处理方法的相应参数中。\",\"Spring MVC 对控制器处理方法签名的限制是很宽松的，几乎可以按喜欢的任何方式对方法进行签名。\",\"必要时可以对方法及方法入参标注相应的注解（ @PathVariable 、@RequestParam、 @RequestHeader 等）\",\"Spring MVC 框架会将 HTTP 请求的信息绑定到相应的方法入参中，并根据方法的返回值类型做出相应的后续处理。\"]},{\"header\":\"1、@RequestParam\",\"slug\":\"_1、-requestparam\",\"contents\":[\"在处理方法入参处使用 @RequestParam 可以把请求参数传递给请求方法\",\"value：参数名\",\"required：是否必须。默认为 true, 表示请求参数中必须包含对应的参数，若不存在，将抛出异常\",\"defaultValue: 默认值，当没有传递传递使用该值\",\"@RequestMapping(\\\"/a1\\\") public String requestParam(@RequestParam(value = \\\"name\\\") String name, @RequestParam(value=\\\"age\\\",required=false,defaultValue=\\\"0\\\") int age, Model model){ model.addAttribute(\\\"msg\\\",\\\"name=\\\"+name); model.addAttribute(\\\"msg1\\\",\\\"age=\\\"+age); return \\\"a\\\"; } \",\"<!--测试 请求参数 @RequestParam 注解使用 --> <a href=\\\"a1?name=xiaobear&age=3\\\">testRequestParam</a> \"]},{\"header\":\"2、@RequestHeader\",\"slug\":\"_2、-requestheader\",\"contents\":[\"使用 @RequestHeader 绑定请求报头的属性值\",\"请求头包含了若干个属性，服务器可据此获知客户端的信息，通过 @RequestHeader 即可将请求头中的\",\"属性值绑定到处理方法的入参中\",\"@RequestMapping(\\\"/a2\\\") public String testRequestHeader(@RequestHeader(\\\"Accept-Encoding\\\") String encoding ,Model model){ model.addAttribute(\\\"msg\\\",\\\"获取Accept-Encoding标头的值:\\\"+encoding); return \\\"a\\\"; } \",\"<!-- 测试 请求头@RequestHeader 注解使用 --> <a href=\\\"a2\\\">testRequestHeader</a> \"]},{\"header\":\"3、@CookieValue\",\"slug\":\"_3、-cookievalue\",\"contents\":[\"使用 @CookieValue 绑定请求中的 Cookie 值\",\"@CookieValue 可让处理方法入参绑定某个 Cookie 值\",\"@RequestMapping(\\\"/a3\\\") public String testCookieValue(@CookieValue(\\\"JSESSIONID\\\") String sessionId,Model model){ model.addAttribute(\\\"msg\\\",\\\"cookie=\\\"+sessionId); return \\\"a\\\"; } \",\"<!-- 测试 请求头@CookieValue 注解使用 --> <a href=\\\"a3\\\">testCookieValue</a><br> \"]},{\"header\":\"4、使用POJO 作为参数\",\"slug\":\"_4、使用pojo-作为参数\",\"contents\":[\"使用 POJO 对象绑定请求参数值\",\"Spring MVC 会按请求参数名和 POJO 属性名进行自动匹配，自动为该对象填充属性值。支持级联属性。\",\"如：dept.deptId、dept.address.tel 等\",\"表单\",\"<!-- 测试 POJO 对象传参，支持级联属性 --> <form action=\\\"a4\\\" method=\\\"POST\\\"> username: <input type=\\\"text\\\" name=\\\"username\\\"/><br> password: <input type=\\\"password\\\" name=\\\"password\\\"/><br> email: <input type=\\\"text\\\" name=\\\"email\\\"/><br> age: <input type=\\\"text\\\" name=\\\"age\\\"/><br> city: <input type=\\\"text\\\" name=\\\"address.city\\\"/><br> province: <input type=\\\"text\\\" name=\\\"address.province\\\"/> <input type=\\\"submit\\\" value=\\\"Submit\\\"/> </form> \",\"@RequestMapping(\\\"/a4\\\") public String testPojo(User user,Model model){ model.addAttribute(\\\"msg\\\",\\\"\\\"+user); return \\\"a\\\"; } \",\"实体类\",\"@Data @AllArgsConstructor @NoArgsConstructor public class User { private Integer id ; private String username; private String password; private String email; private int age; private Address address; } \",\"@Data @AllArgsConstructor @NoArgsConstructor public class Address { private String province; private String city; } \",\"中文乱码解决：\",\"<!--配置springMVC的乱码过滤--> <filter> <filter-name>characterEncodingFilter</filter-name> <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class> <init-param> <param-name>encoding</param-name> <param-value>UTF-8</param-value> </init-param> <init-param> <param-name>forceEncoding</param-name> <param-value>true</param-value> </init-param> </filter> <filter-mapping> <filter-name>characterEncodingFilter</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> \"]},{\"header\":\"2、处理响应数据\",\"slug\":\"_2、处理响应数据\",\"contents\":[]},{\"header\":\"1、SpringMVC 输出模型数据概述\",\"slug\":\"_1、springmvc-输出模型数据概述\",\"contents\":[\"ModelAndView: 处理方法返回值类型为 ModelAndView 时, 方法体即可通过该对象添加模型数据\",\"Map 及 Model: 入参为 org.springframework.ui.Model、 org.springframework.ui.ModelMap 或 java.uti.Map 时，处理方法返回时，Map 中的数据会自动添加到模型中。\",\"@SessionAttributes: 将模型中的某个属性暂存到 HttpSession 中，以便多个请求之间可以共享这个属性\",\"@ModelAttribute: 方法入参标注该注解后, 入参的对象就会放到数据模型中\",\"ModelAndView\",\"控制器处理方法的返回值如果为 ModelAndView, 则其既包含视图信息，也包含模型数据信息。\",\"添加模型数据:\",\"MoelAndView addObject(String attributeName, Object attributeValue)\",\"ModelAndView addAllObject(Map<String, ?> modelMap)\",\"设置视图:\",\"void setView(View view)\",\"void setViewName(String viewName)\",\"@RequestMapping(\\\"/b1\\\") public ModelAndView testModel(){ ModelAndView modelAndView = new ModelAndView(); modelAndView.setViewName(\\\"success\\\"); modelAndView.addObject(\\\"time\\\",new Date().toString()); return modelAndView; } \",\"处理模型数据之 Map\",\"Spring MVC 在内部使用了一个 org.springframework.ui.Model 接口存储模型数据具体使用步骤\",\"Spring MVC 在调用方法前会创建一个隐含的模型对象作为模型数据的存储容器。\",\"如果方法的入参为 Map 或 Model 类型，Spring MVC 会将隐含模型的引用传递给这些入参。\",\"在方法体内，开发者可以通过这个入参对象访问到模型中的所有数据，也可以向模型中添加新的属\",\"性数据\",\"@RequestMapping(\\\"/b2\\\") public String testMap(Map<String,Object> map){ map.put(\\\"name\\\", Arrays.asList(\\\"xiaobear\\\",\\\"yhx\\\",\\\"xiaohua\\\")); return \\\"success\\\"; }\",\"处理模型数据之 SessionAttributes \",\"若希望在多个请求之间共用某个模型属性数据，则可以在控制器类上标注一个@SessionAttributes, Spring MVC 将在模型中对应的属性暂存到 HttpSession 中。\",\"@SessionAttributes 除了可以通过属性名指定需要放到会话中的属性外，还可以通过模型属性的对象\",\"类型指定哪些模型属性需要放到会话中\",\"例如：\",\"@SessionAttributes(types=User.class) 会 将 隐 含 模 型 中 所 有 类 型 为User.class 的属性添加到会话中。\",\"@SessionAttributes(value={“user1”, “user2”})\",\"@SessionAttributes(types={User.class, Dept.class})\",\"@SessionAttributes(value={“user1”, “user2”}, types={Dept.class})\",\" @RequestMapping(\\\"/b3\\\") public String testSessionAttributes(Map<String,Object> map){ User user = new User(\\\"xiaobear\\\",\\\"123\\\",\\\"Xxx@qq.com\\\",18); map.put(\\\"user\\\", user); map.put(\\\"school\\\", \\\"xiaobear\\\"); //默认是被存放到request 域，如果设置了@SessionAttribute 注解，就同时存放到session 域中 return \\\"success\\\"; } \",\"@ModelAttribute\",\"在方法定义上使用 @ModelAttribute 注解：Spring MVC 在调用目标处理方法前，会先逐个调用在方法级上标注了 @ModelAttribute 的方法。\",\"在方法的入参前使用 @ModelAttribute 注解：可以从隐含对象中获取隐含的模型数据中获取对象，再将请求参数绑定到对象中，再传入入参。\",\"将方法入参对象添加到模型中。\",\"@RequestMapping(\\\"/b4\\\") public String testModelAttribute(User user){ System.out.println(user); return \\\"success\\\"; } @ModelAttribute public void getUser(@RequestParam(value=\\\"id\\\",required=false) Integer id, Map<String,Object> map){ if(id!=null){ //模拟从数据库中获取到的user 对象 User user = new User(1,\\\"xiaobear\\\",\\\"123456\\\",\\\"xxx@qq.com\\\",18); System.out.println(\\\"从数据库中查询的对象：user=\\\"+user ); map.put(\\\"user\\\", user); } }\",\" ```html <form action=\\\"b4\\\" method=\\\"POST\\\"> <input type=\\\"hidden\\\" name=\\\"id\\\" value=\\\"1\\\"><br> username: <input type=\\\"text\\\" name=\\\"username\\\" value=\\\"xiaobear\\\"/><br> email: <input type=\\\"text\\\" name=\\\"email\\\" value=\\\"xxx@qq.com\\\"/><br> age: <input type=\\\"text\\\" name=\\\"age\\\" value=\\\"3\\\"/><br> <input type=\\\"submit\\\" value=\\\"Submit\\\"/> </form> \"]},{\"header\":\"1、 @ModelAttribute 之运行原理执行\",\"slug\":\"_1、-modelattribute-之运行原理执行\",\"contents\":[\"@ModelAttribute 注解所修饰的方法，将从数据库中获取的对象存放到 Map 集合中，key 为 user\",\"SpringMVC从Map集合中获取 user对象，将表单数据封装到与参数名称对应的user对象属性上\",\"SpringMVC 将 user 对象作为参数，传递给目标方法。\",\"注意：@ModelAttribute注解修饰的方法中，放入到 Map 集合中的 key 值，应该和目标方法参数类型的类名称首字母小写一致。\"]},{\"header\":\"2、@ModelAttribute 之源码\",\"slug\":\"_2、-modelattribute-之源码\",\"contents\":[\"@Target({ElementType.PARAMETER, ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface ModelAttribute { @AliasFor(\\\"name\\\") String value() default \\\"\\\"; @AliasFor(\\\"value\\\") String name() default \\\"\\\"; boolean binding() default true; } \"]},{\"header\":\"3、重定向\",\"slug\":\"_3、重定向\",\"contents\":[\"一般情况下，控制器方法返回字符串类型的值会被当成逻辑视图名处理\",\"如果返回的字符串中带 forward: 或 redirect: 前缀时，SpringMVC 会对他们进行特殊处理：将 forward: 和 redirect: 当成指示符，其后的字符串作为 URL 来处理\",\"redirect:success.jsp：会完成一个到 success.jsp 的重定向的操作\",\"forward:success.jsp：会完成一个到 success.jsp 的转发操作\",\"@RequestMapping(\\\"/b5\\\") public String redirect(Model model){ model.addAttribute(\\\"msg\\\",\\\"sucess!\\\"); return \\\"redirect:/index.jsp\\\"; // return \\\"forward:/index.jsp\\\"; } \"]}]},\"/study-tutorial/frame/spring-mvc/helloword.html\":{\"title\":\"2、永远的HelloWorld\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"新建一个module,添加Web框架\"]},{\"header\":\"1、在web.xml 中配置DispatcherServlet\",\"slug\":\"_1、在web-xml-中配置dispatcherservlet\",\"contents\":[\"<!-- 配置SpringMVC 核心控制器： --> <servlet> <servlet-name>springDispatcherServlet</servlet-name> <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class> <!-- 配置DispatcherServlet 的初始化參數：设置文件的路径和文件名称--> <init-param> <param-name>contextConfigLocation</param-name> <param-value>classpath:springmvc.xml</param-value> </init-param> <load-on-startup>1</load-on-startup> </servlet> <servlet-mapping> <servlet-name>springDispatcherServlet</servlet-name> <url-pattern>/</url-pattern> </servlet-mapping> \",\"解释配置文件的名称定义规则:\",\"实际上也可以不通过contextConfigLocation 来配置SpringMVC 的配置文件, 而使用默认\",\"的.默认的配置文件为: /WEB-INF/<servlet-name>-servlet.xml\",\"所有请求都将自动拦截下来，拦截下来后，请求交给DispatcherServlet去处理，在加载 DispatcherServlet 时，还需要指定配置文件路径。这里有一个默认的规则，如果配置文件放在 webapp/WEB-INF/ 目录下，并且配置文件的名字等于 DispatcherServlet 的名字+ -servlet（即这里的配置文件路径是 webapp/WEB-INF/springmvc-servlet.xml），如果是这样的话，可以不用添加 init-param 参数，即不用手动配置 springmvc 的配置文件，框架会自动加载。\"]},{\"header\":\"2、加载 springmvc 配置文件\",\"slug\":\"_2、加载-springmvc-配置文件\",\"contents\":[\"在 resources 目录下，创建一个名为 spring.xml 的 springmvc 的配置文件\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\\\"> <!--spring mvc为我们做好的--> <bean class=\\\"org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping\\\" /> <bean class=\\\"org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter\\\" id=\\\"handlerAdapter\\\"/> <!--视图解析器--> <bean class=\\\"org.springframework.web.servlet.view.InternalResourceViewResolver\\\" id=\\\"viewResolver\\\"> <!--前缀--> <property name=\\\"prefix\\\" value=\\\"/WEB-INF/jsp/\\\"/> <!--后缀--> <property name=\\\"suffix\\\" value=\\\".jsp\\\"/> </bean> <bean id=\\\"/hello\\\" class=\\\"com.xiaobear.controller.HelloController\\\"/> </beans> \"]},{\"header\":\"3、Controller，处理浏览器请求的接口\",\"slug\":\"_3、controller-处理浏览器请求的接口\",\"contents\":[\"public class HelloController implements Controller { /** * 这就是一个请求处理接口 * @param httpServletRequest 这就是前端发送来的请求 * @param httpServletResponse 这就是服务端给前端的响应 * @return 返回值是一个 ModelAndView，Model 相当于是我们的数据模型 View 是我们的视图 * @throws Exception */ public ModelAndView handleRequest(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception { ModelAndView view = new ModelAndView(); view.addObject(\\\"msg\\\",\\\"hello,spring mvc!\\\"); view.setViewName(\\\"hello\\\"); //WEB-INF/jsp/hello.jsp return view; } } \"]},{\"header\":\"4、创建视图\",\"slug\":\"_4、创建视图\",\"contents\":[\"在WEB-INF下新建一个jsp目录,新建hello.jsp\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> </head> <body> ${msg} </body> </html> \",\"访问出现404的原因：\",\"仔细检查自己的路径\",\"在classes目录下新建一个lib目录，导入相关依赖 \"]}]},\"/study-tutorial/frame/spring-mvc/interceptor.html\":{\"title\":\"11、拦截器\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"SpringMVC 中的拦截器，相当于 Jsp/Servlet 中的过滤器，只不过拦截器的功能更为强大。\",\"public interface HandlerInterceptor { //这个是请求预处理的方法，只有当这个方法返回值为 true 的时候，后面的方法才会执行 default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return true; } default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { } default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception { } \",\"Spring MVC 也可以使用拦截器对请求进行拦截处理，用户可以自定义拦截器来实现特定的功能，自定义的拦截器必须实现HandlerInterceptor 接口\",\"preHandle()：这个方法在业务处理器处理请求之前被调用，在该方法中对用户请求request 进行处理。如果程序员决定该拦截器对请求进行拦截处理后还要调用其他的拦截器，或者是业务处理器去进行处理，则返回true；如果程序员决定不需要再调用其他的组件去处理请求，则返回false。\",\"postHandle()：这个方法在业务处理器处理完请求后，但是DispatcherServlet 向客户端返回响应前被调用，在该方法中对用户请求request 进行处理。\",\"afterCompletion()：这个方法在DispatcherServlet 完全处理完请求后被调用，可以在该方法中进行一些资源清理的操作。\"]},{\"header\":\"1、单个拦截器\",\"slug\":\"_1、单个拦截器\",\"contents\":[\"public class MyInterceptor implements HandlerInterceptor { /** * 这个是请求预处理的方法，只有当这个方法返回值为 true 的时候，后面的方法才会执行 * @param request * @param response * @param handler * @return * @throws Exception */ public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\\\"==========处理前==========\\\"); return true; } public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\\\"==========处理后==========\\\"); } public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\\\"==========清理==========\\\"); } } \",\"controller\",\"@Controller public class InterceptorController { @RequestMapping(\\\"/t1\\\") @ResponseBody public String test(){ return \\\"hello\\\"; } } \",\"在applicationContext.xml配置拦截器\",\" <!--配置拦截器--> <mvc:interceptors> <mvc:interceptor> <mvc:mapping path=\\\"/**\\\"/> <bean class=\\\"com.xiaobear.config.MyInterceptor\\\"/> </mvc:interceptor> </mvc:interceptors> \"]},{\"header\":\"2、拦截登录实例\",\"slug\":\"_2、拦截登录实例\",\"contents\":[\"登录表单login.jsp\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>登录页面</title> </head> <body> <form action=\\\"${pageContext.request.contextPath}/user/login\\\" method=\\\"post\\\"> 用户名：<input type=\\\"text\\\" name=\\\"username\\\"> 密码： <input type=\\\"text\\\" name=\\\"password\\\"> <input type=\\\"submit\\\" value=\\\"login\\\"> </form> </body> </html> \",\"index.jsp实现跳转\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>$Title$</title> </head> <body> <h1> <a href=\\\"${pageContext.request.contextPath}/user/toLogin\\\">登录页面</a> <a href=\\\"${pageContext.request.contextPath}/user/main\\\">首页</a> </h1> </body> </html> \",\"登录成功后进入首页\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> </head> <body> <h1>首页</h1> <h3>${username}</h3> <p> <a href=\\\"${pageContext.request.contextPath}/user/goOut\\\">注销</a> </p> </body> </html> \",\"登录拦截器\",\"public class LoginInterceptor implements HandlerInterceptor { public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { HttpSession session = request.getSession(); if (request.getRequestURI().contains(\\\"toLogin\\\")){ return true; } if (request.getRequestURI().contains(\\\"login\\\")){ return true; } if (session.getAttribute(\\\"userLoginInfo\\\")!=null){ return true; } request.getRequestDispatcher(\\\"/WEB-INF/jsp/login.jsp\\\").forward(request,response); return true; } } \",\"controller层\",\"@Controller @RequestMapping(\\\"/user\\\") public class LoginController { @RequestMapping(\\\"/login\\\") public String login(String username, String password, HttpSession session, Model model){ session.setAttribute(\\\"userLoginInfo\\\",username); model.addAttribute(\\\"username\\\",username); return \\\"main\\\"; } @RequestMapping(\\\"/goOut\\\") public String goOut(String username, String password, HttpSession session, Model model){ session.removeAttribute(\\\"userLoginInfo\\\"); return \\\"login\\\"; } @RequestMapping(\\\"/main\\\") public String main(){ return \\\"main\\\"; } @RequestMapping(\\\"/toLogin\\\") public String main1(){ return \\\"login\\\"; } } \",\"配置拦截器\",\"<!--配置拦截器--> <mvc:interceptors> <mvc:interceptor> <mvc:mapping path=\\\"/**\\\"/> <bean class=\\\"com.xiaobear.config.LoginInterceptor\\\"/> </mvc:interceptor> </mvc:interceptors> \"]}]},\"/study-tutorial/frame/spring-mvc/json.html\":{\"title\":\"9、JSON\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"目前主流的 JSON 处理工具主要有三种：\",\"jackson\",\"gson\",\"fastjson\",\"在 SpringMVC 中，对 jackson 和 gson 都提供了相应的支持，就是如果使用这两个作为 JSON 转换器，只需要添加对应的依赖就可以了，返回的对象和返回的集合、Map 等都会自动转为 JSON，但是，如果使用 fastjson，除了添加相应的依赖之外，还需要自己手动配置 HttpMessageConverter 转换器。其实前两个也是使用 HttpMessageConverter 转换器，但是是 SpringMVC 自动提供的，SpringMVC 没有给 fastjson 提供相应的转换器。\"]},{\"header\":\"1、Jackson\",\"slug\":\"_1、jackson\",\"contents\":[\"jackson 是一个使用比较多，时间也比较长的 JSON 处理工具，在 SpringMVC 中使用 jackson ，只需要添加 jackson 的依赖即可：\",\"<!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind --> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-databind</artifactId> <version>2.11.1</version> </dependency> \",\"依赖添加成功后，凡是在接口中直接返回的对象，集合等等，都会自动转为 JSON.\",\"pojo类\",\"@Data @AllArgsConstructor @NoArgsConstructor public class User { private String name; private int age; private String sex; } \",\"controller层\",\"@RestController public class UserController { @RequestMapping(value = \\\"/j1\\\"/*,produces = \\\"application/json; charset=utf-8\\\"*/) public String json1() throws JsonProcessingException { ObjectMapper mapper = new ObjectMapper(); User user = new User(); user.setName(\\\"鄢汉雄\\\"); user.setAge(18); user.setSex(\\\"boy\\\"); String value = mapper.writeValueAsString(user); return value; } @RequestMapping(\\\"/j2\\\") public String json2() throws JsonProcessingException { ObjectMapper objectMapper = new ObjectMapper(); List<User> users = new ArrayList<User>(); User user = new User(\\\"yhx\\\", 2, \\\"boy\\\"); User user1 = new User(\\\"yhx\\\", 3, \\\"boy\\\"); User user2 = new User(\\\"yhx\\\", 4, \\\"boy\\\"); User user3 = new User(\\\"yhx\\\", 5, \\\"boy\\\"); users.add(user); users.add(user2); users.add(user3); users.add(user1); String s = objectMapper.writeValueAsString(users); return s; } @RequestMapping(\\\"/j3\\\") public String json3() throws JsonProcessingException { /** * 修改配置文件 Date date = new Date(); return date; */ //时间格式一 /*SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\\\"yyyy-MM-dd HH:mm:ss\\\"); return objectMapper.writeValueAsString(simpleDateFormat.format(date));*/ /** * 时间格式三 */ /*ObjectMapper objectMapper = new ObjectMapper(); objectMapper.configure(SerializationFeature.WRITE_DATE_KEYS_AS_TIMESTAMPS,false); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\\\"yyyy-MM-dd HH:mm:ss\\\"); objectMapper.setDateFormat(simpleDateFormat); Date date = new Date(); return objectMapper.writeValueAsString(date);*/ Date date = new Date(); return JsonUtils.getJson(date,\\\"yyyy-MM-dd HH:mm:ss\\\"); } } \"]},{\"header\":\"1、json中文字符串乱码问题\",\"slug\":\"_1、json中文字符串乱码问题\",\"contents\":[\"解决方案一，设置生成环境为UTF-8\",\" @RequestMapping(value = \\\"/j1\\\",produces = \\\"application/json; charset=utf-8\\\") \",\"在spring配置文件中配置属性\",\"<!--处理json字符串乱码问题--> <mvc:annotation-driven> <mvc:message-converters register-defaults=\\\"true\\\"> <bean class=\\\"org.springframework.http.converter.StringHttpMessageConverter\\\"> <constructor-arg value=\\\"UTF-8\\\"/> </bean> <bean class=\\\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\\\"> <property name=\\\"objectMapper\\\"> <bean class=\\\"org.springframework.http.converter.json.Jackson2ObjectMapperFactoryBean\\\"> <property name=\\\"failOnEmptyBeans\\\" value=\\\"false\\\"></property> </bean> </property> </bean> </mvc:message-converters> </mvc:annotation-driven> \"]},{\"header\":\"2、json处理时间格式问题\",\"slug\":\"_2、json处理时间格式问题\",\"contents\":[\"解决方案一\",\"@RequestMapping(\\\"/j3\\\") public String json3() throws JsonProcessingException { ObjectMapper objectMapper = new ObjectMapper(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\\\"yyyy-MM-dd HH:mm:ss\\\"); Date date = new Date(); return objectMapper.writeValueAsString(simpleDateFormat.format(date)); } \",\"解决方案二\",\"@RequestMapping(\\\"/j3\\\") public String json3() throws JsonProcessingException { ObjectMapper objectMapper = new ObjectMapper(); objectMapper.configure(SerializationFeature.WRITE_DATE_KEYS_AS_TIMESTAMPS,false); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\\\"yyyy-MM-dd HH:mm:ss\\\"); objectMapper.setDateFormat(simpleDateFormat); Date date = new Date(); return objectMapper.writeValueAsString(date); } \",\"在spring配置文件中配置\",\"<!--json日期格式化--> <mvc:annotation-driven> <mvc:message-converters> <ref bean=\\\"httpMessageConverter\\\"/> </mvc:message-converters> </mvc:annotation-driven> <bean class=\\\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\\\" id=\\\"httpMessageConverter\\\"> <property name=\\\"objectMapper\\\"> <bean class=\\\"com.fasterxml.jackson.databind.ObjectMapper\\\"> <property name=\\\"dateFormat\\\"> <bean class=\\\"java.text.SimpleDateFormat\\\"> <constructor-arg name=\\\"pattern\\\" value=\\\"yyyy-MM-dd HH:mm:ss\\\"/> </bean> </property> <property name=\\\"timeZone\\\" value=\\\"Asia/Shanghai\\\"/> </bean> </property> </bean> \",\"@RequestMapping(\\\"/j3\\\") public Date json3() throws JsonProcessingException { /** * 修改配置文件 */ Date date = new Date(); return date; } \",\"针对方案二我们可以自己封装为一个工具类JsonUtils\",\"public class JsonUtils { public static String getJson(Object object){ return getJson(object,\\\"yyyy-MM-dd HH:mm:ss\\\"); } /** * Json时间格式工具类 * @param o 时间对象 * @param dateFormat 时间格式 * @return json格式 */ public static String getJson(Object o,String dateFormat){ ObjectMapper objectMapper = new ObjectMapper(); objectMapper.configure(SerializationFeature.WRITE_DATE_KEYS_AS_TIMESTAMPS,false); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(dateFormat); objectMapper.setDateFormat(simpleDateFormat); try { return objectMapper.writeValueAsString(o); } catch (JsonProcessingException e) { e.printStackTrace(); } return null; } } \",\"@RequestMapping(\\\"/j3\\\") public String json3() throws JsonProcessingException { Date date = new Date(); return JsonUtils.getJson(date,\\\"yyyy-MM-dd HH:mm:ss\\\"); } \"]},{\"header\":\"2、fastjson\",\"slug\":\"_2、fastjson\",\"contents\":[\"提供服务器端、安卓客户端两种解析工具，性能表现较好。\",\"提供了 toJSONString() 和 parseObject() 方法来将 Java 对象与 JSON 相互转换。调用toJSONString方 法即可将对象转换成 JSON 字符串，parseObject 方法则反过来将 JSON 字符串转换成对象。\",\"允许转换预先存在的无法修改的对象（只有class、无源代码）。\",\"Java泛型的广泛支持。\",\"允许对象的自定义表示、允许自定义序列化类。\",\"支持任意复杂对象（具有深厚的继承层次和广泛使用的泛型类型）。\",\"使用 fastjson，我们首先添加 fastjson 依赖\",\"<!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --> <dependency> <groupId>com.alibaba</groupId> <artifactId>fastjson</artifactId> <version>1.2.72</version> </dependency> \",\"在 SpringMVC 并没针对 fastjson 提供相应的 HttpMessageConverter，所以，fastjson 在使用时，一定要自己手动配置 HttpMessageConverter\",\"<mvc:annotation-driven> <mvc:message-converters> <ref bean=\\\"httpMessageConverter\\\"/> </mvc:message-converters> </mvc:annotation-driven> <bean class=\\\"com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter\\\" id=\\\"httpMessageConverter\\\"> <property name=\\\"fastJsonConfig\\\"> <bean class=\\\"com.alibaba.fastjson.support.config.FastJsonConfig\\\"> <property name=\\\"dateFormat\\\" value=\\\"yyyy-MM-dd\\\"/> </bean> </property> </bean> \"]},{\"header\":\"1、fastjson 默认中文乱码\",\"slug\":\"_1、fastjson-默认中文乱码\",\"contents\":[\"添加如下配置解决：\",\"<mvc:annotation-driven> <mvc:message-converters> <ref bean=\\\"httpMessageConverter\\\"/> </mvc:message-converters> </mvc:annotation-driven> <bean class=\\\"com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter\\\" id=\\\"httpMessageConverter\\\"> <property name=\\\"fastJsonConfig\\\"> <bean class=\\\"com.alibaba.fastjson.support.config.FastJsonConfig\\\"> <property name=\\\"dateFormat\\\" value=\\\"yyyy-MM-dd\\\"/> </bean> </property> <property name=\\\"supportedMediaTypes\\\"> <list> <value>application/json;charset=utf-8</value> </list> </property> </bean> \"]},{\"header\":\"2、使用\",\"slug\":\"_2、使用\",\"contents\":[\"@RequestMapping(\\\"/j4\\\") public String json4() throws JsonProcessingException { List<User> users = new ArrayList<User>(); User user = new User(\\\"yhx\\\", 2, \\\"boy\\\"); User user1 = new User(\\\"yhx\\\", 3, \\\"boy\\\"); User user2 = new User(\\\"yhx\\\", 4, \\\"boy\\\"); User user3 = new User(\\\"yhx\\\", 5, \\\"boy\\\"); users.add(user); users.add(user2); users.add(user3); users.add(user1); String s = JSON.toJSONString(users); return s; } \",\"注：访问出现500的原因：记得把fastjson依赖导入lib中\"]}]},\"/study-tutorial/frame/spring-mvc/requestMapping.html\":{\"title\":\"6、@RequestMapping详解\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"@GetMapping == @RequestMapping(method = RequestMethod.GET)\",\"@PostMapping == @RequestMapping(method = RequestMethod.POST)\",\"@PutMapping == @RequestMapping(method = RequestMethod.PUT)\",\"@DeleteMapping == @RequestMapping(method = RequestMethod.DELETE)\",\"@PatchMapping == @RequestMapping(method = RequestMethod.PATCH)\",\"可以使用@RequestMapping批注将请求映射到控制器方法。它具有各种属性，可以通过URL，HTTP方法，请求参数，标头和媒体类型进行匹配。您可以在类级别使用它来表示共享的映射，也可以在方法级别使用它来缩小到特定的端点映射。\",\"SpringMVC 使用@RequestMapping 注解为控制器指定可以处理哪些 URL 请求\",\"在控制器的类定义及方法定义处都可标注 @RequestMapping\",\"标记在类上：提供初步的请求映射信息。相对于 WEB 应用的根目录\",\"标记在方法上：提供进一步的细分映射信息。相对于标记在类上的 URL。\",\"若类上未标注 @RequestMapping，则方法处标记的 URL 相对于 WEB 应用的根目录\",\"作用：DispatcherServlet 截获请求后，就通过控制器上 @RequestMapping 提供的映射信息确定请求所对应的处理方法。\",\"@Controller public class ControllerTest3 { @RequestMapping(\\\"/test3\\\") /** * 映射请求的名称：用于客户端请求；类似Struts2 中action 映射配置的，action 名称 * 1 使用@RequestMapping 注解来映射请求的 URL * 2 返回值会通过视图解析器解析为实际的物理视图, * 对于 InternalResourceViewResolver 视图解析器, * 会做如下的解析: * 通过 prefix + returnVal + 后缀 这样的方式得到实际的物理视图, 然会做转发操作. * /WEB-INF/jsp/test.jsp */ public String test3(){ return \\\"test\\\"; } } \"]},{\"header\":\"1、映射请求参数、请求方法或请求头\",\"slug\":\"_1、映射请求参数、请求方法或请求头\",\"contents\":[\"@RequestMapping 除了可以使用请求 URL 映射请求外，还可以使用请求方法、请求参数及请求头映射请求\",\"@RequestMapping 的 value【重点】、method【重点】、params【了解】 及 heads 【了解】 分别表示请求 URL、请求方法、请求参数及请求头的映射条件，他们之间是与的关系，联合使用多个条件可让请求映射更加精确化。\",\"params 和 headers 支持简单的表达式：\",\"param1: 表示请求必须包含名为 param1 的请求参数\",\"!param1: 表示请求不能包含名为 param1 的请求参数\",\"param1 != value1: 表示请求包含名为 param1 的请求参数，但其值不能为 value1 {\\\"param1=value1\\\", \\\"param2\\\"}: 请求必须包含名为 param1 和 param2 的两个请求参数，且 param1 参数的值必须为 value1\",\"@Controller public class RequestController { @RequestMapping(value = \\\"/t1\\\",method = RequestMethod.POST) public String test1(Model model){ model.addAttribute(\\\"msg\\\",\\\"get请求方式成功\\\"); return \\\"test1\\\"; } } \",\"发生错误\"]},{\"header\":\"2、URL模式\",\"slug\":\"_2、url模式\",\"contents\":[\"模式\",\"描述\",\"例\",\"?\",\"匹配一个字符\",\"\\\"/pages/t?st.html\\\"比赛\\\"/pages/test.html\\\" 和\\\"/pages/t3st.html\\\"\",\"*\",\"匹配路径段中的零个或多个字符\",\"\\\"/resources/*.png\\\" 火柴 \\\"/resources/file.png\\\"``\\\"/projects/*/versions\\\"匹配\\\"/projects/spring/versions\\\"但不匹配\\\"/projects/spring/boot/versions\\\"\",\"**\",\"匹配零个或多个路径段，直到路径结束\",\"\\\"/resources/**\\\"比赛\\\"/resources/file.png\\\"和\\\"/resources/images/file.png\\\"\",\"{name}\",\"匹配路径段并将其捕获为名为“ name”的变量\",\"\\\"/projects/{project}/versions\\\"比赛\\\"/projects/spring/versions\\\"和捕获project=spring\",\"{name:[a-z]+}\",\"将正则表达式匹配\\\"[a-z]+\\\"为名为“名称”的路径变量\",\"\\\"/projects/{project:[a-z]+}/versions\\\"匹配\\\"/projects/spring/versions\\\"但不匹配\\\"/projects/spring1/versions\\\"\",\" /** *@RequestMapping(\\\"/t2/ab??\\\") 匹配http://localhost:8080/springmvc_04_controller_war_exploded/t2/abcd。。。 */ @RequestMapping(\\\"/t2/ab??\\\") public String testUrl(Model model){ model.addAttribute(\\\"msg\\\",\\\"请求成功！xiaobear\\\"); return \\\"test1\\\"; } } \"]},{\"header\":\"3、 RequestMapping 映射请求占位符 PathVariable 注解\",\"slug\":\"_3、requestmapping-映射请求占位符pathvariable-注解\",\"contents\":[]},{\"header\":\"1、 @PathVariable\",\"slug\":\"_1、-pathvariable\",\"contents\":[\"通过 @PathVariable 可以将 URL 中占位符参数绑定到控制器处理方法的入参中\",\"URL 中的 {xxx} 占位符可以通过@PathVariable(\\\"xxx\\\")绑定到操作方法的入参中。\",\"@GetMapping(\\\"/t3/{a}/{b}\\\") public String testPath(@PathVariable int a, @PathVariable int b, Model model){ int restPath = a+b; model.addAttribute(\\\"msg\\\",\\\"结果为：\\\"+restPath); return \\\"test1\\\"; } \"]}]},\"/study-tutorial/frame/spring-mvc/restFul.html\":{\"title\":\"7、RestFul\",\"contents\":[{\"header\":\"1、REST 是什么？\",\"slug\":\"_1、rest-是什么\",\"contents\":[\"REST：即 Representational State Transfer。（资源）表现层状态转化。是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用\",\"资源（Resources）：网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、\",\"一首歌曲、一种服务，总之就是一个具体的存在。可以用一个 URI（统一资源定位符）指向它，每种资源对\",\"应一个特定的 URI 。获取这个资源，访问它的 URI就可以，因此 URI 即为每一个资源的独一无二的识别符。\",\"表现层（Representation）：把资源具体呈现出来的形式，叫做它的表现层（Representation）。比如，文本\",\"可以用 txt 格式表现，也可以用 HTML 格式、XML格式、JSON 格式表现，甚至可以采用二进制格式。\",\"状态转化（State Transfer）：每发出一个请求，就代表了客户端和服务器的一次交互过程。HTTP 协议，是一\",\"个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，\",\"让服务器端发生“状态转化” （State Transfer）而这种转化是建立在表现层之上的，所以就是 “表现层状态转\",\"化”。\",\"具体说，就是 HTTP 协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种\",\"基本操作：GET 用来获取资源，POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。\"]},{\"header\":\"2、URL风格\",\"slug\":\"_2、url风格\",\"contents\":[\"我们传统的传入参数是/xiaobear/id=1?method=add\",\"/order/1 HTTP GET ：得到 id = 1 的 order /order/1 HTTP DELETE：删除 id = 1 的 order /order HTTP PUT：更新 order /order HTTP POST：新增 order \",\"符合REST设计风格的Web API称为RESTful API。它从以下三个方面资源进行定义：\",\"直观简短的资源地址：URI，比如：http://example.com/resources。\",\"传输的资源：Web服务接受与返回的互联网媒体类型，比如：JSON，XML，YAML等。\",\"对资源的操作：Web服务在该资源上所支持的一系列请求方法（比如：POST，GET，PUT或DELETE）。\",\"资源\",\"GET\",\"PUT\",\"POST\",\"DELETE\",\"一组资源的URI，比如https://example.com/resources\",\"列出URI，以及该资源组中每个资源的详细信息（后者可选）。\",\"使用给定的一组资源替换当前整组资源。\",\"在本组资源中创建/追加一个新的资源。该操作往往返回新资源的URL。\",\"删除整组资源。\",\"单个资源的URI，比如https://example.com/resources/142\",\"获取指定的资源的详细信息，格式可以自选一个合适的网络媒体类型（比如：XML、JSON等）\",\"替换/创建指定的资源。并将其追加到相应的资源组中。\",\"把指定的资源当做一个资源组，并在其下创建/追加一个新的元素，使其隶属于当前资源。\",\"删除指定的元素\"]},{\"header\":\"3、 HiddenHttpMethodFilter\",\"slug\":\"_3、hiddenhttpmethodfilter\",\"contents\":[\"浏览器 form 表单只支持 GET 与 POST 请求，而 DELETE、PUT 等 method 并不支持，Spring3.0 添加了一个过滤器，可以将这些请求转换为标准的 http 方法，使得支持 GET、POST、PUT 与 DELETE 请求。\"]},{\"header\":\"源码分析：\",\"slug\":\"源码分析\",\"contents\":[\"为什么请求隐含参数名称必须叫做”_method”\",\"hiddenHttpMethodFilter 的处理过程\",\"protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { HttpServletRequest requestToUse = request; if (\\\"POST\\\".equals(request.getMethod()) && request.getAttribute(\\\"javax.servlet.error.exception\\\") == null) { String paramValue = request.getParameter(this.methodParam); if (StringUtils.hasLength(paramValue)) { String method = paramValue.toUpperCase(Locale.ENGLISH); if (ALLOWED_METHODS.contains(method)) { requestToUse = new HiddenHttpMethodFilter.HttpMethodRequestWrapper(request, method); } } } filterChain.doFilter((ServletRequest)requestToUse, response); } private static class HttpMethodRequestWrapper extends HttpServletRequestWrapper { private final String method; public HttpMethodRequestWrapper(HttpServletRequest request, String method) { super(request); this.method = method; } public String getMethod() { return this.method; } } \"]},{\"header\":\"代码测试\",\"slug\":\"代码测试\",\"contents\":[\"在web.xml中配置filter\",\"<!-- 支持 REST 风格的过滤器：可以将 POST 请求转换为 PUT 或 DELETE 请求 --> <filter> <filter-name>HiddenHttpMethodFilter</filter-name> <filter-class>org.springframework.web.filter.HiddenHttpMethodFilter</filter-class> </filter> <filter-mapping> <filter-name>HiddenHttpMethodFilter</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> \",\"@Controller public class RestController { @GetMapping(value=\\\"/testRESTGet/{id}\\\") public String testRESTGet(@PathVariable(value=\\\"id\\\") Integer id, Model model) { model.addAttribute(\\\"msg\\\",\\\"ID=\\\"+id); return \\\"test1\\\"; } @PostMapping(\\\"/testRESTPost\\\") public String testRESTPost( Model model) { model.addAttribute(\\\"msg\\\",\\\"Post方式请求成功！\\\"); return \\\"test1\\\"; } @PutMapping(\\\"/testRESTPut/{id}\\\") public String testRESTPut(@PathVariable(value=\\\"id\\\") Integer id, Model model) { model.addAttribute(\\\"msg\\\",\\\"Put方式请求成功！id=\\\"+id); return \\\"test1\\\"; } @PutMapping(\\\"/testRESTDelete/{id}\\\") public String testRESTDelete(@PathVariable(value=\\\"id\\\") Integer id, Model model) { model.addAttribute(\\\"msg\\\",\\\"Delete方式请求成功！id=\\\"+id); return \\\"test1\\\"; } } \",\"index.jsp\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>$Title$</title> </head> <body> <!-- 实验 1 测试 REST 风格 GET 请求 --> <a href=\\\"testRESTGet/1\\\">testREST GET</a><br/> <!-- 实验 2 测试 REST 风格 POST 请求 --> <form action=\\\"testRESTPost\\\" method=\\\"POST\\\"> <input type=\\\"submit\\\" value=\\\"testRESTPost\\\"> </form> <!-- 实验 3 测试 REST 风格 PUT 请求 --> <form action=\\\"testRESTPut/1\\\" method=\\\"POST\\\"> <input type=\\\"hidden\\\" name=\\\"_method\\\" value=\\\"PUT\\\"> <input type=\\\"submit\\\" value=\\\"testRESTPut\\\"> </form> <!-- 实验 4 测试 REST 风格 DELETE 请求 --> <form action=\\\"testRESTDelete/1\\\" method=\\\"POST\\\"> <input type=\\\"hidden\\\" name=\\\"_method\\\" value=\\\"DELETE\\\"> <input type=\\\"submit\\\" value=\\\"testRESTDelete\\\"> </form> </body> </html> \"]}]},\"/study-tutorial/frame/spring-mvc/spring.html\":{\"title\":\"1、Spring简介\",\"contents\":[{\"header\":\"1.常用组件\",\"slug\":\"_1-常用组件\",\"contents\":[\"1、DispatcherServlet：前端控制器\",\"2 、Controller：处理器/页面控制器，做的是 MVC 中的 C 的事情，但控制逻辑转移到前端控制器了，用于对请求进行处理\",\"3 、HandlerMapping ：请求映射到处理器，找谁来处理，如果映射成功返回一个 HandlerExecutionChain 对象（包含一个 Handler 处理器(页面控制器)对象、多 个 HandlerInterceptor 拦截器对象）\",\"4 、View Resolver : 视图解析器，找谁来处理返回的页面。把逻辑视图解析为具体 的 View,进行这种策略模式，很容易更换其他视图技术； 如 InternalResourceViewResolver 将逻辑视图名映射为 JSP 视图\",\"5 、LocalResolver：本地化、国际化\",\"6 、MultipartResolver：文件上传解析器\",\"7 、HandlerExceptionResolver：异常处理器\"]},{\"header\":\"2、SpringMVC和Struts的区别\",\"slug\":\"_2、springmvc和struts的区别\",\"contents\":[]},{\"header\":\"共同点：\",\"slug\":\"共同点\",\"contents\":[\"都是表现层框架，基于MVC模型编写的\",\"都离不开原始的ServletAPI\",\"处理机制都是一个核心控制器--DispatcherServlet\",\"SpringMVC\",\"Struts\",\"入口是Servlet\",\"Filter\",\"基于方法设计的\",\"基于类设计的\",\"简洁，支持JSR303，处理Ajax请求更方便\"]},{\"header\":\"3、SpringMVC的特点\",\"slug\":\"_3、springmvc的特点\",\"contents\":[\"天生与Spring 框架集成，如：(IOC,AOP)\",\"支持Restful 风格\",\"进行更简洁的Web 层开发\",\"支持灵活的URL 到页面控制器的映射\",\"非常容易与其他视图技术集成，如:Velocity、FreeMarker 等等\",\"因为模型数据不存放在特定的API 里，而是放在一个Model 里(Map 数据结构实现，因此很容易被其他框架使用)\",\"非常灵活的数据验证、格式化和数据绑定机制、能使用任何对象进行数据绑定，不必实现特定框架的API\",\"更加简单、强大的异常处理\",\"对静态资源的支持\",\"支持灵活的本地化、主题等解析\"]},{\"header\":\"4、回顾Servlet\",\"slug\":\"_4、回顾servlet\",\"contents\":[\"1.在web.xml中配置servlet\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <web-app xmlns=\\\"http://xmlns.jcp.org/xml/ns/javaee\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\\\" version=\\\"4.0\\\"> <servlet> <servlet-name>ReServlet</servlet-name> <servlet-class>com.xiaobear.servlet.ReServlet</servlet-class> </servlet> <servlet-mapping> <servlet-name>ReServlet</servlet-name> <url-pattern>/reServlet</url-pattern> </servlet-mapping> <!-- <session-config> <session-timeout>15</session-timeout> </session-config> <welcome-file-list> <welcome-file>index.jsp</welcome-file> </welcome-file-list>--> </web-app> \",\"2、建立一个servlet\",\"/** * @author xiaobear * @date 2020/6/27 0027 16:39 * @Description 回顾Servlet */ public class ReServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String method = req.getParameter(\\\"method\\\"); if (method.equals(\\\"add\\\")) { req.getSession().setAttribute(\\\"msg\\\",\\\"执行了add方法\\\"); } if (method.equals(\\\"delete\\\")) { req.getSession().setAttribute(\\\"msg\\\",\\\"执行了delete方法\\\"); } req.getRequestDispatcher(\\\"/WEB-INF/jsp/test.jsp\\\").forward(req, resp); } } \",\"3、视图/WEB-INF/jsp/test.jsp\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>test.jsp</title> </head> <body> ${msg} </body> </html> \",\"4、测试\",\"http://localhost:8080/springmvc_01_servlet_war_exploded/reServlet?method=add\",\"http://localhost:8080/springmvc_01_servlet_war_exploded/reServlet?method=delete\"]}]},\"/study-tutorial/frame/spring-mvc/static_resource.html\":{\"title\":\"12、静态资源访问\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在 SpringMVC 中，静态资源，默认都是被拦截的，例如 html、js、css、jpg、png、txt、pdf 等等，都是无法直接访问的。因为所有请求都被拦截了，所以，针对静态资源，我们要做额外处理，处理方式很简单，直接在 SpringMVC 的配置文件中，添加如下内容：\",\"<mvc:resources mapping=\\\"/static/html/**\\\" location=\\\"/static/html/\\\"/> \",\"mapping 表示映射规则，也是拦截规则，就是说，如果请求地址是 /static/html 这样的格式的话，那么对应的资源就去 /static/html/ 这个目录下查找。\",\"在映射路径的定义中，最后是两个 *，这是一种 Ant 风格的路径匹配符号，一共有三个通配符：\",\"通配符\",\"含义\",\"**\",\"匹配多层路径\",\"*\",\"匹配一层路径\",\"？\",\"匹配任意单个字符\",\"一个比较原始的配置方式可能如下：\",\"<mvc:resources mapping=\\\"/static/html/**\\\" location=\\\"/static/html/\\\"/> <mvc:resources mapping=\\\"/static/js/**\\\" location=\\\"/static/js/\\\"/> <mvc:resources mapping=\\\"/static/css/**\\\" location=\\\"/static/css/\\\"/> \",\"但是，由于 ** 可以表示多级路径，所以，以上配置，我们可以进行简化：\",\"<mvc:resources mapping=\\\"/**\\\" location=\\\"/\\\"/> \"]}]},\"/study-tutorial/frame/spring-mvc/upload.html\":{\"title\":\"10、文件上传\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"SpringMVC 中对文件上传做了封装，我们可以更加方便的实现文件上传。从 Spring3.1 开始，对于文件上传，提供了两个处理器：\",\"CommonsMultipartResolver\",\"StandardServletMultipartResolver\",\"第一个处理器兼容性较好，可以兼容 Servlet3.0 之前的版本，但是它依赖了 commons-fileupload 这个第三方工具，所以如果使用这个，一定要添加 commons-fileupload 依赖。\",\"第二个处理器兼容性较差，它适用于 Servlet3.0 之后的版本，它不依赖第三方工具，使用它，可以直接做文件上传。\"]},{\"header\":\"1、 CommonsMultipartResolver\",\"slug\":\"_1、commonsmultipartresolver\",\"contents\":[\"使用 CommonsMultipartResolver 做文件上传，需要首先添加 commons-fileupload 依赖\",\"<!-- https://mvnrepository.com/artifact/commons-fileupload/commons-fileupload --> <dependency> <groupId>commons-fileupload</groupId> <artifactId>commons-fileupload</artifactId> <version>1.4</version> </dependency> \"]},{\"header\":\"1、在 SpringMVC 的配置文件中，配置 MultipartResolver：\",\"slug\":\"_1、在-springmvc-的配置文件中-配置-multipartresolver\",\"contents\":[\"<!--配置 MultipartResolver id必须是multipartResolver--> <bean class=\\\"org.springframework.web.multipart.commons.CommonsMultipartResolver\\\" id=\\\"multipartResolver\\\"/> \"]},{\"header\":\"2、Jsp页面\",\"slug\":\"_2、jsp页面\",\"contents\":[\"<form action=\\\"/upload\\\" method=\\\"post\\\" enctype=\\\"multipart/form-data\\\"> <input type=\\\"file\\\" name=\\\"file\\\"> <input type=\\\"submit\\\" value=\\\"上传\\\"> </form> \",\"注意文件上传请求是 POST 请求，enctype 一定是 multipart/form-data\",\"upload.jsp\",\"<%@ page contentType=\\\"text/html;charset=UTF-8\\\" language=\\\"java\\\" %> <html> <head> <title>Title</title> </head> <body> <% Object msg = request.getAttribute(\\\"msg\\\"); %> <%=msg%> </body> </html> \"]},{\"header\":\"3、controller\",\"slug\":\"_3、controller\",\"contents\":[\"@Controller public class UploadController { @RequestMapping(\\\"/upload\\\") public String upload(@RequestParam(\\\"file\\\") List<MultipartFile> files, HttpServletRequest request) throws IOException { /*System.out.println(\\\"OriginalFilename : \\\"+file.getOriginalFilename()); InputStream inputStream = file.getInputStream(); System.out.println(\\\"inputStream.available() : \\\"+inputStream.available()); System.out.println(\\\"inputStream : \\\"+inputStream);*/ String msg = \\\"\\\"; // 判断文件是否上传 if (!files.isEmpty()) { // 设置上传文件的保存目录 String basePath = request.getServletContext().getRealPath(\\\"/upload/\\\"); System.out.println(basePath); // 判断文件目录是否存在 File uploadFile = new File(basePath); if (!uploadFile.exists()) { uploadFile.mkdirs(); } for (MultipartFile file : files) { String originalFilename = file.getOriginalFilename(); if (originalFilename != null && !originalFilename.equals(\\\"\\\")) { try { // 对文件名做加UUID值处理 originalFilename = UUID.randomUUID() + \\\"_\\\" + originalFilename; file.transferTo(new File(basePath + originalFilename)); } catch (IOException e) { e.printStackTrace(); msg = \\\"文件上传失败！\\\"; } } else { msg = \\\"上传的文件为空！\\\"; } } msg = \\\"文件上传成功！\\\"; } else { msg = \\\"没有文件被上传！\\\"; } request.setAttribute(\\\"msg\\\", msg); return \\\"upload\\\"; } } \",\"这里还有一个小问题，在 SpringMVC 中，静态资源默认都是被自动拦截的，无法访问，意味着上传成功的图片无法访问，因此，还需要我们在 SpringMVC 的配置文件中，再添加如下配置：\",\"<mvc:resources mapping=\\\"/**\\\" location=\\\"/\\\"/> \",\"当然，默认的配置不一定满足我们的需求，我们还可以自己手动配置文件上传大小等：\",\"<bean class=\\\"org.springframework.web.multipart.commons.CommonsMultipartResolver\\\" id=\\\"multipartResolver\\\"> <!--默认的编码--> <property name=\\\"defaultEncoding\\\" value=\\\"UTF-8\\\"/> <!--上传的总文件大小--> <property name=\\\"maxUploadSize\\\" value=\\\"1048576\\\"/> <!--上传的单个文件大小--> <property name=\\\"maxUploadSizePerFile\\\" value=\\\"1048576\\\"/> <!--内存中最大的数据量，超过这个数据量，数据就要开始往硬盘中写了--> <property name=\\\"maxInMemorySize\\\" value=\\\"4096\\\"/> <!--临时目录，超过 maxInMemorySize 配置的大小后，数据开始往临时目录写，等全部上传完成后，再将数据合并到正式的文件上传目录--> <property name=\\\"uploadTempDir\\\" value=\\\"file:///E:\\\\\\\\tmp\\\"/> </bean> \"]},{\"header\":\"2、 StandardServletMultipartResolver\",\"slug\":\"_2、standardservletmultipartresolver\",\"contents\":[\"这种文件上传方式，不需要依赖第三方 jar（主要是不需要添加 commons-fileupload 这个依赖），但是也不支持 Servlet3.0 之前的版本。 使用 StandardServletMultipartResolver ，那我们首先在 SpringMVC 的配置文件中，配置这个 Bean：\",\"<bean class=\\\"org.springframework.web.multipart.support.StandardServletMultipartResolver\\\" id=\\\"multipartResolver\\\"> </bean> \",\"注意，这里 Bean 的名字依然叫 multipartResolver\",\"配置完成后，注意，这个 Bean 无法直接配置上传文件大小等限制。需要在 web.xml 中进行配置（这里，即使不需要限制文件上传大小，也需要在 web.xml 中配置 multipart-config）\",\"<multipart-config> <!--文件保存的临时目录，这个目录系统不会主动创建--> <location>E:\\\\\\\\temp</location> <!--上传的单个文件大小--> <max-file-size>1048576</max-file-size> <!--上传的总文件大小--> <max-request-size>1048576</max-request-size> <!--这个就是内存中保存的文件最大大小--> <file-size-threshold>4096</file-size-threshold> </multipart-config> \",\"配置完成后，就可以测试文件上传了，测试方式和上面一样。\"]},{\"header\":\"3、多文件上传\",\"slug\":\"_3、多文件上传\",\"contents\":[\"主要是 input 节点中多了 multiple 属性。后端用一个数组来接收文件即可：\",\"@RequestMapping(\\\"/upload2\\\") public String uploadFile(MultipartFile[] files, HttpServletRequest req) throws IOException { for (MultipartFile file : files) { if(!file.isEmpty()){ file.transferTo(new File(\\\"D:/\\\"+file.getOriginalFilename())); req.getSession().setAttribute(\\\"msg\\\",\\\"success\\\"); } } return \\\"upload\\\"; } \"]}]},\"/study-tutorial/frame/spring-mvc/working_principle.html\":{\"title\":\"3、Spring MVC工作流程\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"面试99%会问到\",\"客户端请求提交到DispatcherServlet\",\"由DispatcherServlet 控制器查询一个或多个HandlerMapping，找到处理请求的 Controller DispatcherServlet\",\"请求提交到 Controller（也称为Handler）\",\"Controller 调用业务逻辑处理后，返回 ModelAndView\",\"DispatcherServlet 查询一个或多个 ViewResoler 视图解析器，找到 ModelAndView 指定的视图\",\"视图负责将结果显示到客户端\"]}]},\"/study-tutorial/frame/mybatis-plus/crud.html\":{\"title\":\"2、CRUD扩展\",\"contents\":[{\"header\":\"Insert\",\"slug\":\"insert\",\"contents\":[\"// 测试插入 @Test public void testInsert(){ User user = new User(); user.setName(\\\"小熊\\\"); user.setAge(18); user.setEmail(\\\"2861184805@qq.com\\\"); int insert = userMapper.insert(user); //自动生成id System.out.println(insert); System.out.println(user); } } \",\"结果\",\"数据库插入的id默认值为：全局唯一的id\"]},{\"header\":\"主键生成策略\",\"slug\":\"主键生成策略\",\"contents\":[]},{\"header\":\"雪花算法：Twitter 利用 zookeeper 实现了一个全局ID生成的服务 Snowflake： github.com/twitter/sno… ：\",\"slug\":\"雪花算法-twitter-利用-zookeeper-实现了一个全局id生成的服务-snowflake-github-com-twitter-sno\",\"contents\":[\"snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0\",\"主键自增\",\"我们配置主键自增\",\"实体类字段上加上@TableId(type = IdType.AUTO)\",\"数据库字段上一定要自增\",\"再次测试即可\",\"public enum IdType { AUTO(0), //数据库id自增 NONE(1), //未设置主键 INPUT(2), //手动输入 ASSIGN_ID(3), //默认全局id ASSIGN_UUID(4), //全局唯一id @Deprecated ID_WORKER(3), @Deprecated ID_WORKER_STR(3), //ID_WORKER字符串表示法 @Deprecated UUID(4); private final int key; private IdType(int key) { this.key = key;} public int getKey() { return this.key;} } \"]},{\"header\":\"update\",\"slug\":\"update\",\"contents\":[\" @Test public void testupdate(){ //通过条件自动拼接动态sql User user = new User(); user.setId(6L); user.setName(\\\"你是最棒的\\\"); int i = userMapper.updateById(user); //updateById参数是一个对象 System.out.println(i); } } \"]},{\"header\":\"自动填充\",\"slug\":\"自动填充\",\"contents\":[\"数据库级别（工作中不使用）\",\"1、在表中字段增加create_time、update_time\",\"2、通过测试插入方法\",\"private Date createTime; private Date updateTime; \",\"3、查看结果\",\"代码级别\",\"1、实体类属性上增加注解\",\"@TableField(fill = FieldFill.INSERT) private Date createTime; @TableField(fill = FieldFill.INSERT_UPDATE) private Date updateTime; \",\"2、实现元对象处理器接口：com.baomidou.mybatisplus.core.handlers.MetaObjectHandler\",\"自定义实现类 MyMetaObjectHandler\",\"@Component //加入到IOC容器里 @Slf4j public class MyMetaObjectHandler implements MetaObjectHandler { @Override public void insertFill(MetaObject metaObject) { log.info(\\\"start insert fill ....\\\"); this.setFieldValByName(\\\"createTime\\\",new Date(),metaObject); this.setFieldValByName(\\\"updateTime\\\",new Date(),metaObject); } @Override public void updateFill(MetaObject metaObject) { log.info(\\\"start update fill ....\\\"); this.setFieldValByName(\\\"updateTime\\\",new Date(),metaObject); } } \",\"3、测试插入，观察时间\"]},{\"header\":\"乐观锁\",\"slug\":\"乐观锁\",\"contents\":[\"十分乐观，它总是认为不会出现问题，无论干什么都不上锁，如果出现了问题，再次更新值测试！\",\"乐观锁实现方式：\",\"取出记录时，获取当前version\",\"更新时，带上这个version\",\"执行更新时， set version = newVersion where version = oldVersion\",\"如果version不对，就更新失败\",\"1、给数据库新增字段verison，默认值为1\",\"2、实体类新增字段\",\"@Version //乐观锁注解 private Integer version; \",\"3、注册插件\",\"@Configuration @MapperScan(\\\"com.xiaobear.mapper\\\") public class MybatisPlusConfig { //注册乐观锁插件 @Bean public OptimisticLockerInterceptor optimisticLockerInterceptor() { return new OptimisticLockerInterceptor(); } } \",\"4、测试\",\"@Test public void testOptimisticLocker(){ //查询用户信息 User user = userMapper.selectById(1L); //修改信息 user.setName(\\\"xiaobear\\\"); user.setAge(2); //执行 int update = userMapper.updateById(user); System.out.println(update); } //测试失败 @Test public void testOptimisticLocker2(){ User user = userMapper.selectById(1L); user.setName(\\\"xiaobear\\\"); user.setAge(2); User user2 = userMapper.selectById(1L); user2.setName(\\\"xiaobear111\\\"); user2.setAge(10); //模拟另一个线程进行插队操作 int update = userMapper.updateById(user); int update2 = userMapper.updateById(user2); System.out.println(update); System.out.println(update2); } \"]},{\"header\":\"悲观锁\",\"slug\":\"悲观锁\",\"contents\":[\"十分悲观，它总是认为会出现问题，无论干什么都会上锁，再去操作！\"]},{\"header\":\"查询操作\",\"slug\":\"查询操作\",\"contents\":[\" //单个查询 @Test public void testSelect(){ User user = userMapper.selectById(2L); System.out.println(user); } //批量查询 @Test public void testSelectByBatchId(){ List<User> users = userMapper.selectBatchIds(Arrays.asList(1, 2, 3)); users.forEach(System.out::println); } //条件查询map @Test public void testSelectByBatchIds(){ HashMap<String, Object> map = new HashMap<>(); map.put(\\\"name\\\",\\\"xiaobear\\\"); List<User> users = userMapper.selectByMap(map); users.forEach(System.out::println); } \"]},{\"header\":\"分页查询\",\"slug\":\"分页查询\",\"contents\":[\"分页插件\",\" //分页插件 @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); /* PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); // 设置请求的页面大于最大页后操作， true调回到首页，false 继续请求 默认false // paginationInterceptor.setOverflow(false); // 设置最大单页限制数量，默认 500 条，-1 不受限制 // paginationInterceptor.setLimit(500); // 开启 count 的 join 优化,只针对部分 left join paginationInterceptor.setCountSqlParser(new JsqlParserCountOptimize(true)); return paginationInterceptor;*/ } \",\"测试\",\"//测试分页插件 @Test public void testPage(){ //参数一：当前页 参数二：页面大小 Page<User> page = new Page<>(1,5); userMapper.selectPage(page,null); page.getRecords().forEach(System.out::println); System.out.println(page.getTotal()); } \"]},{\"header\":\"删除操作\",\"slug\":\"删除操作\",\"contents\":[\"//测试删除 @Test public void deleteById(){ int i = userMapper.deleteById(1L); System.out.println(i); } //测试id批量删除 @Test public void deleteById2(){ userMapper.deleteBatchIds(Arrays.asList(1L,2L,3L)); } //通过map删除 @Test public void testDeleteMap(){ HashMap<String, Object> map = new HashMap<>(); map.put(\\\"name\\\",\\\"小熊\\\"); userMapper.deleteByMap(map); } \"]},{\"header\":\"逻辑删除\",\"slug\":\"逻辑删除\",\"contents\":[\"物理删除：从数据库中直接删除\",\"逻辑删除：在数据库没有移除，而是通过一个变量来让他失效！防止数据丢失，类似回收站\",\"数据库添加字段，实体类上加上字段\",\"@TableLogic private Integer deleted; \",\"测试删除、查询\"]}]},\"/study-tutorial/frame/mybatis-plus/other.html\":{\"title\":\"3、条件、代码生成\",\"contents\":[{\"header\":\"6、性能分析插件\",\"slug\":\"_6、性能分析插件\",\"contents\":[\"性能分析拦截器，用于输出每条 SQL 语句及其执行时间\",\"/** * SQL执行效率插件 */ @Bean @Profile({\\\"dev\\\",\\\"test\\\"})// 设置 dev test 环境开启 public PerformanceInterceptor performanceInterceptor() { return new PerformanceInterceptor(); } \"]},{\"header\":\"7、条件构造器 wrapper\",\"slug\":\"_7、条件构造器wrapper\",\"contents\":[\" @Autowired private UserMapper userMapper; @Test void contextLoads() { QueryWrapper<User> wrapper = new QueryWrapper<> (); wrapper .isNotNull(\\\"name\\\") .isNotNull(\\\"email\\\") .ge(\\\"age\\\",3); userMapper.selectList(wrapper).forEach(System.out::println); } @Test void test(){ QueryWrapper<User> wrapper = new QueryWrapper<>(); wrapper.eq(\\\"name\\\",\\\"Tom\\\"); //查询名字 User user = userMapper.selectOne(wrapper); //查询一个数据 System.out.println(user); } @Test void test2(){ //查询年龄10-20 QueryWrapper<User> wrapper = new QueryWrapper<>(); wrapper.between(\\\"age\\\",10,20); Integer count = userMapper.selectCount(wrapper); System.out.println(count); } @Test void test3(){ //模糊查询 QueryWrapper<User> wrapper = new QueryWrapper<>(); wrapper .notLike(\\\"name\\\",\\\"e\\\") .likeRight(\\\"email\\\",\\\"t\\\"); List<Map<String, Object>> maps = userMapper.selectMaps(wrapper); maps.forEach(System.out::println); } @Test void test4(){ // QueryWrapper<User> wrapper = new QueryWrapper<>(); wrapper.inSql(\\\"id\\\",\\\"select id form user where id<3\\\"); List<Object> objects = userMapper.selectObjs(wrapper); objects.forEach(System.out::println); } \"]},{\"header\":\"8、代码生成器\",\"slug\":\"_8、代码生成器\",\"contents\":[\"public class XiaoBearCode { public static void main(String[] args) { AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\\\"user.dir\\\"); gc.setOutputDir(projectPath + \\\"/src/main/java\\\"); gc.setAuthor(\\\"xiaobear\\\"); gc.setOpen(false); // gc.setSwagger2(true); 实体属性 Swagger2 注解 mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\\\"jdbc:mysql://localhost:3306/mybatisplus?useSSL=true&useUnicode=true&characterEncoding=UTF-8&serverTimezone=GMT\\\"); // dsc.setSchemaName(\\\"public\\\"); dsc.setDriverName(\\\"com.mysql.cj.jdbc.Driver\\\"); dsc.setUsername(\\\"root\\\"); dsc.setPassword(\\\"密码\\\"); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(\\\"blog\\\"); pc.setParent(\\\"com.xiaobear\\\"); pc.setEntity(\\\"pojo\\\"); pc.setMapper(\\\"mapper\\\"); pc.setService(\\\"com/xiaobear/service\\\"); pc.setServiceImpl(\\\"com.xiaobear/Service/Impl\\\"); pc.setController(\\\"com/xiaobear/controller\\\"); mpg.setPackageInfo(pc); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setEntityLombokModel(true); //自动lombok strategy.setRestControllerStyle(true); // 公共父类 strategy.setSuperControllerClass(\\\"你自己的父类控制器,没有就不用设置!\\\"); // 写于父类中的公共字段 strategy.setSuperEntityColumns(\\\"id\\\"); strategy.setInclude(\\\"user\\\");//映射的表 strategy.setLogicDeleteFieldName(\\\"deleted\\\"); //逻辑删除字段 //自动填充策略 TableFill gmt_create = new TableFill(\\\"gmt_create\\\", FieldFill.INSERT); TableFill gmt_modified = new TableFill(\\\"gmt_strate\\\", FieldFill.INSERT); ArrayList<TableFill> list = new ArrayList<>(); list.add(gmt_create); list.add(gmt_modified); strategy.setTableFillList(list); //乐观锁 strategy.setVersionFieldName(\\\"version\\\"); strategy.setRestControllerStyle(true); //驼峰 strategy.setControllerMappingHyphenStyle(true); //localhost:8080/hello_id_1 strategy.setTablePrefix(pc.getModuleName() + \\\"_\\\"); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); //执行 } } \",\"错误：\",\"19:39:38.943 [main] DEBUG com.baomidou.mybatisplus.generator.AutoGenerator - ==========================准备生成文件...========================== Exception in thread \\\"main\\\" java.lang.NoClassDefFoundError: freemarker/template/Configuration at com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine.init(FreemarkerTemplateEngine.java:41) at com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine.init(FreemarkerTemplateEngine.java:34) at com.baomidou.mybatisplus.generator.AutoGenerator.execute(AutoGenerator.java:103) at com.xiaobear.XiaoBearCode.main(XiaoBearCode.java:66) Caused by: java.lang.ClassNotFoundException: freemarker.template.Configuration at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:418) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ... 4 more \",\"解决：\",\"导入模板引擎依赖\",\"<dependency> <groupId>org.freemarker</groupId> <artifactId>freemarker</artifactId> <version>2.3.30</version> </dependency> \"]}]},\"/study-tutorial/frame/mybatis-plus/overview.html\":{\"title\":\"1、概述\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"为简化开发而生\",\"Mybatis简化JDBC操作\",\"MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。\"]},{\"header\":\"1、特性\",\"slug\":\"_1、特性\",\"contents\":[\"无侵入：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑\",\"损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作\",\"强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求\",\"支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错\",\"支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题\",\"支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作\",\"支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ）\",\"内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用\",\"内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询\",\"分页插件支持多种数据库：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer 等多种数据库\",\"内置性能分析插件：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询\",\"内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作\"]},{\"header\":\"2、支持数据库\",\"slug\":\"_2、支持数据库\",\"contents\":[\"mysql 、 mariadb 、 oracle 、 db2 、 h2 、 hsql 、 sqlite 、 postgresql 、 sqlserver\",\"达梦数据库 、 虚谷数据库 、 人大金仓数据库\"]},{\"header\":\"3、快速开始\",\"slug\":\"_3、快速开始\",\"contents\":[]},{\"header\":\"1、创建数据库\",\"slug\":\"_1、创建数据库\",\"contents\":[\"DROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT(20) NOT NULL COMMENT '主键ID', name VARCHAR(30) NULL DEFAULT NULL COMMENT '姓名', age INT(11) NULL DEFAULT NULL COMMENT '年龄', email VARCHAR(50) NULL DEFAULT NULL COMMENT '邮箱', PRIMARY KEY (id) ); INSERT INTO user (id, name, age, email) VALUES (1, 'Jone', 18, 'test1@baomidou.com'), (2, 'Jack', 20, 'test2@baomidou.com'), (3, 'Tom', 28, 'test3@baomidou.com'), (4, 'Sandy', 21, 'test4@baomidou.com'), (5, 'Billie', 24, 'test5@baomidou.com'); \"]},{\"header\":\"2、新建Spring Boot项目\",\"slug\":\"_2、新建spring-boot项目\",\"contents\":[\"导入依赖\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <!--mybatis-plus--> <dependency> <groupId>com.baomidou</groupId> <artifactId>mybatis-plus-boot-starter</artifactId> <version>3.3.1.tmp</version> </dependency> \",\"连接数据库\",\"spring: datasource: url: jdbc:mysql://localhost:3306/mybatisplus?useSSL=true&useUnicode=true&characterEncoding=UTF-8&serverTimezone=GMT #useSSL安全连接 useUnicode编码 characterEncoding编码格式 serverTimezone时区 username: root password: 密码 driver-class-name: com.mysql.cj.jdbc.Driver \",\"创建实体类User\",\"@Data @AllArgsConstructor @NoArgsConstructor public class User { private Long id; private String name; private Integer age; private String email; } \",\"实现接口UserMapper\",\"//@Mapper @Repository //代表持久层 public interface UserMapper extends BaseMapper<User> { //所有的CRUD已经编写完成 } \",\"在 Spring Boot 启动类中添加 @MapperScan 注解，扫描 Mapper 文件夹\",\"@SpringBootApplication @MapperScan(\\\"com.xiaobear.mapper\\\") //扫描文件夹 public class MybatisplusApplication { public static void main(String[] args) { SpringApplication.run(MybatisplusApplication.class, args); } } \",\"测试\",\"@SpringBootTest class MybatisplusApplicationTests { @Autowired private UserMapper userMapper; @Test void contextLoads() { //查询所有用户 List<User> list = userMapper.selectList(null); list.forEach(System.out::println); } } \"]},{\"header\":\"4、配置日志\",\"slug\":\"_4、配置日志\",\"contents\":[\"使用默认控制台输出日志\",\"#配置日志 mybatis-plus: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl \"]}]},\"/study-tutorial/microservice/spring-cloud/consul.html\":{\"title\":\"5、Consul服务注册与发现\",\"contents\":[{\"header\":\"1、简介\",\"slug\":\"_1、简介\",\"contents\":[\"https://www.consul.io/docs/intro\",\"Consul is a service mesh solution providing a full featured control plane with service discovery, configuration, and segmentation functionality. Each of these features can be used individually as needed, or they can be used together to build a full service mesh. Consul requires a data plane and supports both a proxy and native integration model. Consul ships with a simple built-in proxy so that everything works out of the box, but also supports 3rd party proxy integrations such as Envoy.\",\"Consul 是一套开源的分布式服务发现和配置管理系统，由 HashiCorp 公司用 Go 语言开发。\",\"提供了微服务系统中的服务治理、配置中心、控制总线等功能。这些功能中的每一个都可以根据需要单独使用，也可以一起使用以构建全方位的服务网格，总之Consul提供了一种完整的服务网格解决方案。 它具有很多优点。包括： 基于 raft 协议，比较简洁； 支持健康检查, 同时支持 HTTP 和 DNS 协议 支持跨数据中心的 WAN 集群 提供图形界面 跨平台，支持 Linux、Mac、Windows \",\"特性\",\"服务发现\",\"提供HTTP和DNS两种服务发现方式\",\"健康监测\",\"支持多种方式，Http、TCP、Docker、Shell脚本定制化监控\",\"KV存储\",\"key、value的存储方式\",\"多数据中心\",\"支持多数据中心\",\"可视化Web界面\",\"下载\",\"https://www.consul.io/downloads\"]},{\"header\":\"2、安装并运行\",\"slug\":\"_2、安装并运行\",\"contents\":[\"https://learn.hashicorp.com/tutorials/consul/get-started-install?in=consul/getting-started\",\"解压之后就是一个exe文件，双击运行即可\"]},{\"header\":\"查看版本信息\",\"slug\":\"查看版本信息\",\"contents\":[\"consul --version \"]},{\"header\":\"使用开发者模式启动\",\"slug\":\"使用开发者模式启动\",\"contents\":[\"consul agent -dev \",\"通过地址即可访问到consul的首页：http://localhost:8500/\"]},{\"header\":\"3、服务提供者\",\"slug\":\"_3、服务提供者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-provider-consul-payment8006-7 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom\",\"contents\":[\"<dependencies> <!--SpringCloud consul-server --> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-consul-discovery</artifactId> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yml\",\"slug\":\"_3、写yml\",\"contents\":[\"server: port: 8006 spring: application: name: consul-provider-payment #consul服务注册地址 cloud: consul: host: localhost port: 8500 discovery: service-name: ${spring.application.name} \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication public class ConsulProviderPayment { public static void main(String[] args) { SpringApplication.run(ConsulProviderPayment.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[]},{\"header\":\"controller\",\"slug\":\"controller\",\"contents\":[\"@RestController public class PaymentController { @Value(\\\"${server.port}\\\") private String serverPort; @GetMapping(\\\"/consul/payment\\\") public String paymentInfo(){ return \\\"hello consul,\\\"+serverPort; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"http://localhost:8006/consul/payment\"]},{\"header\":\"4、服务消费者\",\"slug\":\"_4、服务消费者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-consumer-consul-order82 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom-1\",\"contents\":[\" <dependencies> <!--SpringCloud consul-server --> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-consul-discovery</artifactId> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yml\",\"slug\":\"_3、写yml-1\",\"contents\":[\"###consul服务端口号 server: port: 80 spring: application: name: cloud-consumer-order ####consul注册中心地址 cloud: consul: host: localhost port: 8500 discovery: #hostname: 127.0.0.1 service-name: ${spring.application.name} \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-1\",\"contents\":[\"@SpringBootApplication public class ConsulOrderMain { public static void main(String[] args) { SpringApplication.run(ConsulOrderMain.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-1\",\"contents\":[]},{\"header\":\"1、配置bean\",\"slug\":\"_1、配置bean\",\"contents\":[\"@Configuration public class ApplicationContextBean { @Bean @LoadBalanced public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \"]},{\"header\":\"2、controller\",\"slug\":\"_2、controller\",\"contents\":[\"@RestController public class OrderConsulController { public static final String INVOKE_URL = \\\"http://consul-provider-payment\\\"; //consul-provider-payment @Resource private RestTemplate restTemplate; @GetMapping(value = \\\"/consumer/payment/consul\\\") public String paymentInfo() { String result = restTemplate.getForObject(INVOKE_URL+\\\"/consul/payment\\\", String.class); System.out.println(\\\"消费者调用支付服务(consule)--->result:\\\" + result); return result; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试-1\",\"contents\":[\"http://localhost/consumer/payment/consul\"]},{\"header\":\"5、三个注册中心的异同点\",\"slug\":\"_5、三个注册中心的异同点\",\"contents\":[]},{\"header\":\"CAP\",\"slug\":\"cap\",\"contents\":[\"C：consistency(强一致性)\",\"A：Availability(可用性)\",\"P：Partition tolerance(分区容错性)\",\"CAP理论关注粒度是数据，而不是整体设计的策略\",\"最多只能同时较好的满足两个。\",\"CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，\",\"因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类：\",\"CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。\",\"CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。\",\"AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。\"]},{\"header\":\"Eureka（AP）\",\"slug\":\"eureka-ap\",\"contents\":[\"AP架构\",\"当网络分区出现后，为了保证可用性，系统B可以返回旧值，保证系统的可用性。\",\"结论：违背了一致性C的要求，只满足可用性和分区容错，即AP\"]},{\"header\":\"Zookeeper/Consul（CP）\",\"slug\":\"zookeeper-consul-cp\",\"contents\":[\"CP架构\",\"当网络分区出现后，为了保证一致性，就必须拒接请求，否则无法保证一致性\",\"结论：违背了可用性A的要求，只满足一致性和分区容错，即CP\"]}]},\"/study-tutorial/microservice/spring-cloud/consumer_provider.html\":{\"title\":\"3、消费者直接调用提供者（案例）\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"建立一个父工程，父工程新建module\",\"步骤\",\"建module\",\"改pom.xml\",\"写yaml\",\"主启动\",\"业务类 \",\"建表\",\"entities\",\"dao\",\"service\",\"controller\",\"测试\"]},{\"header\":\"1、微服务提供者module\",\"slug\":\"_1、微服务提供者module\",\"contents\":[]},{\"header\":\"1、建module\",\"slug\":\"_1、建module\",\"contents\":[\"名字为：xiaobear-provider-payment-8001-1\"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>com.alibaba</groupId> <artifactId>druid-spring-boot-starter</artifactId> <version>1.1.10</version> </dependency> <!--mysql-connector-java--> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> </dependency> <!--jdbc--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"在rescourse目录下新建application.yml\",\"server: port: 8001 spring: application: name: xiaobear-cloud-payment-service datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring-cloud-xiaobear?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8 username: root password: mima mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: com.xiaobear.entity \"]},{\"header\":\"4、主启动\",\"slug\":\"_4、主启动\",\"contents\":[\"在java目录下，新建一个主启动类\",\"package com.xiaobear; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; /** * @Author xiaobear * @Date 2021/4/6 21:41 * @Description 主启动类 * @Version 1.0 */ @SpringBootApplication public class Payment8001Application { public static void main(String[] args) { SpringApplication.run(Payment8001Application.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[]},{\"header\":\"1、建表\",\"slug\":\"_1、建表\",\"contents\":[\"CREATE TABLE `payment` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ID', `serial` varchar(200) DEFAULT '', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 \"]},{\"header\":\"2、entities\",\"slug\":\"_2、entities\",\"contents\":[\"/** * (Payment)实体类 * * @author xiaobear * @since 2021-04-06 21:52:42 */ public class Payment implements Serializable { private static final long serialVersionUID = -82310885557668418L; /** * ID */ private Long id; private String serial; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getSerial() { return serial; } public void setSerial(String serial) { this.serial = serial; } } \"]},{\"header\":\"3、dao\",\"slug\":\"_3、dao\",\"contents\":[\"@Mapper public interface PaymentDao { /** * 通过ID查询单条数据 * * @param id 主键 * @return 实例对象 */ Payment queryById(@Param(\\\"id\\\")Long id); /** * 查询指定行数据 * * @param offset 查询起始位置 * @param limit 查询条数 * @return 对象列表 */ List<Payment> queryAllByLimit(@Param(\\\"offset\\\") int offset, @Param(\\\"limit\\\") int limit); /** * 通过实体作为筛选条件查询 * * @param payment 实例对象 * @return 对象列表 */ List<Payment> queryAll(Payment payment); /** * 新增数据 * * @param payment 实例对象 * @return 影响行数 */ int insert(Payment payment); /** * 批量新增数据（MyBatis原生foreach方法） * * @param entities List<Payment> 实例对象列表 * @return 影响行数 */ int insertBatch(@Param(\\\"entities\\\") List<Payment> entities); /** * 批量新增或按主键更新数据（MyBatis原生foreach方法） * * @param entities List<Payment> 实例对象列表 * @return 影响行数 */ int insertOrUpdateBatch(@Param(\\\"entities\\\") List<Payment> entities); /** * 修改数据 * * @param payment 实例对象 * @return 影响行数 */ int update(Payment payment); /** * 通过主键删除数据 * * @param id 主键 * @return 影响行数 */ int deleteById(Long id); } \"]},{\"header\":\"xml\",\"slug\":\"xml\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <!DOCTYPE mapper PUBLIC \\\"-//mybatis.org//DTD Mapper 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\\\"> <mapper namespace=\\\"com.xiaobear.dao.PaymentDao\\\"> <resultMap type=\\\"com.xiaobear.entity.Payment\\\" id=\\\"PaymentMap\\\"> <result property=\\\"id\\\" column=\\\"id\\\" jdbcType=\\\"INTEGER\\\"/> <result property=\\\"serial\\\" column=\\\"serial\\\" jdbcType=\\\"VARCHAR\\\"/> </resultMap> <!--查询单个--> <select id=\\\"queryById\\\" resultMap=\\\"PaymentMap\\\"> select id, serial from payment where id = #{id} </select> <!--查询指定行数据--> <select id=\\\"queryAllByLimit\\\" resultMap=\\\"PaymentMap\\\"> select id, serial from payment limit #{offset} , #{limit} </select> <!--通过实体作为筛选条件查询--> <select id=\\\"queryAll\\\" resultMap=\\\"PaymentMap\\\"> select id, serial from payment <where> <if test=\\\"id != null\\\"> and id = #{id} </if> <if test=\\\"serial != null and serial != ''\\\"> and serial = #{serial} </if> </where> </select> <!--新增所有列--> <insert id=\\\"insert\\\" keyProperty=\\\"id\\\" useGeneratedKeys=\\\"true\\\"> insert into payment(serial) values (#{serial}) </insert> <insert id=\\\"insertBatch\\\" keyProperty=\\\"id\\\" useGeneratedKeys=\\\"true\\\"> insert into payment(serial) values <foreach collection=\\\"entities\\\" item=\\\"entity\\\" separator=\\\",\\\"> (#{entity.serial}) </foreach> </insert> <insert id=\\\"insertOrUpdateBatch\\\" keyProperty=\\\"id\\\" useGeneratedKeys=\\\"true\\\"> insert into payment(serial) values <foreach collection=\\\"entities\\\" item=\\\"entity\\\" separator=\\\",\\\"> (#{entity.serial}) </foreach> on duplicate key update serial = values(serial) </insert> <!--通过主键修改数据--> <update id=\\\"update\\\"> update payment <set> <if test=\\\"serial != null and serial != ''\\\"> serial = #{serial}, </if> </set> where id = #{id} </update> <!--通过主键删除--> <delete id=\\\"deleteById\\\"> delete from payment where id = #{id} </delete> </mapper> \"]},{\"header\":\"4、service\",\"slug\":\"_4、service\",\"contents\":[\"public interface PaymentService { /** * 通过ID查询单条数据 * * @param id 主键 * @return 实例对象 */ Payment queryById(Long id); /** * 查询多条数据 * * @param offset 查询起始位置 * @param limit 查询条数 * @return 对象列表 */ List<Payment> queryAllByLimit(int offset, int limit); /** * 新增数据 * * @param payment 实例对象 * @return 实例对象 */ Payment insert(Payment payment); /** * 修改数据 * * @param payment 实例对象 * @return 实例对象 */ Payment update(Payment payment); /** * 通过主键删除数据 * * @param id 主键 * @return 是否成功 */ boolean deleteById(Long id); } \"]},{\"header\":\"impl\",\"slug\":\"impl\",\"contents\":[\"@Service(\\\"paymentService\\\") public class PaymentServiceImpl implements PaymentService { @Resource private PaymentDao paymentDao; /** * 通过ID查询单条数据 * * @param id 主键 * @return 实例对象 */ @Override public Payment queryById(Long id) { return this.paymentDao.queryById(id); } /** * 查询多条数据 * * @param offset 查询起始位置 * @param limit 查询条数 * @return 对象列表 */ @Override public List<Payment> queryAllByLimit(int offset, int limit) { return this.paymentDao.queryAllByLimit(offset, limit); } /** * 新增数据 * * @param payment 实例对象 * @return 实例对象 */ @Override public Payment insert(Payment payment) { this.paymentDao.insert(payment); return payment; } /** * 修改数据 * * @param payment 实例对象 * @return 实例对象 */ @Override public Payment update(Payment payment) { this.paymentDao.update(payment); return this.queryById(payment.getId()); } /** * 通过主键删除数据 * * @param id 主键 * @return 是否成功 */ @Override public boolean deleteById(Long id) { return this.paymentDao.deleteById(id) > 0; } } \"]},{\"header\":\"5、controller\",\"slug\":\"_5、controller\",\"contents\":[\"@RestController @RequestMapping(\\\"payment\\\") @Slf4j public class PaymentController { /** * 服务对象 */ @Resource private PaymentService paymentService; /** * 通过主键查询单条数据 * * @param id 主键 * @return 单条数据 */ @GetMapping(\\\"/selectOne/{id}\\\") public Payment selectOne(@PathVariable(\\\"id\\\")Long id) { return this.paymentService.queryById(id); } @PostMapping(\\\"/insert\\\") public CommonResult createPayment(@RequestBody Payment payment){ Payment insert = paymentService.insert(payment); log.info(\\\"插入成功！\\\"); if (null != insert){ return new CommonResult(200,\\\"插入数据库成功！\\\",insert); }else { return new CommonResult(500,\\\"插入数据库失败！\\\",null); } } } \",\"这里采用的是通用返回CommonResult\",\"/** * @Author xiaobear * @Date 2021/4/10 20:39 * @Description 通用返回类型 * @Version 1.0 */ public class CommonResult<T>{ private Integer code; private String message; private T data; public CommonResult() { } public CommonResult(Integer code, String message, T data) { this.code = code; this.message = message; this.data = data; } public CommonResult( Integer code,String message) { this( code, message,null); } public CommonResult(T data) { this(200, \\\"操作成功\\\", data); } //setter--getter public T getData() { return data; } public void setData(T data) { this.data = data; } public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public Integer getCode() { return code; } public void setCode(Integer code) { this.code = code; } } \",\"6、测试\"]},{\"header\":\"2、微服务消费者module\",\"slug\":\"_2、微服务消费者module\",\"contents\":[]},{\"header\":\"1、建module\",\"slug\":\"_1、建module-1\",\"contents\":[\"xiaobear-consumer-order80-2\"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-1\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <parent> <artifactId>SpringCloud-xiaobear</artifactId> <groupId>com.xiaobear</groupId> <version>1.0-SNAPSHOT</version> </parent> <modelVersion>4.0.0</modelVersion> <artifactId>xiaobear-consumer-order80-2</artifactId> <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!-- <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency>--> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> </project> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml-1\",\"contents\":[\"server: port: 80 \"]},{\"header\":\"4、主启动\",\"slug\":\"_4、主启动-1\",\"contents\":[\"@SpringBootApplication public class Order80Application { public static void main(String[] args) { SpringApplication.run(Order80Application.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-1\",\"contents\":[\"把消费者的entities以及通用返回类型复制即可\"]},{\"header\":\"1、RestTemplate\",\"slug\":\"_1、resttemplate\",\"contents\":[\"RestTemplate提供了多种便捷访问远程Http服务的方法，\",\"是一种简单便捷的访问restful服务模板类，是Spring提供的用于访问Rest服务的客户端模板工具集\",\"https://docs.spring.io/spring-framework/docs/5.3.x/javadoc-api/org/springframework/web/client/RestTemplate.html\"]},{\"header\":\"使用\",\"slug\":\"使用\",\"contents\":[\"使用restTemplate访问restful接口非常的简单粗暴无脑。\",\"(url, requestMap, ResponseBean.class)这三个参数分别代表\",\"REST请求地址、请求参数、HTTP响应转换被转换成的对象类型。\"]},{\"header\":\"2、config配置类\",\"slug\":\"_2、config配置类\",\"contents\":[\"/** * @Author xiaobear * @Date 2021/4/11 16:23 * @Description RestTemplate配置类 * @Version 1.0 */ @Configuration public class ApplicationContextConfig { @Bean public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \"]},{\"header\":\"3、controller\",\"slug\":\"_3、controller\",\"contents\":[\"@RestController @Slf4j public class OrderController { private static final String PAYMENT_URL = \\\"http://localhost:8001\\\"; @Resource private RestTemplate restTemplate; @GetMapping(\\\"/consumer/payment/insert\\\") public CommonResult<Payment> create(Payment payment){ return restTemplate.postForObject(PAYMENT_URL+\\\"/payment/insert\\\",payment,CommonResult.class); } @GetMapping(\\\"/consumer/payment/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable Long id){ return restTemplate.getForObject(PAYMENT_URL + \\\"/payment/selectOne/\\\"+id,CommonResult.class, id); } } \",\"注意：两个服务需同时启动\"]},{\"header\":\"3、问题\",\"slug\":\"_3、问题\",\"contents\":[\"两个module都存在entities，而且两个实体类都是相同的，这时候我们可以工程重构\"]},{\"header\":\"4、工程重构\",\"slug\":\"_4、工程重构\",\"contents\":[\"新建一个module --名字为common-api\"]},{\"header\":\"1、修改pom.xml\",\"slug\":\"_1、修改pom-xml\",\"contents\":[\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <parent> <artifactId>SpringCloud-xiaobear</artifactId> <groupId>com.xiaobear</groupId> <version>1.0-SNAPSHOT</version> </parent> <modelVersion>4.0.0</modelVersion> <artifactId>xiaobear-common-api-3</artifactId> <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>cn.hutool</groupId> <artifactId>hutool-all</artifactId> <version>5.1.0</version> </dependency> </dependencies> </project> \"]},{\"header\":\"2、entities\",\"slug\":\"_2、entities-1\",\"contents\":[\"放入实体类payment和CommonResult\"]},{\"header\":\"3、对通用module进行clean install\",\"slug\":\"_3、对通用module进行clean-install\",\"contents\":[]},{\"header\":\"4、分别对前面两个module进行改造\",\"slug\":\"_4、分别对前面两个module进行改造\",\"contents\":[]},{\"header\":\"1、删除原来的entities目录\",\"slug\":\"_1、删除原来的entities目录\",\"contents\":[]},{\"header\":\"2、粘贴pom的内容\",\"slug\":\"_2、粘贴pom的内容\",\"contents\":[\" <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> \"]}]},\"/study-tutorial/microservice/spring-cloud/eureka.html\":{\"title\":\"4、Eureka服务注册与发现\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"什么是服务治理？\",\"Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务治理。\",\"在传统的rpc远程调用框架中，管理每个服务与服务之间依赖关系比较复杂，管理比较复杂，所以需要使用服\",\"务治理，管理服务于服务之间依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册。\",\"什么是服务注册？\",\"Eureka采用了CS的设计架构，Eureka Server 作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用 Eureka的客户端连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。\",\"在服务注册与发现中，有一个注册中心。当服务器启动的时候，会把当前自己服务器的信息 比如 服务地址通讯地址等以别名方式注册到注册中心上。另一方（消费者|服务提供者），以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用RPC远程调用框架核心设计思想：在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。在任何rpc远程框架中，都会有一个注册中心(存放服务地址相关信息(接口地址))\",\"Eureka包含两个组件：Eureka Server和Eureka Client\",\"Eureka Server提供服务注册服务\",\"各个微服务节点通过配置启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观看到。\",\"EurekaClient通过注册中心进行访问\",\"是一个Java客户端，用于简化Eureka Server的交互，客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期为30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，EurekaServer将会从服务注册表中把这个服务节点移除（默认90秒）\"]},{\"header\":\"1、单机Eurake构建\",\"slug\":\"_1、单机eurake构建\",\"contents\":[]},{\"header\":\"1、创建微服务注册中心\",\"slug\":\"_1、创建微服务注册中心\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-eurake-server-7001-4\"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <!--eureka-server--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!--boot web actuator--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--一般通用配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> </dependency> </dependencies> \"]},{\"header\":\"3、写Yml\",\"slug\":\"_3、写yml\",\"contents\":[\"server: port: 7001 eureka: instance: hostname: localhost #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 \"]},{\"header\":\"4、主启动\",\"slug\":\"_4、主启动\",\"contents\":[\"@SpringBootApplication @EnableEurekaServer //标注这个一个eureka服务端 public class EurekaMainIn7001Application { public static void main(String[] args) { SpringApplication.run(EurekaMainIn7001Application.class,args); } } \"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"http://localhost:7001/\"]},{\"header\":\"2、将提供者8001和消费者80分别注册进eureka服务\",\"slug\":\"_2、将提供者8001和消费者80分别注册进eureka服务\",\"contents\":[\"因为两个注册过程一致，这里以8001为例\"]},{\"header\":\"1、修改8001的pom.xml\",\"slug\":\"_1、修改8001的pom-xml\",\"contents\":[\"//增加客户端依赖 <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> \"]},{\"header\":\"2、写Yml\",\"slug\":\"_2、写yml\",\"contents\":[\"#添加如下 eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://localhost:7001/eureka \"]},{\"header\":\"3、主启动\",\"slug\":\"_3、主启动\",\"contents\":[\"@SpringBootApplication @EnableEurekaClient public class Payment8001Application { public static void main(String[] args) { SpringApplication.run(Payment8001Application.class,args); } } \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"http://localhost:7001/\",\"微服务注册名说明\"]},{\"header\":\"2、集群Eureka构建\",\"slug\":\"_2、集群eureka构建\",\"contents\":[\"原理说明 互相注册，相互守望\",\"**微服务RPC远程服务调用最核心的是：**高可用\",\"试想：如果只有一个注册中心，它挂掉了，就会导致整个服务都可不用\",\"解决：搭建Eureka注册中心集群，实现负载均衡+故障容错\"]},{\"header\":\"1、创建微服务注册中心7002\",\"slug\":\"_1、创建微服务注册中心7002\",\"contents\":[\"参考单机注册中心7001的创建\"]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-eureka-server-7002-5 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-1\",\"contents\":[\" <dependencies> <!--eureka-server--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!--boot web actuator--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--一般通用配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> </dependency> </dependencies> \"]},{\"header\":\"3、修改映射配置\",\"slug\":\"_3、修改映射配置\",\"contents\":[\"因为有两个注册中心，倘若实例的主机都为localhost,无法区分到底是哪个注册中心，所以这时候需要我们修改C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts文件，增加如下:\",\"#######################Spring Boot Eureka学习########## 127.0.0.1 eureka7001.com 127.0.0.1 eureka7002.com \"]},{\"header\":\"4、写yml\",\"slug\":\"_4、写yml\",\"contents\":[\"7001的yaml修改：\",\"server: port: 7001 eureka: instance: hostname: eureka7001.com client: register-with-eureka: false #不向注册中心注册自己 fetch-registry: false #自己就是注册中心 service-url: ## 与eureka-server交互的地址查询服务和注册服务器需要这个地址 defaultZone: http://eureka7002.com:7002/eureka/ \",\"因为之前只有一个注册中心，以自己的地址为server，现在有7001和7002，两个服务需要相互注册，互相守望\",\"server: port: 7002 eureka: instance: hostname: eureka7002.com client: register-with-eureka: false #不向注册中心注册自己 fetch-registry: false #自己就是注册中心 service-url: ## 与eureka-server交互的地址查询服务和注册服务器需要这个地址 defaultZone: http://eureka7001.com:7001/eureka/ \"]},{\"header\":\"5、主启动\",\"slug\":\"_5、主启动\",\"contents\":[\"@SpringBootApplication @EnableEurekaServer public class EurekaMainIn7002Application { public static void main(String[] args) { SpringApplication.run(EurekaMainIn7002Application.class,args); } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"http://eureka7001.com:7001/\",\"http://eureka7002.com:7002/\"]},{\"header\":\"2、将微服务8001和80分别发布到2台Eureka集群配置中\",\"slug\":\"_2、将微服务8001和80分别发布到2台eureka集群配置中\",\"contents\":[\"只需修改yml的配置文件，以前是挂在一台server上，而现在\",\"eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 ## defaultZone: http://localhost:7001/eureka 单机版 \"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"启动顺序\",\"先启动EurekaServer，7001、7002服务\",\"再启动服务提供者，8001\",\"启动消费者80\",\"测试 http://localhost/consumer/payment/get/1 OK\"]},{\"header\":\"3、创建服务提供者8001集群环境--8002\",\"slug\":\"_3、创建服务提供者8001集群环境-8002\",\"contents\":[\"参考微服务提供者8001\"]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-2\",\"contents\":[\"xiaobear-provider-payment8002-6 \"]},{\"header\":\"2、写pom.xml\",\"slug\":\"_2、写pom-xml\",\"contents\":[\" <dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>com.alibaba</groupId> <artifactId>druid-spring-boot-starter</artifactId> <version>1.1.10</version> </dependency> <!--mysql-connector-java--> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> </dependency> <!--jdbc--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 8002 spring: application: name: xiaobear-cloud-payment-service datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring-cloud-xiaobear?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8 username: root password: root eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 ## defaultZone: http://localhost:7001/eureka 单机版 mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: com.xiaobear.entities \"]},{\"header\":\"4、主启动\",\"slug\":\"_4、主启动-1\",\"contents\":[\"@SpringBootApplication @EnableEurekaClient public class Payment8002Application { public static void main(String[] args) { SpringApplication.run(Payment8002Application.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"直接复制8001的业务类即可\"]},{\"header\":\"6、修改8001与8002的controller\",\"slug\":\"_6、修改8001与8002的controller\",\"contents\":[\"为了区分调用的是哪个服务\",\"@RestController @RequestMapping(\\\"payment\\\") @Slf4j public class PaymentController { /** * 服务对象 */ @Resource private PaymentService paymentService; @Value(\\\"${server.port}\\\") private String serverPort; /** * 通过主键查询单条数据 * * @param id 主键 * @return 单条数据 */ @GetMapping(\\\"/selectOne/{id}\\\") public CommonResult selectOne(@PathVariable(\\\"id\\\")Long id) { Payment payment = paymentService.queryById(id); if (null != payment){ return new CommonResult(200,\\\"查询成功！serverPort:\\\"+serverPort,payment); }else { return new CommonResult(404,\\\"查询失败！serverPort\\\"+serverPort,null); } } @PostMapping(\\\"/insert\\\") public CommonResult createPayment(@RequestBody Payment payment){ Payment insert = paymentService.insert(payment); log.info(\\\"插入成功！\\\"); if (null != insert){ return new CommonResult(200,\\\"插入数据库成功！serverPort:\\\"+serverPort,insert); }else { return new CommonResult(500,\\\"插入数据库失败！serverPort:\\\"+serverPort,null); } } } \"]},{\"header\":\"7、测试\",\"slug\":\"_7、测试\",\"contents\":[\"http://localhost/consumer/payment/get/1\",\"这时候发现，无论怎么刷新，都是调用的8001的服务====>因为80的地址是写死的\",\"解决：需变成动态的地址，服务我们都已经注册到server端了，需把地址换成spring.application.name\",\"修改如下：\",\"@RestController @Slf4j public class OrderController { private static final String PAYMENT_URL = \\\"http://localhost:8001\\\"; // 通过在eureka上注册过的微服务名称调用 public static final String PAYMENT_SRV = \\\"http://XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"; @Resource private RestTemplate restTemplate; @GetMapping(\\\"/consumer/payment/insert\\\") public CommonResult<Payment> create(Payment payment){ return restTemplate.postForObject(PAYMENT_SRV+\\\"/payment/insert\\\",payment,CommonResult.class); } @GetMapping(\\\"/consumer/payment/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable(\\\"id\\\") Long id){ return restTemplate.getForObject(PAYMENT_SRV + \\\"/payment/selectOne/\\\"+id,CommonResult.class, id); } } \",\"重启然后进行访问，http://localhost/consumer/payment/get/1\",\"未知的服务名称，因为两个提供者不知道该调用哪一个，这时候需用到负载均衡\",\"使用@LoadBalanced注解赋予RestTemplate负载均衡的能力\",\"修改80的config\",\"@Configuration public class ApplicationContextConfig { @Bean @LoadBalanced //使用@LoadBalanced注解赋予RestTemplate负载均衡的能力 public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \",\"重启再接着测试，会发现，8001与8002交替出现\"]},{\"header\":\"3、Actuator微服务信息完善\",\"slug\":\"_3、actuator微服务信息完善\",\"contents\":[]},{\"header\":\"1、主机名称修改\",\"slug\":\"_1、主机名称修改\",\"contents\":[\"存在的问题：含有电脑主机的名称\",\"解决：\",\"必须有以下依赖才可\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> \",\"修改8001以及8002的yaml文件\",\"server: port: 8002 spring: application: name: xiaobear-cloud-payment-service datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring-cloud-xiaobear?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8 username: root password: 密码 eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 ## defaultZone: http://localhost:7001/eureka 单机版 instance: instance-id: payment8002 mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: com.xiaobear.entities \",\"修改之后的效果\",\"查询状态是否打开 http://localhost:8001/actuator/health\"]},{\"header\":\"2、访问信息ip提示\",\"slug\":\"_2、访问信息ip提示\",\"contents\":[\"修改8001和8002的yaml\",\"eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka ## 集群版 #defaultZone: http://localhost:7001/eureka ## 单机版 instance: instance-id: payment8001 prefer-ip-address: true #访问路径可以显示IP地址 \",\"修改之前\",\"修改之后\"]},{\"header\":\"4、服务发现Discovery\",\"slug\":\"_4、服务发现discovery\",\"contents\":[\"对于注册进Eureka里面的微服务，可以通过服务发现来获得服务的信息\",\"以8001Controller为例：\"]},{\"header\":\"1、自动装配\",\"slug\":\"_1、自动装配\",\"contents\":[\"@Resource private DiscoveryClient discoveryClient; \"]},{\"header\":\"2、写接口\",\"slug\":\"_2、写接口\",\"contents\":[\" @GetMapping(\\\"/discovery\\\") public Object discovery(){ List<String> services = discoveryClient.getServices(); for (String service : services) { System.out.println(service); } List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); for (ServiceInstance element : instances) { System.out.println(element.getServiceId() + \\\"\\\\t\\\" + element.getHost() + \\\"\\\\t\\\" + element.getPort() + \\\"\\\\t\\\" + element.getUri()); } return this.discoveryClient; } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"http://localhost:8001/payment/discovery\"]},{\"header\":\"5、Eureka的自我保护\",\"slug\":\"_5、eureka的自我保护\",\"contents\":[\"保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式，\",\"Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据，也就是不会注销任何微服\",\"务。\",\"如果在Eureka Server的首页看到以下这段提示，则说明Eureka进入了保护模式：\",\"EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY'RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE \"]},{\"header\":\"1、原因\",\"slug\":\"_1、原因\",\"contents\":[\"为什么会产生Eureka自我保护机制？\",\"为了防止EurekaClient可以正常运行，但是 与 EurekaServer网络不通情况下，EurekaServer不会立刻将EurekaClient服务剔除.\",\"什么是自我保护模式？\",\"默认情况下，如果EurekaServer在一定时间内没有接收到某个微服务实例的心跳，EurekaServer将会注销该实例（默认90秒）。但是当网络分区故障发生(延时、卡顿、拥挤)时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题——当EurekaServer节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。\",\"在自我保护模式中，Eureka Server会保护服务注册表中的信息，不再注销任何服务实例。\",\"它的设计哲学就是宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例。一句话讲解：好死不如赖活着.\",\"综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留）也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。\"]},{\"header\":\"2、如何禁止\",\"slug\":\"_2、如何禁止\",\"contents\":[\"出厂默认，自我保护机制是开启的\",\"server端以7001为例，client以8001为例\"]},{\"header\":\"1、修改7001的yaml\",\"slug\":\"_1、修改7001的yaml\",\"contents\":[\"server: port: 7001 eureka: instance: hostname: eureka7001.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://eureka7002.com:7002/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 server: #关闭自我保护机制，保证不可用服务被及时踢除 enable-self-preservation: false eviction-interval-timer-in-ms: 1000 \",\"关闭效果\"]},{\"header\":\"2、Client8001\",\"slug\":\"_2、client8001\",\"contents\":[\"修改yaml文件\",\"eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: true #是否从EurekaServer抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetchRegistry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 ## defaultZone: http://localhost:7001/eureka 单机版 instance: instance-id: payment8001 prefer-ip-address: true #访问路径可以显示IP地址 #Eureka服务端在收到最后一次心跳后等待时间上限，单位为秒(默认是90秒)，超时将剔除服务 lease-expiration-duration-in-seconds: 2 #Eureka客户端向服务端发送心跳的时间间隔，单位为秒(默认是30秒) lease-renewal-interval-in-seconds: 1 \"]},{\"header\":\"3、模拟测试\",\"slug\":\"_3、模拟测试\",\"contents\":[\"7001和8001都配置好\",\"先启动7001，后启动8001\",\"关闭8001，7001上立马被删除了\",\"Eureka停止更新了：https://github.com/Netflix/eureka\"]}]},\"/study-tutorial/microservice/spring-cloud/hystrix.html\":{\"title\":\"8、Hystrix断路器\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"分布式面临的问题\",\"复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败。\",\"服务雪崩\",\"多个微服务之间调用的时候，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”. 对于高流量的应用来说，单一的后端依赖可能会导致所有服务器上的所有资源都在几秒钟内饱和。比失败更糟糕的是，这些应用程序还可能导致服务之间的延迟增加，备份队列，线程和其他系统资源紧张，导致整个系统发生更多的级联故障。这些都表示需要对故障和延迟进行隔离和管理，以便单个依赖关系的失败，不能取消整个应用程序或系统。 所以，通常当你发现一个模块下的某个实例失败后，这时候这个模块依然还会接收流量，然后这个有问题的模块还调用了其他的模块，这样就会发生级联故障，或者叫雪崩。 \"]},{\"header\":\"1、Hystrix\",\"slug\":\"_1、hystrix\",\"contents\":[\"Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。\",\"“断路器”本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。\",\"Hystrix是由Netflix开源的一个延迟和容错库，用于隔离访问远程系统、服务或者第三方库，防止级联失 败，从而提升系统的可用性与容错性。Hystrix主要通过以下几点实现延迟和容错。\",\"包裹请求：使用HystrixCommand包裹对依赖的调用逻辑，每个命令在独立线程中执行。这使用 了设计模式中的“命令模式”。\",\"跳闸机制：当某服务的错误率超过一定的阈值时，Hystrix可以自动或手动跳闸，停止请求该服务 一段时间。\",\"资源隔离：Hystrix为每个依赖都维护了一个小型的线程池（或者信号量）。如果该线程池已满， 发往该依赖的请求就被立即拒绝，而不是排队等待，从而加速失败判定。\",\"监控：Hystrix可以近乎实时地监控运行指标和配置的变化，例如成功、失败、超时、以及被拒绝 的请求等。\",\"回退机制：当请求失败、超时、被拒绝，或当断路器打开时，执行回退逻辑。回退逻辑由开发人员 自行提供，例如返回一个缺省值。\",\"自我修复：断路器打开一段时间后，会自动进入“半开”状态。\"]},{\"header\":\"2、作用\",\"slug\":\"_2、作用\",\"contents\":[\"服务降级\",\"服务熔断\",\"接近实时的监控\"]},{\"header\":\"3、官网\",\"slug\":\"_3、官网\",\"contents\":[\"如何使用：https://github.com/Netflix/Hystrix/wiki/How-To-Use\",\"官网：https://github.com/Netflix/Hystrix\",\"但现在已经停止更新了，不在发布新版本，被动修复bug\"]},{\"header\":\"2、服务容错的核心知识\",\"slug\":\"_2、服务容错的核心知识\",\"contents\":[]},{\"header\":\"1、雪崩效应\",\"slug\":\"_1、雪崩效应\",\"contents\":[\"在微服务架构中，一个请求需要调用多个服务是非常常见的。如客户端访问A服务，而A服务需要调用B 服务，B服务需要调用C服务，由于网络原因或者自身的原因，如果B服务或者C服务不能及时响应，A服 务将处于阻塞状态，直到B服务C服务响应。此时若有大量的请求涌入，容器的线程资源会被消耗完毕， 导致服务瘫痪。服务与服务之间的依赖性，故障会传播，造成连锁反应，会对整个微服务系统造成灾难 性的严重后果，这就是服务故障的“雪崩”效应。\",\"雪崩是系统中的蝴蝶效应导致其发生的原因多种多样，有不合理的容量设计，或者是高并发下某一个方 法响应变慢，亦或是某台机器的资源耗尽。从源头上我们无法完全杜绝雪崩源头的发生，但是雪崩的根 本原因来源于服务之间的强依赖，所以我们可以提前评估，做好熔断，隔离，限流。\"]},{\"header\":\"2、服务降级\",\"slug\":\"_2、服务降级\",\"contents\":[\"熔断这一概念来源于电子工程中的断路器（Circuit Breaker）。在互联网系统中，当下游服务因访问压 力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。这 种牺牲局部，保全整体的措施就叫做熔断。\",\"服务器忙，请稍后重试，不让客户端等待并立即返回一个友好提示，fallback\",\"所谓降级，就是当某个服务熔断之后，服务器将不再被调用，此时客户端可以自己准备一个本地的 fallback回调，返回一个缺省值。 也可以理解为兜底\",\"会发生降级的情况\",\"程序运行异常\",\"超时\",\"服务熔断触发服务降级\",\"线程池、信号量打满也会导致服务降级\"]},{\"header\":\"3、服务熔断\",\"slug\":\"_3、服务熔断\",\"contents\":[\"类比保险丝达到最大的服务访问后，直接拒绝访问，拉闸限电，然后调用服务降级的方法并返回友好提示\",\"就相当于保险丝，服务的降级-----》进而熔断-----》恢复调用链路\"]},{\"header\":\"4、服务限流\",\"slug\":\"_4、服务限流\",\"contents\":[\"限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说 系统的吞吐量是可以被测算的，为了保证系统的稳固运行，一旦达到的需要限制的阈值，就需要限制流 量并采取少量措施以完成限制流量的目的。比方：推迟解决，拒绝解决，或者者部分拒绝解决等等。\",\"秒杀高并发等操作，严禁一窝蜂的一样拥挤，排队有序进行，一秒钟N个，有序进行\"]},{\"header\":\"3、Hystrix案例\",\"slug\":\"_3、hystrix案例\",\"contents\":[]},{\"header\":\"1、构建\",\"slug\":\"_1、构建\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-provider-hystrix-payment8001-12 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-hystrix</artifactId> </dependency> <!--eureka client--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yml\",\"slug\":\"_3、写yml\",\"contents\":[\"server: port: 8001 spring: application: name: cloud-provider-hystrix-payment eureka: client: register-with-eureka: true fetch-registry: true service-url: #defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka defaultZone: http://eureka7001.com:7001/eureka \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication(exclude={DataSourceAutoConfiguration.class, HibernateJpaAutoConfiguration.class}) @EnableEurekaClient //本服务启动后会自动注册进eureka服务中 public class HystrixApplication { public static void main(String[] args) { SpringApplication.run(HystrixApplication.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"service\",\"@Service public class PaymentService { /** * 正常访问，一切OK * @param id * @return */ public String paymentInfo_OK(Integer id) { return \\\"线程池:\\\"+Thread.currentThread().getName()+\\\"paymentInfo_OK,id: \\\"+id+\\\"\\\\t\\\"+\\\"O(∩_∩)O\\\"; } /** * 超时访问，演示降级 * @param id * @return */ public String paymentInfo_TimeOut(Integer id) { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return \\\"线程池:\\\"+Thread.currentThread().getName()+\\\"paymentInfo_TimeOut,id: \\\"+id+\\\"\\\\t\\\"+\\\"O(∩_∩)O，耗费3秒\\\"; } } \",\"controller\",\"@RestController @Slf4j public class HystrixController { @Resource private PaymentService paymentService; @Value(\\\"${server.port}\\\") private String serverPort; @GetMapping(\\\"/payment/hystrix/ok/{id}\\\") public String paymentInfo_OK(@PathVariable(\\\"id\\\") Integer id) { String result = paymentService.paymentInfo_OK(id); log.info(\\\"****result: \\\"+result); return result; } @GetMapping(\\\"/payment/hystrix/timeout/{id}\\\") public String paymentInfo_TimeOut(@PathVariable(\\\"id\\\") Integer id) throws InterruptedException { String result = paymentService.paymentInfo_TimeOut(id); log.info(\\\"****result: \\\"+result); return result; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"启动顺序：\",\"先启动Eureka7001\",\"再启动启动类\",\"测试 \",\"正常访问的：http://localhost:8001/payment/hystrix/ok/1\",\"调用耗时5秒的：http://localhost:8001/payment/hystrix/timeout/1\"]},{\"header\":\"2、高并发测试\",\"slug\":\"_2、高并发测试\",\"contents\":[\"上述在非高并发的情况下还能勉强满足，但是在高并发的情况下就会延迟。\"]},{\"header\":\"1、Jmeter压测\",\"slug\":\"_1、jmeter压测\",\"contents\":[\"安装请参考：https://bbs.huaweicloud.com/blogs/222391\"]},{\"header\":\"1、开启Jmeter\",\"slug\":\"_1、开启jmeter\",\"contents\":[\"双击即可\",\"创建线程组\",\"添加HTTP默认请求\",\"启动执行\",\"发现之前正常的也在转圈，访问延迟了，这是因为：\",\"tomcat的默认的工作线程数被打满 了，没有多余的线程来分解压力和处理。\",\"总结：\",\"上述服务还是用于自测，倘若此时外部的80也来访问，那么消费者只能等着自测先访问完，最终导致80消费者不满意，服务端8001被拖死了\"]},{\"header\":\"2、新建消费者80\",\"slug\":\"_2、新建消费者80\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-consumer-openFeign-hystrix-order80-13 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-1\",\"contents\":[\" <dependencies> <!--openfeign--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-openfeign</artifactId> </dependency> <!--hystrix--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-hystrix</artifactId> </dependency> <!--eureka client--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <dependency> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!--web--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--一般基础通用配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yml\",\"slug\":\"_3、写yml-1\",\"contents\":[\"server: port: 80 eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka/ \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-1\",\"contents\":[\"@SpringBootApplication @EnableFeignClients public class OrderHystrixMain80 { public static void main(String[] args) { SpringApplication.run(OrderHystrixMain80.class,args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-1\",\"contents\":[\"service\",\"@Component @FeignClient(value = \\\"CLOUD-PROVIDER-HYSTRIX-PAYMENT\\\") public interface PaymentHystrixService { @GetMapping(\\\"/payment/hystrix/ok/{id}\\\") String paymentInfoOK(@PathVariable(\\\"id\\\") Integer id); @GetMapping(\\\"/payment/hystrix/timeout/{id}\\\") String paymentInfoTimeOut(@PathVariable(\\\"id\\\") Integer id); } \",\"controller\",\"@RestController public class OrderHystrixController { @Resource private PaymentHystrixService paymentHystrixService; @GetMapping(\\\"/consumer/payment/hystrix/ok/{id}\\\") public String paymentInfo_OK(@PathVariable(\\\"id\\\") Integer id) { String result = paymentHystrixService.paymentInfoOK(id); return result; } @GetMapping(\\\"/consumer/payment/hystrix/timeout/{id}\\\") public String paymentInfo_TimeOut(@PathVariable(\\\"id\\\") Integer id) { String result = paymentHystrixService.paymentInfoTimeOut(id); return result; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试-1\",\"contents\":[\"正常测试：http://localhost/consumer/payment/hystrix/ok/1\",\"高并发压测8001，发现80访问的时候要么就是超时错误，要么就是等待\",\"故障出现以及导致的原因：\",\"8001的其他服务被困死，因tomcat的线程池里面的工作线程已经被挤占了\",\"80此时调用8001，客户端响应缓慢\",\"正因为有了上述的故障，才有了服务的降级、限流等技术\",\"如何解决？\",\"超时导致服务器变慢----->超时不再等待\",\"出错（宕机或程序出错）----------->出错要有兜底，也就是最终的解决方法\"]},{\"header\":\"3、服务降级\",\"slug\":\"_3、服务降级\",\"contents\":[\"降级的配置：@HystrixCommand\"]},{\"header\":\"1、8001从自身找问题\",\"slug\":\"_1、8001从自身找问题\",\"contents\":[\"设置自身调用服务超时时间的峰值，在峰值内可以正常运行，超过了需要兜底的方法进行处理，做服务降级fallback\"]},{\"header\":\"1、修改8001service\",\"slug\":\"_1、修改8001service\",\"contents\":[\"一旦调用的方法抛出错误信息后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法\",\"@Service public class PaymentService { /** * 正常访问，一切OK * @param id * @return */ public String paymentInfo_OK(Integer id) { return \\\"线程池:\\\"+Thread.currentThread().getName()+\\\"paymentInfo_OK,id: \\\"+id+\\\"\\\\t\\\"+\\\"O(∩_∩)O\\\"; } /** * 超时访问，演示降级 * @param id * @return */ /**自身超时时间3秒钟 */ @HystrixCommand(fallbackMethod = \\\"paymentInfo_TimeOutHandler\\\",commandProperties = {@HystrixProperty(name=\\\"execution.isolation.thread.timeoutInMilliseconds\\\",value=\\\"3000\\\")}) public String paymentInfo_TimeOut(Integer id) { try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } return \\\"线程池:\\\"+Thread.currentThread().getName()+\\\"paymentInfo_TimeOut,id: \\\"+id+\\\"\\\\t\\\"+\\\"O(∩_∩)O，耗费3秒\\\"; } /** * 如果paymentInfo_OK超时或者出错，即会调用该方法 * @param id * @return */ public String paymentInfo_TimeOutHandler(Integer id){ return \\\"/(ㄒoㄒ)/调用支付接口超时或异常：\\\\t\\\"+ \\\"\\\\t当前线程池名字\\\" + Thread.currentThread().getName(); } } \"]},{\"header\":\"2、主启动类激活\",\"slug\":\"_2、主启动类激活\",\"contents\":[\"添加新注解@EnableCircuitBreaker\",\"@SpringBootApplication(exclude={DataSourceAutoConfiguration.class, HibernateJpaAutoConfiguration.class}) @EnableEurekaClient //本服务启动后会自动注册进eureka服务中 @EnableCircuitBreaker public class HystrixApplication { public static void main(String[] args) { SpringApplication.run(HystrixApplication.class,args); } } \"]},{\"header\":\"2、80fallback\",\"slug\":\"_2、80fallback\",\"contents\":[\"80订单微服务，也可以更好的保护自己，自己也可以降级处理\",\"**注意：**我们自己配置热部署对java代码改动明显，但对@HystrixCommand建议重启微服务\"]},{\"header\":\"1、修改yaml\",\"slug\":\"_1、修改yaml\",\"contents\":[\"server: port: 80 eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka/ ## 用于服务降级 在注解@FeignClient中添加fallbackFactory属性值 feign: hystrix: enabled: true \"]},{\"header\":\"2、主启动类\",\"slug\":\"_2、主启动类\",\"contents\":[\"@SpringBootApplication @EnableFeignClients @EnableHystrix public class OrderHystrixMain80 { public static void main(String[] args) { SpringApplication.run(OrderHystrixMain80.class,args); } } \"]},{\"header\":\"3、业务类\",\"slug\":\"_3、业务类\",\"contents\":[\"controller\",\"@RestController public class OrderHystrixController { @Resource private PaymentHystrixService paymentHystrixService; @GetMapping(\\\"/consumer/payment/hystrix/ok/{id}\\\") public String paymentInfo_OK(@PathVariable(\\\"id\\\") Integer id) { String result = paymentHystrixService.paymentInfoOK(id); return result; } @GetMapping(\\\"/consumer/payment/hystrix/timeout/{id}\\\") @HystrixCommand(fallbackMethod = \\\"paymentTimeOutFallbackMethod\\\",commandProperties = { @HystrixProperty(name=\\\"execution.isolation.thread.timeoutInMilliseconds\\\",value=\\\"1500\\\") }) public String paymentInfo_TimeOut(@PathVariable(\\\"id\\\") Integer id) { String result = paymentHystrixService.paymentInfoTimeOut(id); return result; } public String paymentTimeOutFallbackMethod(@PathVariable(\\\"id\\\") Integer id) { return \\\"我是消费者80,对方支付系统繁忙请10秒钟后再试或者自己运行出错请检查自己,o(╥﹏╥)o\\\"; } } \",\"目前存在的问题：\",\"每一个业务方法都有一个兜底的方法，代码膨胀\",\"解决\",\"统一和自定义的分开\"]},{\"header\":\"3、解决问题\",\"slug\":\"_3、解决问题\",\"contents\":[\"本次服务的降级处理是在客户端80实现的，与服务端8001没有关系，只需要feign客户端定义的接口添加一个服务降级处理实现类即可实现解耦\",\"以后我们时常需要面对的：\",\"运行\",\"宕机\",\"超时\"]},{\"header\":\"1、修改xiaobear-consumer-openFeign-hystrix-order80-13\",\"slug\":\"_1、修改xiaobear-consumer-openfeign-hystrix-order80-13\",\"contents\":[\"该模块已经有了PaymentHystrixService接口，我们需要重新建一个PaymentFallBackHystrixService实现该接口，统一为该接口里面的方法做异常处理\",\"@Component public class PaymentFallBackHystrixService implements PaymentHystrixService{ @Override public String paymentInfoOK(Integer id) { return \\\"服务调用失败，提示来自：paymentInfoOK\\\"; } @Override public String paymentInfoTimeOut(Integer id) { return \\\"服务调用失败，提示来自：paymentInfoTimeOut\\\"; } } \",\"PaymentHystrixService\",\"@Component @FeignClient(value = \\\"CLOUD-PROVIDER-HYSTRIX-PAYMENT\\\",fallback = PaymentFallBackHystrixService.class) public interface PaymentHystrixService { @GetMapping(\\\"/payment/hystrix/ok/{id}\\\") String paymentInfoOK(@PathVariable(\\\"id\\\") Integer id); @GetMapping(\\\"/payment/hystrix/timeout/{id}\\\") String paymentInfoTimeOut(@PathVariable(\\\"id\\\") Integer id); } \"]},{\"header\":\"2、测试\",\"slug\":\"_2、测试\",\"contents\":[\"启动顺序：\",\"先启动7001\",\"再启动8001\",\"正常测试：http://localhost/consumer/payment/hystrix/ok/1\",\"此时我们故意关闭8001，服务器已经宕机了，但是我们统一做了服务降级处理，让客户端在服务器不可用时也会获得提示信息而不会挂起耗死服务器\"]},{\"header\":\"4、服务熔断\",\"slug\":\"_4、服务熔断\",\"contents\":[\"熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务出错不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息。\",\"当检测到该节点微服务调用响应正常后，恢复调用链路。\",\"在Spring Cloud框架里，熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，\",\"当失败的调用到一定阈值，缺省是5秒内20次调用失败，就会启动熔断机制。熔断机制的注解是@HystrixCommand。\",\"简单来说，就是家里的保险丝\"]},{\"header\":\"1、修改xiaobear-provider-hystrix-payment8001-12\",\"slug\":\"_1、修改xiaobear-provider-hystrix-payment8001-12\",\"contents\":[\"####### 1、修改service，添加两个方法\",\"/** * 服务熔断 * @param id * @return */ @HystrixCommand(fallbackMethod = \\\"paymentCircuitBreaker_fallback\\\",commandProperties = { @HystrixProperty(name = \\\"circuitBreaker.enabled\\\",value = \\\"true\\\"), @HystrixProperty(name = \\\"circuitBreaker.requestVolumeThreshold\\\",value = \\\"10\\\"), @HystrixProperty(name = \\\"circuitBreaker.sleepWindowInMilliseconds\\\",value = \\\"10000\\\"), @HystrixProperty(name = \\\"circuitBreaker.errorThresholdPercentage\\\",value = \\\"60\\\"), }) public String paymentCircuitBreaker(@PathVariable(\\\"id\\\") Integer id) { if(id < 0) { throw new RuntimeException(\\\"******id 不能负数\\\"); } String serialNumber = IdUtil.simpleUUID(); return Thread.currentThread().getName()+\\\"\\\\t\\\"+\\\"调用成功，流水号: \\\" + serialNumber; } public String paymentCircuitBreaker_fallback(@PathVariable(\\\"id\\\") Integer id) { return \\\"id 不能负数，请稍后再试，/(ㄒoㄒ)/~~ id: \\\" +id; } \",\"####### 2、在controller调用service方法\",\"@GetMapping(\\\"/payment/circuit/{id}\\\") public String paymentCircuitBreaker(@PathVariable(\\\"id\\\") Integer id) { String result = paymentService.paymentCircuitBreaker(id); log.info(\\\"****result: \\\"+result); return result; } \",\"####### 3、测试\",\"http://localhost:8001/payment/circuit/12\",\"一次正确一次错误的测试\",\"多次错误，然后慢慢正确，刚开始不满足条件，就算是正确的地址也不能进行\"]},{\"header\":\"2、总结\",\"slug\":\"_2、总结\",\"contents\":[\"####### 1、熔断类型\",\"熔断打开\",\"请求不在调用当前服务，内部设置时钟一般为MTTR（平均故障处理时间），当打开时长达到所设时钟则进入半熔断状态\",\"熔断关闭\",\"熔断关闭不会对服务进行熔断\",\"熔断半开\",\"部分请求根据规则调用当前服务，如果请求成功而且符合规则，认为当前服务恢复正常，关闭熔断\",\"####### 2、断路器什么时候开始起作用\",\"涉及到断路器的三个重要参数：快照时间窗、请求总数阀值、错误百分比阀值。\",\"1：快照时间窗：断路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的10秒。\",\"2：请求总数阀值：在快照时间窗内，必须满足请求总数阀值才有资格熔断。默认为20，意味着在10秒内，如果该hystrix命令的调用次数不足20次，即使所有的请求都超时或其他原因失败，断路器都不会打开。\",\"3：错误百分比阀值：当请求总数在快照时间窗内超过了阀值，比如发生了30次调用，如果在这30次调用中，有15次发生了超时异常，也就是超过50%的错误百分比，在默认设定50%阀值情况下，这时候就会将断路器打开。\",\"####### 3、断路器开启或关闭的条件\",\"当满足一定的阈值的时候（默认10s内超过20个请求）\",\"当失败率达到一定的时候（默认10s超过50%的请求失败）\",\"以上两个，断路器则会开启\",\"开启后，所有的请求都不会进行转发\",\"一段时间之后（默认5秒），这个时候断路器是半开状态，会让其中一个请求进行转发，如果成功，断路器则会关闭，若失败，则会继续开启\",\"####### 4、断路器开启之后\",\"再有请求调用的时候，将不会调用主逻辑，而是直接调用降级fallback。通过断路器，实现了自动地发现错误并将降级逻辑切换为主逻辑，减少响应延迟的效果。\",\"原来的主逻辑要如何恢复呢？\",\"对于这一问题，hystrix也为我们实现了自动恢复功能\",\"当断路器打开，对主逻辑进行熔断之后，hystrix会启动一个休眠时间窗，在这个时间窗内，降级逻辑是临时的成为主逻辑，\",\"当休眠时间窗到期，断路器将进入半开状态，释放一次请求到原来的主逻辑上，如果此次请求正常返回，那么断路器将继续闭合，\",\"主逻辑恢复，如果这次请求依然有问题，断路器继续进入打开状态，休眠时间窗重新计时。\",\"####### 5、HystrixProperty相关配置\",\"@HystrixCommand(fallbackMethod = \\\"str_fallbackMethod\\\", groupKey = \\\"strGroupCommand\\\", commandKey = \\\"strCommand\\\", threadPoolKey = \\\"strThreadPool\\\", commandProperties = { // 设置隔离策略，THREAD 表示线程池 SEMAPHORE：信号池隔离 @HystrixProperty(name = \\\"execution.isolation.strategy\\\", value = \\\"THREAD\\\"), // 当隔离策略选择信号池隔离的时候，用来设置信号池的大小（最大并发数） @HystrixProperty(name = \\\"execution.isolation.semaphore.maxConcurrentRequests\\\", value = \\\"10\\\"), // 配置命令执行的超时时间 @HystrixProperty(name = \\\"execution.isolation.thread.timeoutinMilliseconds\\\", value = \\\"10\\\"), // 是否启用超时时间 @HystrixProperty(name = \\\"execution.timeout.enabled\\\", value = \\\"true\\\"), // 执行超时的时候是否中断 @HystrixProperty(name = \\\"execution.isolation.thread.interruptOnTimeout\\\", value = \\\"true\\\"), // 执行被取消的时候是否中断 @HystrixProperty(name = \\\"execution.isolation.thread.interruptOnCancel\\\", value = \\\"true\\\"), // 允许回调方法执行的最大并发数 @HystrixProperty(name = \\\"fallback.isolation.semaphore.maxConcurrentRequests\\\", value = \\\"10\\\"), // 服务降级是否启用，是否执行回调函数 @HystrixProperty(name = \\\"fallback.enabled\\\", value = \\\"true\\\"), // 是否启用断路器 @HystrixProperty(name = \\\"circuitBreaker.enabled\\\", value = \\\"true\\\"), // 该属性用来设置在滚动时间窗中，断路器熔断的最小请求数。例如，默认该值为 20 的时候， // 如果滚动时间窗（默认10秒）内仅收到了19个请求， 即使这19个请求都失败了，断路器也不会打开。 @HystrixProperty(name = \\\"circuitBreaker.requestVolumeThreshold\\\", value = \\\"20\\\"), // 该属性用来设置在滚动时间窗中，表示在滚动时间窗中，在请求数量超过 // circuitBreaker.requestVolumeThreshold 的情况下，如果错误请求数的百分比超过50, // 就把断路器设置为 \\\"打开\\\" 状态，否则就设置为 \\\"关闭\\\" 状态。 @HystrixProperty(name = \\\"circuitBreaker.errorThresholdPercentage\\\", value = \\\"50\\\"), // 该属性用来设置当断路器打开之后的休眠时间窗。 休眠时间窗结束之后， // 会将断路器置为 \\\"半开\\\" 状态，尝试熔断的请求命令，如果依然失败就将断路器继续设置为 \\\"打开\\\" 状态， // 如果成功就设置为 \\\"关闭\\\" 状态。 @HystrixProperty(name = \\\"circuitBreaker.sleepWindowinMilliseconds\\\", value = \\\"5000\\\"), // 断路器强制打开 @HystrixProperty(name = \\\"circuitBreaker.forceOpen\\\", value = \\\"false\\\"), // 断路器强制关闭 @HystrixProperty(name = \\\"circuitBreaker.forceClosed\\\", value = \\\"false\\\"), // 滚动时间窗设置，该时间用于断路器判断健康度时需要收集信息的持续时间 @HystrixProperty(name = \\\"metrics.rollingStats.timeinMilliseconds\\\", value = \\\"10000\\\"), // 该属性用来设置滚动时间窗统计指标信息时划分\\\"桶\\\"的数量，断路器在收集指标信息的时候会根据 // 设置的时间窗长度拆分成多个 \\\"桶\\\" 来累计各度量值，每个\\\"桶\\\"记录了一段时间内的采集指标。 // 比如 10 秒内拆分成 10 个\\\"桶\\\"收集这样，所以 timeinMilliseconds 必须能被 numBuckets 整除。否则会抛异常 @HystrixProperty(name = \\\"metrics.rollingStats.numBuckets\\\", value = \\\"10\\\"), // 该属性用来设置对命令执行的延迟是否使用百分位数来跟踪和计算。如果设置为 false, 那么所有的概要统计都将返回 -1。 @HystrixProperty(name = \\\"metrics.rollingPercentile.enabled\\\", value = \\\"false\\\"), // 该属性用来设置百分位统计的滚动窗口的持续时间，单位为毫秒。 @HystrixProperty(name = \\\"metrics.rollingPercentile.timeInMilliseconds\\\", value = \\\"60000\\\"), // 该属性用来设置百分位统计滚动窗口中使用 “ 桶 ”的数量。 @HystrixProperty(name = \\\"metrics.rollingPercentile.numBuckets\\\", value = \\\"60000\\\"), // 该属性用来设置在执行过程中每个 “桶” 中保留的最大执行次数。如果在滚动时间窗内发生超过该设定值的执行次数， // 就从最初的位置开始重写。例如，将该值设置为100, 滚动窗口为10秒，若在10秒内一个 “桶 ”中发生了500次执行， // 那么该 “桶” 中只保留 最后的100次执行的统计。另外，增加该值的大小将会增加内存量的消耗，并增加排序百分位数所需的计算时间。 @HystrixProperty(name = \\\"metrics.rollingPercentile.bucketSize\\\", value = \\\"100\\\"), // 该属性用来设置采集影响断路器状态的健康快照（请求的成功、 错误百分比）的间隔等待时间。 @HystrixProperty(name = \\\"metrics.healthSnapshot.intervalinMilliseconds\\\", value = \\\"500\\\"), // 是否开启请求缓存 @HystrixProperty(name = \\\"requestCache.enabled\\\", value = \\\"true\\\"), // HystrixCommand的执行和事件是否打印日志到 HystrixRequestLog 中 @HystrixProperty(name = \\\"requestLog.enabled\\\", value = \\\"true\\\"), }, threadPoolProperties = { // 该参数用来设置执行命令线程池的核心线程数，该值也就是命令执行的最大并发量 @HystrixProperty(name = \\\"coreSize\\\", value = \\\"10\\\"), // 该参数用来设置线程池的最大队列大小。当设置为 -1 时，线程池将使用 SynchronousQueue 实现的队列， // 否则将使用 LinkedBlockingQueue 实现的队列。 @HystrixProperty(name = \\\"maxQueueSize\\\", value = \\\"-1\\\"), // 该参数用来为队列设置拒绝阈值。 通过该参数， 即使队列没有达到最大值也能拒绝请求。 // 该参数主要是对 LinkedBlockingQueue 队列的补充,因为 LinkedBlockingQueue // 队列不能动态修改它的对象大小，而通过该属性就可以调整拒绝请求的队列大小了。 @HystrixProperty(name = \\\"queueSizeRejectionThreshold\\\", value = \\\"5\\\"), } )public String strConsumer() { return \\\"hello 2020\\\"; } public String str_fallbackMethod() { return \\\"*****fall back str_fallbackMethod\\\"; } \"]},{\"header\":\"4、Hystrix工作流程\",\"slug\":\"_4、hystrix工作流程\",\"contents\":[\"序号\",\"操作\",\"1\",\"创建 HystrixCommand（用在依赖的服务返回单个操作结果的时候） 或 HystrixObserableCommand（用在依赖的服务返回多个操作结果的时候） 对象。\",\"2\",\"命令执行。其中 HystrixComand 实现了下面前两种执行方式；而 HystrixObservableCommand 实现了后两种执行方式：execute()：同步执行，从依赖的服务返回一个单一的结果对象， 或是在发生错误的时候抛出异常。queue()：异步执行， 直接返回 一个Future对象， 其中包含了服务执行结束时要返回的单一结果对象。observe()：返回 Observable 对象，它代表了操作的多个结果，它是一个 Hot Obserable（不论 \\\"事件源\\\" 是否有 \\\"订阅者\\\"，都会在创建后对事件进行发布，所以对于 Hot Observable 的每一个 \\\"订阅者\\\" 都有可能是从 \\\"事件源\\\" 的中途开始的，并可能只是看到了整个操作的局部过程）。toObservable()： 同样会返回 Observable 对象，也代表了操作的多个结果，但它返回的是一个Cold Observable（没有 \\\"订阅者\\\" 的时候并不会发布事件，而是进行等待，直到有 \\\"订阅者\\\" 之后才发布事件，所以对于 Cold Observable 的订阅者，它可以保证从一开始看到整个操作的全部过程）。\",\"3\",\"若当前命令的请求缓存功能是被启用的， 并且该命令缓存命中， 那么缓存的结果会立即以 Observable 对象的形式 返回。\",\"4\",\"检查断路器是否为打开状态。如果断路器是打开的，那么Hystrix不会执行命令，而是转接到 fallback 处理逻辑（第 8 步）；如果断路器是关闭的，检查是否有可用资源来执行命令（第 5 步）。\",\"5\",\"线程池/请求队列/信号量是否占满。如果命令依赖服务的专有线程池和请求队列，或者信号量（不使用线程池的时候）已经被占满， 那么 Hystrix 也不会执行命令， 而是转接到 fallback 处理逻辑（第8步）。\",\"6\",\"Hystrix 会根据我们编写的方法来决定采取什么样的方式去请求依赖服务。HystrixCommand.run() ：返回一个单一的结果，或者抛出异常。HystrixObservableCommand.construct()： 返回一个Observable 对象来发射多个结果，或通过 onError 发送错误通知。\",\"7\",\"Hystrix会将 \\\"成功\\\"、\\\"失败\\\"、\\\"拒绝\\\"、\\\"超时\\\" 等信息报告给断路器， 而断路器会维护一组计数器来统计这些数据。断路器会使用这些统计数据来决定是否要将断路器打开，来对某个依赖服务的请求进行 \\\"熔断/短路\\\"。\",\"8\",\"当命令执行失败的时候， Hystrix 会进入 fallback 尝试回退处理， 我们通常也称该操作为 \\\"服务降级\\\"。而能够引起服务降级处理的情况有下面几种：第4步： 当前命令处于\\\"熔断/短路\\\"状态，断路器是打开的时候。第5步： 当前命令的线程池、 请求队列或 者信号量被占满的时候。第6步：HystrixObservableCommand.construct() 或 HystrixCommand.run() 抛出异常的时候。\",\"9\",\"当Hystrix命令执行成功之后， 它会将处理结果直接返回或是以Observable 的形式返回。\",\"tips：如果我们没有为命令实现降级逻辑或者在降级处理逻辑中抛出了异常， Hystrix 依然会返回一个 Observable 对象， 但是它不会发射任何结果数据， 而是通过 onError 方法通知命令立即中断请求，并通过onError()方法将引起命令失败的异常发送给调用者。\"]},{\"header\":\"5、服务监控 HystrixDashBoard\",\"slug\":\"_5、服务监控hystrixdashboard\",\"contents\":[\"除了隔离依赖服务的调用以外，Hystrix还提供了准实时的调用监控（Hystrix Dashboard），Hystrix会持续地记录所有通过Hystrix发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求多少成功，多少失败等。Netflix通过hystrix-metrics-event-stream项目实现了对以上指标的监控。Spring Cloud也提供了Hystrix Dashboard的整合，对监控内容转化成可视化界面。\"]},{\"header\":\"1、案例module9001\",\"slug\":\"_1、案例module9001\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-2\",\"contents\":[\"xiaobear-consumer-hystrix-dashboard9001-14 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-2\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-hystrix-dashboard</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 9001 \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-2\",\"contents\":[\"@SpringBootApplication @EnableHystrixDashboard public class HystrixDashBoard9001 { public static void main(String[] args) { SpringApplication.run(HystrixDashBoard9001.class,args); } } \"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"http://localhost:9001/hystrix\"]},{\"header\":\"2、监控测试\",\"slug\":\"_2、监控测试\",\"contents\":[]},{\"header\":\"1、修改xiaobear-provider-hystrix-payment8001-12\",\"slug\":\"_1、修改xiaobear-provider-hystrix-payment8001-12-1\",\"contents\":[\"注意：新版本Hystrix需要在启动类上指定监控路径，否则就会报404错误。\",\"@SpringBootApplication(exclude={DataSourceAutoConfiguration.class, HibernateJpaAutoConfiguration.class}) @EnableEurekaClient //本服务启动后会自动注册进eureka服务中 @EnableCircuitBreaker public class HystrixApplication { public static void main(String[] args) { SpringApplication.run(HystrixApplication.class,args); } /** *此配置是为了服务监控而配置，与服务容错本身无关，springcloud升级后的坑 *ServletRegistrationBean因为springboot的默认路径不是\\\"/hystrix.stream\\\"， *只要在自己的项目里配置上下面的servlet就可以了 */ @Bean public ServletRegistrationBean getServlet() { HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet(); ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet); registrationBean.setLoadOnStartup(1); registrationBean.addUrlMappings(\\\"/hystrix.stream\\\"); registrationBean.setName(\\\"HystrixMetricsStreamServlet\\\"); return registrationBean; } } \"]},{\"header\":\"2、测试\",\"slug\":\"_2、测试-1\",\"contents\":[\"启动一个Eureka或者Eureka集群\",\"观察监控窗口\"]},{\"header\":\"1、观察监控窗口\",\"slug\":\"_1、观察监控窗口\",\"contents\":[\"Delay：该参数用来控制服务器上轮询监控信息的延迟时间，默认为2000毫秒，可以通过配置该属性来降低客户端的网络和CPU消耗。\",\"Title：该参数对应了头部标题Hystrix Stream之后的内容，默认会使用具体监控实例的URL，可以通过配置该信息来展示更合适的标题。\",\"遇到的问题：\",\"启动8001的时候报错了，\",\"Cannot execute request on any known server\",\"原因：在默认设置下，eureka服务注册中心也会将自己作为客户端来尝试注册它自己，所以我们需要禁用它的客户端注册行为\",\"解决：查看配置文件application.yml，发现register-with-eureka 属性值设置为true了，将其改为false,重启eureka服务，启动成功\",\"打开另一个浏览器窗口，测试8001的其他接口地址\",\"例如：http://localhost:8001/payment/circuit/1\",\"监控成功\",\"监控失败\"]},{\"header\":\"2、如何观看监控图？\",\"slug\":\"_2、如何观看监控图\",\"contents\":[\"实心圆：共有两种含义。它通过颜色的变化代表了实例的健康程度，它的健康度从绿色<黄色<橙色<红色递减。\",\"该实心圆除了颜色的变化之外，它的大小也会根据实例的请求流量发生变化，流量越大该实心圆就越大。所以通过该实心圆的展示，就可以在大量的实例中快速的发现故障实例和高压力实例。\",\"曲线：用来记录2分钟内流量的相对变化，可以通过它来观察到流量的上升和下降趋势。\"]}]},\"/study-tutorial/microservice/spring-cloud/microservice.html\":{\"title\":\"1、微服务的概念\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在系统架构与设计的实践中，从宏观上可以总结为三个阶段；\",\"集中式架构：就是把所有的功能、模块都集中到一个项目中，部署在一台服务器上，从而对外提供服务（单体架构、单体服务、单体应用）；\",\"直白一点：就是只有一个项目，只有一个war；\",\"**分布式架构：**就是把所有的功能、模块拆分成不同的子项目，部署在多台不同的服务器上，这些子项目相互协作共同对外提供服务。\",\"直白一点：就是有很多项目，有很多war包，这些项目相互协作完成需要的功能，不是一个war能完成的，一个war包完成不了；\",\"比如：\",\"Shop项目：单体应用\",\"Shop项目：拆分--> (user-center, order-center, trade-center) 分布式应用\",\"**微服务架构：**分布式强调系统的拆分，微服务也是强调系统的拆分，微服务架构属于分布式架构的范畴；\",\"并且到目前为止，微服务并没有一个统一的标准的定义，那么微服务究竟是什\",\"么？\"]},{\"header\":\"1、什么是微服务\",\"slug\":\"_1、什么是微服务\",\"contents\":[\"The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services , which may be written in different programming languages and use different data storage technologies. \",\"首先微服务并没有一个官方的定义，想要直接描述微服务比较困难，我们可以通过对比传统WEB应用，来理解什么是微服务。\"]},{\"header\":\"1、传统的web项目VS微服务\",\"slug\":\"_1、传统的web项目vs微服务\",\"contents\":[]},{\"header\":\"1、传统的Web项目\",\"slug\":\"_1、传统的web项目\",\"contents\":[\"传统的WEB应用核心分为业务逻辑、适配器以及API或通过UI访问的WEB界面。业务逻辑定义业务流程、业务规则以及领域实体。适配器包括数据库访问组件、消息组件以及访问接口等。\",\"尽管也是遵循模块化开发，但最终它们会打包并部署为单体式应用。例如Java应用程序会被打包成WAR，部署在Tomcat或者Jetty上。\",\"这种单体应用比较适合于小项目，优点是：\",\"开发简单直接，集中式管理\",\"基本不会重复开发\",\"功能都在本地，没有分布式的管理开销和调用开销\",\"它的缺点也十分明显，特别对于互联网公司来说：\",\"开发效率低：所有的开发在一个项目改代码，递交代码相互等待，代码冲突不断\",\"代码维护难：代码功能耦合在一起，新人不知道何从下手\",\"部署不灵活：构建时间长，任何小修改必须重新构建整个项目，这个过程往往很长\",\"稳定性不高：一个微不足道的小问题，可以导致整个应用挂掉\",\"扩展性不够：无法满足高并发情况下的业务需求\"]},{\"header\":\"2、微服务项目\",\"slug\":\"_2、微服务项目\",\"contents\":[\"现在主流的设计一般会采用微服务架构。其思路不是开发一个巨大的单体式应用，而是将应用分解为小的、互相连接的微服务。一个微服务完成某个特定功能，比如乘客管理和下单管理等。每个微服务都有自己的业务逻辑和适配器。一些微服务还会提供API接口给其他微服务和应用客户端使用\",\"微服务架构的优点\",\"解决了复杂性问题\",\"它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务。这些服务定义了明确的RPC或消息驱动的API边界。微服务架构强化了应用模块化的水平，而这通过单体代码库很难实现。因此，微服务开发的速度要快很多，更容易理解和维护。\",\"单独开发每个服务，与其他服务互不干扰\",\"只要符合服务API契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响。\",\"可以独立部署每个微服务\",\"开发人员无需协调对服务升级或更改的部署。这些更改可以在测试通过后立即部署。所以微服务架构也使得CI／CD成为可能。\",\"微服务的缺点\",\"多服务运维难度\",\"系统部署依赖\",\"服务间通信成本\",\"数据一致性\",\"系统集成测试\",\"重复工作\",\"性能监控\"]},{\"header\":\"2、如何实现微服务\",\"slug\":\"_2、如何实现微服务\",\"contents\":[\"四个问题\",\"客户端如何访问这些服务？\",\"服务之间如何通信？\",\"这么多服务，怎么找?\",\"服务挂了怎么办？\"]},{\"header\":\"1、客户端如何访问这些服务\",\"slug\":\"_1、客户端如何访问这些服务\",\"contents\":[\"原来的服务都是可以进行单独调用，现在按功能拆分成独立的服务，变成了一个独立的Java进程了。客户端UI如何\",\"访问他的？后台有N个服务，前台就需要记住管理N个服务，一个服务下线/更新/升级，前台就要重新部署，这明显\",\"不服务我们拆分的理念，特别当前台是移动应用的时候，通常业务变化的节奏更快。另外，N个小服务的调用也是\",\"一个不小的网络开销。还有一般微服务在系统内部，通常是无状态的，用户登录信息和权限管理最好有一个统一的\",\"地方维护管理（OAuth）。\",\"所以，一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway，他的作用包括\",\"提供统一服务入口，让微服务对前台透明\",\"聚合后台的服务，节省流量，提升性能\",\"提供安全，过滤，流控等API管理功能\",\"我的理解其实这个API Gateway可以有很多广义的实现办法，可以是一个软硬一体的盒子，也可以是一个简单\",\"MVC框架，甚至是一个Node.js的服务端。他们最重要的作用是为前台（通常是移动应用）提供后台服务的聚合，提\",\"供一个统一的服务出口，解除他们之间的耦合，不过API Gateway也有可能成为单点故障点或者性能的瓶颈。\"]},{\"header\":\"2、 服务之间如何通信？\",\"slug\":\"_2、-服务之间如何通信\",\"contents\":[\"因为所有的微服务都是独立的Java进程跑在独立的虚拟机上，所以服务间的通行就是IPC（inter process communication），已经有很多成熟的方案。现在基本最通用的有两种方式。\",\"同步调用 \",\"REST（JAX-RS）\",\"RPC（Dubbo）\",\"异步消息调用(Kafka, Notify, MetaQ)\",\"一般同步调用比较简单，一致性强，但是容易出调用问题，性能体验上也会差些，特别是调用层次多的时候。RESTful和RPC的比较也是一个很有意思的话题。一般REST基于HTTP，更容易实现，更容易被接受，服务端实现技术也更灵活些，各个语言都能支持，同时能跨客户端，对客户端没有特殊的要求，只要封装了HTTP的SDK就能调用，所以相对使用的广一些。RPC也有自己的优点，传输协议更高效，安全更可控，特别在一个公司内部，如果有统一个的开发规范和统一的服务框架时，他的开发效率优势更明显些。就看各自的技术积累实际条件，自己的选择了。\",\"而异步消息的方式在分布式系统中有特别广泛的应用，他既能减低调用服务之间的耦合，又能成为调用之间的缓冲，确保消息积压不会冲垮被调用方，同时能保证调用方的服务体验，继续干自己该干的活，不至于被后台性能拖慢。不过需要付出的代价是一致性的减弱，需要接受数据最终一致性；还有就是后台服务一般要实现幂等性，因为消息发送出于性能的考虑一般会有重复（保证消息的被收到且仅收到一次对性能是很大的考验）；最后就是必须引入一个独立的broker，如果公司内部没有技术积累，对broker分布式管理也是一个很大的挑战。\"]},{\"header\":\"3、这么多服务，怎么找?\",\"slug\":\"_3、这么多服务-怎么找\",\"contents\":[\"在微服务架构中，一般每一个服务都是有多个拷贝，来做负载均衡。一个服务随时可能下线，也可能应对临时访问\",\"压力增加新的服务节点。服务之间如何相互感知？服务如何管理？这就是服务发现的问题了。一般有两类做法，也\",\"各有优缺点。基本都是通过zookeeper等类似技术做服务注册信息的分布式管理。当服务上线时，服务提供者将自\",\"己的服务信息注册到ZK（或类似框架），并通过心跳维持长链接，实时更新链接信息。服务调用者通过ZK寻址，\",\"根据可定制算法，找到一个服务，还可以将服务信息缓存在本地以提高性能。当服务下线时，ZK会发通知给服务\",\"客户端。\",\"客户端做：优点是架构简单，扩展灵活，只对服务注册器依赖。缺点是客户端要维护所有调用服务的地址，有\",\"技术难度，一般大公司都有成熟的内部框架支持，比如Dubbo。\",\"服务端做：优点是简单，所有服务对于前台调用方透明，一般在小公司在云服务上部署的应用采用的比较多。\"]},{\"header\":\"4、这么多服务，服务挂了怎么办？\",\"slug\":\"_4、这么多服务-服务挂了怎么办\",\"contents\":[\"前面提到，Monolithic方式开发一个很大的风险是，把所有鸡蛋放在一个篮子里，一荣俱荣，一损俱损。而分布式\",\"最大的特性就是网络是不可靠的。通过微服务拆分能降低这个风险，不过如果没有特别的保障，结局肯定是噩梦。\",\"我们刚遇到一个线上故障就是一个很不起眼的SQL计数功能，在访问量上升时，导致数据库load彪高，影响了所在\",\"应用的性能，从而影响所有调用这个应用服务的前台应用。所以当我们的系统是由一系列的服务调用链组成的时\",\"候，我们必须确保任一环节出问题都不至于影响整体链路。相应的手段有很多：\",\"重试机制\",\"限流\",\"熔断机制\",\"负载均衡\",\"降级（本地缓存）\"]},{\"header\":\"3、经典面试\",\"slug\":\"_3、经典面试\",\"contents\":[]},{\"header\":\"1、分布式和微服务有什么区别？\",\"slug\":\"_1、分布式和微服务有什么区别\",\"contents\":[\"分布式，就是将巨大的一个系统划分为多个模块，这一点和微服务是一样的，都是要把系统进行拆分，部署到不同机器上，因为一台机器可能承受不了这么大的访问压力，或者说要支撑这么大的访问压力需要采购一台性能超级好的服务器，其财务成本非常高，有这些预算完全可以采购很多台普通的服务器了，分布式系统各个模块通过接口进行数据交互，其实分布式也是一种微服务，因为都是把模块拆分变为独立的单元，提供接口来调用，那么它们本质的区别是什么？\",\"它们的本质的区别体现在“目标”上， 何为目标，就是你采用分布式架构或者采用微服务架构，你最终是为了什么，要达到什么目的？\",\"分布式架构的目标是什么？ 就是访问量很大一台机器承受不了，或者是成本问题，不得不使用多台机器来完成服务的部署；\",\"而微服务的目标是什么？只是让各个模块拆分开来，不会被互相影响，比如模块的升级或者出现BUG或者是重构等等都不要影响到其他模块，微服务它是可以在一台机器上部署；\",\"但是：分布式也是微服务的一种，微服务也属于分布式；\"]},{\"header\":\"2、微服务与Spring-Cloud的关系或区别？\",\"slug\":\"_2、微服务与spring-cloud的关系或区别\",\"contents\":[\"微服务只是一种项目的架构方式、架构理念，或者说是一种概念，就如同我们的MVC架构一样， 那么Spring Cloud便是对这种架构方式的技术落地实现；\"]},{\"header\":\"3、微服务一定要使用Spring Cloud吗？\",\"slug\":\"_3、微服务一定要使用spring-cloud吗\",\"contents\":[\"微服务只是一种项目的架构方式、架构理念，所以任何技术都可以实现这种架构理念，只是微服务架构里面有很多问题需要我们去解决，比如：负载均衡，服务的注册与发现，服务调用，服务路由，服务熔断等等一系列问题，如果你自己从0开始实现微服务的架构理念，那头发都掉光了，所以Spring Cloud 帮我们做了这些事情，Spring Cloud将处理这些问题的的技术全部打包好了，我们只需要开箱即用；\",\"对于微服务的文章，可参考：\",\"一篇文章快速了解微服务\",\"微服务那点事\"]}]},\"/study-tutorial/microservice/spring-cloud/openFeign.html\":{\"title\":\"7、OpenFeign服务接口调用\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"官网：https://docs.spring.io/spring-cloud-openfeign/docs/current/reference/html/\",\"Feign是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单。\",\"它的使用方法是定义一个服务接口然后在上面添加注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡\",\"Feign is a declarative web service client. It makes writing web service clients easier. To use Feign create an interface and annotate it. It has pluggable annotation support including Feign annotations and JAX-RS annotations. Feign also supports pluggable encoders and decoders. Spring Cloud adds support for Spring MVC annotations and for using the same HttpMessageConverters used by default in Spring Web. Spring Cloud integrates Eureka, Spring Cloud CircuitBreaker, as well as Spring Cloud LoadBalancer to provide a load-balanced http client when using Feign.\"]},{\"header\":\"1、作用\",\"slug\":\"_1、作用\",\"contents\":[\"Feign旨在使编写Java Http客户端变得更容易。\",\"前面在使用Ribbon+RestTemplate时，利用RestTemplate对http请求的封装处理，形成了一套模版化的调用方法。但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。所以，Feign在此基础上做了进一步封装，由他来帮助我们定义和实现依赖服务接口的定义。在Feign的实现下，我们只需创建一个接口并使用注解的方式来配置它(以前是Dao接口上面标注Mapper注解,现在是一个微服务接口上面标注一个Feign注解即可)，即可完成对服务提供方的接口绑定，简化了使用Spring cloud Ribbon时，自动封装服务调用客户端的开发量。\",\"Feign集成了Ribbon\",\"利用Ribbon维护了Payment的服务列表信息，并且通过轮询实现了客户端的负载均衡。而与Ribbon不同的是，通过feign只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用\"]},{\"header\":\"2、Feign与OpenFeign的区别\",\"slug\":\"_2、feign与openfeign的区别\",\"contents\":[\"Feign\",\"OpenFeign\",\"Feign是Spring Cloud组件中的一个轻量级RESTful的HTTP服务客户端；Feign内置了Ribbon，用来做客户端负载均衡，去调用服务注册中心的服务。Feign的使用方式是：使用Feign的注解定义接口，调用这个接口，就可以调用服务注册中心的服务\",\"OpenFeign是Spring Cloud 在Feign的基础上支持了SpringMVC的注解，如@RequesMapping等等。OpenFeign的@FeignClient可以解析SpringMVC的@RequestMapping注解下的接口，并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务。\"]},{\"header\":\"2、使用\",\"slug\":\"_2、使用\",\"contents\":[\"还是跟之前一样的步骤\"]},{\"header\":\"1、建module\",\"slug\":\"_1、建module\",\"contents\":[\"xiaobear-openFeign-order80-11 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom\",\"contents\":[\"<dependencies> <!--openfeign--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-openfeign</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \",\"这里我遇到的问题:\",\"突然就报数据库连接的错误，而这里我并没有做任何数据库相关的配置，一看pom文件，发现导入了数据库相关的依赖\"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 80 eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication @EnableFeignClients public class OpenFeignOrderMain { public static void main(String[] args) { SpringApplication.run(OpenFeignOrderMain.class,args); } } \"]},{\"header\":\"5、业务层\",\"slug\":\"_5、业务层\",\"contents\":[]},{\"header\":\"1、service\",\"slug\":\"_1、service\",\"contents\":[\"@Component @FeignClient(value = \\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\") public interface PaymentFeignService { @GetMapping(\\\"/payment/selectOne/{id}\\\") //这里调用的8001或8002的查询 CommonResult<Payment> getPaymentById(@PathVariable(\\\"id\\\") Long id); } \"]},{\"header\":\"2、controller\",\"slug\":\"_2、controller\",\"contents\":[\"@RestController public class OpenFeignController { @Resource private PaymentFeignService feignService; @GetMapping(\\\"/feign/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable(\\\"id\\\") Long id){ return feignService.getPaymentById(id); } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"http://localhost/feign/get/1\",\"8001和8002服务交替出现\"]},{\"header\":\"3、OpenFeign的超时控制\",\"slug\":\"_3、openfeign的超时控制\",\"contents\":[]},{\"header\":\"1、服务8002controller写暂停代码\",\"slug\":\"_1、服务8002controller写暂停代码\",\"contents\":[\"@GetMapping(value = \\\"/payment/feign/timeout\\\") public String handleTimeOut(){ try{ //暂停3秒 TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return serverPort; } \"]},{\"header\":\"2、服务消费端80\",\"slug\":\"_2、服务消费端80\",\"contents\":[\"@Component @FeignClient(value = \\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\") public interface PaymentFeignService { @GetMapping(\\\"/payment/selectOne/{id}\\\") CommonResult<Payment> getPaymentById(@PathVariable(\\\"id\\\") Long id); @GetMapping(value = \\\"/payment/feign/timeout\\\") String handleTimeOut(); } \"]},{\"header\":\"3、80controller添加超时方法\",\"slug\":\"_3、80controller添加超时方法\",\"contents\":[\"@RestController public class OpenFeignController { @Resource private PaymentFeignService feignService; @GetMapping(\\\"/feign/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable(\\\"id\\\") Long id){ return feignService.getPaymentById(id); } @GetMapping(value = \\\"/consumer/feign/timeout\\\") public String handleTimeOut(){ return feignService.handleTimeOut(); } } \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试\",\"contents\":[\"先测试-----》http://localhost:8002/payment/payment/feign/timeout 3秒后才显示端口\",\"再http://localhost/consumer/feign/timeout![image-20210429133442333](../../images/image-20210429133442333.png)\",\"原因\",\"OpenFeign默认等待1秒钟，超时就会报错\",\"默认Feign客户端只等待一秒钟，但是服务端处理需要超过1秒钟，导致Feign客户端不想等待了，直接返回报错。\",\"为了避免这样的情况，有时候我们需要设置Feign客户端的超时控制。\",\"在yaml中进行配置\",\"#设置feign客户端超时时间(OpenFeign默认支持ribbon) ribbon: #指的是建立连接所用的时间，适用于网络状况正常的情况下,两端连接所用的时间 ReadTimeout: 5000 #指的是建立连接后从服务器读取到可用资源所用的时间 ConnectTimeout: 5000 \",\"完整版\",\"server: port: 80 eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 #设置feign客户端超时时间(OpenFeign默认支持ribbon) ribbon: #指的是建立连接所用的时间，适用于网络状况正常的情况下,两端连接所用的时间 ReadTimeout: 5000 #指的是建立连接后从服务器读取到可用资源所用的时间 ConnectTimeout: 5000 \",\"再接着访问http://localhost/consumer/feign/timeout 等待5秒就会显示接口\",\"遇到的错误，启动80的启动类\",\"Error starting ApplicationContext. To display the auto-configuration report re-run your application with 'debug' enabled. 2018-08-05 10:56:04,870 ERROR [LoggingFailureAnalysisReporter.java:42] : *************************** APPLICATION FAILED TO START *************************** Description: Cannot determine embedded database driver class for database type NONE Action: If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active). \",\"解决\",\"因为这个module是没有用到数据库的，这时候，我们需要排除，防止自动注入数据库的链接\",\"在@SpringBootApplication中排除其注入 @SpringBootApplication(exclude={DataSourceAutoConfiguration.class,HibernateJpaAutoConfiguration.class})\",\"@SpringBootApplication(exclude={DataSourceAutoConfiguration.class, HibernateJpaAutoConfiguration.class}) @EnableFeignClients public class OpenFeignOrderMain { public static void main(String[] args) { SpringApplication.run(OpenFeignOrderMain.class,args); } } \"]},{\"header\":\"4、OpenFeign日志打印\",\"slug\":\"_4、openfeign日志打印\",\"contents\":[\"Feign 提供了日志打印功能，我们可以通过配置来调整日志级别，从而了解 Feign 中 Http 请求的细节。\",\"说白了就是对Feign接口的调用情况进行监控和输出\"]},{\"header\":\"1、日志级别\",\"slug\":\"_1、日志级别\",\"contents\":[\"NONE：默认的，不显示任何日志；\",\"BASIC：仅记录请求方法、URL、响应状态码及执行时间；\",\"HEADERS：除了 BASIC 中定义的信息之外，还有请求和响应的头信息；\",\"FULL：除了 HEADERS 中定义的信息之外，还有请求和响应的正文及元数据。\"]},{\"header\":\"2、配置\",\"slug\":\"_2、配置\",\"contents\":[\"//日志打印配置类 @Configuration public class OpenFeignConfig { @Bean Logger.Level feignLoggerLevel() { return Logger.Level.FULL; } } \"]},{\"header\":\"3、在yaml文件中开始OpenFeign的客户端\",\"slug\":\"_3、在yaml文件中开始openfeign的客户端\",\"contents\":[\"server: port: 80 eureka: client: #表示是否将自己注册进EurekaServer默认为true。 register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 #设置feign客户端超时时间(OpenFeign默认支持ribbon) ribbon: #指的是建立连接所用的时间，适用于网络状况正常的情况下,两端连接所用的时间 ReadTimeout: 5000 #指的是建立连接后从服务器读取到可用资源所用的时间 ConnectTimeout: 5000 logging: level: ## feign日志以什么级别监控哪个接口 com.xiaobear.service.PaymentFeignService: debug \"]},{\"header\":\"4、测试\",\"slug\":\"_4、测试-1\",\"contents\":[\"http://localhost/consumer/feign/timeout\"]}]},\"/study-tutorial/microservice/spring-cloud/ribbon.html\":{\"title\":\"6、Ribbon负载均衡服务与调用\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"Spring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具。\",\"简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法和服务调用。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们很容易使用Ribbon实现自定义的负载均衡算法\"]},{\"header\":\"1、负载均衡分类\",\"slug\":\"_1、负载均衡分类\",\"contents\":[\"载均和分为硬件负载均衡和软件负载均衡：\",\"硬件负载均衡：比如 F5、深信服、Array 等；\",\"软件负载均衡：比如 Nginx、LVS、HAProxy 等；（是一个服务器实现的）\"]},{\"header\":\"2、作用\",\"slug\":\"_2、作用\",\"contents\":[]},{\"header\":\"1、LB负载均衡(Load Balance)是什么\",\"slug\":\"_1、lb负载均衡-load-balance-是什么\",\"contents\":[\"简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA（高可用）。\"]},{\"header\":\"2、Ribbon本地负载均衡客户端 VS Nginx服务端负载均衡区别\",\"slug\":\"_2、ribbon本地负载均衡客户端-vs-nginx服务端负载均衡区别\",\"contents\":[\"Nginx是服务器负载均衡，客户端所有请求都会交给nginx，然后由nginx实现转发请求。即负载均衡是由服务端实现的。\",\"Ribbon本地负载均衡，在调用微服务接口时候，会在注册中心上获取注册信息服务列表之后缓存到JVM本地，从而在本地实现RPC远程服务调用技术。\"]},{\"header\":\"3、进程式LB与集中式LB\",\"slug\":\"_3、进程式lb与集中式lb\",\"contents\":[]},{\"header\":\"进程式LB\",\"slug\":\"进程式lb\",\"contents\":[\"将LB逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。\",\"Ribbon就属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。\"]},{\"header\":\"集中式LB\",\"slug\":\"集中式lb\",\"contents\":[\"即在服务的消费方和提供方之间使用独立的LB设施(可以是硬件，如F5, 也可以是软件，如nginx), 由该设施负责把访问请求通过某种策略转发至服务的提供方；\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"Ribbon就是负载均衡+RestTemplate调用\",\"我们在Eureka就用到了负载均衡，8001和8002的访问\"]},{\"header\":\"2、负载均衡演示\",\"slug\":\"_2、负载均衡演示\",\"contents\":[\"Ribbon在工作时分成两步\",\"先选择 EurekaServer ,它优先选择在同一个区域内负载较少的server.\",\"再根据用户指定的策略，在从server取到的服务注册列表中选择一个地址。\",\"其中Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权。\",\"**总结：**Ribbon就是一个软负载均衡的客户端组件，他可以根据所需请求的客户端结合使用\",\"那么，前面用到的Eureka的负载均衡是哪来的呢？\",\"我们没有引入，猜测就是Eureka的客户端自带了ribbon，事实也是\"]},{\"header\":\"1、RestTempalte的作用\",\"slug\":\"_1、resttempalte的作用\",\"contents\":[\"官网地址：https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/javadoc-api/org/springframework/web/client/RestTemplate.html\"]},{\"header\":\"1、getForObject/getForEntity方法\",\"slug\":\"_1、getforobject-getforentity方法\",\"contents\":[]},{\"header\":\"getForObject方法\",\"slug\":\"getforobject方法\",\"contents\":[\"返回对象为响应体中数据转化成的对象，基本上可以理解为Json\",\"@GetMapping(\\\"/consumer/payment/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable(\\\"id\\\") Long id){ return restTemplate.getForObject(PAYMENT_SRV + \\\"/payment/selectOne/\\\"+id,CommonResult.class, id); } \"]},{\"header\":\"getForEntity方法\",\"slug\":\"getforentity方法\",\"contents\":[\"返回对象为ResponseEntity对象，包含了响应中的一些重要信息，比如响应头、响应状态码、响应体等\",\" @GetMapping(\\\"/consumer/payment/getForEntity/{id}\\\") public CommonResult<Payment> getPayment2(@PathVariable(\\\"id\\\") Long id){ ResponseEntity<CommonResult> entity = restTemplate.getForEntity(PAYMENT_SRV + \\\"/payment/selectOne/\\\" + id, CommonResult.class, id); if (entity.getStatusCode().is2xxSuccessful()){ return entity.getBody(); }else { return new CommonResult<>(400,\\\"操作失败\\\"); } } \"]},{\"header\":\"2、postForObject/postForEntity方法\",\"slug\":\"_2、postforobject-postforentity方法\",\"contents\":[]},{\"header\":\"1、postForObject方法\",\"slug\":\"_1、postforobject方法\",\"contents\":[\"@GetMapping(\\\"/consumer/payment/insert\\\") public CommonResult<Payment> create(Payment payment){ return restTemplate.postForObject(PAYMENT_SRV+\\\"/payment/insert\\\",payment,CommonResult.class); } \"]},{\"header\":\"2、postForEntity方法\",\"slug\":\"_2、postforentity方法\",\"contents\":[\" @GetMapping(\\\"/consumer/payment/postForEntity\\\") public CommonResult<Payment> create2(Payment payment){ ResponseEntity<CommonResult> postForEntity = restTemplate.postForEntity(PAYMENT_SRV + \\\"/payment/insert\\\", payment, CommonResult.class); if (postForEntity.getStatusCode().is2xxSuccessful()){ return postForEntity.getBody(); }else { return new CommonResult<>(400,\\\"新增失败\\\"); } } \"]},{\"header\":\"3、get/post方法\",\"slug\":\"_3、get-post方法\",\"contents\":[]},{\"header\":\"1、get方法\",\"slug\":\"_1、get方法\",\"contents\":[\"<T> T getForObject(String url, Class<T> responseType, Object... uriVariables); <T> T getForObject(String url, Class<T> responseType, Map<String, ?> uriVariables); <T> T getForObject(URI url, Class<T> responseType); <T> ResponseEntity<T> getForEntity(String url, Class<T> responseType, Object... uriVariables); <T> ResponseEntity<T> getForEntity(String url, Class<T> responseType, Map<String, ?> uriVariables); <T> ResponseEntity<T> getForEntity(URI var1, Class<T> responseType); \"]},{\"header\":\"2、post方法\",\"slug\":\"_2、post方法\",\"contents\":[\"<T> T postForObject(String url, @Nullable Object request, Class<T> responseType, Object... uriVariables); <T> T postForObject(String url, @Nullable Object request, Class<T> responseType, Map<String, ?> uriVariables); <T> T postForObject(URI url, @Nullable Object request, Class<T> responseType); <T> ResponseEntity<T> postForEntity(String url, @Nullable Object request, Class<T> responseType, Object... uriVariables); <T> ResponseEntity<T> postForEntity(String url, @Nullable Object request, Class<T> responseType, Map<String, ?> uriVariables); <T> ResponseEntity<T> postForEntity(URI url, @Nullable Object request, Class<T> responseType); \"]},{\"header\":\"3、核心组件 IRule\",\"slug\":\"_3、核心组件-irule\",\"contents\":[\"根据特定算法中从服务列表中选取一个要访问的服务\",\"RoundRobinRule：轮询\",\"RandomRule：随机\",\"RetryRule：先按照RoundRobinRule的 策略获取服务，如果服务获取失败，则在指定的时间内重试，获取可用的服务\",\"RestAvailableRule：先过滤调由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务\",\"AvailabilityFulteringRule：先过滤调故障实例，再选择并发量最小的实例\",\"WeightedResponseTimeRule：对RoundRobinRule的扩展，响应速度越快的实例选择权重越大，越容易被选择\",\"ZoneAvoidanceRule：默认规则，复合判断server所在区域的性能和server的可用性选择服务器\"]},{\"header\":\"1、如何进行替换\",\"slug\":\"_1、如何进行替换\",\"contents\":[\"我们将对order80进行修改\",\"配置注意\",\"官方文档明确给出了警告：\",\"这个自定义配置类不能放在@ComponentScan所扫描的当前包下以及子包下，\",\"否则我们自定义的这个配置类就会被所有的Ribbon客户端所共享，达不到特殊化定制的目的了。\",\"也就是启动类所在的包\"]},{\"header\":\"新建规则类\",\"slug\":\"新建规则类\",\"contents\":[\"@Configuration public class XiaoBearRule { //随机替换 @Bean public IRule myRule(){ return new RandomRule(); } } \"]},{\"header\":\"主启动类加注解@RibbonClient\",\"slug\":\"主启动类加注解-ribbonclient\",\"contents\":[\"@SpringBootApplication @EnableEurekaClient @RibbonClient(name = \\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\",configuration= XiaoBearRule.class) public class Order80Application { public static void main(String[] args) { SpringApplication.run(Order80Application.class,args); } } \",\"name：是我们提供者的服务名称\",\"configuration：新建的规则类\"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"http://localhost/consumer/payment/get/1\",\"刷新会发现调用服务是随机的\"]},{\"header\":\"4、Ribbon负载均衡算法\",\"slug\":\"_4、ribbon负载均衡算法\",\"contents\":[\"负载均衡算法：rest接口第几次请求数 % 服务器集群总数量 = 实际调用服务器位置下标 ，每次服务重启动后rest接口计数从1开始。\",\"List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); 如： List [0] instances = 127.0.0.1:8002 List [1] instances = 127.0.0.1:8001 \",\"8001+ 8002 组合成为集群，它们共计2台机器，集群总数为2， 按照轮询算法原理：\",\"当总请求数为1时： 1 % 2 =1 对应下标位置为1 ，则获得服务地址为127.0.0.1:8001\",\"当总请求数位2时： 2 % 2 =0 对应下标位置为0 ，则获得服务地址为127.0.0.1:8002\",\"当总请求数位3时： 3 % 2 =1 对应下标位置为1 ，则获得服务地址为127.0.0.1:8001\",\"当总请求数位4时： 4 % 2 =0 对应下标位置为0 ，则获得服务地址为127.0.0.1:8002\",\"如此类推......\"]},{\"header\":\"1、手写负载均衡\",\"slug\":\"_1、手写负载均衡\",\"contents\":[]},{\"header\":\"1、8001和8002的controller改造\",\"slug\":\"_1、8001和8002的controller改造\",\"contents\":[\"@RestController @RequestMapping(\\\"payment\\\") @Slf4j public class PaymentController { /** * 服务对象 */ @Resource private PaymentService paymentService; @Resource private DiscoveryClient discoveryClient; @Value(\\\"${server.port}\\\") private String serverPort; /** * 通过主键查询单条数据 * * @param id 主键 * @return 单条数据 */ @GetMapping(\\\"/selectOne/{id}\\\") public CommonResult selectOne(@PathVariable(\\\"id\\\")Long id) { Payment payment = paymentService.queryById(id); if (null != payment){ return new CommonResult(200,\\\"查询成功！serverPort:\\\"+serverPort,payment); }else { return new CommonResult(404,\\\"查询失败！serverPort\\\"+serverPort,null); } } @PostMapping(\\\"/insert\\\") public CommonResult createPayment(@RequestBody Payment payment){ Payment insert = paymentService.insert(payment); log.info(\\\"插入成功！\\\"); if (null != insert){ return new CommonResult(200,\\\"插入数据库成功！serverPort:\\\"+serverPort,insert); }else { return new CommonResult(500,\\\"插入数据库失败！serverPort:\\\"+serverPort,null); } } @GetMapping(\\\"/discovery\\\") public Object discovery(){ List<String> services = discoveryClient.getServices(); for (String service : services) { System.out.println(service); } List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); for (ServiceInstance element : instances) { System.out.println(element.getServiceId() + \\\"\\\\t\\\" + element.getHost() + \\\"\\\\t\\\" + element.getPort() + \\\"\\\\t\\\" + element.getUri()); } return this.discoveryClient; } @GetMapping(value = \\\"/payment/lb\\\") public String getPaymentLB() { return serverPort; } } \"]},{\"header\":\"2、80订单微服务改造\",\"slug\":\"_2、80订单微服务改造\",\"contents\":[\"@Configuration public class ApplicationContextConfig { @Bean // @LoadBalanced //使用@LoadBalanced注解赋予RestTemplate负载均衡的能力 public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \"]},{\"header\":\"3、自定义接口与实现类\",\"slug\":\"_3、自定义接口与实现类\",\"contents\":[\"public interface XiaobearLoadBalanced { ServiceInstance instances(List<ServiceInstance> serviceInstances); } \",\"@Component public class XiaobearLoadBalancedImpl implements XiaobearLoadBalanced{ private AtomicInteger atomicInteger = new AtomicInteger(0); /** * 利用CAS和自旋锁 CAS比较并交换 * @return */ public int getServiceInstanceIndex(){ int current; int index; do { current = atomicInteger.get(); index = current >= 2147483647 ? 0 : current + 1; }while (!this.atomicInteger.compareAndSet(current,index)); System.out.println(\\\"*****next: \\\"+index); return index; } /** *负载均衡算法：rest接口第几次请求数 % 服务器集群总数量 = 实际调用服务器位置下标 ，每次服务重启动后rest接口计数从1开始。 * @param serviceInstances * @return */ @Override public ServiceInstance instances(List<ServiceInstance> serviceInstances) { int i = getServiceInstanceIndex() % serviceInstances.size(); return serviceInstances.get(i); } } \"]},{\"header\":\"4、80的controller\",\"slug\":\"_4、80的controller\",\"contents\":[\"@RestController @Slf4j public class OrderController { private static final String PAYMENT_URL = \\\"http://localhost:8001\\\"; // 通过在eureka上注册过的微服务名称调用 public static final String PAYMENT_SRV = \\\"http://XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"; @Resource private RestTemplate restTemplate; @Resource private DiscoveryClient discoveryClient; @Resource private XiaobearLoadBalanced xiaobearLoadBalanced; public CommonResult<Payment> create(Payment payment){ return restTemplate.postForObject(PAYMENT_SRV+\\\"/payment/insert\\\",payment,CommonResult.class); } @GetMapping(\\\"/consumer/payment/postForEntity\\\") public CommonResult<Payment> create2(Payment payment){ ResponseEntity<CommonResult> postForEntity = restTemplate.postForEntity(PAYMENT_SRV + \\\"/payment/insert\\\", payment, CommonResult.class); if (postForEntity.getStatusCode().is2xxSuccessful()){ return postForEntity.getBody(); }else { return new CommonResult<>(400,\\\"新增失败\\\"); } } @GetMapping(\\\"/consumer/payment/get/{id}\\\") public CommonResult<Payment> getPayment(@PathVariable(\\\"id\\\") Long id){ return restTemplate.getForObject(PAYMENT_SRV + \\\"/payment/selectOne/\\\"+id,CommonResult.class, id); } @GetMapping(\\\"/consumer/payment/getForEntity/{id}\\\") public CommonResult<Payment> getPayment2(@PathVariable(\\\"id\\\") Long id){ ResponseEntity<CommonResult> entity = restTemplate.getForEntity(PAYMENT_SRV + \\\"/payment/selectOne/\\\" + id, CommonResult.class, id); if (entity.getStatusCode().is2xxSuccessful()){ return entity.getBody(); }else { return new CommonResult<>(400,\\\"操作失败\\\"); } } //测试自定义的负载均衡 @GetMapping(\\\"/consumer/payment/lb\\\") public String getServerPort(){ //获取服务 List<ServiceInstance> instances = discoveryClient.getInstances(\\\"XIAOBEAR-CLOUD-PAYMENT-SERVICE\\\"); if(instances == null || instances.size()<=0) { return null; } ServiceInstance serviceInstance = xiaobearLoadBalanced.instances(instances); URI uri = serviceInstance.getUri(); return restTemplate.getForObject(uri+\\\"/payment/payment/lb\\\",String.class); } } \"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"http://localhost/consumer/payment/lb\",\"可以发现8001和8002交替出现，重启之后，又从1开始\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud.html\":{\"title\":\"2、Spring Cloud是啥\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Spring Cloud provides tools for developers to quickly build some of the common patterns in distributed systems (e.g. configuration management, service discovery, circuit breakers, intelligent routing, micro-proxy, control bus, one-time tokens, global locks, leadership election, distributed sessions, cluster state). Coordination of distributed systems leads to boiler plate patterns, and using Spring Cloud developers can quickly stand up services and applications that implement those patterns. They will work well in any distributed environment, including the developer’s own laptop, bare metal data centres, and managed platforms such as Cloud Foundry.\",\"Spring Cloud为开发人员提供了工具，以快速构建分布式系统中的一些常见模式（例如，配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁，领导选举，分布式会话，群集状态）。分布式系统的协调导致样板式样，并且使用Spring Cloud开发人员可以快速站起来实现这些样板的服务和应用程序。它们将在任何分布式环境中都能很好地工作，包括开发人员自己的笔记本电脑，裸机数据中心以及诸如Cloud Foundry之类的托管平台.\",\"特性\",\"分布式/版本化配置\",\"服务注册和发现\",\"路由\",\"服务与服务间的调用\",\"负载均衡\",\"断路器\",\"全局锁\",\"领导选举与集群状态\",\"分布式消息传递\"]},{\"header\":\"1、Spring Cloud版本选择\",\"slug\":\"_1、spring-cloud版本选择\",\"contents\":[]},{\"header\":\"1、版本关系\",\"slug\":\"_1、版本关系\",\"contents\":[\"Spring Cloud 采用了英国伦敦地铁站的名称来命名，并由地铁站名称字母A-Z依次类推的形式来发布迭代版本\",\"SpringCloud是一个由许多子项目组成的综合项目，各子项目有不同的发布节奏。为了管理SpringCloud与各子项目的版本依赖关系，发布了一个清单，其中包括了某个SpringCloud版本对应的子项目版本。为了避免SpringCloud版本号与子项目版本号混淆，SpringCloud版本采用了名称而非版本号的命名，这些版本的名字采用了伦敦地铁站的名字，根据字母表的顺序来对应版本时间顺序。例如Angel是第一个版本, Brixton是第二个版本。\",\"当SpringCloud的发布内容积累到临界点或者一个重大BUG被解决后，会发布一个\\\"service releases\\\"版本，简称SRX版本，比如Greenwich.SR2就是SpringCloud发布的Greenwich版本的第2个SRX版本。\"]},{\"header\":\"2、Spring Cloud与Spring Boot依赖关系\",\"slug\":\"_2、spring-cloud与spring-boot依赖关系\",\"contents\":[\"官方：https://spring.io/projects/spring-cloud#overview\",\"更详细版本对应的查看：https://start.spring.io/actuator/info\",\"json格式化：截止（2021.04.13）\",\"{ \\\"git\\\": { \\\"branch\\\": \\\"709ff1cec88a124ef588f6185e5dc237766f3236\\\", \\\"commit\\\": { \\\"id\\\": \\\"709ff1c\\\", \\\"time\\\": \\\"2021-04-10T08:56:10Z\\\" } }, \\\"build\\\": { \\\"version\\\": \\\"0.0.1-SNAPSHOT\\\", \\\"artifact\\\": \\\"start-site\\\", \\\"versions\\\": { \\\"spring-boot\\\": \\\"2.4.4\\\", \\\"initializr\\\": \\\"0.11.0-SNAPSHOT\\\" }, \\\"name\\\": \\\"start.spring.io website\\\", \\\"time\\\": \\\"2021-04-12T11:22:19.209Z\\\", \\\"group\\\": \\\"io.spring.start\\\" }, \\\"bom-ranges\\\": { \\\"azure\\\": { \\\"2.2.4\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"3.2.0\\\": \\\"Spring Boot >=2.3.0.M1 and <2.4.0-M1\\\", \\\"3.3.0\\\": \\\"Spring Boot >=2.4.0.M1 and <2.5.0-M1\\\" }, \\\"codecentric-spring-boot-admin\\\": { \\\"2.2.4\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"2.3.1\\\": \\\"Spring Boot >=2.3.0.M1 and <2.5.0-M1\\\" }, \\\"solace-spring-boot\\\": { \\\"1.0.0\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"1.1.0\\\": \\\"Spring Boot >=2.3.0.M1 and <2.5.0-M1\\\" }, \\\"solace-spring-cloud\\\": { \\\"1.0.0\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"1.1.1\\\": \\\"Spring Boot >=2.3.0.M1 and <2.4.0-M1\\\", \\\"2.0.0\\\": \\\"Spring Boot >=2.4.0.M1 and <2.5.0-M1\\\" }, \\\"spring-cloud\\\": { \\\"Hoxton.SR10\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.10.BUILD-SNAPSHOT\\\", \\\"Hoxton.BUILD-SNAPSHOT\\\": \\\"Spring Boot >=2.3.10.BUILD-SNAPSHOT and <2.4.0.M1\\\", \\\"2020.0.0-M3\\\": \\\"Spring Boot >=2.4.0.M1 and <=2.4.0.M1\\\", \\\"2020.0.0-M4\\\": \\\"Spring Boot >=2.4.0.M2 and <=2.4.0-M3\\\", \\\"2020.0.0\\\": \\\"Spring Boot >=2.4.0.M4 and <=2.4.0\\\", \\\"2020.0.2\\\": \\\"Spring Boot >=2.4.1 and <2.5.0-M1\\\", \\\"2020.0.3-SNAPSHOT\\\": \\\"Spring Boot >=2.4.5-SNAPSHOT\\\" }, \\\"spring-cloud-alibaba\\\": { \\\"2.2.1.RELEASE\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\" }, \\\"spring-cloud-gcp\\\": { \\\"2.0.0\\\": \\\"Spring Boot >=2.4.0-M1 and <2.5.0-M1\\\" }, \\\"spring-cloud-services\\\": { \\\"2.2.6.RELEASE\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.RELEASE\\\", \\\"2.3.0.RELEASE\\\": \\\"Spring Boot >=2.3.0.RELEASE and <2.4.0-M1\\\", \\\"2.4.1\\\": \\\"Spring Boot >=2.4.0-M1 and <2.5.0-M1\\\" }, \\\"spring-geode\\\": { \\\"1.2.12.RELEASE\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"1.3.9.RELEASE\\\": \\\"Spring Boot >=2.3.0.M1 and <2.4.0-M1\\\", \\\"1.4.4\\\": \\\"Spring Boot >=2.4.0-M1 and <2.5.0-M1\\\", \\\"1.5.0-M3\\\": \\\"Spring Boot >=2.5.0-M1\\\" }, \\\"vaadin\\\": { \\\"14.5.2\\\": \\\"Spring Boot >=2.1.0.RELEASE and <2.5.0-M1\\\" }, \\\"wavefront\\\": { \\\"2.0.2\\\": \\\"Spring Boot >=2.1.0.RELEASE and <2.4.0-M1\\\", \\\"2.1.0\\\": \\\"Spring Boot >=2.4.0-M1\\\" } }, \\\"dependency-ranges\\\": { \\\"native\\\": { \\\"0.9.0\\\": \\\"Spring Boot >=2.4.3 and <2.4.4\\\", \\\"0.9.1\\\": \\\"Spring Boot >=2.4.4 and <2.4.5-SNAPSHOT\\\", \\\"0.9.2-SNAPSHOT\\\": \\\"Spring Boot >=2.4.5-SNAPSHOT and <2.5.0-M1\\\" }, \\\"okta\\\": { \\\"1.4.0\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.4.0-M1\\\", \\\"1.5.1\\\": \\\"Spring Boot >=2.4.0-M1 and <2.4.1\\\", \\\"2.0.1\\\": \\\"Spring Boot >=2.4.1 and <2.5.0-M1\\\" }, \\\"mybatis\\\": { \\\"2.1.4\\\": \\\"Spring Boot >=2.1.0.RELEASE and <2.5.0-M1\\\" }, \\\"camel\\\": { \\\"3.3.0\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"3.5.0\\\": \\\"Spring Boot >=2.3.0.M1 and <2.4.0-M1\\\", \\\"3.9.0\\\": \\\"Spring Boot >=2.4.0.M1 and <2.5.0-M1\\\" }, \\\"open-service-broker\\\": { \\\"3.1.1.RELEASE\\\": \\\"Spring Boot >=2.2.0.RELEASE and <2.3.0.M1\\\", \\\"3.2.0\\\": \\\"Spring Boot >=2.3.0.M1 and <2.4.0-M1\\\", \\\"3.3.0\\\": \\\"Spring Boot >=2.4.0-M1 and <2.5.0-M1\\\" } } } \"]},{\"header\":\"3、Spring Cloud组件的停更、升级、替换\",\"slug\":\"_3、spring-cloud组件的停更、升级、替换\",\"contents\":[]},{\"header\":\"4、环境版本定稿\",\"slug\":\"_4、环境版本定稿\",\"contents\":[\" <!-- 统一管理jar包版本 --> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <maven.compiler.source>1.8</maven.compiler.source> <maven.compiler.target>1.8</maven.compiler.target> <junit.version>4.12</junit.version> <log4j.version>1.2.17</log4j.version> <lombok.version>1.18.12</lombok.version> <mysql.version>8.0.21</mysql.version> <druid.version>1.2.4</druid.version> <mybatis.spring.boot.version>2.1.3</mybatis.spring.boot.version> </properties> <!-- 子模块继承之后，提供作用：锁定版本+子modlue不用写groupId和version --> <dependencyManagement> <dependencies> <!--spring boot 2.2.2--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-dependencies</artifactId> <version>2.3.3.RELEASE</version> <type>pom</type> <scope>import</scope> </dependency> <!--spring cloud Hoxton.SR1--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-dependencies</artifactId> <version>Hoxton.SR4</version> <type>pom</type> <scope>import</scope> </dependency> <!--spring cloud alibaba 2.1.0.RELEASE--> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-alibaba-dependencies</artifactId> <version>2.2.1.RELEASE</version> <type>pom</type> <scope>import</scope> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>${mysql.version}</version> </dependency> <dependency> <groupId>com.alibaba</groupId> <artifactId>druid</artifactId> <version>${druid.version}</version> </dependency> <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> <version>${mybatis.spring.boot.version}</version> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>${junit.version}</version> </dependency> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <version>${lombok.version}</version> <optional>true</optional> </dependency> </dependencies> </dependencyManagement> <build> <plugins> <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <configuration> <fork>true</fork> <addResources>true</addResources> </configuration> </plugin> </plugins> </build> \"]},{\"header\":\"1、maven工程复习\",\"slug\":\"_1、maven工程复习\",\"contents\":[\"dependencyManagement\",\"Maven 使用dependencyManagement 元素来提供了一种管理依赖版本号的方式。通常会在一个组织或者项目的最顶层的父POM 中看到dependencyManagement 元素。\",\"使用pom.xml 中的dependencyManagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号。 Maven 会沿着父子层次向上走，直到找到一个拥有dependencyManagement 元素的项目，然后它就会使用这个 dependencyManagement 元素中指定的版本号。\",\"例如：\",\"我们在父项目指定了版本，子项目就可以不用指定版本，直接用父项目的版本\",\"这样做的好处就是：如果有多个子项目都引用同一样依赖，则可以避免在每个使用的子项目里都声明一个版本号，\",\"这样当想升级或切换到另一个版本时，只需要在顶层父容器里更新，而不需要一个一个子项目的修改 ；另外如果\",\"某个子项目需要另外的一个版本，只需要声明version就可。\",\"dependencyManagement里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。 \",\"如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本， 才会从父项目中继承该项，并且version和scope都读取自父pom;\",\"如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 \"]},{\"header\":\"2、mysql驱动说明\",\"slug\":\"_2、mysql驱动说明\",\"contents\":[\"com.mysql.jdbc.Driver和mysql-connector-java 5一起用。\",\"com.mysql.cj.jdbc.Driver和mysql-connector-java 6 一起用。\",\"com.mysql.cj.jdbc.Driver是mysql-connector-java 6 中的特性，相比mysql-connector-java 5 多了一个时区：\",\"serverTimezone，把数据源配置的驱动改一下就好了\",\"org.gjt.mm.mysql.Driver是当时最好的MySQL JDBC，但不是MySQL公司的，然后MySQL将MM的JDBC驱动 收为\",\"官方的JDBC驱动，所以将驱动的package也改了，但还保留了org.gjt.mm.mysql.Driver这个路径的引用，也就\",\"是你使用新版的JDBC驱动时还可以通过这个来引用，打开下载的新版JDBC驱动的jar文件可以看到，只有一个\",\"文件的目录是org.gjt.mm.mysql，就是为了兼容而设计的\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud_bus.html\":{\"title\":\"11、Spring Cloud Bus消息总线\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"Spring Cloud Bus将分布式系统的节点与轻量级消息代理链接。这可以用于广播状态更改（例如配置更改）或其他管理指令。一个关键的想法是，Bus就像一个扩展的Spring Boot应用程序的分布式执行器，但也可以用作应用程序之间的通信渠道。当前唯一的实现是使用AMQP代理作为传输，但是相同的基本功能集（还有一些取决于传输）在其他传输的路线图上。\",\"分布式自动刷新配置功能\",\"配合Spring Cloud Config实现配置的动态的刷新\",\"Bus支持的两种消息代码：\",\"RabbitMQ\",\"kafka\"]},{\"header\":\"1、作用\",\"slug\":\"_1、作用\",\"contents\":[\"Spring Cloud Bus能管理和传播分布式系统间的消息，就像一个分布式执行器，可用于广播状态更改、事件推送等，也可以当作微服务间的通信通道。\"]},{\"header\":\"2、为何叫做总线\",\"slug\":\"_2、为何叫做总线\",\"contents\":[]},{\"header\":\"什么是总线\",\"slug\":\"什么是总线\",\"contents\":[\"在微服务架构的系统中，通常会使用轻量级的消息代理来构建一个共用的消息主题，并让系统中所有微服务实例都连接上来。由于该主题中产生的消息会被所有实例监听和消费，所以称它为消息总线。在总线上的各个实例，都可以方便地广播一些需要让其他连接在该主题上的实例都知道的消息。\"]},{\"header\":\"基本原理\",\"slug\":\"基本原理\",\"contents\":[\"ConfigClient实例都监听MQ中同一个topic(默认是springCloudBus)。当一个服务刷新数据的时候，它会把这个信息放入到Topic中，这样其它监听同一Topic的服务就能得到通知，然后去更新自身的配置。\",\"https://www.bilibili.com/video/av55976700?from=search&seid=15010075915728605208\"]},{\"header\":\"2、 RabbitMQ 安装与配置\",\"slug\":\"_2、rabbitmq安装与配置\",\"contents\":[\"参考：\",\"https://www.huaweicloud.com/articles/8b843c2fd653346beba18d2b21af0c3d.html\",\"https://cloud.tencent.com/developer/article/1582235\"]},{\"header\":\"3、Spring Cloud Bus动态刷新全局广播\",\"slug\":\"_3、spring-cloud-bus动态刷新全局广播\",\"contents\":[\"切记：必须安装好rabbitMQ的环境\"]},{\"header\":\"1、新建一个module3366\",\"slug\":\"_1、新建一个module3366\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-cloud-config-client-3366 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-config</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml( bootstrap.yaml )\",\"slug\":\"_3、写yaml-bootstrap-yaml\",\"contents\":[\"server: port: 3366 spring: application: name: config-client cloud: #Config客户端配置 config: label: master #分支名称 name: config #配置文件名称 profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取http://config-3344.com:3344/master/config-dev.yml uri: http://localhost:3344 #配置中心地址 #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka ## 暴露监控端点 management: endpoints: web: exposure: include: \\\"*\\\" \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@EnableEurekaClient @SpringBootApplication public class BusConfigClient3366 { public static void main(String[] args) { SpringApplication.run(BusConfigClient3366.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"controller\",\"@RestController @RefreshScope public class BusClientController { @Value(\\\"${server.port}\\\") private String serverPort; @Value(\\\"${spring.cloud.nacos.config.server-addr}\\\") private String configInfo; @GetMapping(\\\"/configInfo\\\") public String configInfo() { return \\\"serverPort: \\\"+serverPort+\\\"\\\\t\\\\n\\\\n configInfo: \\\"+configInfo; } } \"]},{\"header\":\"2、设计思想\",\"slug\":\"_2、设计思想\",\"contents\":[\"利用消息总线触发一个客户端/bus/refresh，而刷新客户端所有配置\",\"利用消息总线触发一个服务端ConfigServer的/bus/refresh端点，而刷新所有客户端配置\",\"**推荐：**第二个思想更合适一点，第一个不适合原因如下：\",\"打破了微服务的单一职责性，因为微服务本身是业务模块，它本不应该承担配置刷新的职责\",\"破坏了微服务各节点的对等性\",\"有一定的局限性。比如微服务在迁移时，它的网络地址时常发生变化，这时要想做到自动刷新，那就会增加更多的修改\"]},{\"header\":\"3、 3344服务端增加消息总线\",\"slug\":\"_3、3344服务端增加消息总线\",\"contents\":[]},{\"header\":\"1、pom文件增加依赖\",\"slug\":\"_1、pom文件增加依赖\",\"contents\":[\"<!--添加消息总线RabbitMQ支持--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-bus-amqp</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> \"]},{\"header\":\"2、yaml增加配置\",\"slug\":\"_2、yaml增加配置\",\"contents\":[\"server: port: 3344 spring: application: name: cloud-config-center #注册进Eureka服务器的微服务名 cloud: config: server: git: uri: https://github.com/yhx1001/Spring-Cloud-Config-Test.git #GitHub上面的git仓库名字 #rabbit配置 rabbitmq: host: localhost port: 5672 username: guest password: guest #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka ##rabbitmq相关配置,暴露bus刷新配置的端点 management: endpoints: #暴露bus刷新配置的端点 web: exposure: include: 'bus-refresh' \"]},{\"header\":\"4、3355客户端增加消息总线\",\"slug\":\"_4、3355客户端增加消息总线\",\"contents\":[]},{\"header\":\"1、pom文件增加依赖\",\"slug\":\"_1、pom文件增加依赖-1\",\"contents\":[\"<!--添加消息总线RabbitMQ支持--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-bus-amqp</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> \"]},{\"header\":\"2、yaml增加配置\",\"slug\":\"_2、yaml增加配置-1\",\"contents\":[\"server: port: 3355 spring: application: name: config-client cloud: #Config客户端配置 config: label: master #分支名称 name: config #配置文件名称 profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取http://config-3344.com:3344/master/config-dev.yml uri: http://localhost:3344 #配置中心地址 #rabbitmq相关配置 15672是Web管理界面的端口；5672是MQ访问的端口 rabbitmq: host: localhost port: 5672 username: guest password: guest #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka ## 暴露监控端点 ## 暴露监控端点 management: endpoints: web: exposure: include: \\\"*\\\" ## 'refresh' \"]},{\"header\":\"5、3366客户端增加消息总线\",\"slug\":\"_5、3366客户端增加消息总线\",\"contents\":[]},{\"header\":\"1、pom文件增加依赖\",\"slug\":\"_1、pom文件增加依赖-2\",\"contents\":[\"<!--添加消息总线RabbitMQ支持--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-bus-amqp</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> \"]},{\"header\":\"2、yaml增加配置\",\"slug\":\"_2、yaml增加配置-2\",\"contents\":[\"server: port: 3366 spring: application: name: config-client cloud: #Config客户端配置 config: label: master #分支名称 name: config #配置文件名称 profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取http://config-3344.com:3344/master/config-dev.yml uri: http://localhost:3344 #配置中心地址 #rabbitmq相关配置 15672是Web管理界面的端口；5672是MQ访问的端口 rabbitmq: host: localhost port: 5672 username: guest password: guest #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka ## 暴露监控端点 management: endpoints: web: exposure: include: \\\"*\\\" \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"启动3355，3366报错\",\"org.springframework.context.ApplicationContextException: Failed to start bean 'webServerStartStop'; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat server \",\"升级依赖：https://spring.io/projects/spring-cloud#overview\",\"修改GitHub配置文件\",\"curl -X POST \\\"http://localhost:3344/actuator/bus ，一次发送处处生效\",\"服务端：http://localhost:3344/config-dev.yml\",\"客户端：http://localhost:3355/configInfo\",\"客户端：http://localhost:3366/configInfo\",\"获取配置信息，发现都已经刷新了\"]},{\"header\":\"4、Spring Cloud Bus动态刷新定点通知\",\"slug\":\"_4、spring-cloud-bus动态刷新定点通知\",\"contents\":[\"不想全部通知，只通知一部分\",\"指定具体一个实例生效而不是全部\",\"公式：http://localhost:配置中心的端口号/actuator/bus-refresh/{destination}\",\"/bus/refresh请求不再发送到具体的服务实例上，而是发给config server并通过destination参数类指定需要更新配置的服务或实例\",\"案例：以刷新运行在3355端口上的config-client为例\",\"curl -X POST \\\"http://localhost:3344/actuator/bus-refresh/config-client:3355\\\"\",\"总结\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud_config.html\":{\"title\":\"10、Spring Cloud Config分布式配置中心\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"分布式系统面临的问题--配置问题\",\"微服务意味着要将单体应用中的业务拆分成一个个子服务，每个服务的粒度相对较小，因此系统中会出现大量的服务。由于每个服务都需要必要的配置信息才能运行，所以一套集中式的、动态的配置管理设施是必不可少的。\",\"SpringCloud提供了ConfigServer来解决这个问题，我们每一个微服务自己带着一个application.yml，上百个配置文件的管理......\"]},{\"header\":\"1、什么是 Spring Cloud Config\",\"slug\":\"_1、什么是spring-cloud-config\",\"contents\":[\"SpringCloud Config为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置。\",\"SpringCloud Config分为服务端和客户端两部分。\",\"服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口\",\"客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。\"]},{\"header\":\"1、作用\",\"slug\":\"_1、作用\",\"contents\":[\"集中管理配置文件\",\"不同环境不同配置，动态化的配置更新，分环境部署，比如/dev、/test、/prod、/beta、/release\",\"运行期间动态调整配置，不再需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息\",\"当配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置\",\"将配置信息以REST接口的形式暴露 \",\"post、curl访问刷新即可\"]},{\"header\":\"2、与 GitHub 的整合\",\"slug\":\"_2、与github的整合\",\"contents\":[\"Spring Cloud Config默认使用Git来存储配置文件（也有其他方式，比如SVN和本地文件），但最推荐还是Git，而且使用的是http/https访问的形式。\"]},{\"header\":\"3、官网\",\"slug\":\"_3、官网\",\"contents\":[\"https://docs.spring.io/spring-cloud-config/docs/current/reference/html/\"]},{\"header\":\"2、Config服务端配置与测试\",\"slug\":\"_2、config服务端配置与测试\",\"contents\":[]},{\"header\":\"1、在GitHub创建仓库\",\"slug\":\"_1、在github创建仓库\",\"contents\":[\"同时，新建分支dev，yml的内容随便填写\"]},{\"header\":\"2、整合\",\"slug\":\"_2、整合\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-cloud-config-center-3344 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-config-server</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 3344 spring: application: name: cloud-config-center #注册进Eureka服务器的微服务名 cloud: config: server: git: uri: https://github.com/yhx1001/Spring-Cloud-Config-Test.git #GitHub上面的git仓库名字 #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@EnableConfigServer @EnableEurekaClient @SpringBootApplication public class ConfigApplication3344 { public static void main(String[] args) { SpringApplication.run(ConfigApplication3344.class, args); } } \"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"http://localhost:3344/master/config-dev.yml\",\"http://localhost:3344/config-dev.yml\",\"http://localhost:3344/config/dev/dev\"]},{\"header\":\"3、配置读取规则\",\"slug\":\"_3、配置读取规则\",\"contents\":[\"/{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properti \",\"其中“应用程序”作为SpringApplication中的spring.config.name注入（即常规Spring Boot应用程序中通常为“应用程序”），“配置文件”是活动配置文件（或逗号分隔列表）的属性），“label”是可选的git标签（默认为“master”）。\"]},{\"header\":\"1、 /{application}/{profile}[/{label}]\",\"slug\":\"_1、-application-profile-label\",\"contents\":[\"以刚刚测试的地址来测试：\",\"master分支 \",\"http://localhost:3344/config/dev/master\",\"http://localhost:3344/config/pro/master\",\"http://localhost:3344/config/test/master\",\"dev分支 \",\"http://localhost:3344/config/dev/dev\",\"http://localhost:3344/config/pro/dev\",\"http://localhost:3344/config/test/dev\"]},{\"header\":\"2、 /{application}-{profile}.yml\",\"slug\":\"_2、-application-profile-yml\",\"contents\":[\"http://localhost:3344/config-dev.yml\",\"http://localhost:3344/config-pro.yml\",\"http://localhost:3344/config-test.yml\"]},{\"header\":\"3、 /{label}/{application}-{profile}.yml\",\"slug\":\"_3、-label-application-profile-yml\",\"contents\":[\"master \",\"http://localhost:3344/master/config-dev.yml\",\"http://localhost:3344/master/config-pro.yml\",\"http://localhost:3344/master/config-test.yml\",\"dev分支 \",\"http://localhost:3344/dev/config-dev.yml\",\"http://localhost:3344/dev/config-pro.yml\",\"http://localhost:3344/dev/config-test.yml\"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[\"/{name}-{profiles}.yml\",\"/{label}-{name}-{profiles}.yml\",\"label：分支(branch)\",\"name ：服务名\",\"profiles：环境(dev/test/prod)\"]},{\"header\":\"3、config客户端配置与测试\",\"slug\":\"_3、config客户端配置与测试\",\"contents\":[]},{\"header\":\"1、案例\",\"slug\":\"_1、案例\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-cloud-config-client-3355 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-1\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-config</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml-1\",\"contents\":[\"这里写的是bootstrap.yaml\",\"server: port: 3355 spring: application: name: config-client cloud: #Config客户端配置 config: label: master #分支名称 name: config #配置文件名称 profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取http://config-3344.com:3344/master/config-dev.yml uri: http://localhost:3344 #配置中心地址k #服务注册到eureka地址 eureka: client: service-url: defaultZone: http://localhost:7001/eureka \"]},{\"header\":\"1、什么是bootstrap.yml\",\"slug\":\"_1、什么是bootstrap-yml\",\"contents\":[\"applicaiton.yml是用户级的资源配置项\",\"bootstrap.yml是系统级的，优先级更加高\",\"Spring Cloud会创建一个“Bootstrap Context”，作为Spring应用的Application Context的父上下文。初始化的时候，Bootstrap Context负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的Environment。\",\"Bootstrap属性有高优先级，默认情况下，它们不会被本地配置覆盖。 Bootstrap context和Application Context有着不同的约定，所以新增了一个bootstrap.yml文件，保证Bootstrap Context和Application Context配置的分离。\",\"要将Client模块下的application.yml文件改为bootstrap.yml,这是很关键的，\",\"因为bootstrap.yml是比application.yml先加载的。bootstrap.yml优先级高于application.yml\"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-1\",\"contents\":[\"@SpringBootApplication @EnableEurekaClient public class ConfigClientApplication3355 { public static void main(String[] args) { SpringApplication.run(ConfigClientApplication3355.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"controller\",\"@RestController public class ConfigClientController { @Value(\\\"${spring.cloud.nacos.config.server-addr}\\\") private String configInfo; @GetMapping(\\\"/configInfo\\\") public String getConfigInfo() { return configInfo; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"启动3344自测\",\"启动3355\",\"测试地址：http://localhost:3344/config/dev/master\",\"http://localhost:3355/configInfo\",\"成功实现了客户端3355访问3344通过GitHub获取配置信息\",\"存在的问题：动态刷新\",\"运维人员修改GitHub的配置\",\"刷新3344，发现config server立即响应\",\"刷新3355，发现没有任何反应\",\"3355除非重新启动\"]},{\"header\":\"4、config客户端动态刷新\",\"slug\":\"_4、config客户端动态刷新\",\"contents\":[]},{\"header\":\"1、动态刷新\",\"slug\":\"_1、动态刷新\",\"contents\":[]},{\"header\":\"1、修改3355pom\",\"slug\":\"_1、修改3355pom\",\"contents\":[\"引入spring-boot-starter-actuator\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> \"]},{\"header\":\"2、修改yml，暴露监控端口\",\"slug\":\"_2、修改yml-暴露监控端口\",\"contents\":[\"## 暴露监控端点 management: endpoints: web: exposure: include: \\\"*\\\" \"]},{\"header\":\"3、修改GitHub文件\",\"slug\":\"_3、修改github文件\",\"contents\":[\"http://localhost:3355/configInfo 没有任何反应\"]},{\"header\":\"4、运维人员激活3355\",\"slug\":\"_4、运维人员激活3355\",\"contents\":[\"发送POST请求\",\"curl -X POST \\\"http://localhost:3355/actuator/refresh\\\"\"]},{\"header\":\"5、再次请求\",\"slug\":\"_5、再次请求\",\"contents\":[\"http://localhost:3355/configInfo\",\"成功刷到最新配置内容--避免了服务重启\",\"可否广播，一次通知，处处生效？\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud_gateWay.html\":{\"title\":\"9、Spring Cloud GateWay服务网关\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"官网：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/\",\"该项目提供了一个用于在Spring WebFlux之上构建API网关的库。Spring Cloud Gateway旨在提供一种简单而有效的方法来路由到API，并为它们提供跨领域关注，例如：安全性，监视/指标和弹性。\"]},{\"header\":\"1、什么是 GateWay\",\"slug\":\"_1、什么是gateway\",\"contents\":[\"Cloud全家桶中有个很重要的组件就是网关，在1.x版本中都是采用的Zuul网关；\",\"但在2.x版本中，zuul的升级一直跳票，SpringCloud最后自己研发了一个网关替代Zuul，\",\"那就是SpringCloud Gateway一句话：gateway是原zuul1.x版的替代\"]},{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"提供了一个在Spring生态系统之上构建的API网关，包括：Spring 5，Spring Boot 2和Project Reactor\",\"pringCloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在Spring Cloud 2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 1.x非Reactor模式的老版本。\",\"而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。\",\"Spring Cloud Gateway的目标提供统一的路由方式且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。\"]},{\"header\":\"2、特性\",\"slug\":\"_2、特性\",\"contents\":[\"反向代理\",\"鉴权\",\"流量控制\",\"熔断\",\"日志监控\"]},{\"header\":\"3、微服务的网关在哪\",\"slug\":\"_3、微服务的网关在哪\",\"contents\":[]},{\"header\":\"4、有 Zuul ，为什么又出现了 GateWay\",\"slug\":\"_4、有zuul-为什么又出现了gateway\",\"contents\":[]},{\"header\":\"1、为什么我们选择 GateWay\",\"slug\":\"_1、为什么我们选择gateway\",\"contents\":[\"netflix不靠谱，迟迟不发布\",\"方面因为Zuul1.0已经进入了维护阶段，而且Gateway是SpringCloud团队研发的，是亲儿子产品，值得信赖。而且很多功能Zuul都没有用起来也非常的简单便捷。\",\"Gateway是基于异步非阻塞模型上进行开发的，性能方面不需要担心。虽然Netflix早就发布了最新的 Zuul 2.x，但 Spring Cloud 貌似没有整合计划。而且Netflix相关组件都宣布进入维护期；不知前景如何？\",\"多方面综合考虑Gateway是很理想的网关选择。\",\"Spring Cloud GateWay有很多特性\",\"基于Spring Framework 5, Project Reactor 和 Spring Boot 2.0 进行构建；\",\"动态路由：能够匹配任何请求属性；\",\"可以对路由指定 Predicate（断言）和 Filter（过滤器）；\",\"集成Hystrix的断路器功能；\",\"集成 Spring Cloud 服务发现功能；\",\"易于编写的 Predicate（断言）和 Filter（过滤器）；\",\"请求限流功能；\",\"支持路径重写。\",\"Spring Cloud Gateway 与 Zuul的区别\",\"在SpringCloud Finchley 正式版之前，Spring Cloud 推荐的网关是 Netflix 提供的Zuul：\",\"Zuul 1.x，是一个基于阻塞 I/ O 的 API Gateway\",\"Zuul 1.x 基于Servlet 2. 5使用阻塞架构它不支持任何长连接(如 WebSocket) Zuul 的设计模式和Nginx较像，每次 I/ O 操作都是从工作线程中选择一个执行，请求线程被阻塞到工作线程完成，但是差别是Nginx 用C++ 实现，Zuul 用 Java 实现，而 JVM 本身会有第一次加载较慢的情况，使得Zuul 的性能相对较差。\",\"Zuul 2.x理念更先进，想基于Netty非阻塞和支持长连接，但SpringCloud目前还没有整合。 Zuul 2.x的性能较 Zuul 1.x 有较大提升。在性能方面，根据官方提供的基准测试， Spring Cloud Gateway 的 RPS（每秒请求数）是Zuul 的 1. 6 倍。\",\"Spring Cloud Gateway 建立 在 Spring Framework 5、 Project Reactor 和 Spring Boot 2 之上， 使用非阻塞 API。\",\"Spring Cloud Gateway 还 支持 WebSocket， 并且与Spring紧密集成拥有更好的开发体验\"]},{\"header\":\"2、Zuul 1.x模型\",\"slug\":\"_2、zuul-1-x模型\",\"contents\":[\"Springcloud中所集成的Zuul版本，采用的是Tomcat容器，使用的是传统的Servlet IO处理模型。\",\"学web课程都知道一个题目，Servlet的生命周期?servlet由servlet container进行生命周期管理。\",\"container启动时构造servlet对象并调用servlet init()进行初始化；\",\"container运行时接受请求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用service()。\",\"container关闭时调用servlet destory()销毁servlet；\",\"上述模式的缺点：\",\"servlet是一个简单的网络IO模型，当请求进入servlet container时，servlet container就会为其绑定一个线程，在并发不高的场景下这种模型是适用的。但是一旦高并发(比如抽风用jemeter压)，线程数量就会上涨，而线程资源代价是昂贵的（上线文切换，内存消耗大）严重影响请求的处理时间。在一些简单业务场景下，不希望为每个request分配一个线程，只需要1个或几个线程就能应对极大并发的请求，这种业务场景下servlet模型没有优势\",\"所以Zuul 1.X是基于servlet之上的一个阻塞式处理模型，即spring实现了处理所有request请求的一个\",\"servlet（DispatcherServlet）并由该servlet阻塞式处理处理。所以Springcloud Zuul无法摆脱servlet模型的弊端\"]},{\"header\":\"3、GateWay模型\",\"slug\":\"_3、gateway模型\",\"contents\":[\"传统的Web框架，比如说：struts2，springmvc等都是基于Servlet API与Servlet容器基础之上运行的。但是\",\"在Servlet3.1之后有了异步非阻塞的支持。而WebFlux是一个典型非阻塞异步的框架，它的核心是基于Reactor的相关API实现的。相对于传统的web框架来说，它可以运行在诸如Netty，Undertow及支持Servlet3.1的容器上。非阻塞式+函数式编程（Spring5必须让你使用java8）\",\"Spring WebFlux 是 Spring 5.0 引入的新的响应式框架，区别于 Spring MVC，它不需要依赖Servlet API，它是完全异步非阻塞的，并且基于 Reactor 来实现响应式流规范。\"]},{\"header\":\"2、核心概念\",\"slug\":\"_2、核心概念\",\"contents\":[]},{\"header\":\"1、 Route 路由\",\"slug\":\"_1、route路由\",\"contents\":[\"路由是构建网关的基本模块，它由ID、目标url，一系列的断言和过滤组成，如果断言为true则匹配该路由\"]},{\"header\":\"2、 Predicate 断言\",\"slug\":\"_2、predicate断言\",\"contents\":[\"参考Java8的函数式编程\",\"开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数）， 如果请求与断言匹配则进行路由\"]},{\"header\":\"3、 Filter 过滤\",\"slug\":\"_3、filter过滤\",\"contents\":[\"Spring框架中GateWayFilter的实例，使用过滤器，可以在请求被路由前或之后队请求进行修改\",\"总结：\",\"web请求，通过一些匹配条件，定位到真正的服务节点。并在这个转发过程的前后，进行一些精细化控制。\",\"predicate就是我们的匹配条件；\",\"而filter，就可以理解为一个无所不能的拦截器。有了这两个元素，再加上目标uri，就可以实现一个具体的路由了\"]},{\"header\":\"3、GateWay工作流程\",\"slug\":\"_3、gateway工作流程\",\"contents\":[\"客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。\",\"Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。\",\"过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。\",\"Filter在“pre”类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等，\",\"在“post”类型的过滤器中可以做响应内容、响应头的修改，日志的输出，流量监控等有着非常重要的作用。\",\"核心逻辑：路由转发+执行过滤链\"]},{\"header\":\"4、入门配置\",\"slug\":\"_4、入门配置\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-gateway-gateway9527 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <!--gateway--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-gateway</artifactId> </dependency> <!--eureka-client--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <dependency> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!--一般基础配置类--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \",\"注意：spring-cloud-starter-gateway的依赖，是不需要web的依赖\"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 9527 eureka: instance: hostname: cloud-gateway-service client: #服务提供者provider注册进eureka服务列表内 service-url: register-with-eureka: true fetch-registry: true defaultZone: http://eureka7001.com:7001/eureka \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication @EnableEurekaClient public class GateWayApplication { public static void main(String[] args) { SpringApplication.run(GateWayApplication.class,args); } } \"]},{\"header\":\"5、增加yaml的配置\",\"slug\":\"_5、增加yaml的配置\",\"contents\":[\"作为路由映射\",\"server: port: 9527 spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 eureka: instance: hostname: cloud-gateway-service client: #服务提供者provider注册进eureka服务列表内 service-url: register-with-eureka: true fetch-registry: true defaultZone: http://eureka7001.com:7001/eureka \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"启动顺序：\",\"启动7001\",\"启动8001\",\"启动9527\",\"访问：\",\"添加网关前：http://localhost:8001/payment/selectOne/1\",\"添加网关后：http://localhost:9527/payment/selectOne/1\"]},{\"header\":\"7、 gateWay 网关配置的两种方式\",\"slug\":\"_7、gateway网关配置的两种方式\",\"contents\":[]},{\"header\":\"1、yaml配置，前面我们用到的就是\",\"slug\":\"_1、yaml配置-前面我们用到的就是\",\"contents\":[]},{\"header\":\"2、代码中注入 RouteLocator 的 Bean\",\"slug\":\"_2、代码中注入routelocator的bean\",\"contents\":[\"官网：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#configuration\",\"@Configuration public class GateWayConfig { @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder routeBuilder){ return routeBuilder.routes().route(\\\"xiaobear-config\\\",r -> r.path(\\\"/spring-cloud-gateway-configuration\\\").uri(\\\"https://www.bilibili.com/video/BV18E411x7eT\\\")).build(); } } \"]},{\"header\":\"5、动态路由\",\"slug\":\"_5、动态路由\",\"contents\":[\"通过微服务名实现动态路由，默认情况下GateWay会根据注册中心注册的服务列表，以注册中心上微服务名为路径创建动态路由进行转发，从而实现动态路由的功能。\",\"启动：\",\"一个Eureka7001\",\"两个服务提供者8001，8002\"]},{\"header\":\"1、修改yaml\",\"slug\":\"_1、修改yaml\",\"contents\":[\"server: port: 9527 spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 eureka: instance: hostname: cloud-gateway-service client: #服务提供者provider注册进eureka服务列表内 service-url: register-with-eureka: true fetch-registry: true defaultZone: http://eureka7001.com:7001/eureka \",\"注意：uri的协议为lb，表示启用GateWay的负载均衡功能\",\"lb://serviceName是Spring Cloud GateWay在微服务中自动为我们创建的负载均衡uri\"]},{\"header\":\"2、测试\",\"slug\":\"_2、测试\",\"contents\":[\"测试地址：http://localhost:9527/payment/payment/lb\",\"发现在端口8001与8002来回切换\"]},{\"header\":\"6、 Predicate 断言\",\"slug\":\"_6、predicate断言\",\"contents\":[\"在启动网关模块的时候，控制输出如下：\",\"官网地址：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories\"]},{\"header\":\"1、常用的 Route Predicate\",\"slug\":\"_1、常用的route-predicate\",\"contents\":[\"Spring Cloud Gateway将路由匹配作为Spring WebFlux HandlerMapping基础架构的一部分。\",\"Spring Cloud Gateway包括许多内置的Route Predicate工厂。所有这些Predicate都与HTTP请求的不同属性匹配。多个Route Predicate工厂可以进行组合\",\"Spring Cloud Gateway 创建 Route 对象时， 使用 RoutePredicateFactory 创建 Predicate 对象，Predicate 对象可以赋值给 Route。 Spring Cloud Gateway 包含许多内置的Route Predicate Factories。\",\"所有这些谓词都匹配HTTP请求的不同属性。多种谓词工厂可以组合，并通过逻辑and。\"]},{\"header\":\"1、 After Route Predicate Factory\",\"slug\":\"_1、after-route-predicate-factory\",\"contents\":[\"表示在这个时区里的时间点才允许访问\",\"这个时间格式，是Java 8新特性的时区时间格式\",\"public class ZoneDateTimeTest { public static void main(String[] args) { ZonedDateTime zbj = ZonedDateTime.now(); // 默认时区 System.out.println(zbj); // ZonedDateTime zny = ZonedDateTime.now(ZoneId.of(\\\"America/New_York\\\")); // 用指定时区获取当前时间 // System.out.println(zny); } } \",\"server: port: 9527 spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 - After=2021-05-24T15:17:53.623+08:00[Asia/Shanghai] ## 断言，路径相匹配的进行路由 eureka: instance: hostname: cloud-gateway-service client: #服务提供者provider注册进eureka服务列表内 service-url: register-with-eureka: true fetch-registry: true defaultZone: http://eureka7001.com:7001/eureka \"]},{\"header\":\"2、 Before Route Predicate Factory\",\"slug\":\"_2、before-route-predicate-factory\",\"contents\":[\"在时间点前才允许访问\",\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 - Before=2021-05-24T15:17:53.623+08:00[Asia/Shanghai] \"]},{\"header\":\"3、 Between Route Predicate Factory\",\"slug\":\"_3、between-route-predicate-factory\",\"contents\":[\"在两者之间\",\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 - Between=2021-05-24T15:17:53.623+08:00[Asia/Shanghai], 2021-05-24T15:30:53.623+08:00[Asia/Shanghai] \"]},{\"header\":\"4、 Cookie Route Predicate Factory\",\"slug\":\"_4、cookie-route-predicate-factory\",\"contents\":[\"Cookie Route Predicate需要两个参数，一个是 Cookie name ,一个是正则表达式。\",\"路由规则会通过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由，如果没有匹配上则不执行\",\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 - Cookie=uname, xiaobear \"]},{\"header\":\"1、不带cookie访问\",\"slug\":\"_1、不带cookie访问\",\"contents\":[\"curl http://localhost:9527/payment/payment/lb\"]},{\"header\":\"2、带cookie访问\",\"slug\":\"_2、带cookie访问\",\"contents\":[\"curl http://localhost:9527/payment/payment/lb --cookie \\\"uname=xiaobear\\\"\"]},{\"header\":\"5、 Header Route Predicate Factory\",\"slug\":\"_5、header-route-predicate-factory\",\"contents\":[\"两个参数：一个是属性名称和一个正则表达式，这个属性值和正则表达式匹配则执行。\",\"curl http://localhost:9526/payment/payment/lb -H \\\"X-Request-Id:123\\\"\",\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 - Header=X-Request-Id, \\\\d+ ## 请求头要有X-Request-Id属性并且值为整数的正则表达式 #- Cookie=uname, xiaobear \"]},{\"header\":\"6、 Host Route Predicate Factory\",\"slug\":\"_6、host-route-predicate-factory\",\"contents\":[\"Host Route Predicate 接收一组参数，一组匹配的域名列表，这个模板是一个 ant 分隔的模板，用.号作为分隔符。它通过参数中的主机地址作为匹配规则。\",\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 #- Header=X-Request-Id, \\\\d+ ## 请求头要有X-Request-Id属性并且值为整数的正则表达式 #- Cookie=uname, xiaobear - Host=**.xiaobear.com \",\"其他的请参照官网：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories\",\"总结：Predicate就是为了实现一组匹配规则，让请求找到对应的Route进行处理\"]},{\"header\":\"7、 Filter 过滤\",\"slug\":\"_7、filter过滤\",\"contents\":[\"https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gatewayfilter-factories\",\"路由过滤器可用于修改进入的HTTP请求和返回的HTTP响应，路由过滤器只能指定路由进行使用。\",\"Spring Cloud Gateway 内置了多种路由过滤器，他们都由GatewayFilter的工厂类来产生。\"]},{\"header\":\"1、分类\",\"slug\":\"_1、分类\",\"contents\":[]},{\"header\":\"1、按生命周期分类\",\"slug\":\"_1、按生命周期分类\",\"contents\":[\"Pre\",\"Post\"]},{\"header\":\"2、按种类分类\",\"slug\":\"_2、按种类分类\",\"contents\":[\"GatewayFilter Factories\",\"Global Filters\"]},{\"header\":\"2、常用的 Filter\",\"slug\":\"_2、常用的filter\",\"contents\":[\"spring: application: name: xiaobear-cloud-gateway cloud: gateway: routes: - id: payment_routh #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: http://localhost:8001 #匹配后提供服务的路由地址 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE predicates: - Path=/payment/selectOne/** ## 断言，路径相匹配的进行路由 - id: payment_routh2 #payment_route #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://XIAOBEAR-CLOUD-PAYMENT-SERVICE filters: - AddRequestParameter=X-Request-Id,1024 #过滤器工厂会在匹配的请求头加上一对请求头，名称为X-Request-Id值为1024 #uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/payment/lb/** ## 断言，路径相匹配的进行路由 #- Header=X-Request-Id, \\\\d+ ## 请求头要有X-Request-Id属性并且值为整数的正则表达式 #- Cookie=uname, xiaobear - Host=**.xiaobear.com \"]},{\"header\":\"3、自定义过滤器\",\"slug\":\"_3、自定义过滤器\",\"contents\":[\"自定义全局过滤器Global Filters\",\"主要是实现两个接口 implements GlobalFilter,Ordered\"]},{\"header\":\"作用\",\"slug\":\"作用\",\"contents\":[\"全局日志处理\",\"统一网关鉴权\",\"@Component public class XiaoBearGateWayFilter implements GlobalFilter, Ordered { private static final Logger log = LoggerFactory.getLogger(XiaoBearGateWayFilter.class); @Override public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) { log.info(\\\"time:\\\"+new Date()+\\\"\\\\t 执行了自定义的全局过滤器: \\\"+\\\"MyLogGateWayFilter\\\"+\\\"hello\\\"); String uname = exchange.getRequest().getQueryParams().getFirst(\\\"uname\\\"); if (uname == null) { log.error(\\\"用户名为空！！！！！！\\\"); exchange.getResponse().setStatusCode(HttpStatus.NOT_ACCEPTABLE); return exchange.getResponse().setComplete(); } return chain.filter(exchange); } @Override public int getOrder() { return 0; } } \"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[\"错误的：http://localhost:9527/payment/payment/lb\",\"正确的：http://localhost:9527/payment/payment/lb?uname=123\",\"**注意：**之前我们使用Predicate的代码记得注释，以及yaml的过滤\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud_sleuth.html\":{\"title\":\"13、SpringCloud Sleuth`分布式请求链路跟踪\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"在微服务框架中，一个由客户端发起的请求在后端系统中会经过多个不同的的服务节点调用来协同产生最后的请求结果，每一个前段请求都会形成一条复杂的分布式服务调用链路，链路中的任何一环出现高延时或错误都会引起整个请求最后的失败。\",\"官网资料：https://github.com/spring-cloud/spring-cloud-sleuth\",\"Spring Cloud Sleuth提供了一套完整的服务跟踪的解决方案，在分布式系统中提供追踪解决方案并且兼容支持了zipkin。\"]},{\"header\":\"2、搭建链路监控\",\"slug\":\"_2、搭建链路监控\",\"contents\":[\"SpringCloud从F版起已不需要自己构建Zipkin Server了，只需调用jar包即可\",\"下载地址：https://search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec\",\"zipkin-server-2.12.9-exec.jar\",\"运行：java -jar zipkin-server-2.12.9-exec.jar \",\"访问地址：http://localhost:9411/zipkin/\"]},{\"header\":\"1、完整的链路图\",\"slug\":\"_1、完整的链路图\",\"contents\":[\"表示一请求链路，一条链路通过Trace Id唯一标识，Span标识发起的请求信息，各span通过parent id 关联起来\"]},{\"header\":\"2、解释\",\"slug\":\"_2、解释\",\"contents\":[\"一条链路通过Trace Id唯一标识，Span标识发起的请求信息，各span通过parent id 关联起来\"]},{\"header\":\"3、名词\",\"slug\":\"_3、名词\",\"contents\":[\"**Trace：**类似于树结构的Span集合，表示一条调用链路，存在唯一标识\",\"**span：**表示调用链路来源，通俗的理解span就是一次请求信息\"]},{\"header\":\"3、监控案例\",\"slug\":\"_3、监控案例\",\"contents\":[\"这里以起初的xiaobear-provider-payment8001和xiaobear-consumer-order80-2进行监控\"]},{\"header\":\"1、修改 xiaobear-consumer-order80-2\",\"slug\":\"_1、修改xiaobear-consumer-order80-2\",\"contents\":[]},{\"header\":\"1、pom文件增加监控依赖\",\"slug\":\"_1、pom文件增加监控依赖\",\"contents\":[\"<!--包含了sleuth+zipkin--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-zipkin</artifactId> </dependency> \"]},{\"header\":\"2、yaml增加配置\",\"slug\":\"_2、yaml增加配置\",\"contents\":[\"spring: application: name: xiaobear-consumer-order zipkin: base-url: http://localhost:9411 sleuth: sampler: probability: 1 \"]},{\"header\":\"3、业务层controller\",\"slug\":\"_3、业务层controller\",\"contents\":[\"@GetMapping(\\\"/payment/zipkin\\\") public String paymentZipkin() { return \\\"hi ,i'am paymentzipkin server fall back，O(∩_∩)O哈哈~\\\"; } \"]},{\"header\":\"2、修改xiaobear-provider-payment8001\",\"slug\":\"_2、修改xiaobear-provider-payment8001\",\"contents\":[]},{\"header\":\"1、pom文件增加监控依赖\",\"slug\":\"_1、pom文件增加监控依赖-1\",\"contents\":[\" <!--包含了sleuth+zipkin--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-zipkin</artifactId> </dependency> \"]},{\"header\":\"2、yaml增加配置\",\"slug\":\"_2、yaml增加配置-1\",\"contents\":[\"spring: application: name: xiaobear-cloud-payment-service zipkin: base-url: http://localhost:9411 sleuth: sampler: #采样率值介于 0 到 1 之间，1 则表示全部采集 probability: 1 datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/spring-cloud-xiaobear?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8 username: root password: 密码 \"]},{\"header\":\"3、业务层controller\",\"slug\":\"_3、业务层controller-1\",\"contents\":[\"@GetMapping(\\\"/payment/zipkin\\\") public String paymentZipkin() { return \\\"hi ,i'am paymentzipkin server fall back，O(∩_∩)O哈哈~\\\"; } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"启动：\",\"7001\",\"8001\",\"80\",\"分别访问：\",\"http://localhost:8001/payment/payment/zipkin\",\"http://localhost/payment/zipkin\"]}]},\"/study-tutorial/microservice/spring-cloud/spring_cloud_stream.html\":{\"title\":\"12、Spring Cloud Stream消息驱动\",\"contents\":[{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"屏蔽底层消息中间件的差异,降低切换成本，统一消息的编程模型\",\"官网资料：\",\"https://docs.spring.io/spring-cloud-stream/docs/3.1.3/reference/html/\",\"中文指导手册：https://m.wang1314.com/doc/webapp/topic/20971999.html\"]},{\"header\":\"1、设计思想\",\"slug\":\"_1、设计思想\",\"contents\":[]},{\"header\":\"1、标准的MQ\",\"slug\":\"_1、标准的mq\",\"contents\":[\"生产者/消费者之间靠消息媒介传递信息内容 ----Message\",\"消息必须走特定的通道 ----消息通道MessageChannel\",\"消息通道里的消息如何被消费呢，谁负责收发处理 ----消息通道MessageChannel的子接口SubscribableChannel，由MessageHandler消息处理器所订阅\"]},{\"header\":\"2、为什么要用 Spring Cloud Stream\",\"slug\":\"_2、为什么要用spring-cloud-stream\",\"contents\":[\"Stream凭什么可以统一底层差异\",\"在没有绑定器这个概念的情况下，我们的SpringBoot应用要直接与消息中间件进行信息交互的时候，\",\"由于各消息中间件构建的初衷不同，它们的实现细节上会有较大的差异性\",\"通过定义绑定器作为中间层，完美地实现了应用程序与消息中间件细节之间的隔离。\",\"通过向应用程序暴露统一的Channel通道，使得应用程序不需要再考虑各种不同的消息中间件实现。\",\"通过定义绑定器Binder作为中间层，实现了应用程序与消息中间件细节之间的隔离。\",\"Binder\",\"在没有绑定器这个概念的情况下，我们的SpringBoot应用要直接与消息中间件进行信息交互的时候，由于各消息中间件构建的初衷不同，它们的实现细节上会有较大的差异性，通过定义绑定器作为中间层，完美地实现了应用程序与消息中间件细节之间的隔离。Stream对消息中间件的进一步封装，可以做到代码层面对中间件的无感知，甚至于动态的切换中间件(rabbitmq切换为kafka)，使得微服务开发的高度解耦，服务可以关注更多自己的业务流程\",\"![下载 (1)](../../images/下载 (1).png)\",\"通过定义绑定器Binder作为中间层，实现了应用程序与消息中间件细节之间的隔离。\",\"Binder可以生成Binding，Binding用来绑定消息容器的生产者和消费者，它有两种类型，INPUT和OUTPUT，INPUT对应于消费者，OUTPUT对应于生产者。\"]},{\"header\":\"3、 Stream 的通信方式遵循了发布-订阅模式\",\"slug\":\"_3、stream的通信方式遵循了发布-订阅模式\",\"contents\":[\"Topic主题进行广播\",\"在RabbitMQ就是Exchange\",\"在Kafka就是Topic\"]},{\"header\":\"2、标准流程套路\",\"slug\":\"_2、标准流程套路\",\"contents\":[]},{\"header\":\"1、 Binder\",\"slug\":\"_1、binder\",\"contents\":[\"很方便的连接中间件，屏蔽差异\"]},{\"header\":\"2、 Channel\",\"slug\":\"_2、channel\",\"contents\":[\"通道，是队列Queue的一种抽象，在消息通讯系统中就是实现存储和转发的媒介，通过Channel对队列进行配置\"]},{\"header\":\"3、 Source 和 Sink\",\"slug\":\"_3、source和sink\",\"contents\":[\"简单的可理解为参照对象是Spring Cloud Stream自身，从Stream发布消息就是输出，接受消息就是输入。\"]},{\"header\":\"3、编码API和常用注解\",\"slug\":\"_3、编码api和常用注解\",\"contents\":[]},{\"header\":\"2、案例说明\",\"slug\":\"_2、案例说明\",\"contents\":[\"xiaobear-cloud-stream-rabbitmq-provider8801， 作为生产者进行发消息模块\",\"xiaobear-cloud-stream-rabbitmq-consumer8802，作为消息接收模块\",\"xiaobear-cloud-stream-rabbitmq-consumer8803 作为消息接收模块\"]},{\"header\":\"3、消息驱动生产者\",\"slug\":\"_3、消息驱动生产者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-cloud-stream-rabbitmq-provider8801 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-stream-rabbit</artifactId> </dependency> <!--基础配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 8801 spring: application: name: cloud-stream-provider cloud: stream: binders: ## 在此处配置要绑定的rabbitmq的服务信息； defaultRabbit: ## 表示定义的名称，用于于binding整合 type: rabbit ## 消息组件类型 environment: ## 设置rabbitmq的相关的环境配置 spring: rabbitmq: host: localhost port: 5672 username: guest password: guest bindings: ## 服务的整合处理 output: ## 这个名字是一个通道的名称 destination: studyExchange ## 表示要使用的Exchange名称定义 content-type: application/json ## 设置消息类型，本次为json，文本则设置“text/plain” binder: ## 设置要绑定的消息服务的具体设置 eureka: client: ## 客户端进行Eureka注册的配置 service-url: defaultZone: http://localhost:7001/eureka instance: lease-renewal-interval-in-seconds: 2 ## 设置心跳的时间间隔（默认是30秒） lease-expiration-duration-in-seconds: 5 ## 如果现在超过了5秒的间隔（默认是90秒） instance-id: send-8801.com ## 在信息列表时显示主机名称 prefer-ip-address: true ## 访问的路径变为IP地址 \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication public class StreamApplication8801 { public static void main(String[] args) { SpringApplication.run(StreamApplication8801.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"service\",\"public interface IMessageService { /** * 生产者发送消息 * @return */ public String send() ; } \",\"impl\",\"@EnableBinding(Source.class) public class IMessageServiceImpl implements IMessageService { /** * 消息的发送管道 */ @Resource private MessageChannel output; @Override public String send() { String s = UUID.randomUUID().toString(); output.send(MessageBuilder.withPayload(s).build()); return s; } } \",\"controller\",\"@RestController public class StreamController { @Resource private IMessageService iMessageService; @GetMapping(\\\"/getMessage\\\") public String getMessage(){ String send = iMessageService.send(); return send; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"http://localhost:8801/getMessage\",\"在rabbitMq上面可以看到有峰值波动：http://localhost:15672/#/\"]},{\"header\":\"4、消息驱动消费者\",\"slug\":\"_4、消息驱动消费者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-cloud-stream-rabbitmq-consumer8802 \"]},{\"header\":\"2、改pom.xml\",\"slug\":\"_2、改pom-xml-1\",\"contents\":[\"<dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId> </dependency> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-stream-rabbit</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--基础配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml-1\",\"contents\":[\"server: port: 8802 spring: application: name: cloud-stream-consumer cloud: stream: binders: ## 在此处配置要绑定的rabbitmq的服务信息； defaultRabbit: ## 表示定义的名称，用于于binding整合 type: rabbit ## 消息组件类型 environment: ## 设置rabbitmq的相关的环境配置 spring: rabbitmq: host: localhost port: 5672 username: guest password: guest bindings: ## 服务的整合处理 input: ## 这个名字是一个通道的名称 destination: studyExchange ## 表示要使用的Exchange名称定义 content-type: application/json ## 设置消息类型，本次为对象json，如果是文本则设置“text/plain” binder: ## 设置要绑定的消息服务的具体设置 group: xiaobear8803 eureka: client: ## 客户端进行Eureka注册的配置 service-url: defaultZone: http://localhost:7001/eureka instance: lease-renewal-interval-in-seconds: 2 ## 设置心跳的时间间隔（默认是30秒） lease-expiration-duration-in-seconds: 5 ## 如果现在超过了5秒的间隔（默认是90秒） instance-id: receive-8802.com ## 在信息列表时显示主机名称 prefer-ip-address: true ## 访问的路径变为IP地址 \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-1\",\"contents\":[\"@SpringBootApplication public class StreamMQMain8802 { public static void main(String[] args) { SpringApplication.run(StreamMQMain8802.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-1\",\"contents\":[\"监听器\",\"@Component @EnableBinding(Sink.class) public class ReceiveMessageListener { @Value(\\\"${server.port}\\\") private String serverPort; @StreamListener(Sink.INPUT) public void input(Message<String> message) { System.out.println(\\\"消费者1号，------->接收到的消息：\\\" + message.getPayload()+\\\"\\\\t port: \\\"+serverPort); } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试-1\",\"contents\":[\"http://localhost:8801/getMessage\",\"8802的控制台输出\"]},{\"header\":\"5、分组消费与持久化\",\"slug\":\"_5、分组消费与持久化\",\"contents\":[\"按照xiaobear-cloud-stream-rabbitmq-consumer88032克隆出一份xiaobear-cloud-stream-rabbitmq-consumer8803\",\"运行后产生的问题：\",\"重复消费问题\",\"消息持久化问题\"]},{\"header\":\"1、消费\",\"slug\":\"_1、消费\",\"contents\":[]},{\"header\":\"1、现象\",\"slug\":\"_1、现象\",\"contents\":[\"8802和8803都收到了8801的消息，存在重复消费的问题\"]},{\"header\":\"2、解决\",\"slug\":\"_2、解决\",\"contents\":[\"分组与持久化属性Group\",\"比如在如下场景中，订单系统我们做集群部署，都会从RabbitMQ中获取订单信息，那如果一个订单同时被两个服务获取到，那么就会造成数据错误，我们得避免这种情况。这时我们就可以使用Stream中的消息分组来解决\",\"注意在Stream中处于同一个group中的多个消费者是竞争关系，就能够保证消息只会被其中一个应用消费一次。\",\"不同组是可以全面消费的(重复消费)，同一组内会发生竞争关系，只有其中一个可以消费。\"]},{\"header\":\"2、分组\",\"slug\":\"_2、分组\",\"contents\":[\"原理：微服务应用放置于同一个Group中，就能够保证消息只会被其中一个应用消费一次。不同的组是可以消费的，同一个组内会发生竞争关系，只有其中一个可以消费。\",\"分组：\",\"修改8802与8803的yaml，重点是加入了Group的配置，8803同理\",\"server: port: 8802 spring: application: name: cloud-stream-consumer cloud: stream: binders: ## 在此处配置要绑定的rabbitmq的服务信息； defaultRabbit: ## 表示定义的名称，用于于binding整合 type: rabbit ## 消息组件类型 environment: ## 设置rabbitmq的相关的环境配置 spring: rabbitmq: host: localhost port: 5672 username: guest password: guest bindings: ## 服务的整合处理 input: ## 这个名字是一个通道的名称 destination: studyExchange ## 表示要使用的Exchange名称定义 content-type: application/json ## 设置消息类型，本次为对象json，如果是文本则设置“text/plain” binder: ## 设置要绑定的消息服务的具体设置 group: xiaobear8802 eureka: client: ## 客户端进行Eureka注册的配置 service-url: defaultZone: http://localhost:7001/eureka instance: lease-renewal-interval-in-seconds: 2 ## 设置心跳的时间间隔（默认是30秒） lease-expiration-duration-in-seconds: 5 ## 如果现在超过了5秒的间隔（默认是90秒） instance-id: receive-8802.com ## 在信息列表时显示主机名称 prefer-ip-address: true ## 访问的路径变为IP地址 \",\"分布式微服务应用为了实现高可用和负载均衡，实际上都会部署多个实例，本次启动了两个消费微服务(8802/8803) 多数情况，生产者发送消息给某个具体微服务时只希望被消费一次，按照上面我们启动两个应用的例子，虽然它们同属一个应用，但是这个消息出现了被重复消费两次的情况。为了解决这个问题，在Spring Cloud Stream中提供了消费组的概念。 \",\"结论：加入消费组之后，还是存在重复消费\",\"解决：\",\"8802与8803实现轮询分组，每次只有一个消费者能接收到消息\",\"8802与8803变成相同组，Group两个相同，同时变成xiaobear8802或xiaobear8803\"]},{\"header\":\"3、持久化\",\"slug\":\"_3、持久化\",\"contents\":[\"停止8802并去除8802的分组属性Group\",\"8801先发送几条消息（调用http://localhost:8801/getMessage）\",\"启动8802，没有分组属性，并没有接收到消息\",\"启动8803，有分组属性配置，后台接收了8801发送的消息\"]}]},\"/study-tutorial/microservice/spring-boot2/config.html\":{\"title\":\"2、配置文件\",\"contents\":[{\"header\":\"1、配置文件\",\"slug\":\"_1、配置文件\",\"contents\":[\"SpringBoot使用一个全局的配置文件，配置文件名是固定的；\",\"application.properties\",\"application.yml\",\"配置文件的作用：修改SpringBoot自动配置的默认值；SpringBoot在底层都给我们自动配置好；\",\"YAML（YAML Ain't Markup Language）\",\"YAML A Markup Language：是一个标记语言\",\"YAML isn't Markup Language：不是一个标记语言；\",\"标记语言：\",\"​ 以前的配置文件；大多都使用的是 xxxx.xml文件；\",\"​ YAML：以数据为中心，比json、xml等更适合做配置文件；\",\"​ YAML：配置例子\",\"server: port: 8081 \",\"​ XML：\",\"<server> <port>8081</port> </server> \"]},{\"header\":\"2、YAML语法：\",\"slug\":\"_2、yaml语法\",\"contents\":[]},{\"header\":\"1、基本语法\",\"slug\":\"_1、基本语法\",\"contents\":[\"k:(空格)v：表示一对键值对（空格必须有）；\",\"以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的\",\"server: port: 8081 path: /hello \",\"属性和值也是大小写敏感；\"]},{\"header\":\"2、值的写法\",\"slug\":\"_2、值的写法\",\"contents\":[]},{\"header\":\"字面量：普通的值（数字，字符串，布尔）\",\"slug\":\"字面量-普通的值-数字-字符串-布尔\",\"contents\":[\"​ k: v：字面直接来写；\",\"​ 字符串默认不用加上单引号或者双引号；\",\"​ \\\"\\\"：双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表示的意思\",\"​ name: \\\"zhangsan \\\\n lisi\\\"：输出；zhangsan 换行 lisi\",\"​ ''：单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据\",\"​ name: ‘zhangsan \\\\n lisi’：输出；zhangsan \\\\n lisi\"]},{\"header\":\"对象、Map（属性和值）（键值对）：\",\"slug\":\"对象、map-属性和值-键值对\",\"contents\":[\"​ k: v：在下一行来写对象的属性和值的关系；注意缩进\",\"​ 对象还是k: v的方式\",\"friends: lastName: zhangsan age: 20 \",\"行内写法：\",\"friends: {lastName: zhangsan,age: 18} \"]},{\"header\":\"数组（List、Set）：\",\"slug\":\"数组-list、set\",\"contents\":[\"用- 值表示数组中的一个元素\",\"pets: - cat - dog - pig \",\"行内写法\",\"pets: [cat,dog,pig] \"]},{\"header\":\"3、配置文件值注入\",\"slug\":\"_3、配置文件值注入\",\"contents\":[\"配置文件\",\"person: lastName: hello age: 18 boss: false birth: 2017/12/12 maps: {k1: v1,k2: 12} lists: - lisi - zhaoliu dog: name: 小狗 age: 12 \",\"javaBean：\",\"/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = \\\"person\\\"：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能； * */ @Component @ConfigurationProperties(prefix = \\\"person\\\") public class Person { private String lastName; private Integer age; private Boolean boss; private Date birth; private Map<String,Object> maps; private List<Object> lists; private Dog dog; \",\"我们可以导入配置文件处理器，以后编写配置就有提示了\",\"<!--导入配置文件处理器，配置文件进行绑定就会有提示--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-configuration-processor</artifactId> <optional>true</optional> </dependency> \"]},{\"header\":\"1、properties配置文件在idea中默认utf-8可能会乱码\",\"slug\":\"_1、properties配置文件在idea中默认utf-8可能会乱码\",\"contents\":[\"调整\"]},{\"header\":\"2、@Value获取值和@ConfigurationProperties获取值比较\",\"slug\":\"_2、-value获取值和-configurationproperties获取值比较\",\"contents\":[\"@ConfigurationProperties\",\"@Value\",\"功能\",\"批量注入配置文件中的属性\",\"一个个指定\",\"松散绑定（松散语法）\",\"支持\",\"不支持\",\"SpEL\",\"不支持\",\"支持\",\"JSR303数据校验\",\"支持\",\"不支持\",\"复杂类型封装\",\"支持\",\"不支持\",\"配置文件yml还是properties他们都能获取到值；\",\"如果说，我们只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value；\",\"如果说，我们专门编写了一个javaBean来和配置文件进行映射，我们就直接使用@ConfigurationProperties；\"]},{\"header\":\"3、配置文件注入值数据校验\",\"slug\":\"_3、配置文件注入值数据校验\",\"contents\":[\"@Component @ConfigurationProperties(prefix = \\\"person\\\") @Validated public class Person { /** * <bean class=\\\"Person\\\"> * <property name=\\\"lastName\\\" value=\\\"字面量/${key}从环境变量、配置文件中获取值/#{SpEL}\\\"></property> * <bean/> */ //lastName必须是邮箱格式 @Email //@Value(\\\"${person.last-name}\\\") private String lastName; //@Value(\\\"#{11*2}\\\") private Integer age; //@Value(\\\"true\\\") private Boolean boss; private Date birth; private Map<String,Object> maps; private List<Object> lists; private Dog dog; \"]},{\"header\":\"4、@PropertySource & @ImportResource & @Bean\",\"slug\":\"_4、-propertysource-importresource-bean\",\"contents\":[\"@PropertySource：加载指定的配置文件；\",\"/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = \\\"person\\\"：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能； * @ConfigurationProperties(prefix = \\\"person\\\")默认从全局配置文件中获取值； * */ @PropertySource(value = {\\\"classpath:person.properties\\\"}) @Component @ConfigurationProperties(prefix = \\\"person\\\") //@Validated public class Person { /** * <bean class=\\\"Person\\\"> * <property name=\\\"lastName\\\" value=\\\"字面量/${key}从环境变量、配置文件中获取值/#{SpEL}\\\"></property> * <bean/> */ //lastName必须是邮箱格式 // @Email //@Value(\\\"${person.last-name}\\\") private String lastName; //@Value(\\\"#{11*2}\\\") private Integer age; //@Value(\\\"true\\\") private Boolean boss; \",\"@ImportResource：导入Spring的配置文件，让配置文件里面的内容生效；\",\"Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别；\",\"想让Spring的配置文件生效，加载进来；@ImportResource标注在一个配置类上\",\"@ImportResource(locations = {\\\"classpath:beans.xml\\\"}) /导入Spring的配置文件让其生效 \",\"不来编写Spring的配置文件\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\\\"> <bean id=\\\"helloService\\\" class=\\\"com.atguigu.springboot.service.HelloService\\\"></bean> </beans> \",\"SpringBoot推荐给容器中添加组件的方式；推荐使用全注解的方式\",\"1、配置类**@Configuration**------>Spring配置文件\",\"2、使用**@Bean**给容器中添加组件\",\"/** * @Configuration：指明当前类是一个配置类；就是来替代之前的Spring配置文件 * * 在配置文件中用<bean><bean/>标签添加组件 * */ @Configuration public class MyAppConfig { //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名 @Bean public HelloService helloService02(){ System.out.println(\\\"配置类@Bean给容器中添加组件了...\\\"); return new HelloService(); } } \",\"##4、配置文件占位符\"]},{\"header\":\"1、随机数\",\"slug\":\"_1、随机数\",\"contents\":[\"${random.value}、${random.int}、${random.long} ${random.int(10)}、${random.int[1024,65536]} \"]},{\"header\":\"2、占位符获取之前配置的值，如果没有可以是用:指定默认值\",\"slug\":\"_2、占位符获取之前配置的值-如果没有可以是用-指定默认值\",\"contents\":[\"person.last-name=张三${random.uuid} person.age=${random.int} person.birth=2017/12/15 person.boss=false person.maps.k1=v1 person.maps.k2=14 person.lists=a,b,c person.dog.name=${person.hello:hello}_dog person.dog.age=15 \"]},{\"header\":\"5、Profile\",\"slug\":\"_5、profile\",\"contents\":[]},{\"header\":\"1、多Profile文件\",\"slug\":\"_1、多profile文件\",\"contents\":[\"我们在主配置文件编写的时候，文件名可以是 application-{profile}.properties/yml\",\"默认使用application.properties的配置；\"]},{\"header\":\"2、yml支持多文档块方式\",\"slug\":\"_2、yml支持多文档块方式\",\"contents\":[\" server: port: 8081 spring: profiles: active: prod --- server: port: 8083 spring: profiles: dev --- server: port: 8084 spring: profiles: prod #指定属于哪个环境 \"]},{\"header\":\"3、激活指定profile\",\"slug\":\"_3、激活指定profile\",\"contents\":[\"​ 1、在配置文件中指定 spring.profiles.active=dev\",\"​ 2、命令行：\",\"​ java -jar spring-boot-02-config-0.0.1-SNAPSHOT.jar --spring.profiles.active=dev；\",\"​ 可以直接在测试的时候，配置传入命令行参数\",\"​ 3、虚拟机参数；\",\"​ -Dspring.profiles.active=dev\"]},{\"header\":\"6、配置文件加载位置\",\"slug\":\"_6、配置文件加载位置\",\"contents\":[\"springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件\",\"–file:./config/\",\"–file:./\",\"–classpath:/config/\",\"–classpath:/\",\"优先级由高到底，高优先级的配置会覆盖低优先级的配置；\",\"SpringBoot会从这四个位置全部加载主配置文件；互补配置；\",\"我们还可以通过spring.config.location来改变默认的配置文件位置\",\"项目打包好以后，我们可以使用命令行参数的形式，启动项目的时候来指定配置文件的新位置；指定配置文件和默认加载的这些配置文件共同起作用形成互补配置；\",\"java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --spring.config.location=G:/application.properties\"]},{\"header\":\"7、外部配置加载顺序\",\"slug\":\"_7、外部配置加载顺序\",\"contents\":[\"SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置\",\"1.命令行参数\",\"所有的配置都可以在命令行上进行指定\",\"java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --server.port=8087 --server.context-path=/abc\",\"多个配置用空格分开； --配置项=值\",\"2.来自java:comp/env的JNDI属性\",\"3.Java系统属性（System.getProperties()）\",\"4.操作系统环境变量\",\"5.RandomValuePropertySource配置的random.*属性值\",\"由jar包外向jar包内进行寻找；\",\"优先加载带profile\",\"6.jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件\",\"7.jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件\",\"再来加载不带profile\",\"8.jar包外部的application.properties或application.yml(不带spring.profile)配置文件\",\"9.jar包内部的application.properties或application.yml(不带spring.profile)配置文件\",\"10.@Configuration注解类上的@PropertySource\",\"11.通过SpringApplication.setDefaultProperties指定的默认属性\",\"所有支持的配置加载来源；\",\"参考官方文档\"]},{\"header\":\"8、自动配置原理\",\"slug\":\"_8、自动配置原理\",\"contents\":[\"配置文件到底能写什么？怎么写？自动配置原理；\",\"配置文件能配置的属性参照\"]},{\"header\":\"1、 自动配置原理：\",\"slug\":\"_1、自动配置原理\",\"contents\":[\"1）、SpringBoot启动的时候加载主配置类，开启了自动配置功能 @EnableAutoConfiguration\",\"2）、@EnableAutoConfiguration 作用：\",\"利用EnableAutoConfigurationImportSelector给容器中导入一些组件？\",\"可以查看selectImports()方法的内容；\",\"List<String> configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置\",\"SpringFactoriesLoader.loadFactoryNames() 扫描所有jar包类路径下 META-INF/spring.factories 把扫描到的这些文件的内容包装成properties对象 从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中 \",\"将 类路径下 META-INF/spring.factories 里面配置的所有EnableAutoConfiguration的值加入到了容器中；\",\"## Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.ldap.LdapDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.elasticsearch.jest.JestAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mobile.DeviceResolverAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mobile.DeviceDelegatingViewResolverAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mobile.SitePreferenceAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.reactor.ReactorAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.security.SecurityAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.security.SecurityFilterAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.security.oauth2.OAuth2AutoConfiguration,\\\\ org.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.social.SocialWebAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.social.FacebookAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.social.LinkedInAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.social.TwitterAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.DispatcherServletAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.HttpEncodingAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.HttpMessageConvertersAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.MultipartAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.WebClientAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.websocket.WebSocketAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.websocket.WebSocketMessagingAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration \",\"每一个这样的 xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置；\",\"3）、每一个自动配置类进行自动配置功能；\",\"4）、以**HttpEncodingAutoConfiguration（Http编码自动配置）**为例解释自动配置原理；\",\"@Configuration //表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件 @EnableConfigurationProperties(HttpEncodingProperties.class) //启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpEncodingProperties绑定起来；并把HttpEncodingProperties加入到ioc容器中 @ConditionalOnWebApplication //Spring底层@Conditional注解（Spring注解版），根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效； 判断当前应用是否是web应用，如果是，当前配置类生效 @ConditionalOnClass(CharacterEncodingFilter.class) //判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器； @ConditionalOnProperty(prefix = \\\"spring.http.encoding\\\", value = \\\"enabled\\\", matchIfMissing = true) //判断配置文件中是否存在某个配置 spring.http.encoding.enabled；如果不存在，判断也是成立的 //即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的； public class HttpEncodingAutoConfiguration { //他已经和SpringBoot的配置文件映射了 private final HttpEncodingProperties properties; //只有一个有参构造器的情况下，参数的值就会从容器中拿 public HttpEncodingAutoConfiguration(HttpEncodingProperties properties) { this.properties = properties; } @Bean //给容器中添加一个组件，这个组件的某些值需要从properties中获取 @ConditionalOnMissingBean(CharacterEncodingFilter.class) //判断容器没有这个组件？ public CharacterEncodingFilter characterEncodingFilter() { CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE)); return filter; } \",\"根据当前不同的条件判断，决定这个配置类是否生效？\",\"一但这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性是从对应的properties类中获取的，这些类里面的每一个属性又是和配置文件绑定的；\",\"5）、所有在配置文件中能配置的属性都是在xxxxProperties类中封装者‘；配置文件能配置什么就可以参照某个功能对应的这个属性类\",\"@ConfigurationProperties(prefix = \\\"spring.http.encoding\\\") //从配置文件中获取指定的值和bean的属性进行绑定 public class HttpEncodingProperties { public static final Charset DEFAULT_CHARSET = Charset.forName(\\\"UTF-8\\\"); \",\"精髓：\",\"​ 1）、SpringBoot启动会加载大量的自动配置类\",\"​ 2）、我们看我们需要的功能有没有SpringBoot默认写好的自动配置类；\",\"​ 3）、我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件有，我们就不需要再来配置了）\",\"​ 4）、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值；\",\"xxxxAutoConfigurartion：自动配置类；\",\"给容器中添加组件\",\"xxxxProperties:封装配置文件中相关属性；\"]},{\"header\":\"2、细节\",\"slug\":\"_2、细节\",\"contents\":[]},{\"header\":\"1、@Conditional派生注解（Spring注解版原生的@Conditional作用）\",\"slug\":\"_1、-conditional派生注解-spring注解版原生的-conditional作用\",\"contents\":[\"作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效；\",\"@Conditional扩展注解\",\"作用（判断是否满足当前指定条件）\",\"@ConditionalOnJava\",\"系统的java版本是否符合要求\",\"@ConditionalOnBean\",\"容器中存在指定Bean；\",\"@ConditionalOnMissingBean\",\"容器中不存在指定Bean；\",\"@ConditionalOnExpression\",\"满足SpEL表达式指定\",\"@ConditionalOnClass\",\"系统中有指定的类\",\"@ConditionalOnMissingClass\",\"系统中没有指定的类\",\"@ConditionalOnSingleCandidate\",\"容器中只有一个指定的Bean，或者这个Bean是首选Bean\",\"@ConditionalOnProperty\",\"系统中指定的属性是否有指定的值\",\"@ConditionalOnResource\",\"类路径下是否存在指定资源文件\",\"@ConditionalOnWebApplication\",\"当前是web环境\",\"@ConditionalOnNotWebApplication\",\"当前不是web环境\",\"@ConditionalOnJndi\",\"JNDI存在指定项\",\"自动配置类必须在一定的条件下才能生效；\",\"我们怎么知道哪些自动配置类生效；\",\"我们可以通过启用 debug=true属性；来让控制台打印自动配置报告，这样我们就可以很方便的知道哪些自动配置类生效；\",\"************************= AUTO-CONFIGURATION REPORT ************************= Positive matches:（自动配置类启用的） ----------------- DispatcherServletAutoConfiguration matched: - @ConditionalOnClass found required class 'org.springframework.web.servlet.DispatcherServlet'; @ConditionalOnMissingClass did not find unwanted class (OnClassCondition) - @ConditionalOnWebApplication (required) found StandardServletEnvironment (OnWebApplicationCondition) Negative matches:（没有启动，没有匹配成功的自动配置类） ----------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes 'javax.jms.ConnectionFactory', 'org.apache.activemq.ActiveMQConnectionFactory' (OnClassCondition) AopAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice' (OnClassCondition) \"]}]},\"/study-tutorial/microservice/spring-boot2/data_access.html\":{\"title\":\"5、SpringBoot与数据访问\",\"contents\":[{\"header\":\"1、JDBC\",\"slug\":\"_1、jdbc\",\"contents\":[\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <scope>runtime</scope> </dependency> \",\"spring: datasource: username: root password: 123456 url: jdbc:mysql://192.168.15.22:3306/jdbc driver-class-name: com.mysql.jdbc.Driver \",\"效果：\",\"​ 默认是用org.apache.tomcat.jdbc.pool.DataSource作为数据源；\",\"​ 数据源的相关配置都在DataSourceProperties里面；\",\"自动配置原理：\",\"org.springframework.boot.autoconfigure.jdbc：\",\"1、参考DataSourceConfiguration，根据配置创建数据源，默认使用Tomcat连接池；可以使用spring.datasource.type指定自定义的数据源类型；\",\"2、SpringBoot默认可以支持；\",\"org.apache.tomcat.jdbc.pool.DataSource、HikariDataSource、BasicDataSource、 \",\"3、自定义数据源类型\",\"/** * Generic DataSource configuration. */ @ConditionalOnMissingBean(DataSource.class) @ConditionalOnProperty(name = \\\"spring.datasource.type\\\") static class Generic { @Bean public DataSource dataSource(DataSourceProperties properties) { //使用DataSourceBuilder创建数据源，利用反射创建响应type的数据源，并且绑定相关属性 return properties.initializeDataSourceBuilder().build(); } } \",\"4、DataSourceInitializer：ApplicationListener；\",\"​ 作用：\",\"​ 1）、runSchemaScripts();运行建表语句；\",\"​ 2）、runDataScripts();运行插入数据的sql语句；\",\"默认只需要将文件命名为：\",\"schema-*.sql、data-*.sql 默认规则：schema.sql，schema-all.sql； 可以使用 schema: - classpath:department.sql 指定位置 \",\"5、操作数据库：自动配置了JdbcTemplate操作数据库\"]},{\"header\":\"2、整合Druid数据源\",\"slug\":\"_2、整合druid数据源\",\"contents\":[\"导入druid数据源 @Configuration public class DruidConfig { @ConfigurationProperties(prefix = \\\"spring.datasource\\\") @Bean public DataSource druid(){ return new DruidDataSource(); } //配置Druid的监控 //1、配置一个管理后台的Servlet @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), \\\"/druid/*\\\"); Map<String,String> initParams = new HashMap<>(); initParams.put(\\\"loginUsername\\\",\\\"admin\\\"); initParams.put(\\\"loginPassword\\\",\\\"123456\\\"); initParams.put(\\\"allow\\\",\\\"\\\");//默认就是允许所有访问 initParams.put(\\\"deny\\\",\\\"192.168.15.21\\\"); bean.setInitParameters(initParams); return bean; } //2、配置一个web监控的filter @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map<String,String> initParams = new HashMap<>(); initParams.put(\\\"exclusions\\\",\\\"*.js,*.css,/druid/*\\\"); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList(\\\"/*\\\")); return bean; } } \"]},{\"header\":\"3、整合MyBatis\",\"slug\":\"_3、整合mybatis\",\"contents\":[\" <dependency> <groupId>org.mybatis.spring.boot</groupId> <artifactId>mybatis-spring-boot-starter</artifactId> <version>1.3.1</version> </dependency> \",\"步骤：\",\"​ 1）、配置数据源相关属性（见上一节Druid）\",\"​ 2）、给数据库建表\",\"​ 3）、创建JavaBean\"]},{\"header\":\"4）、注解版\",\"slug\":\"_4-、注解版\",\"contents\":[\"//指定这是一个操作数据库的mapper @Mapper public interface DepartmentMapper { @Select(\\\"select * from department where id=#{id}\\\") public Department getDeptById(Integer id); @Delete(\\\"delete from department where id=#{id}\\\") public int deleteDeptById(Integer id); @Options(useGeneratedKeys = true,keyProperty = \\\"id\\\") @Insert(\\\"insert into department(departmentName) values(#{departmentName})\\\") public int insertDept(Department department); @Update(\\\"update department set departmentName=#{departmentName} where id=#{id}\\\") public int updateDept(Department department); } \",\"问题：\",\"自定义MyBatis的配置规则；给容器中添加一个ConfigurationCustomizer；\",\"@org.springframework.context.annotation.Configuration public class MyBatisConfig { @Bean public ConfigurationCustomizer configurationCustomizer(){ return new ConfigurationCustomizer(){ @Override public void customize(Configuration configuration) { configuration.setMapUnderscoreToCamelCase(true); } }; } } \",\"使用MapperScan批量扫描所有的Mapper接口； @MapperScan(value = \\\"com.atguigu.springboot.mapper\\\") @SpringBootApplication public class SpringBoot06DataMybatisApplication { public static void main(String[] args) { SpringApplication.run(SpringBoot06DataMybatisApplication.class, args); } } \"]},{\"header\":\"5）、配置文件版\",\"slug\":\"_5-、配置文件版\",\"contents\":[\"mybatis: config-location: classpath:mybatis/mybatis-config.xml 指定全局配置文件的位置 mapper-locations: classpath:mybatis/mapper/*.xml 指定sql映射文件的位置 \",\"更多使用参照\",\"http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/\"]},{\"header\":\"4、整合SpringData JPA\",\"slug\":\"_4、整合springdata-jpa\",\"contents\":[]},{\"header\":\"1）、SpringData简介\",\"slug\":\"_1-、springdata简介\",\"contents\":[]},{\"header\":\"2）、整合SpringData JPA\",\"slug\":\"_2-、整合springdata-jpa\",\"contents\":[\"JPA:ORM（Object Relational Mapping）；\",\"1）、编写一个实体类（bean）和数据表进行映射，并且配置好映射关系；\",\"//使用JPA注解配置映射关系 @Entity //告诉JPA这是一个实体类（和数据表映射的类） @Table(name = \\\"tbl_user\\\") //@Table来指定和哪个数据表对应;如果省略默认表名就是user； public class User { @Id //这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//自增主键 private Integer id; @Column(name = \\\"last_name\\\",length = 50) //这是和数据表对应的一个列 private String lastName; @Column //省略默认列名就是属性名 private String email; \",\"2）、编写一个Dao接口来操作实体类对应的数据表（Repository）\",\"//继承JpaRepository来完成对数据库的操作 public interface UserRepository extends JpaRepository<User,Integer> { } \",\"3）、基本的配置JpaProperties\",\"spring: jpa: hibernate: ## 更新或者创建数据表结构 ddl-auto: update ## 控制台显示SQL show-sql: true \"]}]},\"/study-tutorial/microservice/spring-boot2/diy_starter.html\":{\"title\":\"7、自定义starter\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"starter：\",\"​ 1、这个场景需要使用到的依赖是什么？\",\"​ 2、如何编写自动配置\",\"@Configuration //指定这个类是一个配置类 @ConditionalOnXXX //在指定条件成立的情况下自动配置类生效 @AutoConfigureAfter //指定自动配置类的顺序 @Bean //给容器中添加组件 @ConfigurationPropertie结合相关xxxProperties类来绑定相关的配置 @EnableConfigurationProperties //让xxxProperties生效加入到容器中 自动配置类要能加载 将需要启动就加载的自动配置类，配置在META-INF/spring.factories org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\\\ \",\"​ 3、模式：\",\"启动器只用来做依赖导入；\",\"专门来写一个自动配置模块；\",\"启动器依赖自动配置；别人只需要引入启动器（starter）\",\"mybatis-spring-boot-starter；自定义启动器名-spring-boot-starter\",\"步骤：\",\"1）、启动器模块\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <modelVersion>4.0.0</modelVersion> <groupId>com.atguigu.starter</groupId> <artifactId>atguigu-spring-boot-starter</artifactId> <version>1.0-SNAPSHOT</version> <!--启动器--> <dependencies> <!--引入自动配置模块--> <dependency> <groupId>com.atguigu.starter</groupId> <artifactId>atguigu-spring-boot-starter-autoconfigurer</artifactId> <version>0.0.1-SNAPSHOT</version> </dependency> </dependencies> </project> \",\"2）、自动配置模块\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\"> <modelVersion>4.0.0</modelVersion> <groupId>com.atguigu.starter</groupId> <artifactId>atguigu-spring-boot-starter-autoconfigurer</artifactId> <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> <name>atguigu-spring-boot-starter-autoconfigurer</name> <description>Demo project for Spring Boot</description> <parent> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-parent</artifactId> <version>1.5.10.RELEASE</version> <relativePath/> <!-- lookup parent from repository --> </parent> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding> <java.version>1.8</java.version> </properties> <dependencies> <!--引入spring-boot-starter；所有starter的基本配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter</artifactId> </dependency> </dependencies> </project> \",\"package com.atguigu.starter; import org.springframework.boot.context.properties.ConfigurationProperties; @ConfigurationProperties(prefix = \\\"atguigu.hello\\\") public class HelloProperties { private String prefix; private String suffix; public String getPrefix() { return prefix; } public void setPrefix(String prefix) { this.prefix = prefix; } public String getSuffix() { return suffix; } public void setSuffix(String suffix) { this.suffix = suffix; } } \",\"package com.atguigu.starter; public class HelloService { HelloProperties helloProperties; public HelloProperties getHelloProperties() { return helloProperties; } public void setHelloProperties(HelloProperties helloProperties) { this.helloProperties = helloProperties; } public String sayHellAtguigu(String name){ return helloProperties.getPrefix()+\\\"-\\\" +name + helloProperties.getSuffix(); } } \",\"package com.atguigu.starter; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication; import org.springframework.boot.context.properties.EnableConfigurationProperties; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration @ConditionalOnWebApplication //web应用才生效 @EnableConfigurationProperties(HelloProperties.class) public class HelloServiceAutoConfiguration { @Autowired HelloProperties helloProperties; @Bean public HelloService helloService(){ HelloService service = new HelloService(); service.setHelloProperties(helloProperties); return service; } } \"]}]},\"/study-tutorial/microservice/spring-boot2/log.html\":{\"title\":\"3、日志\",\"contents\":[{\"header\":\"1、日志框架\",\"slug\":\"_1、日志框架\",\"contents\":[\"小张；开发一个大型系统；\",\"​ 1、System.out.println(\\\"\\\")；将关键数据打印在控制台；去掉？写在一个文件？\",\"​ 2、框架来记录系统的一些运行时信息；日志框架 ； zhanglogging.jar；\",\"​ 3、高大上的几个功能？异步模式？自动归档？xxxx？ zhanglogging-good.jar？\",\"​ 4、将以前框架卸下来？换上新的框架，重新修改之前相关的API；zhanglogging-prefect.jar；\",\"​ 5、JDBC---数据库驱动；\",\"​ 写了一个统一的接口层；日志门面（日志的一个抽象层）；logging-abstract.jar；\",\"​ 给项目中导入具体的日志实现就行了；我们之前的日志框架都是实现的抽象层；\",\"市面上的日志框架；\",\"JUL、JCL、Jboss-logging、logback、log4j、log4j2、slf4j....\",\"日志门面 （日志的抽象层）\",\"日志实现\",\"JCL（Jakarta Commons Logging） SLF4j（Simple Logging Facade for Java） jboss-logging\",\"Log4j JUL（java.util.logging） Log4j2 Logback\",\"左边选一个门面（抽象层）、右边来选一个实现；\",\"日志门面： SLF4J；\",\"日志实现：Logback；\",\"SpringBoot：底层是Spring框架，Spring框架默认是用JCL；‘\",\"​ SpringBoot选用 SLF4j和logback；\"]},{\"header\":\"2、SLF4j使用\",\"slug\":\"_2、slf4j使用\",\"contents\":[]},{\"header\":\"1、如何在系统中使用SLF4j https://www.slf4j.org\",\"slug\":\"_1、如何在系统中使用slf4j-https-www-slf4j-org\",\"contents\":[\"以后开发的时候，日志记录方法的调用，不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法；\",\"给系统里面导入slf4j的jar和 logback的实现jar\",\"import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class HelloWorld { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(\\\"Hello World\\\"); } } \",\"图示；\",\"每一个日志的实现框架都有自己的配置文件。使用slf4j以后，配置文件还是做成日志实现框架自己本身的配置文件；\"]},{\"header\":\"2、遗留问题\",\"slug\":\"_2、遗留问题\",\"contents\":[\"a（slf4j+logback）: Spring（commons-logging）、Hibernate（jboss-logging）、MyBatis、xxxx\",\"统一日志记录，即使是别的框架和我一起统一使用slf4j进行输出？\",\"如何让系统中所有的日志都统一到slf4j；\",\"1、将系统中其他日志框架先排除出去；\",\"2、用中间包来替换原有的日志框架；\",\"3、我们导入slf4j其他的实现\"]},{\"header\":\"3、SpringBoot日志关系\",\"slug\":\"_3、springboot日志关系\",\"contents\":[\" <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter</artifactId> </dependency> \",\"SpringBoot使用它来做日志功能；\",\" <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-logging</artifactId> </dependency> \",\"底层依赖关系\",\"总结：\",\"​ 1）、SpringBoot底层也是使用slf4j+logback的方式进行日志记录\",\"​ 2）、SpringBoot也把其他的日志都替换成了slf4j；\",\"​ 3）、中间替换包？\",\"@SuppressWarnings(\\\"rawtypes\\\") public abstract class LogFactory { static String UNSUPPORTED_OPERATION_IN_JCL_OVER_SLF4J = \\\"http://www.slf4j.org/codes.html#unsupported_operation_in_jcl_over_slf4j\\\"; static LogFactory logFactory = new SLF4JLogFactory(); \",\"​ 4）、如果我们要引入其他框架？一定要把这个框架的默认日志依赖移除掉？\",\"​ Spring框架用的是commons-logging；\",\" <dependency> <groupId>org.springframework</groupId> <artifactId>spring-core</artifactId> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> \",\"SpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要把这个框架依赖的日志框架排除掉即可；\"]},{\"header\":\"4、日志使用；\",\"slug\":\"_4、日志使用\",\"contents\":[]},{\"header\":\"1、默认配置\",\"slug\":\"_1、默认配置\",\"contents\":[\"SpringBoot默认帮我们配置好了日志；\",\" //记录器 Logger logger = LoggerFactory.getLogger(getClass()); @Test public void contextLoads() { //System.out.println(); //日志的级别； //由低到高 trace<debug<info<warn<error //可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效 logger.trace(\\\"这是trace日志...\\\"); logger.debug(\\\"这是debug日志...\\\"); //SpringBoot默认给我们使用的是info级别的，没有指定级别的就用SpringBoot默认规定的级别；root级别 logger.info(\\\"这是info日志...\\\"); logger.warn(\\\"这是warn日志...\\\"); logger.error(\\\"这是error日志...\\\"); } \",\" 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度 %logger{50} 表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 --> %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n \",\"SpringBoot修改日志的默认配置\",\"logging.level.com.atguigu=trace #logging.path= ## 不指定路径在当前项目下生成springboot.log日志 ## 可以指定完整的路径； #logging.file=G:/springboot.log ## 在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用 spring.log 作为默认文件 logging.path=/spring/log ## 在控制台输出的日志的格式 logging.pattern.console=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n ## 指定文件中日志输出的格式 logging.pattern.file=%d{yyyy-MM-dd} **= [%thread] **= %-5level **= %logger{50} **** %msg%n \",\"logging.file\",\"logging.path\",\"Example\",\"Description\",\"(none)\",\"(none)\",\"只在控制台输出\",\"指定文件名\",\"(none)\",\"my.log\",\"输出日志到my.log文件\",\"(none)\",\"指定目录\",\"/var/log\",\"输出到指定目录的 spring.log 文件中\"]},{\"header\":\"2、指定配置\",\"slug\":\"_2、指定配置\",\"contents\":[\"给类路径下放上每个日志框架自己的配置文件即可；SpringBoot就不使用他默认配置的了\",\"Logging System\",\"Customization\",\"Logback\",\"logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy\",\"Log4j2\",\"log4j2-spring.xml or log4j2.xml\",\"JDK (Java Util Logging)\",\"logging.properties\",\"logback.xml：直接就被日志框架识别了；\",\"logback-spring.xml：日志框架就不直接加载日志的配置项，由SpringBoot解析日志配置，可以使用SpringBoot的高级Profile功能\",\"<springProfile name=\\\"staging\\\"> <!-- configuration to be enabled when the \\\"staging\\\" profile is active --> 可以指定某段配置只在某个环境下生效 </springProfile> \",\"如：\",\"<appender name=\\\"stdout\\\" class=\\\"ch.qos.logback.core.ConsoleAppender\\\"> <!-- 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度 %logger{50} 表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 --> <layout class=\\\"ch.qos.logback.classic.PatternLayout\\\"> <springProfile name=\\\"dev\\\"> <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} ----> [%thread] ---> %-5level %logger{50} - %msg%n</pattern> </springProfile> <springProfile name=\\\"!dev\\\"> <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} **** [%thread] **** %-5level %logger{50} - %msg%n</pattern> </springProfile> </layout> </appender> \",\"如果使用logback.xml作为日志配置文件，还要使用profile功能，会有以下错误\",\"no applicable action for [springProfile]\"]},{\"header\":\"5、切换日志框架\",\"slug\":\"_5、切换日志框架\",\"contents\":[\"可以按照slf4j的日志适配图，进行相关的切换；\",\"slf4j+log4j的方式；\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> <exclusions> <exclusion> <artifactId>logback-classic</artifactId> <groupId>ch.qos.logback</groupId> </exclusion> <exclusion> <artifactId>log4j-over-slf4j</artifactId> <groupId>org.slf4j</groupId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </dependency> \",\"切换为log4j2\",\" <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> <exclusions> <exclusion> <artifactId>spring-boot-starter-logging</artifactId> <groupId>org.springframework.boot</groupId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-log4j2</artifactId> </dependency> \"]}]},\"/study-tutorial/microservice/spring-boot2/started.html\":{\"title\":\"1、Spring Boot 入门\",\"contents\":[{\"header\":\"1、Spring Boot 简介\",\"slug\":\"_1、spring-boot-简介\",\"contents\":[\"简化Spring应用开发的一个框架；\",\"整个Spring技术栈的一个大整合；\",\"J2EE开发的一站式解决方案；\"]},{\"header\":\"2、微服务\",\"slug\":\"_2、微服务\",\"contents\":[\"2014，martin fowler\",\"微服务：架构风格（服务微化）\",\"一个应用应该是一组小型服务；可以通过HTTP的方式进行互通；\",\"单体应用：ALL IN ONE\",\"微服务：每一个功能元素最终都是一个可独立替换和独立升级的软件单元；\",\"详细参照微服务文档\"]},{\"header\":\"3、环境准备\",\"slug\":\"_3、环境准备\",\"contents\":[\"环境约束\",\"–jdk1.8：Spring Boot 推荐jdk1.7及以上；java version \\\"1.8.0_112\\\"\",\"–maven3.x：maven 3.3以上版本；Apache Maven 3.3.9\",\"–IntelliJIDEA2017：IntelliJ IDEA 2017.2.2 x64、STS\",\"–SpringBoot 1.5.9.RELEASE：1.5.9；\",\"统一环境；\"]},{\"header\":\"1、MAVEN设置；\",\"slug\":\"_1、maven设置\",\"contents\":[\"给maven 的settings.xml配置文件的profiles标签添加\",\"<profile> <id>jdk-1.8</id> <activation> <activeByDefault>true</activeByDefault> <jdk>1.8</jdk> </activation> <properties> <maven.compiler.source>1.8</maven.compiler.source> <maven.compiler.target>1.8</maven.compiler.target> <maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion> </properties> </profile> \"]},{\"header\":\"2、IDEA设置\",\"slug\":\"_2、idea设置\",\"contents\":[\"整合maven进来；\"]},{\"header\":\"4、Spring Boot HelloWorld\",\"slug\":\"_4、spring-boot-helloworld\",\"contents\":[\"一个功能：\",\"浏览器发送hello请求，服务器接受请求并处理，响应Hello World字符串；\"]},{\"header\":\"1、创建一个maven工程；（jar）\",\"slug\":\"_1、创建一个maven工程-jar\",\"contents\":[]},{\"header\":\"2、导入spring boot相关的依赖\",\"slug\":\"_2、导入spring-boot相关的依赖\",\"contents\":[\" <parent> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-parent</artifactId> <version>1.5.9.RELEASE</version> </parent> <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> </dependencies> \"]},{\"header\":\"3、编写一个主程序；启动Spring Boot应用\",\"slug\":\"_3、编写一个主程序-启动spring-boot应用\",\"contents\":[\" /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } \"]},{\"header\":\"4、编写相关的Controller、Service\",\"slug\":\"_4、编写相关的controller、service\",\"contents\":[\"@Controller public class HelloController { @ResponseBody @RequestMapping(\\\"/hello\\\") public String hello(){ return \\\"Hello World!\\\"; } } \"]},{\"header\":\"5、运行主程序测试\",\"slug\":\"_5、运行主程序测试\",\"contents\":[]},{\"header\":\"6、简化部署\",\"slug\":\"_6、简化部署\",\"contents\":[\" <!-- 这个插件，可以将应用打包成一个可执行的jar包；--> <build> <plugins> <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> </plugin> </plugins> </build> \",\"将这个应用打成jar包，直接使用java -jar的命令进行执行；\"]},{\"header\":\"5、Hello World探究\",\"slug\":\"_5、hello-world探究\",\"contents\":[]},{\"header\":\"1、POM文件\",\"slug\":\"_1、pom文件\",\"contents\":[]},{\"header\":\"1、父项目\",\"slug\":\"_1、父项目\",\"contents\":[\"<parent> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-parent</artifactId> <version>1.5.9.RELEASE</version> </parent> <!--他的父项目是--> <parent> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-dependencies</artifactId> <version>1.5.9.RELEASE</version> <relativePath>../../spring-boot-dependencies</relativePath> </parent> <!--他来真正管理Spring Boot应用里面的所有依赖版本；--> \",\"Spring Boot的版本仲裁中心；\",\"以后我们导入依赖默认是不需要写版本；（没有在dependencies里面管理的依赖自然需要声明版本号）\"]},{\"header\":\"2、启动器\",\"slug\":\"_2、启动器\",\"contents\":[\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> \",\"spring-boot-starter-web：spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件；\",\"Spring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖\",\"都会导入进来。要用什么功能就导入什么场景的启动器\"]},{\"header\":\"2、主程序类，主入口类\",\"slug\":\"_2、主程序类-主入口类\",\"contents\":[\"/** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } \",\"@SpringBootApplication：Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用；\",\"@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { \",\"@SpringBootConfiguration：Spring Boot的配置类；标注在某个类上，表示这是一个Spring Boot的配置类；\",\"@Configuration：配置类上来标注这个注解；配置类 ----- 配置文件；配置类也是容器中的一个组件；@Component\",\"@EnableAutoConfiguration：开启自动配置功能；\",\"​ 以前我们需要配置的东西，Spring Boot帮我们自动配置；@EnableAutoConfiguration告诉SpringBoot开启自动配置功能；这样自动配置才能生效；\",\"@AutoConfigurationPackage @Import(EnableAutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { \",\"@AutoConfigurationPackage：自动配置包\",\"@Import(AutoConfigurationPackages.Registrar.class)：\",\"​ Spring的底层注解@Import，给容器中导入一个组件；导入的组件由AutoConfigurationPackages.Registrar.class；\",\"将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器；\",\"​ @Import(EnableAutoConfigurationImportSelector.class)；\",\"​ 给容器中导入组件？\",\"​ EnableAutoConfigurationImportSelector：导入哪些组件的选择器；\",\"​ 将所有需要导入的组件以全类名的方式返回；这些组件就会被添加到容器中；\",\"​ 会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；就是给容器中导入这个场景需要的所有组件，并配置好这些组件； \",\"有了自动配置类，免去了我们手动编写配置注入功能组件等的工作；\",\"SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,classLoader)； \",\"**Spring Boot在启动的时候从类路径下的META-INF/spring.factories中获取EnableAutoConfiguration指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作；**以前我们需要自己配置的东西，自动配置类都帮我们；\",\"J2EE的整体整合解决方案和自动配置都在spring-boot-autoconfigure-1.5.9.RELEASE.jar；\"]},{\"header\":\"6、使用Spring Initializer快速创建Spring Boot项目\",\"slug\":\"_6、使用spring-initializer快速创建spring-boot项目\",\"contents\":[]},{\"header\":\"1、IDEA：使用 Spring Initializer快速创建项目\",\"slug\":\"_1、idea-使用-spring-initializer快速创建项目\",\"contents\":[\"IDE都支持使用Spring的项目创建向导快速创建一个Spring Boot项目；\",\"选择我们需要的模块；向导会联网创建Spring Boot项目；\",\"默认生成的Spring Boot项目；\",\"主程序已经生成好了，我们只需要我们自己的逻辑\",\"resources文件夹中目录结构 \",\"static：保存所有的静态资源； js css images；\",\"templates：保存所有的模板页面；（Spring Boot默认jar包使用嵌入式的Tomcat，默认不支持JSP页面）；可以使用模板引擎（freemarker、thymeleaf）；\",\"application.properties：Spring Boot应用的配置文件；可以修改一些默认设置；\"]}]},\"/study-tutorial/microservice/spring-boot2/start_principle.html\":{\"title\":\"6、启动配置原理\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"几个重要的事件回调机制\",\"配置在META-INF/spring.factories\",\"ApplicationContextInitializer\",\"SpringApplicationRunListener\",\"只需要放在ioc容器中\",\"ApplicationRunner\",\"CommandLineRunner\",\"启动流程：\"]},{\"header\":\"1、创建SpringApplication对象\",\"slug\":\"_1、创建springapplication对象\",\"contents\":[\"initialize(sources); private void initialize(Object[] sources) { //保存主配置类 if (sources != null && sources.length > 0) { this.sources.addAll(Arrays.asList(sources)); } //判断当前是否一个web应用 this.webEnvironment = deduceWebEnvironment(); //从类路径下找到META-INF/spring.factories配置的所有ApplicationContextInitializer；然后保存起来 setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); //从类路径下找到ETA-INF/spring.factories配置的所有ApplicationListener setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //从多个配置类中找到有main方法的主配置类 this.mainApplicationClass = deduceMainApplicationClass(); } \"]},{\"header\":\"2、运行run方法\",\"slug\":\"_2、运行run方法\",\"contents\":[\"public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; FailureAnalyzers analyzers = null; configureHeadlessProperty(); //获取SpringApplicationRunListeners；从类路径下META-INF/spring.factories SpringApplicationRunListeners listeners = getRunListeners(args); //回调所有的获取SpringApplicationRunListener.starting()方法 listeners.starting(); try { //封装命令行参数 ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); //准备环境 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); //创建环境完成后回调SpringApplicationRunListener.environmentPrepared()；表示环境准备完成 Banner printedBanner = printBanner(environment); //创建ApplicationContext；决定创建web的ioc还是普通的ioc context = createApplicationContext(); analyzers = new FailureAnalyzers(context); //准备上下文环境;将environment保存到ioc中；而且applyInitializers()； //applyInitializers()：回调之前保存的所有的ApplicationContextInitializer的initialize方法 //回调所有的SpringApplicationRunListener的contextPrepared()； // prepareContext(context, environment, listeners, applicationArguments, printedBanner); //prepareContext运行完成以后回调所有的SpringApplicationRunListener的contextLoaded（）； //s刷新容器；ioc容器初始化（如果是web应用还会创建嵌入式的Tomcat）；Spring注解版 //扫描，创建，加载所有组件的地方；（配置类，组件，自动配置） refreshContext(context); //从ioc容器中获取所有的ApplicationRunner和CommandLineRunner进行回调 //ApplicationRunner先回调，CommandLineRunner再回调 afterRefresh(context, applicationArguments); //所有的SpringApplicationRunListener回调finished方法 listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } //整个SpringBoot应用启动完成以后返回启动的ioc容器； return context; } catch (Throwable ex) { handleRunFailure(context, listeners, analyzers, ex); throw new IllegalStateException(ex); } } \"]},{\"header\":\"3、事件监听机制\",\"slug\":\"_3、事件监听机制\",\"contents\":[\"配置在META-INF/spring.factories\",\"ApplicationContextInitializer\",\"public class HelloApplicationContextInitializer implements ApplicationContextInitializer<ConfigurableApplicationContext> { @Override public void initialize(ConfigurableApplicationContext applicationContext) { System.out.println(\\\"ApplicationContextInitializer...initialize...\\\"+applicationContext); } } \",\"SpringApplicationRunListener\",\"public class HelloSpringApplicationRunListener implements SpringApplicationRunListener { //必须有的构造器 public HelloSpringApplicationRunListener(SpringApplication application, String[] args){ } @Override public void starting() { System.out.println(\\\"SpringApplicationRunListener...starting...\\\"); } @Override public void environmentPrepared(ConfigurableEnvironment environment) { Object o = environment.getSystemProperties().get(\\\"os.name\\\"); System.out.println(\\\"SpringApplicationRunListener...environmentPrepared..\\\"+o); } @Override public void contextPrepared(ConfigurableApplicationContext context) { System.out.println(\\\"SpringApplicationRunListener...contextPrepared...\\\"); } @Override public void contextLoaded(ConfigurableApplicationContext context) { System.out.println(\\\"SpringApplicationRunListener...contextLoaded...\\\"); } @Override public void finished(ConfigurableApplicationContext context, Throwable exception) { System.out.println(\\\"SpringApplicationRunListener...finished...\\\"); } } \",\"配置（META-INF/spring.factories）\",\"org.springframework.context.ApplicationContextInitializer=\\\\ com.atguigu.springboot.listener.HelloApplicationContextInitializer org.springframework.boot.SpringApplicationRunListener=\\\\ com.atguigu.springboot.listener.HelloSpringApplicationRunListener \",\"只需要放在ioc容器中\",\"ApplicationRunner\",\"@Component public class HelloApplicationRunner implements ApplicationRunner { @Override public void run(ApplicationArguments args) throws Exception { System.out.println(\\\"ApplicationRunner...run....\\\"); } } \",\"CommandLineRunner\",\"@Component public class HelloCommandLineRunner implements CommandLineRunner { @Override public void run(String... args) throws Exception { System.out.println(\\\"CommandLineRunner...run...\\\"+ Arrays.asList(args)); } } \"]}]},\"/study-tutorial/microservice/spring-boot2/web_development.html\":{\"title\":\"4、Web开发\",\"contents\":[{\"header\":\"1、简介\",\"slug\":\"_1、简介\",\"contents\":[\"使用SpringBoot；\",\"1）、创建SpringBoot应用，选中我们需要的模块；\",\"2）、SpringBoot已经默认将这些场景配置好了，只需要在配置文件中指定少量配置就可以运行起来\",\"3）、自己编写业务代码；\",\"自动配置原理？\",\"这个场景SpringBoot帮我们配置了什么？能不能修改？能修改哪些配置？能不能扩展？xxx\",\"xxxxAutoConfiguration：帮我们给容器中自动配置组件； xxxxProperties:配置类来封装配置文件的内容； \"]},{\"header\":\"2、SpringBoot对静态资源的映射规则；\",\"slug\":\"_2、springboot对静态资源的映射规则\",\"contents\":[\"@ConfigurationProperties(prefix = \\\"spring.resources\\\", ignoreUnknownFields = false) public class ResourceProperties implements ResourceLoaderAware { //可以设置和静态资源有关的参数，缓存时间等 \",\" WebMvcAuotConfiguration： @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { if (!this.resourceProperties.isAddMappings()) { logger.debug(\\\"Default resource handling disabled\\\"); return; } Integer cachePeriod = this.resourceProperties.getCachePeriod(); if (!registry.hasMappingForPattern(\\\"/webjars/**\\\")) { customizeResourceHandlerRegistration( registry.addResourceHandler(\\\"/webjars/**\\\") .addResourceLocations( \\\"classpath:/META-INF/resources/webjars/\\\") .setCachePeriod(cachePeriod)); } String staticPathPattern = this.mvcProperties.getStaticPathPattern(); //静态资源文件夹映射 if (!registry.hasMappingForPattern(staticPathPattern)) { customizeResourceHandlerRegistration( registry.addResourceHandler(staticPathPattern) .addResourceLocations( this.resourceProperties.getStaticLocations()) .setCachePeriod(cachePeriod)); } } //配置欢迎页映射 @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping( ResourceProperties resourceProperties) { return new WelcomePageHandlerMapping(resourceProperties.getWelcomePage(), this.mvcProperties.getStaticPathPattern()); } //配置喜欢的图标 @Configuration @ConditionalOnProperty(value = \\\"spring.mvc.favicon.enabled\\\", matchIfMissing = true) public static class FaviconConfiguration { private final ResourceProperties resourceProperties; public FaviconConfiguration(ResourceProperties resourceProperties) { this.resourceProperties = resourceProperties; } @Bean public SimpleUrlHandlerMapping faviconHandlerMapping() { SimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping(); mapping.setOrder(Ordered.HIGHEST_PRECEDENCE + 1); //所有 **/favicon.ico mapping.setUrlMap(Collections.singletonMap(\\\"**/favicon.ico\\\", faviconRequestHandler())); return mapping; } @Bean public ResourceHttpRequestHandler faviconRequestHandler() { ResourceHttpRequestHandler requestHandler = new ResourceHttpRequestHandler(); requestHandler .setLocations(this.resourceProperties.getFaviconLocations()); return requestHandler; } } \",\"1）、所有 /webjars/ ，都去 classpath:/META-INF/resources/webjars/ 找资源；**\",\"​ webjars：以jar包的方式引入静态资源；\",\"http://www.webjars.org/\",\"localhost:8080/webjars/jquery/3.3.1/jquery.js\",\"<!--引入jquery-webjar-->在访问的时候只需要写webjars下面资源的名称即可 <dependency> <groupId>org.webjars</groupId> <artifactId>jquery</artifactId> <version>3.3.1</version> </dependency> \",\"2）、\\\"/\\\" 访问当前项目的任何资源，都去（静态资源的文件夹）找映射**\",\"\\\"classpath:/META-INF/resources/\\\", \\\"classpath:/resources/\\\", \\\"classpath:/static/\\\", \\\"classpath:/public/\\\" \\\"/\\\"：当前项目的根路径 \",\"localhost:8080/abc **= 去静态资源文件夹里面找abc\",\"3）、欢迎页； 静态资源文件夹下的所有index.html页面；被\\\"/\\\"映射；**\",\"​ localhost:8080/ 找index页面\",\"**4）、所有的 /favicon.ico 都是在静态资源文件下找；\"]},{\"header\":\"3、模板引擎\",\"slug\":\"_3、模板引擎\",\"contents\":[\"JSP、Velocity、Freemarker、Thymeleaf\",\"SpringBoot推荐的Thymeleaf；\",\"语法更简单，功能更强大；\"]},{\"header\":\"1、引入thymeleaf；\",\"slug\":\"_1、引入thymeleaf\",\"contents\":[\" <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-thymeleaf</artifactId> 2.1.6 </dependency> 切换thymeleaf版本 <properties> <thymeleaf.version>3.0.9.RELEASE</thymeleaf.version> <!-- 布局功能的支持程序 thymeleaf3主程序 layout2以上版本 --> <!-- thymeleaf2 layout1--> <thymeleaf-layout-dialect.version>2.2.2</thymeleaf-layout-dialect.version> </properties> \"]},{\"header\":\"2、Thymeleaf使用\",\"slug\":\"_2、thymeleaf使用\",\"contents\":[\"@ConfigurationProperties(prefix = \\\"spring.thymeleaf\\\") public class ThymeleafProperties { private static final Charset DEFAULT_ENCODING = Charset.forName(\\\"UTF-8\\\"); private static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf(\\\"text/html\\\"); public static final String DEFAULT_PREFIX = \\\"classpath:/templates/\\\"; public static final String DEFAULT_SUFFIX = \\\".html\\\"; // \",\"只要我们把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染；\",\"使用：\",\"1、导入thymeleaf的名称空间\",\"<html lang=\\\"en\\\" xmlns:th=\\\"http://www.thymeleaf.org\\\"> \",\"2、使用thymeleaf语法；\",\"<!DOCTYPE html> <html lang=\\\"en\\\" xmlns:th=\\\"http://www.thymeleaf.org\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>Title</title> </head> <body> <h1>成功！</h1> <!--th:text 将div里面的文本内容设置为 --> <div th:text=\\\"${hello}\\\">这是显示欢迎信息</div> </body> </html> \"]},{\"header\":\"3、语法规则\",\"slug\":\"_3、语法规则\",\"contents\":[\"1）、th:text；改变当前元素里面的文本内容；\",\"​ th：任意html属性；来替换原生属性的值\",\"2）、表达式？\",\"Simple expressions:（表达式语法） Variable Expressions: ${...}：获取变量值；OGNL； 1）、获取对象的属性、调用方法 2）、使用内置的基本对象： #ctx : the context object. #vars: the context variables. #locale : the context locale. #request : (only in Web Contexts) the HttpServletRequest object. #response : (only in Web Contexts) the HttpServletResponse object. #session : (only in Web Contexts) the HttpSession object. #servletContext : (only in Web Contexts) the ServletContext object. ${session.foo} 3）、内置的一些工具对象： #execInfo : information about the template being processed. #messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #{…} syntax. #uris : methods for escaping parts of URLs/URIs #conversions : methods for executing the configured conversion service (if any). #dates : methods for java.util.Date objects: formatting, component extraction, etc. #calendars : analogous to #dates , but for java.util.Calendar objects. #numbers : methods for formatting numeric objects. #strings : methods for String objects: contains, startsWith, prepending/appending, etc. #objects : methods for objects in general. #bools : methods for boolean evaluation. #arrays : methods for arrays. #lists : methods for lists. #sets : methods for sets. #maps : methods for maps. #aggregates : methods for creating aggregates on arrays or collections. #ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). Selection Variable Expressions: *{...}：选择表达式：和${}在功能上是一样； 补充：配合 th:object=\\\"${session.user}： <div th:object=\\\"${session.user}\\\"> <p>Name: <span th:text=\\\"*{firstName}\\\">Sebastian</span>.</p> <p>Surname: <span th:text=\\\"*{lastName}\\\">Pepper</span>.</p> <p>Nationality: <span th:text=\\\"*{nationality}\\\">Saturn</span>.</p> </div> Message Expressions: #{...}：获取国际化内容 Link URL Expressions: @{...}：定义URL； @{/order/process(execId=${execId},execType='FAST')} Fragment Expressions: ~{...}：片段引用表达式 <div th:insert=\\\"~{commons :: main}\\\">...</div> Literals（字面量） Text literals: 'one text' , 'Another one!' ,… Number literals: 0 , 34 , 3.0 , 12.3 ,… Boolean literals: true , false Null literal: null Literal tokens: one , sometext , main ,… Text operations:（文本操作） String concatenation: + Literal substitutions: |The name is ${name}| Arithmetic operations:（数学运算） Binary operators: + , - , * , / , % Minus sign (unary operator): - Boolean operations:（布尔运算） Binary operators: and , or Boolean negation (unary operator): ! , not Comparisons and equality:（比较运算） Comparators: > , < , >= , <= ( gt , lt , ge , le ) Equality operators: ** , != ( eq , ne ) Conditional operators:条件运算（三元运算符） If-then: (if) ? (then) If-then-else: (if) ? (then) : (else) Default: (value) ?: (defaultvalue) Special tokens: No-Operation: _ \"]},{\"header\":\"4、SpringMVC自动配置\",\"slug\":\"_4、springmvc自动配置\",\"contents\":[\"https://docs.spring.io/spring-boot/docs/1.5.10.RELEASE/reference/htmlsingle/#boot-features-developing-web-applications\"]},{\"header\":\"1. Spring MVC auto-configuration\",\"slug\":\"_1-spring-mvc-auto-configuration\",\"contents\":[\"Spring Boot 自动配置好了SpringMVC\",\"以下是SpringBoot对SpringMVC的默认配置:（WebMvcAutoConfiguration）\",\"Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans.\",\"自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View），视图对象决定如何渲染（转发？重定向？））\",\"ContentNegotiatingViewResolver：组合所有的视图解析器的；\",\"如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来；\",\"Support for serving static resources, including support for WebJars (see below).静态资源文件夹路径,webjars\",\"Static index.html support. 静态首页访问\",\"Custom Favicon support (see below). favicon.ico\",\"自动注册了 of Converter, GenericConverter, Formatter beans.\",\"Converter：转换器； public String hello(User user)：类型转换使用Converter\",\"Formatter 格式化器； 2017.12.17**=Date；\",\" @Bean @ConditionalOnProperty(prefix = \\\"spring.mvc\\\", name = \\\"date-format\\\")//在文件中配置日期格式化的规则 public Formatter<Date> dateFormatter() { return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件 } \",\"​ 自己添加的格式化器转换器，我们只需要放在容器中即可\",\"Support for HttpMessageConverters (see below).\",\"HttpMessageConverter：SpringMVC用来转换Http请求和响应的；User---Json；\",\"HttpMessageConverters 是从容器中确定；获取所有的HttpMessageConverter；\",\"自己给容器中添加HttpMessageConverter，只需要将自己的组件注册容器中（@Bean,@Component）\",\"Automatic registration of MessageCodesResolver (see below).定义错误代码生成规则\",\"Automatic use of a ConfigurableWebBindingInitializer bean (see below).\",\"我们可以配置一个ConfigurableWebBindingInitializer来替换默认的；（添加到容器）\",\"初始化WebDataBinder； 请求数据****=JavaBean； \",\"org.springframework.boot.autoconfigure.web：web的所有自动场景；\",\"If you want to keep Spring Boot MVC features, and you just want to add additional MVC configuration (interceptors, formatters, view controllers etc.) you can add your own @Configuration class of type WebMvcConfigurerAdapter, but without@EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter or ExceptionHandlerExceptionResolver you can declare a WebMvcRegistrationsAdapter instance providing such components.\",\"If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc.\"]},{\"header\":\"2、扩展SpringMVC\",\"slug\":\"_2、扩展springmvc\",\"contents\":[\" <mvc:view-controller path=\\\"/hello\\\" view-name=\\\"success\\\"/> <mvc:interceptors> <mvc:interceptor> <mvc:mapping path=\\\"/hello\\\"/> <bean></bean> </mvc:interceptor> </mvc:interceptors> \",\"编写一个配置类（@Configuration），是WebMvcConfigurerAdapter类型；不能标注@EnableWebMvc;\",\"既保留了所有的自动配置，也能用我们扩展的配置；\",\"//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能 @Configuration public class MyMvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController(\\\"/atguigu\\\").setViewName(\\\"success\\\"); } } \",\"原理：\",\"​ 1）、WebMvcAutoConfiguration是SpringMVC的自动配置类\",\"​ 2）、在做其他自动配置时会导入；@Import(EnableWebMvcConfiguration.class)\",\" @Configuration public static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration { private final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite(); //从容器中获取所有的WebMvcConfigurer @Autowired(required = false) public void setConfigurers(List<WebMvcConfigurer> configurers) { if (!CollectionUtils.isEmpty(configurers)) { this.configurers.addWebMvcConfigurers(configurers); //一个参考实现；将所有的WebMvcConfigurer相关配置都来一起调用； @Override // public void addViewControllers(ViewControllerRegistry registry) { // for (WebMvcConfigurer delegate : this.delegates) { // delegate.addViewControllers(registry); // } } } } \",\"​ 3）、容器中所有的WebMvcConfigurer都会一起起作用；\",\"​ 4）、我们的配置类也会被调用；\",\"​ 效果：SpringMVC的自动配置和我们的扩展配置都会起作用；\"]},{\"header\":\"3、全面接管SpringMVC；\",\"slug\":\"_3、全面接管springmvc\",\"contents\":[\"SpringBoot对SpringMVC的自动配置不需要了，所有都是我们自己配置；所有的SpringMVC的自动配置都失效了\",\"我们需要在配置类中添加@EnableWebMvc即可；\",\"//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能 @EnableWebMvc @Configuration public class MyMvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController(\\\"/atguigu\\\").setViewName(\\\"success\\\"); } } \",\"原理：\",\"为什么@EnableWebMvc自动配置就失效了；\",\"1）@EnableWebMvc的核心\",\"@Import(DelegatingWebMvcConfiguration.class) public @interface EnableWebMvc { \",\"2）、\",\"@Configuration public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport { \",\"3）、\",\"@Configuration @ConditionalOnWebApplication @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurerAdapter.class }) //容器中没有这个组件的时候，这个自动配置类才生效 @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10) @AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, ValidationAutoConfiguration.class }) public class WebMvcAutoConfiguration { \",\"4）、@EnableWebMvc将WebMvcConfigurationSupport组件导入进来；\",\"5）、导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能；\"]},{\"header\":\"5、如何修改SpringBoot的默认配置\",\"slug\":\"_5、如何修改springboot的默认配置\",\"contents\":[\"模式：\",\"​ 1）、SpringBoot在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有，才自动配置；如果有些组件可以有多个（ViewResolver）将用户配置的和自己默认的组合起来；\",\"​ 2）、在SpringBoot中会有非常多的xxxConfigurer帮助我们进行扩展配置\",\"​ 3）、在SpringBoot中会有很多的xxxCustomizer帮助我们进行定制配置\"]},{\"header\":\"6、RestfulCRUD\",\"slug\":\"_6、restfulcrud\",\"contents\":[]},{\"header\":\"1）、默认访问首页\",\"slug\":\"_1-、默认访问首页\",\"contents\":[\" //使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能 //@EnableWebMvc 不要接管SpringMVC @Configuration public class MyMvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController(\\\"/atguigu\\\").setViewName(\\\"success\\\"); } //所有的WebMvcConfigurerAdapter组件都会一起起作用 @Bean //将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter(){ WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\\\"/\\\").setViewName(\\\"login\\\"); registry.addViewController(\\\"/index.html\\\").setViewName(\\\"login\\\"); } }; return adapter; } } \"]},{\"header\":\"2）、国际化\",\"slug\":\"_2-、国际化\",\"contents\":[\"1）、编写国际化配置文件；\",\"2）、使用ResourceBundleMessageSource管理国际化资源文件\",\"3）、在页面使用fmt:message取出国际化内容\",\"步骤：\",\"1）、编写国际化配置文件，抽取页面需要显示的国际化消息\",\"2）、SpringBoot自动配置好了管理国际化资源文件的组件；\",\"@ConfigurationProperties(prefix = \\\"spring.messages\\\") public class MessageSourceAutoConfiguration { /** * Comma-separated list of basenames (essentially a fully-qualified classpath * location), each following the ResourceBundle convention with relaxed support for * slash based locations. If it doesn't contain a package qualifier (such as * \\\"org.mypackage\\\"), it will be resolved from the classpath root. */ private String basename = \\\"messages\\\"; //我们的配置文件可以直接放在类路径下叫messages.properties； @Bean public MessageSource messageSource() { ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); if (StringUtils.hasText(this.basename)) { //设置国际化资源文件的基础名（去掉语言国家代码的） messageSource.setBasenames(StringUtils.commaDelimitedListToStringArray( StringUtils.trimAllWhitespace(this.basename))); } if (this.encoding != null) { messageSource.setDefaultEncoding(this.encoding.name()); } messageSource.setFallbackToSystemLocale(this.fallbackToSystemLocale); messageSource.setCacheSeconds(this.cacheSeconds); messageSource.setAlwaysUseMessageFormat(this.alwaysUseMessageFormat); return messageSource; } \",\"3）、去页面获取国际化的值；\",\"<!DOCTYPE html> <html lang=\\\"en\\\" xmlns:th=\\\"http://www.thymeleaf.org\\\"> <head> <meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"> <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1, shrink-to-fit=no\\\"> <meta name=\\\"description\\\" content=\\\"\\\"> <meta name=\\\"author\\\" content=\\\"\\\"> <title>Signin Template for Bootstrap</title> <!-- Bootstrap core CSS --> <link href=\\\"asserts/css/bootstrap.min.css\\\" th:href=\\\"@{/webjars/bootstrap/4.0.0/css/bootstrap.css}\\\" rel=\\\"stylesheet\\\"> <!-- Custom styles for this template --> <link href=\\\"asserts/css/signin.css\\\" th:href=\\\"@{/asserts/css/signin.css}\\\" rel=\\\"stylesheet\\\"> </head> <body class=\\\"text-center\\\"> <form class=\\\"form-signin\\\" action=\\\"dashboard.html\\\"> <img class=\\\"mb-4\\\" th:src=\\\"@{/asserts/img/bootstrap-solid.svg}\\\" src=\\\"asserts/img/bootstrap-solid.svg\\\" alt=\\\"\\\" width=\\\"72\\\" height=\\\"72\\\"> <h1 class=\\\"h3 mb-3 font-weight-normal\\\" th:text=\\\"#{login.tip}\\\">Please sign in</h1> <label class=\\\"sr-only\\\" th:text=\\\"#{login.username}\\\">Username</label> <input type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"Username\\\" th:placeholder=\\\"#{login.username}\\\" required=\\\"\\\" autofocus=\\\"\\\"> <label class=\\\"sr-only\\\" th:text=\\\"#{login.password}\\\">Password</label> <input type=\\\"password\\\" class=\\\"form-control\\\" placeholder=\\\"Password\\\" th:placeholder=\\\"#{login.password}\\\" required=\\\"\\\"> <div class=\\\"checkbox mb-3\\\"> <label> <input type=\\\"checkbox\\\" value=\\\"remember-me\\\"/> [[#{login.remember}]] </label> </div> <button class=\\\"btn btn-lg btn-primary btn-block\\\" type=\\\"submit\\\" th:text=\\\"#{login.btn}\\\">Sign in</button> <p class=\\\"mt-5 mb-3 text-muted\\\">© 2017-2018</p> <a class=\\\"btn btn-sm\\\">中文</a> <a class=\\\"btn btn-sm\\\">English</a> </form> </body> </html> \",\"效果：根据浏览器语言设置的信息切换了国际化；\",\"原理：\",\"​ 国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）；\",\" @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = \\\"spring.mvc\\\", name = \\\"locale\\\") public LocaleResolver localeResolver() { if (this.mvcProperties .getLocaleResolver() ** WebMvcProperties.LocaleResolver.FIXED) { return new FixedLocaleResolver(this.mvcProperties.getLocale()); } AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver; } 默认的就是根据请求头带来的区域信息获取Locale进行国际化 \",\"4）、点击链接切换国际化\",\"/** * 可以在连接上携带区域信息 */ public class MyLocaleResolver implements LocaleResolver { @Override public Locale resolveLocale(HttpServletRequest request) { String l = request.getParameter(\\\"l\\\"); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(l)){ String[] split = l.split(\\\"_\\\"); locale = new Locale(split[0],split[1]); } return locale; } @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) { } } @Bean public LocaleResolver localeResolver(){ return new MyLocaleResolver(); } } \"]},{\"header\":\"3）、登陆\",\"slug\":\"_3-、登陆\",\"contents\":[\"开发期间模板引擎页面修改以后，要实时生效\",\"1）、禁用模板引擎的缓存\",\"## 禁用缓存 spring.thymeleaf.cache=false \",\"2）、页面修改完成以后ctrl+f9：重新编译；\",\"登陆错误消息的显示\",\"<p style=\\\"color: red\\\" th:text=\\\"${msg}\\\" th:if=\\\"${not #strings.isEmpty(msg)}\\\"></p> \"]},{\"header\":\"4）、拦截器进行登陆检查\",\"slug\":\"_4-、拦截器进行登陆检查\",\"contents\":[\"拦截器\",\" /** * 登陆检查， */ public class LoginHandlerInterceptor implements HandlerInterceptor { //目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { Object user = request.getSession().getAttribute(\\\"loginUser\\\"); if(user ** null){ //未登陆，返回登陆页面 request.setAttribute(\\\"msg\\\",\\\"没有权限请先登陆\\\"); request.getRequestDispatcher(\\\"/index.html\\\").forward(request,response); return false; }else{ //已登陆，放行请求 return true; } } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { } } \",\"注册拦截器\",\" //所有的WebMvcConfigurerAdapter组件都会一起起作用 @Bean //将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter(){ WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\\\"/\\\").setViewName(\\\"login\\\"); registry.addViewController(\\\"/index.html\\\").setViewName(\\\"login\\\"); registry.addViewController(\\\"/main.html\\\").setViewName(\\\"dashboard\\\"); } //注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) { //super.addInterceptors(registry); //静态资源； *.css , *.js //SpringBoot已经做好了静态资源映射 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(\\\"/**\\\") .excludePathPatterns(\\\"/index.html\\\",\\\"/\\\",\\\"/user/login\\\"); } }; return adapter; } \"]},{\"header\":\"5）、CRUD-员工列表\",\"slug\":\"_5-、crud-员工列表\",\"contents\":[\"实验要求：\",\"1）、RestfulCRUD：CRUD满足Rest风格；\",\"URI： /资源名称/资源标识 HTTP请求方式区分对资源CRUD操作\",\"普通CRUD（uri来区分操作）\",\"RestfulCRUD\",\"查询\",\"getEmp\",\"emp---GET\",\"添加\",\"addEmp?xxx\",\"emp---POST\",\"修改\",\"updateEmp?id=xxx&xxx=xx\",\"emp/{id}---PUT\",\"删除\",\"deleteEmp?id=1\",\"emp/{id}---DELETE\",\"2）、实验的请求架构;\",\"实验功能\",\"请求URI\",\"请求方式\",\"查询所有员工\",\"emps\",\"GET\",\"查询某个员工(来到修改页面)\",\"emp/1\",\"GET\",\"来到添加页面\",\"emp\",\"GET\",\"添加员工\",\"emp\",\"POST\",\"来到修改页面（查出员工进行信息回显）\",\"emp/1\",\"GET\",\"修改员工\",\"emp\",\"PUT\",\"删除员工\",\"emp/1\",\"DELETE\",\"3）、员工列表：\"]},{\"header\":\"thymeleaf公共页面元素抽取\",\"slug\":\"thymeleaf公共页面元素抽取\",\"contents\":[\"1、抽取公共片段 <div th:fragment=\\\"copy\\\"> &copy; 2011 The Good Thymes Virtual Grocery </div> 2、引入公共片段 <div th:insert=\\\"~{footer :: copy}\\\"></div> ~{templatename::selector}：模板名::选择器 ~{templatename::fragmentname}:模板名::片段名 3、默认效果： insert的公共片段在div标签中 如果使用th:insert等属性进行引入，可以不用写~{}： 行内写法可以加上：[[~{}]];[(~{})]； \",\"三种引入公共片段的th属性：\",\"th:insert：将公共片段整个插入到声明引入的元素中\",\"th:replace：将声明引入的元素替换为公共片段\",\"th:include：将被引入的片段的内容包含进这个标签中\",\"<footer th:fragment=\\\"copy\\\"> &copy; 2011 The Good Thymes Virtual Grocery </footer> 引入方式 <div th:insert=\\\"footer :: copy\\\"></div> <div th:replace=\\\"footer :: copy\\\"></div> <div th:include=\\\"footer :: copy\\\"></div> 效果 <div> <footer> &copy; 2011 The Good Thymes Virtual Grocery </footer> </div> <footer> &copy; 2011 The Good Thymes Virtual Grocery </footer> <div> &copy; 2011 The Good Thymes Virtual Grocery </div> \",\"引入片段的时候传入参数：\",\" <nav class=\\\"col-md-2 d-none d-md-block bg-light sidebar\\\" id=\\\"sidebar\\\"> <div class=\\\"sidebar-sticky\\\"> <ul class=\\\"nav flex-column\\\"> <li class=\\\"nav-item\\\"> <a class=\\\"nav-link active\\\" th:class=\\\"${activeUri**'main.html'?'nav-link active':'nav-link'}\\\" href=\\\"#\\\" th:href=\\\"@{/main.html}\\\"> <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"24\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"currentColor\\\" stroke-width=\\\"2\\\" stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"feather feather-home\\\"> <path d=\\\"M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z\\\"></path> <polyline points=\\\"9 22 9 12 15 12 15 22\\\"></polyline> </svg> Dashboard <span class=\\\"sr-only\\\">(current)</span> </a> </li> <!--引入侧边栏;传入参数--> <div th:replace=\\\"commons/bar::#sidebar(activeUri='emps')\\\"></div> \"]},{\"header\":\"6）、CRUD-员工添加\",\"slug\":\"_6-、crud-员工添加\",\"contents\":[\"添加页面\",\"<form> <div class=\\\"form-group\\\"> <label>LastName</label> <input type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan\\\"> </div> <div class=\\\"form-group\\\"> <label>Email</label> <input type=\\\"email\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan@atguigu.com\\\"> </div> <div class=\\\"form-group\\\"> <label>Gender</label><br/> <div class=\\\"form-check form-check-inline\\\"> <input class=\\\"form-check-input\\\" type=\\\"radio\\\" name=\\\"gender\\\" value=\\\"1\\\"> <label class=\\\"form-check-label\\\">男</label> </div> <div class=\\\"form-check form-check-inline\\\"> <input class=\\\"form-check-input\\\" type=\\\"radio\\\" name=\\\"gender\\\" value=\\\"0\\\"> <label class=\\\"form-check-label\\\">女</label> </div> </div> <div class=\\\"form-group\\\"> <label>department</label> <select class=\\\"form-control\\\"> <option>1</option> <option>2</option> <option>3</option> <option>4</option> <option>5</option> </select> </div> <div class=\\\"form-group\\\"> <label>Birth</label> <input type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan\\\"> </div> <button type=\\\"submit\\\" class=\\\"btn btn-primary\\\">添加</button> </form> \",\"提交的数据格式不对：生日：日期；\",\"2017-12-12；2017/12/12；2017.12.12；\",\"日期的格式化；SpringMVC将页面提交的值需要转换为指定的类型;\",\"2017-12-12---Date； 类型转换，格式化;\",\"默认日期是按照/的方式；\"]},{\"header\":\"7）、CRUD-员工修改\",\"slug\":\"_7-、crud-员工修改\",\"contents\":[\"修改添加二合一表单\",\"<!--需要区分是员工修改还是添加；--> <form th:action=\\\"@{/emp}\\\" method=\\\"post\\\"> <!--发送put请求修改员工数据--> <!-- 1、SpringMVC中配置HiddenHttpMethodFilter;（SpringBoot自动配置好的） 2、页面创建一个post表单 3、创建一个input项，name=\\\"_method\\\";值就是我们指定的请求方式 --> <input type=\\\"hidden\\\" name=\\\"_method\\\" value=\\\"put\\\" th:if=\\\"${emp!=null}\\\"/> <input type=\\\"hidden\\\" name=\\\"id\\\" th:if=\\\"${emp!=null}\\\" th:value=\\\"${emp.id}\\\"> <div class=\\\"form-group\\\"> <label>LastName</label> <input name=\\\"lastName\\\" type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan\\\" th:value=\\\"${emp!=null}?${emp.lastName}\\\"> </div> <div class=\\\"form-group\\\"> <label>Email</label> <input name=\\\"email\\\" type=\\\"email\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan@atguigu.com\\\" th:value=\\\"${emp!=null}?${emp.email}\\\"> </div> <div class=\\\"form-group\\\"> <label>Gender</label><br/> <div class=\\\"form-check form-check-inline\\\"> <input class=\\\"form-check-input\\\" type=\\\"radio\\\" name=\\\"gender\\\" value=\\\"1\\\" th:checked=\\\"${emp!=null}?${emp.gender**1}\\\"> <label class=\\\"form-check-label\\\">男</label> </div> <div class=\\\"form-check form-check-inline\\\"> <input class=\\\"form-check-input\\\" type=\\\"radio\\\" name=\\\"gender\\\" value=\\\"0\\\" th:checked=\\\"${emp!=null}?${emp.gender**0}\\\"> <label class=\\\"form-check-label\\\">女</label> </div> </div> <div class=\\\"form-group\\\"> <label>department</label> <!--提交的是部门的id--> <select class=\\\"form-control\\\" name=\\\"department.id\\\"> <option th:selected=\\\"${emp!=null}?${dept.id ** emp.department.id}\\\" th:value=\\\"${dept.id}\\\" th:each=\\\"dept:${depts}\\\" th:text=\\\"${dept.departmentName}\\\">1</option> </select> </div> <div class=\\\"form-group\\\"> <label>Birth</label> <input name=\\\"birth\\\" type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"zhangsan\\\" th:value=\\\"${emp!=null}?${#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')}\\\"> </div> <button type=\\\"submit\\\" class=\\\"btn btn-primary\\\" th:text=\\\"${emp!=null}?'修改':'添加'\\\">添加</button> </form> \"]},{\"header\":\"8）、CRUD-员工删除\",\"slug\":\"_8-、crud-员工删除\",\"contents\":[\"<tr th:each=\\\"emp:${emps}\\\"> <td th:text=\\\"${emp.id}\\\"></td> <td>[[${emp.lastName}]]</td> <td th:text=\\\"${emp.email}\\\"></td> <td th:text=\\\"${emp.gender}**0?'女':'男'\\\"></td> <td th:text=\\\"${emp.department.departmentName}\\\"></td> <td th:text=\\\"${#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')}\\\"></td> <td> <a class=\\\"btn btn-sm btn-primary\\\" th:href=\\\"@{/emp/}+${emp.id}\\\">编辑</a> <button th:attr=\\\"del_uri=@{/emp/}+${emp.id}\\\" class=\\\"btn btn-sm btn-danger deleteBtn\\\">删除</button> </td> </tr> <script> $(\\\".deleteBtn\\\").click(function(){ //删除当前员工的 $(\\\"#deleteEmpForm\\\").attr(\\\"action\\\",$(this).attr(\\\"del_uri\\\")).submit(); return false; }); </script> \"]},{\"header\":\"7、错误处理机制\",\"slug\":\"_7、错误处理机制\",\"contents\":[]},{\"header\":\"1）、SpringBoot默认的错误处理机制\",\"slug\":\"_1-、springboot默认的错误处理机制\",\"contents\":[\"默认效果：\",\"​ 1）、浏览器，返回一个默认的错误页面\",\"浏览器发送请求的请求头：\",\"​ 2）、如果是其他客户端，默认响应一个json数据\",\"​ \",\"原理：\",\"​ 可以参照ErrorMvcAutoConfiguration；错误处理的自动配置；\",\"给容器中添加了以下组件 \",\"​ 1、DefaultErrorAttributes：\",\"帮我们在页面共享信息； @Override public Map<String, Object> getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) { Map<String, Object> errorAttributes = new LinkedHashMap<String, Object>(); errorAttributes.put(\\\"timestamp\\\", new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes; } \",\"​ 2、BasicErrorController：处理默认/error请求\",\"@Controller @RequestMapping(\\\"${server.error.path:${error.path:/error}}\\\") public class BasicErrorController extends AbstractErrorController { @RequestMapping(produces = \\\"text/html\\\")//产生html类型的数据；浏览器发送的请求来到这个方法处理 public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { HttpStatus status = getStatus(request); Map<String, Object> model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); //去哪个页面作为错误页面；包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView ** null ? new ModelAndView(\\\"error\\\", model) : modelAndView); } @RequestMapping @ResponseBody //产生json数据，其他客户端来到这个方法处理； public ResponseEntity<Map<String, Object>> error(HttpServletRequest request) { Map<String, Object> body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity<Map<String, Object>>(body, status); } \",\"​ 3、ErrorPageCustomizer：\",\" @Value(\\\"${error.path:/error}\\\") private String path = \\\"/error\\\"; 系统出现错误以后来到error请求进行处理；（web.xml注册的错误页面规则） \",\"​ 4、DefaultErrorViewResolver：\",\"@Override public ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map<String, Object> model) { ModelAndView modelAndView = resolve(String.valueOf(status), model); if (modelAndView ** null && SERIES_VIEWS.containsKey(status.series())) { modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); } return modelAndView; } private ModelAndView resolve(String viewName, Map<String, Object> model) { //默认SpringBoot可以去找到一个页面？ error/404 String errorViewName = \\\"error/\\\" + viewName; //模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) { //模板引擎可用的情况下返回到errorViewName指定的视图地址 return new ModelAndView(errorViewName, model); } //模板引擎不可用，就在静态资源文件夹下找errorViewName对应的页面 error/404.html return resolveResource(errorViewName, model); } \",\"​ 步骤：\",\"​ 一但系统出现4xx或者5xx之类的错误；ErrorPageCustomizer就会生效（定制错误的响应规则）；就会来到/error请求；就会被BasicErrorController处理；\",\"​ 1）响应页面；去哪个页面是由DefaultErrorViewResolver解析得到的；\",\"protected ModelAndView resolveErrorView(HttpServletRequest request, HttpServletResponse response, HttpStatus status, Map<String, Object> model) { //所有的ErrorViewResolver得到ModelAndView for (ErrorViewResolver resolver : this.errorViewResolvers) { ModelAndView modelAndView = resolver.resolveErrorView(request, status, model); if (modelAndView != null) { return modelAndView; } } return null; } \"]},{\"header\":\"2）、如果定制错误响应：\",\"slug\":\"_2-、如果定制错误响应\",\"contents\":[]},{\"header\":\"1）、如何定制错误的页面；\",\"slug\":\"_1-、如何定制错误的页面\",\"contents\":[\"​ 1）、有模板引擎的情况下；error/状态码; 【将错误页面命名为 错误状态码.html 放在模板引擎文件夹里面的 error文件夹下】，发生此状态码的错误就会来到 对应的页面；\",\"​ 我们可以使用4xx和5xx作为错误页面的文件名来匹配这种类型的所有错误，精确优先（优先寻找精确的状态码.html）；\",\"​ 页面能获取的信息；\",\"​ timestamp：时间戳\",\"​ status：状态码\",\"​ error：错误提示\",\"​ exception：异常对象\",\"​ message：异常消息\",\"​ errors：JSR303数据校验的错误都在这里\",\"​ 2）、没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹下找；\",\"​ 3）、以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面；\"]},{\"header\":\"2）、如何定制错误的json数据；\",\"slug\":\"_2-、如何定制错误的json数据\",\"contents\":[\"​ 1）、自定义异常处理&返回定制json数据；\",\"@ControllerAdvice public class MyExceptionHandler { @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map<String,Object> handleException(Exception e){ Map<String,Object> map = new HashMap<>(); map.put(\\\"code\\\",\\\"user.notexist\\\"); map.put(\\\"message\\\",e.getMessage()); return map; } } //没有自适应效果... \",\"​ 2）、转发到/error进行自适应响应效果处理\",\" @ExceptionHandler(UserNotExistException.class) public String handleException(Exception e, HttpServletRequest request){ Map<String,Object> map = new HashMap<>(); //传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程 /** * Integer statusCode = (Integer) request .getAttribute(\\\"javax.servlet.error.status_code\\\"); */ request.setAttribute(\\\"javax.servlet.error.status_code\\\",500); map.put(\\\"code\\\",\\\"user.notexist\\\"); map.put(\\\"message\\\",e.getMessage()); //转发到/error return \\\"forward:/error\\\"; } \"]},{\"header\":\"3）、将我们的定制数据携带出去；\",\"slug\":\"_3-、将我们的定制数据携带出去\",\"contents\":[\"出现错误以后，会来到/error请求，会被BasicErrorController处理，响应出去可以获取的数据是由getErrorAttributes得到的（是AbstractErrorController（ErrorController）规定的方法）；\",\"​ 1、完全来编写一个ErrorController的实现类【或者是编写AbstractErrorController的子类】，放在容器中；\",\"​ 2、页面上能用的数据，或者是json返回能用的数据都是通过errorAttributes.getErrorAttributes得到；\",\"​ 容器中DefaultErrorAttributes.getErrorAttributes()；默认进行数据处理的；\",\"自定义ErrorAttributes\",\"//给容器中加入我们自己定义的ErrorAttributes @Component public class MyErrorAttributes extends DefaultErrorAttributes { @Override public Map<String, Object> getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) { Map<String, Object> map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put(\\\"company\\\",\\\"atguigu\\\"); return map; } } \",\"最终的效果：响应是自适应的，可以通过定制ErrorAttributes改变需要返回的内容，\"]},{\"header\":\"8、配置嵌入式Servlet容器\",\"slug\":\"_8、配置嵌入式servlet容器\",\"contents\":[\"SpringBoot默认使用Tomcat作为嵌入式的Servlet容器；\",\"问题？\"]},{\"header\":\"1）、如何定制和修改Servlet容器的相关配置；\",\"slug\":\"_1-、如何定制和修改servlet容器的相关配置\",\"contents\":[\"1、修改和server有关的配置（ServerProperties【也是EmbeddedServletContainerCustomizer】）；\",\"server.port=8081 server.context-path=/crud server.tomcat.uri-encoding=UTF-8 //通用的Servlet容器设置 server.xxx //Tomcat的设置 server.tomcat.xxx \",\"2、编写一个EmbeddedServletContainerCustomizer：嵌入式的Servlet容器的定制器；来修改Servlet容器的配置\",\"@Bean //一定要将这个定制器加入到容器中 public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer(){ return new EmbeddedServletContainerCustomizer() { //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) { container.setPort(8083); } }; } \"]},{\"header\":\"2）、注册Servlet三大组件【Servlet、Filter、Listener】\",\"slug\":\"_2-、注册servlet三大组件【servlet、filter、listener】\",\"contents\":[\"由于SpringBoot默认是以jar包的方式启动嵌入式的Servlet容器来启动SpringBoot的web应用，没有web.xml文件。\",\"注册三大组件用以下方式\",\"ServletRegistrationBean\",\"//注册三大组件 @Bean public ServletRegistrationBean myServlet(){ ServletRegistrationBean registrationBean = new ServletRegistrationBean(new MyServlet(),\\\"/myServlet\\\"); return registrationBean; } \",\"FilterRegistrationBean\",\"@Bean public FilterRegistrationBean myFilter(){ FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList(\\\"/hello\\\",\\\"/myServlet\\\")); return registrationBean; } \",\"ServletListenerRegistrationBean\",\"@Bean public ServletListenerRegistrationBean myListener(){ ServletListenerRegistrationBean<MyListener> registrationBean = new ServletListenerRegistrationBean<>(new MyListener()); return registrationBean; } \",\"SpringBoot帮我们自动SpringMVC的时候，自动的注册SpringMVC的前端控制器；DIspatcherServlet；\",\"DispatcherServletAutoConfiguration中：\",\"@Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME) @ConditionalOnBean(value = DispatcherServlet.class, name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public ServletRegistrationBean dispatcherServletRegistration( DispatcherServlet dispatcherServlet) { ServletRegistrationBean registration = new ServletRegistrationBean( dispatcherServlet, this.serverProperties.getServletMapping()); //默认拦截： / 所有请求；包静态资源，但是不拦截jsp请求； /*会拦截jsp //可以通过server.servletPath来修改SpringMVC前端控制器默认拦截的请求路径 registration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); registration.setLoadOnStartup( this.webMvcProperties.getServlet().getLoadOnStartup()); if (this.multipartConfig != null) { registration.setMultipartConfig(this.multipartConfig); } return registration; } \",\"2）、SpringBoot能不能支持其他的Servlet容器；\"]},{\"header\":\"3）、替换为其他嵌入式Servlet容器\",\"slug\":\"_3-、替换为其他嵌入式servlet容器\",\"contents\":[\"默认支持：\",\"Tomcat（默认使用）\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> 引入web模块默认就是使用嵌入式的Tomcat作为Servlet容器； </dependency> \",\"Jetty\",\"<!-- 引入web模块 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> <exclusions> <exclusion> <artifactId>spring-boot-starter-tomcat</artifactId> <groupId>org.springframework.boot</groupId> </exclusion> </exclusions> </dependency> <!--引入其他的Servlet容器--> <dependency> <artifactId>spring-boot-starter-jetty</artifactId> <groupId>org.springframework.boot</groupId> </dependency> \",\"Undertow\",\"<!-- 引入web模块 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> <exclusions> <exclusion> <artifactId>spring-boot-starter-tomcat</artifactId> <groupId>org.springframework.boot</groupId> </exclusion> </exclusions> </dependency> <!--引入其他的Servlet容器--> <dependency> <artifactId>spring-boot-starter-undertow</artifactId> <groupId>org.springframework.boot</groupId> </dependency> \"]},{\"header\":\"4）、嵌入式Servlet容器自动配置原理；\",\"slug\":\"_4-、嵌入式servlet容器自动配置原理\",\"contents\":[\"EmbeddedServletContainerAutoConfiguration：嵌入式的Servlet容器自动配置？\",\"@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @Configuration @ConditionalOnWebApplication @Import(BeanPostProcessorsRegistrar.class) //导入BeanPostProcessorsRegistrar：Spring注解版；给容器中导入一些组件 //导入了EmbeddedServletContainerCustomizerBeanPostProcessor： //后置处理器：bean初始化前后（创建完对象，还没赋值赋值）执行初始化工作 public class EmbeddedServletContainerAutoConfiguration { @Configuration @ConditionalOnClass({ Servlet.class, Tomcat.class })//判断当前是否引入了Tomcat依赖； @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)//判断当前容器没有用户自己定义EmbeddedServletContainerFactory：嵌入式的Servlet容器工厂；作用：创建嵌入式的Servlet容器 public static class EmbeddedTomcat { @Bean public TomcatEmbeddedServletContainerFactory tomcatEmbeddedServletContainerFactory() { return new TomcatEmbeddedServletContainerFactory(); } } /** * Nested configuration if Jetty is being used. */ @Configuration @ConditionalOnClass({ Servlet.class, Server.class, Loader.class, WebAppContext.class }) @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedJetty { @Bean public JettyEmbeddedServletContainerFactory jettyEmbeddedServletContainerFactory() { return new JettyEmbeddedServletContainerFactory(); } } /** * Nested configuration if Undertow is being used. */ @Configuration @ConditionalOnClass({ Servlet.class, Undertow.class, SslClientAuthMode.class }) @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedUndertow { @Bean public UndertowEmbeddedServletContainerFactory undertowEmbeddedServletContainerFactory() { return new UndertowEmbeddedServletContainerFactory(); } } \",\"1）、EmbeddedServletContainerFactory（嵌入式Servlet容器工厂）\",\"public interface EmbeddedServletContainerFactory { //获取嵌入式的Servlet容器 EmbeddedServletContainer getEmbeddedServletContainer( ServletContextInitializer... initializers); } \",\"2）、EmbeddedServletContainer：（嵌入式的Servlet容器）\",\"3）、以TomcatEmbeddedServletContainerFactory为例\",\"@Override public EmbeddedServletContainer getEmbeddedServletContainer( ServletContextInitializer... initializers) { //创建一个Tomcat Tomcat tomcat = new Tomcat(); //配置Tomcat的基本环节 File baseDir = (this.baseDirectory != null ? this.baseDirectory : createTempDir(\\\"tomcat\\\")); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); //将配置好的Tomcat传入进去，返回一个EmbeddedServletContainer；并且启动Tomcat服务器 return getTomcatEmbeddedServletContainer(tomcat); } \",\"4）、我们对嵌入式容器的配置修改是怎么生效？\",\"ServerProperties、EmbeddedServletContainerCustomizer \",\"EmbeddedServletContainerCustomizer：定制器帮我们修改了Servlet容器的配置？\",\"怎么修改的原理？\",\"5）、容器中导入了EmbeddedServletContainerCustomizerBeanPostProcessor\",\"//初始化之前 @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { //如果当前初始化的是一个ConfigurableEmbeddedServletContainer类型的组件 if (bean instanceof ConfigurableEmbeddedServletContainer) { // postProcessBeforeInitialization((ConfigurableEmbeddedServletContainer) bean); } return bean; } private void postProcessBeforeInitialization( ConfigurableEmbeddedServletContainer bean) { //获取所有的定制器，调用每一个定制器的customize方法来给Servlet容器进行属性赋值； for (EmbeddedServletContainerCustomizer customizer : getCustomizers()) { customizer.customize(bean); } } private Collection<EmbeddedServletContainerCustomizer> getCustomizers() { if (this.customizers ** null) { // Look up does not include the parent context this.customizers = new ArrayList<EmbeddedServletContainerCustomizer>( this.beanFactory //从容器中获取所有这葛类型的组件：EmbeddedServletContainerCustomizer //定制Servlet容器，给容器中可以添加一个EmbeddedServletContainerCustomizer类型的组件 .getBeansOfType(EmbeddedServletContainerCustomizer.class, false, false) .values()); Collections.sort(this.customizers, AnnotationAwareOrderComparator.INSTANCE); this.customizers = Collections.unmodifiableList(this.customizers); } return this.customizers; } ServerProperties也是定制器 \",\"步骤：\",\"1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory【TomcatEmbeddedServletContainerFactory】\",\"2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor；\",\"只要是嵌入式的Servlet容器工厂，后置处理器就工作；\",\"3）、后置处理器，从容器中获取所有的EmbeddedServletContainerCustomizer，调用定制器的定制方法\",\"###5）、嵌入式Servlet容器启动原理；\",\"什么时候创建嵌入式的Servlet容器工厂？什么时候获取嵌入式的Servlet容器并启动Tomcat；\",\"获取嵌入式的Servlet容器工厂：\",\"1）、SpringBoot应用启动运行run方法\",\"2）、refreshContext(context);SpringBoot刷新IOC容器【创建IOC容器对象，并初始化容器，创建容器中的每一个组件】；如果是web应用创建AnnotationConfigEmbeddedWebApplicationContext，否则：AnnotationConfigApplicationContext\",\"3）、refresh(context);刷新刚才创建好的ioc容器；\",\"public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\\\"Exception encountered during context initialization - \\\" + \\\"cancelling refresh attempt: \\\" + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } \",\"4）、 onRefresh(); web的ioc容器重写了onRefresh方法\",\"5）、webioc容器会创建嵌入式的Servlet容器；createEmbeddedServletContainer();\",\"6）、获取嵌入式的Servlet容器工厂：\",\"EmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory();\",\"​ 从ioc容器中获取EmbeddedServletContainerFactory 组件；TomcatEmbeddedServletContainerFactory创建对象，后置处理器一看是这个对象，就获取所有的定制器来先定制Servlet容器的相关配置；\",\"7）、使用容器工厂获取嵌入式的Servlet容器：this.embeddedServletContainer = containerFactory .getEmbeddedServletContainer(getSelfInitializer());\",\"8）、嵌入式的Servlet容器创建对象并启动Servlet容器；\",\"先启动嵌入式的Servlet容器，再将ioc容器中剩下没有创建出的对象获取出来；\",\"IOC容器启动创建嵌入式的Servlet容器\"]},{\"header\":\"9、使用外置的Servlet容器\",\"slug\":\"_9、使用外置的servlet容器\",\"contents\":[\"嵌入式Servlet容器：应用打成可执行的jar\",\"​ 优点：简单、便携；\",\"​ 缺点：默认不支持JSP、优化定制比较复杂（使用定制器【ServerProperties、自定义EmbeddedServletContainerCustomizer】，自己编写嵌入式Servlet容器的创建工厂【EmbeddedServletContainerFactory】）；\",\"外置的Servlet容器：外面安装Tomcat---应用war包的方式打包；\"]},{\"header\":\"步骤\",\"slug\":\"步骤\",\"contents\":[\"1）、必须创建一个war项目；（利用idea创建好目录结构）\",\"2）、将嵌入式的Tomcat指定为provided；\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-tomcat</artifactId> <scope>provided</scope> </dependency> \",\"3）、必须编写一个SpringBootServletInitializer的子类，并调用configure方法\",\"public class ServletInitializer extends SpringBootServletInitializer { @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { //传入SpringBoot应用的主程序 return application.sources(SpringBoot04WebJspApplication.class); } } \",\"4）、启动服务器就可以使用；\"]},{\"header\":\"原理\",\"slug\":\"原理\",\"contents\":[\"jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器；\",\"war包：启动服务器，服务器启动SpringBoot应用【SpringBootServletInitializer】，启动ioc容器；\",\"servlet3.0（Spring注解版）：\",\"8.2.4 Shared libraries / runtimes pluggability：\",\"规则：\",\"​ 1）、服务器启动（web应用启动）会创建当前web应用里面每一个jar包里面ServletContainerInitializer实例：\",\"​ 2）、ServletContainerInitializer的实现放在jar包的META-INF/services文件夹下，有一个名为javax.servlet.ServletContainerInitializer的文件，内容就是ServletContainerInitializer的实现类的全类名\",\"​ 3）、还可以使用@HandlesTypes，在应用启动的时候加载我们感兴趣的类；\",\"流程：\",\"1）、启动Tomcat\",\"2）、org\\\\springframework\\\\spring-web\\\\4.3.14.RELEASE\\\\spring-web-4.3.14.RELEASE.jar!\\\\META-INF\\\\services\\\\javax.servlet.ServletContainerInitializer：\",\"Spring的web模块里面有这个文件：org.springframework.web.SpringServletContainerInitializer\",\"3）、SpringServletContainerInitializer将@HandlesTypes(WebApplicationInitializer.class)标注的所有这个类型的类都传入到onStartup方法的Set<Class<?>>；为这些WebApplicationInitializer类型的类创建实例；\",\"4）、每一个WebApplicationInitializer都调用自己的onStartup；\",\"5）、相当于我们的SpringBootServletInitializer的类会被创建对象，并执行onStartup方法\",\"6）、SpringBootServletInitializer实例执行onStartup的时候会createRootApplicationContext；创建容器\",\"protected WebApplicationContext createRootApplicationContext( ServletContext servletContext) { //1、创建SpringApplicationBuilder SpringApplicationBuilder builder = createSpringApplicationBuilder(); StandardServletEnvironment environment = new StandardServletEnvironment(); environment.initPropertySources(servletContext, null); builder.environment(environment); builder.main(getClass()); ApplicationContext parent = getExistingRootWebApplicationContext(servletContext); if (parent != null) { this.logger.info(\\\"Root context already created (using as parent).\\\"); servletContext.setAttribute( WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, null); builder.initializers(new ParentContextApplicationContextInitializer(parent)); } builder.initializers( new ServletContextApplicationContextInitializer(servletContext)); builder.contextClass(AnnotationConfigEmbeddedWebApplicationContext.class); //调用configure方法，子类重写了这个方法，将SpringBoot的主程序类传入了进来 builder = configure(builder); //使用builder创建一个Spring应用 SpringApplication application = builder.build(); if (application.getSources().isEmpty() && AnnotationUtils .findAnnotation(getClass(), Configuration.class) != null) { application.getSources().add(getClass()); } Assert.state(!application.getSources().isEmpty(), \\\"No SpringApplication sources have been defined. Either override the \\\" + \\\"configure method or add an @Configuration annotation\\\"); // Ensure error pages are registered if (this.registerErrorPageFilter) { application.getSources().add(ErrorPageFilterConfiguration.class); } //启动Spring应用 return run(application); } \",\"7）、Spring的应用就启动并且创建IOC容器\",\"public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; FailureAnalyzers analyzers = null; configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); Banner printedBanner = printBanner(environment); context = createApplicationContext(); analyzers = new FailureAnalyzers(context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); //刷新IOC容器 refreshContext(context); afterRefresh(context, applicationArguments); listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } return context; } catch (Throwable ex) { handleRunFailure(context, listeners, analyzers, ex); throw new IllegalStateException(ex); } } \",\"启动Servlet容器，再启动SpringBoot应用\"]}]},\"/study-tutorial/microservice/spring-cloud-alibaba/nacos.html\":{\"title\":\"2、Nacos 服务注册与配置中心\",\"contents\":[{\"header\":\"1、 Nacos 简介\",\"slug\":\"_1、nacos简介\",\"contents\":[]},{\"header\":\"1、为什么叫 nacos\",\"slug\":\"_1、为什么叫nacos\",\"contents\":[\"前四个字母分别是Naming和Configuration的前两个字母，后面的s是Service\"]},{\"header\":\"2、什么是 nacos\",\"slug\":\"_2、什么是nacos\",\"contents\":[\"地址：https://nacos.io/zh-cn/docs/what-is-nacos.html\",\"Nacos是一个易于使用的动态服务发现，配置和服务管理平台，用于构建云本机应用程序。\",\"使用Spring Cloud Alibaba Nacos Discovery，您可以基于Spring Cloud的编程模型快速访问Nacos服务注册功能。\",\"Nacos就是注册中心+配置中心的组合\",\"Nacos = Eureka + Config + Bus\"]},{\"header\":\"3、作用\",\"slug\":\"_3、作用\",\"contents\":[\"替代Eureka做服务注册\",\"替代Config做配置中心\",\"Nacos主要提供以下四大功能：\",\"服务发现与服务健康检查 Nacos使服务更容易注册，并通过DNS或HTTP接口发现其他服务，Nacos还提供服务的实时健康检查，以防 止向不健康的主机或服务实例发送请求。\",\"动态配置管理 动态配置服务允许您在所有环境中以集中和动态的方式管理所有服务的配置。Nacos消除了在更新配置时重新 部署应用程序，这使配置的更改更加高效和灵活。\",\"动态DNS服务 Nacos提供基于DNS 协议的服务发现能力，旨在支持异构语言的服务发现，支持将注册在Nacos上的服务以 域名的方式暴露端点，让三方应用方便的查阅及发现。\",\"服务和元数据管理 Nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周 期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略。\"]},{\"header\":\"4、下载\",\"slug\":\"_4、下载\",\"contents\":[\"https://github.com/alibaba/nacos/releases\"]},{\"header\":\"5、各版本比较\",\"slug\":\"_5、各版本比较\",\"contents\":[\"目前市面上用的比较多的配置中心有：Spring Cloud Config、Apollo、Nacos和Disconf等。 由于Disconf不再维护，下面主要对比一下Spring Cloud Config、Apollo和Nacos。\",\"主流配置中心对比\",\"对比项目\",\"Spring Cloud Config\",\"Apollo\",\"Nacos\",\"配置实时推送\",\"支持(Spring Cloud Bus)\",\"支持(HTTP长轮询1s内)\",\"支持(HTTP长轮询1s内)\",\"版本管理\",\"支持(Git)\",\"支持\",\"支持\",\"配置回滚\",\"支持(Git)\",\"支持\",\"支持\",\"灰度发布\",\"支持\",\"支持\",\"不支持\",\"权限管理\",\"支持(依赖Git)\",\"支持\",\"不支持\",\"多集群\",\"支持\",\"支持\",\"支持\",\"多环境\",\"支持\",\"支持\",\"支持\",\"监听查询\",\"支持\",\"支持\",\"支持\",\"多语言\",\"只支持Java\",\"主流语言，提供了Open API\",\"主流语言，提供了Open API\",\"配置格式校验\",\"不支持\",\"支持\",\"支持\",\"单机读(QPS)\",\"7(限流所致)\",\"9000\",\"15000\",\"单击写(QPS)\",\"5(限流所致)\",\"1100\",\"1800\",\"3节点读(QPS)\",\"21(限流所致)\",\"27000\",\"45000\",\"3节点写(QPS)\",\"5(限流所致)\",\"3300\",\"5600\",\"从配置中心角度来看，性能方面Nacos的读写性能最高，Apollo次之，Spring Cloud Config依赖Git场景不适合开 放的大规模自动化运维API。功能方面Apollo最为完善，nacos具有Apollo大部分配置管理功能，而Spring Cloud Config不带运维管理界面，需要自行开发。Nacos的一大优势是整合了注册中心、配置中心功能，部署和操作相比 Apollo都要直观简单，因此它简化了架构复杂度，并减轻运维及部署工作。 综合来看，Nacos的特点和优势还是比较明显的，下面我们一起进入Nacos的世界。\",\"据说 Nacos 在阿里巴巴内部有超过 10 万的实例运行，已经过了类似双十一等各种大型流量的考验\"]},{\"header\":\"2、 Nacos 安装与配置\",\"slug\":\"_2、nacos安装与配置\",\"contents\":[\"https://nacos.io/zh-cn/docs/quick-start.html\"]},{\"header\":\"3、 Nacos 作为服务注册中心案例\",\"slug\":\"_3、nacos作为服务注册中心案例\",\"contents\":[]},{\"header\":\"1、 Nacos 服务提供者\",\"slug\":\"_1、nacos服务提供者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-cloud-alibaba-payment9001 \"]},{\"header\":\"2、修改pom\",\"slug\":\"_2、修改pom\",\"contents\":[\"父工程增加spring-cloud-alibaba-dependencies的依赖，这样我们就不用每次都要增加这个依赖了。\",\" <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-alibaba-dependencies</artifactId> <version>2.2.1.RELEASE</version> <type>pom</type> <scope>import</scope> </dependency> \",\"<dependencies> <!--SpringCloud ailibaba nacos --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 9001 spring: application: name: nacos-discovery-payment-provider cloud: nacos: discovery: server-addr: 127.0.0.1:8848 management: endpoints: web: exposure: include: '*' \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication @EnableDiscoveryClient public class NacosDiscoveryMain9001 { public static void main(String[] args) { SpringApplication.run(NacosDiscoveryMain9001.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"controller\",\"@RestController public class PaymentController { @Value(\\\"${server.port}\\\") private String serverPort; @GetMapping(value = \\\"/payment/nacos/{id}\\\") public String getPayment(@PathVariable(\\\"id\\\") Integer id) { return \\\"nacos registry, serverPort: \\\"+ serverPort+\\\"\\\\t id\\\"+id; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"http://localhost:9001/payment/nacos/23\",\"为了演示Nacos的负载均衡，复制一份9001，为9002\"]},{\"header\":\"2、 Naocs 服务消费者\",\"slug\":\"_2、naocs服务消费者\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-1\",\"contents\":[\"xiaobear-cloud-alibaba-consumer-order83 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom\",\"contents\":[\"<dependencies> <!--SpringCloud ailibaba nacos --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <dependency><!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml-1\",\"contents\":[\"server: port: 83 spring: application: name: nacos-order-consumer cloud: nacos: discovery: server-addr: localhost:8848 #消费者将要去访问的微服务名称(注册成功进nacos的微服务提供者) service-url: nacos-user-service: http://nacos-discovery-payment-provider \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-1\",\"contents\":[\"@SpringBootApplication @EnableDiscoveryClient public class NacosConsumerMain83 { public static void main(String[] args) { SpringApplication.run(NacosConsumerMain83.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-1\",\"contents\":[\"controller\",\"@RestController public class PaymentController { @Value(\\\"${server.port}\\\") private String serverPort; @GetMapping(value = \\\"/payment/nacos/{id}\\\") public String getPayment(@PathVariable(\\\"id\\\") Integer id) { return \\\"nacos registry, serverPort: \\\"+ serverPort+\\\"\\\\t id\\\"+id; } } \",\"NacosOrderController\",\"@RestController public class NacosOrderController { @Resource private RestTemplate restTemplate; @Value(\\\"${service-url.nacos-user-service}\\\") private String serverURL; @GetMapping(\\\"/consumer/nocas/{id}\\\") public String getPaymentByDiscovery(@PathVariable(\\\"id\\\") Long id){ return restTemplate.getForObject(serverURL+\\\"/payment/nacos/\\\"+id,String.class); } } \",\"ApplicationContextBean\",\"@Configuration public class ApplicationContextBean { @LoadBalanced @Bean public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试-1\",\"contents\":[\"http://localhost:83/consumer/nocas/23\",\"9001和9002轮询负载\"]},{\"header\":\"3、服务注册中心对比\",\"slug\":\"_3、服务注册中心对比\",\"contents\":[]},{\"header\":\"切换\",\"slug\":\"切换\",\"contents\":[\"Naocs支持Ap和Cp的切换\",\"C是所有节点在同一时间看到的数据是一致的；而A的定义是所有的请求都会收到响应。\",\"何时选择使用何种模式？\",\"一般来说，如果不需要存储服务级别的信息且服务实例是通过nacos-client注册，并能够保持心跳上报，那么就可以选择AP模式。当前主流的服务如 Spring cloud 和 Dubbo 服务，都适用于AP模式，AP模式为了服务的可能性而减弱了一致性，因此AP模式下只支持注册临时实例。\",\"如果需要在服务级别编辑或者存储配置信息，那么 CP 是必须，K8S服务和DNS服务则适用于CP模式。\",\"CP模式下则支持注册持久化实例，此时则是以 Raft 协议为集群运行模式，该模式下注册实例之前必须先注册服务，如果服务不存在，则会返回错误。\",\"模式切换\",\"curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'\"]},{\"header\":\"4、 Nacos 作为配置中心\",\"slug\":\"_4、nacos作为配置中心\",\"contents\":[]},{\"header\":\"1、基础配置\",\"slug\":\"_1、基础配置\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module-2\",\"contents\":[\"xiaobear-cloud-alibaba-config-client3377 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom-1\",\"contents\":[\"<dependencies> <!--nacos-config--> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId> </dependency> <!--nacos-discovery--> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <!--web + actuator--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--一般基础配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml-2\",\"contents\":[\"Nacos同springcloud-config一样，在项目初始化时，要保证先从配置中心进行配置拉取，拉取配置之后，才能保证项目的正常启动。\",\"springboot中配置文件的加载是存在优先级顺序的，bootstrap优先级高于application\",\"application.yaml\",\"spring: profiles: active: dev ## 表示开发环境 \",\"bootstrap.yaml\",\"## nacos配置 server: port: 3377 spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 config: server-addr: localhost:8848 #Nacos作为配置中心地址 file-extension: yaml #指定yaml格式的配置 \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类-2\",\"contents\":[\"@SpringBootApplication @EnableDiscoveryClient public class NacosConfigClientMain3377 { public static void main(String[] args) { SpringApplication.run(NacosConfigClientMain3377.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类-2\",\"contents\":[\"controller\",\"@RestController @RefreshScope public class ConfigClientController { @Value(\\\"${config.info}\\\") private String configInfo; @GetMapping(\\\"/config/info\\\") public String getConfigInfo() { return configInfo; } } \"]},{\"header\":\"6、在 Nacos 中增加配置信息\",\"slug\":\"_6、在nacos中增加配置信息\",\"contents\":[\"https://nacos.io/zh-cn/docs/quick-start-spring-cloud.html\",\"在 Nacos Spring Cloud 中，dataId 的完整格式如下：\",\"${prefix}-${spring.profiles.active}.${file-extension} \",\"prefix 默认为 spring.application.name 的值，也可以通过配置项 spring.cloud.nacos.config.prefix来配置。\",\"spring.profiles.active 即为当前环境对应的 profile，详情可以参考 Spring Boot文档。 注意：当 spring.profiles.active 为空时，对应的连接符 - 也将不存在，dataId 的拼接格式变成 ${prefix}.${file-extension}\",\"file-exetension 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\"]},{\"header\":\"实操\",\"slug\":\"实操\",\"contents\":[\"####### 1、配置新增\",\"####### 2、历史配置\",\"Naocs会记录配置文件的历史版本默认保留30天，此外还有一键回滚功能，回滚操作将触发配置更新\"]},{\"header\":\"2、分类配置\",\"slug\":\"_2、分类配置\",\"contents\":[\"存在的问题：\",\"问题1：\",\"实际开发中，通常一个系统会准备\",\"dev开发环境\",\"test测试环境\",\"prod生产环境。\",\"如何保证指定环境启动时服务能正确读取到Nacos上相应环境的配置文件呢？\",\"问题2：\",\"一个大型分布式微服务系统会有很多微服务子项目，\",\"每个微服务项目又都会有相应的开发环境、测试环境、预发环境、正式环境......\",\"那怎么对这些微服务配置进行管理呢？\"]},{\"header\":\"1、 Namespace+Group+Data ID 的关系\",\"slug\":\"_1、namespace-group-data-id的关系\",\"contents\":[\"类似Java里面的package名和类名\",\"最外层的namespace是可以用于区分部署环境的，Group和DataID逻辑上区分两个目标对象。\",\"默认情况：\",\"Namespace=public，Group=DEFAULT_GROUP, 默认Cluster是DEFAULT\",\"Nacos默认的命名空间是public，Namespace主要用来实现隔离。\",\"比方说我们现在有三个环境：开发、测试、生产环境，我们就可以创建三个Namespace，不同的Namespace之间是隔离的。\",\"Group默认是DEFAULT_GROUP，Group可以把不同的微服务划分到同一个分组里面去\",\"Service就是微服务；一个Service可以包含多个Cluster（集群），Nacos默认Cluster是DEFAULT，Cluster是对指定微服务的一个虚拟划分。\",\"比方说为了容灾，将Service微服务分别部署在了杭州机房和广州机房，这时就可以给杭州机房的Service微服务起一个集群名称（HZ），给广州机房的Service微服务起一个集群名称（GZ），还可以尽量让同一个机房的微服务互相调用，以提升性能。\",\"最后是Instance，就是微服务的实例。\"]},{\"header\":\"2、三种加载配置\",\"slug\":\"_2、三种加载配置\",\"contents\":[]},{\"header\":\"1、 Data ID 方案\",\"slug\":\"_1、data-id-方案\",\"contents\":[\"指定spring.profile.active和配置文件的DataID来使不同环境下读取不同的配置\",\"通过spring.profile.active属性就能进行多环境下配置文件的读取\",\"测试：http://localhost:3377/config/info\"]},{\"header\":\"2、 Group 方案\",\"slug\":\"_2、group方案\",\"contents\":[\"通过Group实现环境区分\",\"修改application.yml\",\"spring: profiles: active: dev ## 表示开发环境 #active: test ## 表示测试环境 \",\"修改bootstrap.yml\",\"server: port: 3377 spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 config: server-addr: localhost:8848 #Nacos作为配置中心地址 file-extension: yaml #指定yaml格式的配置 group: DEV_GROUP \",\"在config下增加一条group的配置即可。可配置为DEV_GROUP\"]},{\"header\":\"3、 Namespace 方案\",\"slug\":\"_3、namespace方案\",\"contents\":[\"## nacos配置 server: port: 3377 spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 config: server-addr: localhost:8848 #Nacos作为配置中心地址 file-extension: yaml #指定yaml格式的配置 group: DEV_GROUP namespace: 9c59e043-c297-4c70-84b4-a41a38151fef \"]},{\"header\":\"5、 nacos 集群与持久化配置\",\"slug\":\"_5、nacos集群与持久化配置\",\"contents\":[\"官网地址：https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html\",\"根据官网翻译：\",\"默认Nacos使用嵌入式数据库实现数据的存储。所以，如果启动多个默认配置下的Nacos节点，数据存储是存在一致性问题的。\",\"为了解决这个问题，Nacos采用了集中式存储的方式来支持集群化部署，目前只支持MySQL的存储。\"]},{\"header\":\"1、 Nacos 持久化配置\",\"slug\":\"_1、nacos持久化配置\",\"contents\":[\"Nacos默认自带的是嵌入式数据库derby\",\"https://github.com/alibaba/nacos/blob/develop/config/pom.xml\"]},{\"header\":\"1、切换到MySQL\",\"slug\":\"_1、切换到mysql\",\"contents\":[\"找到该脚本，然后执行\",\"nacos-server-1.1.4\\\\nacos\\\\conf目录下找到application.properties\",\"spring.datasource.platform=mysql db.num=1 db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config&characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true db.user=root db.password=123456 \",\"启动Nacos，可以看到是个全新的空记录界面，以前是记录进derby\",\"新增配置，查询mysql，可以看到刚刚的记录\"]},{\"header\":\"2、 Linux 版 Nacos + MySQL 生产环境配置\",\"slug\":\"_2、linux版nacos-mysql生产环境配置\",\"contents\":[\"预计需要，1个Nginx+3个nacos注册中心+1个mysql\",\"Nginx安装：https://cloud.tencent.com/developer/article/1654844\",\"MySQL安装：https://www.huaweicloud.com/articles/c2f3da4367a296361dfe1b7a2805fd20.html\"]},{\"header\":\"1、Linux服务器上mysql数据库配置\",\"slug\":\"_1、linux服务器上mysql数据库配置\",\"contents\":[\"跟win一样，直接运行脚本即可\"]},{\"header\":\"2、application.properties 配置\",\"slug\":\"_2、application-properties-配置\",\"contents\":[\"spring.datasource.platform=mysql db.num=1 db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true db.user=root db.password=123456 \"]},{\"header\":\"3、Linux服务器上nacos的集群配置cluster.conf\",\"slug\":\"_3、linux服务器上nacos的集群配置cluster-conf\",\"contents\":[\"梳理出3台nacos集器的不同服务端口号\",\"192.168.222.125:3333\",\"192.168.222.125:4444\",\"192.168.222.125:5555\",\"这个IP不能写127.0.0.1，必须是Linux命令hostname -i能够识别的IP\"]},{\"header\":\"4、 编辑Nacos的启动脚本startup.sh ，使它能够接受不同的启动端口\",\"slug\":\"_4、编辑nacos的启动脚本startup-sh-使它能够接受不同的启动端口\",\"contents\":[\"../nacos/bin 目录下有startup.sh\",\"思考：\",\"/nacos/bin 目录下有startup.sh\",\"平时单机版的启动，都是./startup.sh即可。\",\"但是，集群启动，我们希望可以类似其它软件的shell命令，传递不同的端口号启动不同的nacos实例。\",\"命令：./startup.sh -p 3333 表示启动端口号为3333的nacos服务器实例，和上一步的cluster.conf配置的一致。\",\"执行方式：./startup.sh -p 3333\"]},{\"header\":\"5、Nginx的配置，由它作为负载均衡器\",\"slug\":\"_5、nginx的配置-由它作为负载均衡器\",\"contents\":[\"修改nginx的配置文件\",\"注意：记得重启\"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试-2\",\"contents\":[\"测试通过nginx访问nacos：http://192.168.222.125:1111/nacos/#/login\",\"新建一个配置进行测试\",\"linux服务器的mysql插入一条记录\"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[]},{\"header\":\"1、微服务cloudalibaba-provider-payment9002启动注册进nacos集群\",\"slug\":\"_1、微服务cloudalibaba-provider-payment9002启动注册进nacos集群\",\"contents\":[\"server: port: 9002 spring: application: name: nacos-discovery-payment-provider cloud: nacos: discovery: #配置Nacos地址 #server-addr: localhost:8848 ## 换成nginx的1111端口，做集群 server-addr: 192.168.111.144:1111 management: endpoints: web: exposure: include: '*' \",\"总结：\"]}]},\"/study-tutorial/microservice/spring-cloud-alibaba/sentinel.html\":{\"title\":\"3、Sentinel 熔断与限流\",\"contents\":[{\"header\":\"1、简介\",\"slug\":\"_1、简介\",\"contents\":[\"Sentinel 是面向分布式服务架构的流量控制组件，主要以流量为切入点，从流量控制、熔断降级、系统自适应保护等多个维度来帮助您保障微服务的稳定性。\",\"官网：https://github.com/alibaba/Sentinel\",\"中文文档：https://github.com/alibaba/Sentinel/wiki/介绍https://sentinelguard.io/zh-cn/docs/introduction.html\",\"下载地址：https://github.com/alibaba/Sentinel/releases/tag/1.8.1\"]},{\"header\":\"1、Sentinel 基本概念\",\"slug\":\"_1、sentinel-基本概念\",\"contents\":[]},{\"header\":\"资源\",\"slug\":\"资源\",\"contents\":[\"资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。在接下来的文档中，我们都会用资源来描述代码块。\",\"只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。\"]},{\"header\":\"规则\",\"slug\":\"规则\",\"contents\":[\"围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。\"]},{\"header\":\"2、特点\",\"slug\":\"_2、特点\",\"contents\":[\"丰富的适用场景：哨兵在阿里巴巴得到了广泛的应用，几乎覆盖了近10年双11（11.11）购物节的所有核心场景，比如需要限制突发流量的“秒杀”满足系统能力、消息削峰填谷、不依靠业务断路、流量控制等。\",\"实时监控：Sentinel 还提供实时监控能力。可以实时查看单台机器的运行时信息，以及以下 500 个节点的集群运行时信息。\",\"广泛的开源生态：Sentinel 提供与 Spring、Dubbo 和 gRPC 等常用框架和库的开箱即用集成。\",\"多语言支持：Sentinel 为 Java、Go和C++提供了本机支持。\",\"丰富的 SPI 扩展：Sentinel 提供简单易用的 SPI 扩展接口，可以让您快速自定义逻辑，例如自定义规则管理、适配数据源等。\"]},{\"header\":\"3、作用\",\"slug\":\"_3、作用\",\"contents\":[]},{\"header\":\"4、生态\",\"slug\":\"_4、生态\",\"contents\":[\"Sentinel 分为两个部分:\",\"核心库（Java 客户端）不依赖任何框架/库，能够运行于所有 Java 运行时环境，同时对 Dubbo / Spring Cloud 等框架也有较好的支持。\",\"控制台（Dashboard）基于 Spring Boot 开发，打包后可以直接运行，不需要额外的 Tomcat 等应用容器。 \",\"Sentinel Dashboard 是一个轻量级的控制台，提供机器发现、单服务器资源监控、集群资源数据概览以及规则管理等功能\"]},{\"header\":\"5、Sentinel 控制台\",\"slug\":\"_5、sentinel-控制台\",\"contents\":[\"官网地址：https://sentinelguard.io/zh-cn/docs/dashboard.html\",\"Sentinel 提供一个轻量级的开源控制台，它提供机器发现以及健康情况管理、监控（单机和集群），规则管理和推送的功能。这里，我们将会详细讲述如何通过简单的步骤就可以使用这些功能。\",\"接下来，我们将会逐一介绍如何整合 Sentinel 核心库和 Dashboard，让它发挥最大的作用。同时我们也在阿里云上提供企业级的 Sentinel 服务：AHAS Sentinel 控制台，您只需要几个简单的步骤，就能最直观地看到控制台如何实现这些功能，并体验多样化的监控及全自动托管的集群流控能力。\",\"Sentinel 控制台包含如下功能:\",\"查看机器列表以及健康情况：收集 Sentinel 客户端发送的心跳包，用于判断机器是否在线。\",\"监控 (单机和集群聚合)：通过 Sentinel 客户端暴露的监控 API，定期拉取并且聚合应用监控信息，最终可以实现秒级的实时监控。\",\"规则管理和推送：统一管理推送规则。\",\"鉴权：生产环境中鉴权非常重要。这里每个开发者需要根据自己的实际情况进行定制。\"]},{\"header\":\"2、案例module\",\"slug\":\"_2、案例module\",\"contents\":[]},{\"header\":\"1、新建module\",\"slug\":\"_1、新建module\",\"contents\":[\"xiaobear-cloud-alibaba-sentinel-service8401 \"]},{\"header\":\"2、改pom\",\"slug\":\"_2、改pom\",\"contents\":[\"<dependencies> <!--SpringCloud ailibaba nacos --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <!--SpringCloud ailibaba sentinel-datasource-nacos 后续做持久化用到--> <dependency> <groupId>com.alibaba.csp</groupId> <artifactId>sentinel-datasource-nacos</artifactId> </dependency> <!--SpringCloud ailibaba sentinel --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId> </dependency> <!--openfeign--> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-openfeign</artifactId> </dependency> <!-- SpringBoot整合Web组件+actuator --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>cn.hutool</groupId> <artifactId>hutool-all</artifactId> <version>4.6.3</version> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \"]},{\"header\":\"3、写yaml\",\"slug\":\"_3、写yaml\",\"contents\":[\"server: port: 8401 spring: application: name: xiaobear-sentinel-service cloud: #nacos注册服务中心 nacos: discovery: server-addr: localhost:8848 sentinel: transport: #配置Sentinel dashboard地址 dashboard: localhost:8080 #默认8719端口，假如被占用会自动从8719开始依次+1扫描,直至找到未被占用的端口 port: 8719 clientIp: localhost #暴露端点 management: endpoints: web: exposure: include: '*' \"]},{\"header\":\"4、主启动类\",\"slug\":\"_4、主启动类\",\"contents\":[\"@SpringBootApplication @EnableDiscoveryClient public class SentinelServiceApplication8401 { public static void main(String[] args) { SpringApplication.run(SentinelServiceApplication8401.class, args); } } \"]},{\"header\":\"5、业务类\",\"slug\":\"_5、业务类\",\"contents\":[\"controller\",\"@RestController public class FlowLimitController { @GetMapping(\\\"/test1\\\") public String test1(){ return \\\"----------test1--------\\\"; } @GetMapping(\\\"/test2\\\") public String test2(){ return \\\"----------test2--------\\\"; } } \"]},{\"header\":\"6、测试\",\"slug\":\"_6、测试\",\"contents\":[\"启动Nacos，访问http://localhost:8848/nacos/\",\"启动Sentinel，访问http://localhost:8080/\",\"启动工程，访问http://localhost:8401/test1和http://localhost:8401/test2\",\"注意：\",\"Sentinel采用的是懒加载，所以需要访问才会监控到微服务\"]},{\"header\":\"3、流控规则\",\"slug\":\"_3、流控规则\",\"contents\":[]},{\"header\":\"1、概述\",\"slug\":\"_1、概述\",\"contents\":[\"FlowSlot 会根据预设的规则，结合前面 NodeSelectorSlot、ClusterNodeBuilderSlot、StatistcSlot 统计出来的实时信息进行流量控制。\",\"限流的直接表现是在执行 Entry nodeA = SphU.entry(资源名字) 的时候抛出 FlowException 异常。FlowException 是 BlockException 的子类，您可以捕捉 BlockException 来自定义被限流之后的处理逻辑。\",\"同一个资源可以对应多条限流规则。FlowSlot 会对该资源的所有限流规则依次遍历，直到有规则触发限流或者所有规则遍历完毕。\",\"一条限流规则主要由下面几个因素组成，我们可以组合这些元素来实现不同的限流效果：\",\"resource：资源名，即限流规则的作用对象\",\"count: 限流阈值\",\"grade: 限流阈值类型，QPS 或线程数\",\"strategy: 根据调用关系选择策略\",\"Field\",\"说明\",\"默认值\",\"resource\",\"资源名，资源名是限流规则的作用对象\",\"count\",\"限流阈值\",\"grade\",\"限流阈值类型，QPS 或线程数模式\",\"QPS 模式\",\"limitApp\",\"流控针对的调用来源\",\"default，代表不区分调用来源\",\"strategy\",\"调用关系限流策略：直接、链路、关联\",\"根据资源本身（直接）\",\"controlBehavior\",\"流控效果（直接拒绝 / 排队等待 / 慢启动模式），不支持按调用关系限流\",\"直接拒绝\"]},{\"header\":\"2、流控模式\",\"slug\":\"_2、流控模式\",\"contents\":[]},{\"header\":\"1、直接（默认）\",\"slug\":\"_1、直接-默认\",\"contents\":[\"直接->快速失败\",\"表示1秒钟内查询1次就是OK，若超过次数1，就直接-快速失败，报默认错误\",\"Blocked by Sentinel (flow limiting) \",\"思考：\",\"直接报默认错误信息，技术上是OK，但是是否应该有后续处理，比如有一个兜底的fallback\"]},{\"header\":\"2、关联\",\"slug\":\"_2、关联\",\"contents\":[\"当关联的资料达到阈值时，就限流自己。当与A关联的资源B达到阈值后，就限流A\",\"当关联资源/test2的qps阀值超过1时，就限流/test1的Rest访问地址，当关联资源到阈值后限制配置好的资源名\",\"用PostMan模拟并发访问/test2,发现Blocked by Sentinel (flow limiting)\"]},{\"header\":\"3、链路\",\"slug\":\"_3、链路\",\"contents\":[\"链路流控模式指的是，当从某个接口过来的资源达到限流条件时，开启限流；它的功能有点类似于针对 来源配置项，区别在于：针对来源是针对上级微服务，而链路流控是针对上级接口，也就是说它的粒度 更细；\",\"增加service\",\"service\",\"@Service public class SentinelService { @SentinelResource(\\\"hello\\\") public String hello() { return \\\"Hello Sentinel\\\"; } } \",\"@SentinelResource 注释用于识别资源是速率受限还是降级。在上面的示例中，注解的 'hello' 属性指的是资源名称。\",\"controller\",\"@RestController public class FlowLimitController { @Resource private SentinelService sentinelService; @GetMapping(\\\"/test1\\\") public String test1(){ return \\\"----------test1--------\\\"; } @GetMapping(\\\"/test2\\\") public String test2(){ return \\\"----------test2--------\\\"; } @GetMapping(\\\"/test3\\\") public String test3(){ return sentinelService.hello(); } } \"]},{\"header\":\"4、流控效果\",\"slug\":\"_4、流控效果\",\"contents\":[]},{\"header\":\"1、直接（默认的流控处理）\",\"slug\":\"_1、直接-默认的流控处理\",\"contents\":[\"该方式是默认的流量控制方式，当QPS超过任意规则的阈值后，新的请求就会被立即拒绝，拒绝方式为抛出FlowException。\"]},{\"header\":\"2、预热(Warm Up)\",\"slug\":\"_2、预热-warm-up\",\"contents\":[\"https://github.com/alibaba/Sentinel/wiki/流量控制\",\"公式：阈值除以coldFactor(默认为3)，经过预热时长后才达到阈值\",\"冷启动（RuleConstant.CONTROL_BEHAVIOR_WARM_UP）方式。该方式主要用于系统长期处于低水位的情况下，当流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。通过\\\"冷启动\\\"，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮的情况。\"]},{\"header\":\"1、配置\",\"slug\":\"_1、配置\",\"contents\":[\"默认 coldFactor 为 3，即请求QPS从(threshold / 3) 开始，经多少预热时长才逐渐升至设定的 QPS 阈值。\",\"案例，阀值为10+预热时长设置5秒。系统初始化的阀值为10 / 3 约等于3,即阀值刚开始为3；然后过了5秒后阀值才慢慢升高恢复到10\"]},{\"header\":\"2、应用场景\",\"slug\":\"_2、应用场景\",\"contents\":[\"如：秒杀系统在开启的瞬间，会有很多流量上来，很有可能把系统打死，预热方式就是把为了保护系统，可慢慢的把流量放进来，慢慢的把阀值增长到设置的阀值。\"]},{\"header\":\"3、排队等待\",\"slug\":\"_3、排队等待\",\"contents\":[\"匀速器（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER）方式。这种方式严格控制了请求通过的间隔时间，也即是让请求以均匀的速度通过，对应的是漏桶算法。\",\"匀速排队，让请求以均匀的速度通过，阀值类型必须设成QPS，否则无效。\",\"设置含义：/test1每秒1次请求，超过的话就排队等待，等待的超时时间为1000毫秒。\"]},{\"header\":\"5、降级规则\",\"slug\":\"_5、降级规则\",\"contents\":[\"官网文档地址：https://sentinelguard.io/zh-cn/docs/circuit-breaking.html\",\"Sentinel 提供以下几种熔断策略：\",\"慢调用比例 (SLOW_REQUEST_RATIO)：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。\",\"异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。\",\"异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。\",\"RT（平均响应时间，秒级）\",\"平均响应时间 超出阈值 且 在时间窗口内通过的请求>=5，两个条件同时满足后触发降级\",\"窗口期过后关闭断路器\",\"RT最大4900（更大的需要通过-Dcsp.sentinel.statistic.max.rt=XXXX才能生效）\",\"异常比列（秒级）\",\"QPS >= 5 且异常比例（秒级统计）超过阈值时，触发降级；时间窗口结束后，关闭降级\",\"异常数（分钟级）\",\"异常数（分钟统计）超过阈值时，触发降级；时间窗口结束后，关闭降级\",\"Sentinel 熔断降级会在调用链路中某个资源出现不稳定状态时（例如调用超时或异常比例升高），对这个资源的调用进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。\",\"当资源被降级后，在接下来的降级时间窗口之内，对该资源的调用都自动熔断（默认行为是抛出 DegradeException）。\",\"Sentinel的断路器是没有半开状态 \",\"半开的状态系统自动检测是否请求有异常，没有异常就关闭断路器恢复使用，有异常则打开断路器不可用\"]},{\"header\":\"1、慢调用比例\",\"slug\":\"_1、慢调用比例\",\"contents\":[\"以前的版本是：平均响应时间\",\"选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。\",\"controller\",\" @GetMapping(\\\"/test4\\\") public String test4(){ try { Thread.sleep(1000); } catch(InterruptedException e) { e.printStackTrace(); } return \\\"------------test4----------\\\"; } \",\"使用jmeter进行压测，一秒钟访问10次\",\"永远一秒钟打进来10个线程（大于5个了）调用testD，我们希望200毫秒处理完本次任务，\",\"如果超过200毫秒还没处理完，在未来1秒钟的时间窗口内，断路器打开(保险丝跳闸)微服务不可用，保险丝跳闸断电了\",\"后续我停止jmeter，没有这么大的访问量了，断路器关闭(保险丝恢复)，微服务恢复OK\"]},{\"header\":\"2、异常比例\",\"slug\":\"_2、异常比例\",\"contents\":[\"当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。\",\"controller\",\" @GetMapping(\\\"/test5\\\") public String test5(){ int x = 10/0; return \\\"----------5--------------\\\"; } } \",\"单独访问一次，必然来一次报错一次(int x = 10/0)，调一次错一次；开启jmeter后，直接高并发发送请求，多次调用达到我们的配置条件了。断路器开启(保险丝跳闸)，微服务不可用了，不再报错error而是服务降级了。\"]},{\"header\":\"3、异常数\",\"slug\":\"_3、异常数\",\"contents\":[\"当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。\",\"异常数是按分钟统计的\",\"controller\",\" @GetMapping(\\\"/test5\\\") public String test5(){ int x = 10/0; return \\\"----------5--------------\\\"; } \",\"http://localhost:8401/test5，第一次访问绝对报错，因为除数不能为零，我们看到error窗口，但是达到5次报错后，进入熔断后降级。\"]},{\"header\":\"6、热点 Key 限流\",\"slug\":\"_6、热点key限流\",\"contents\":[\"何为热点\",\"热点即经常访问的数据，很多时候我们希望统计或者限制某个热点数据中访问频次最高的TopN数据，并对其访问进行限流或者其它操作\",\"兜底方法分为系统默认和客户自定义，两种\",\"之前的case，限流出问题后，都是用sentinel系统默认的提示：Blocked by Sentinel (flow limiting)\",\"我们能不能自定?类似hystrix，某个方法出问题了，就找对应的兜底降级方法？\",\"结论从HystrixCommand 到@SentinelResource\"]},{\"header\":\"1、配置\",\"slug\":\"_1、配置-1\",\"contents\":[\"限流模式只支持QPS模式，固定写死了。（这才叫热点）\",\"@SentinelResource注解的方法参数索引，0代表第一个参数，1代表第二个参数，以此类推\",\"单机阀值以及统计窗口时长表示在此窗口时间超过阀值就限流。\",\"上面的抓图就是第一个参数有值的话，1秒的QPS为1，超过就限流，限流后调用dealHandler_testHotKey支持方法。\",\"@GetMapping(\\\"/testHotKey\\\") @SentinelResource(value = \\\"testHotKey\\\",blockHandler = \\\"dealHandler_testHotKey\\\") public String testHotKey(@RequestParam(value = \\\"p1\\\",required = false) String p1, @RequestParam(value = \\\"p2\\\",required = false) String p2){ return \\\"------testHotKey-------------\\\"; } public String dealHandler_testHotKey(String p1, String p2, BlockException exception) { return \\\"-----dealHandler_testHotKey-----------\\\"; } \",\"只要第一参数访问的QPS超过1秒，马上降级处理blockHandler = \\\"dealHandler_testHotKey\\\"\",\"测试：\",\"http://localhost:8401/testHotKey?p1=abc\",\"http://localhost:8401/testHotKey?p1=abc&p2=111\",\"http://localhost:8401/testHotKey?p2=abc\"]},{\"header\":\"2、特殊情况\",\"slug\":\"_2、特殊情况\",\"contents\":[\"我们期望p1的某个值达到限流时，它的限流值跟普通的不一样。\",\"例如p1= 10，限流值阈值为200\",\"这里需要先填写参数值跟限流阈值，添加按钮才会有反应\",\"注意：\",\"参数类型必须是基本类型，访问地址的类型跟参数类型进行对应\"]},{\"header\":\"7、系统规则\",\"slug\":\"_7、系统规则\",\"contents\":[\"https://github.com/alibaba/Sentinel/wiki/系统自适应限流#系统规则\",\"系统保护规则是从应用级别的入口流量进行控制，从单台机器的 load、CPU 使用率、平均 RT、入口 QPS 和并发线程数等几个维度监控应用指标，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。\",\"系统保护规则是应用整体维度的，而不是资源维度的，并且仅对入口流量生效。入口流量指的是进入应用的流量（EntryType.IN），比如 Web 服务或 Dubbo 服务端接收的请求，都属于入口流量。\",\"系统规则支持以下的模式：\",\"Load 自适应（仅对 Linux/Unix-like 机器生效）：系统的 load1 作为启发指标，进行自适应系统保护。当系统 load1 超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU cores * 2.5。\",\"CPU usage（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。\",\"平均 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。\",\"并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。\",\"入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。\"]},{\"header\":\"1、原理\",\"slug\":\"_1、原理\",\"contents\":[\"先用经典图来镇楼:\",\"我们把系统处理请求的过程想象为一个水管，到来的请求是往这个水管灌水，当系统处理顺畅的时候，请求不需要排队，直接从水管中穿过，这个请求的RT是最短的；反之，当请求堆积的时候，那么处理请求的时间则会变为：排队时间 + 最短处理时间。\",\"推论一: 如果我们能够保证水管里的水量，能够让水顺畅的流动，则不会增加排队的请求；也就是说，这个时候的系统负载不会进一步恶化。\",\"我们用 T 来表示(水管内部的水量)，用RT来表示请求的处理时间，用P来表示进来的请求数，那么一个请求从进入水管道到从水管出来，这个水管会存在 P * RT 个请求。换一句话来说，当 T ≈ QPS * Avg(RT) 的时候，我们可以认为系统的处理能力和允许进入的请求个数达到了平衡，系统的负载不会进一步恶化。\",\"接下来的问题是，水管的水位是可以达到了一个平衡点，但是这个平衡点只能保证水管的水位不再继续增高，但是还面临一个问题，就是在达到平衡点之前，这个水管里已经堆积了多少水。如果之前水管的水已经在一个量级了，那么这个时候系统允许通过的水量可能只能缓慢通过，RT会大，之前堆积在水管里的水会滞留；反之，如果之前的水管水位偏低，那么又会浪费了系统的处理能力。\",\"推论二: 当保持入口的流量是水管出来的流量的最大的值的时候，可以最大利用水管的处理能力。\",\"然而，和 TCP BBR 的不一样的地方在于，还需要用一个系统负载的值（load1）来激发这套机制启动。\",\"注：这种系统自适应算法对于低 load 的请求，它的效果是一个“兜底”的角色。对于不是应用本身造成的 load 高的情况（如其它进程导致的不稳定的情况），效果不明显。\"]},{\"header\":\"8、 @SentinelResourse\",\"slug\":\"_8、-sentinelresourse\",\"contents\":[]},{\"header\":\"1、按资源名限流\",\"slug\":\"_1、按资源名限流\",\"contents\":[\"启动Nacos\",\"启动Sentinel\"]},{\"header\":\"1、修改8401\",\"slug\":\"_1、修改8401\",\"contents\":[]},{\"header\":\"1、pom文件增加依赖\",\"slug\":\"_1、pom文件增加依赖\",\"contents\":[\"<dependency> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> \"]},{\"header\":\"2、增加业务层controller\",\"slug\":\"_2、增加业务层controller\",\"contents\":[\"增加兜底方法handleException\",\"@RestController public class RateLimitController { @GetMapping(\\\"/byResource\\\") @SentinelResource(value = \\\"/byResource\\\",blockHandler = \\\"handleException\\\") public CommonResult byResourse(){ return new CommonResult(200,\\\"按资源名称限流测试OK\\\",new Payment(2020L,\\\"serial001\\\")); } public CommonResult handleException(BlockException exception) { return new CommonResult(444,exception.getClass().getCanonicalName()+\\\"\\\\t 服务不可用\\\"); } } \"]},{\"header\":\"3、测试\",\"slug\":\"_3、测试\",\"contents\":[\"启动8401\",\"http://localhost:8401/byResource\",\"添加流控规则\",\"表示1秒钟内查询的次数大于1，就会跑到我们自己的限流\",\"接着访问\",\"额外问题\",\"关闭8401，流控规则就消失了，不能够持久化\"]},{\"header\":\"2、按URL限流\",\"slug\":\"_2、按url限流\",\"contents\":[\"通过访问URL来限流，会返回Sentinel自带默认的限流处理\",\"业务层添加接口\",\" @GetMapping(\\\"/rateLimit/byUrl\\\") @SentinelResource(value = \\\"byUrl\\\") public CommonResult byUrl() { return new CommonResult(200,\\\"按url限流测试OK\\\",new Payment(2020L,\\\"serial002\\\")); } \",\"测试：http://localhost:8401/rateLimit/byUrl\",\"添加流控规则：每秒钟访问一次\",\"然后测试：\",\"1 系统默认的，没有体现我们自己的业务要求。\",\"2 依照现有条件，我们自定义的处理方法又和业务代码耦合在一块，不直观。\",\"3 每个业务方法都添加一个兜底的，那代码膨胀加剧。\",\"4 全局统一的处理方法没有体现。\"]},{\"header\":\"3、自定义限流处理规则\",\"slug\":\"_3、自定义限流处理规则\",\"contents\":[\"创建CustomerBlockHandler类用于自定义限流处理逻辑\",\"public class CustomerBlockHandler { public static CommonResult handleException(BlockException exception){ return new CommonResult(2020,\\\"自定义的限流处理信息......CustomerBlockHandler\\\"); } } \",\"RateLimitController增加接口\",\" @GetMapping(\\\"/rateLimit/customerBlockHandler\\\") @SentinelResource(value = \\\"customerBlockHandler\\\", blockHandlerClass = CustomerBlockHandler.class, blockHandler = \\\"handleException\\\") public CommonResult customerBlockHandler() { return new CommonResult(200,\\\"按客户自定义限流处理逻辑\\\"); } \",\"测试：http://localhost:8401/rateLimit/customerBlockHandler\",\"增加流控规则\"]},{\"header\":\"4、更多属性\",\"slug\":\"_4、更多属性\",\"contents\":[\"地址：https://sentinelguard.io/zh-cn/docs/annotation-support.html\"]},{\"header\":\"@SentinelResource 注解\",\"slug\":\"sentinelresource-注解\",\"contents\":[\"注意：注解方式埋点不支持 private 方法。\",\"@SentinelResource 用于定义资源，并提供可选的异常处理和 fallback 配置项。 @SentinelResource 注解包含以下属性：\",\"value：资源名称，必需项（不能为空）\",\"entryType：entry 类型，可选项（默认为 EntryType.OUT）\",\"blockHandler / blockHandlerClass: blockHandler 对应处理 BlockException 的函数名称，可选项。blockHandler 函数访问范围需要是 public，返回类型需要与原方法相匹配，参数类型需要和原方法相匹配并且最后加一个额外的参数，类型为 BlockException。blockHandler 函数默认需要和原方法在同一个类中。若希望使用其他类的函数，则可以指定 blockHandlerClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否则无法解析。\",\"fallback \",\"：fallback 函数名称，可选项，用于在抛出异常的时候提供 fallback 处理逻辑。fallback 函数可以针对所有类型的异常（除了\",\"exceptionsToIgnore \",\"里面排除掉的异常类型）进行处理。fallback 函数签名和位置要求：\",\"返回值类型必须与原函数返回值类型一致；\",\"方法参数列表需要和原函数一致，或者可以额外多一个 Throwable 类型的参数用于接收对应的异常。\",\"fallback 函数默认需要和原方法在同一个类中。若希望使用其他类的函数，则可以指定 fallbackClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否则无法解析。\",\"defaultFallback \",\"（since 1.6.0）：默认的 fallback 函数名称，可选项，通常用于通用的 fallback 逻辑（即可以用于很多服务或方法）。默认 fallback 函数可以针对所以类型的异常（除了\",\"exceptionsToIgnore \",\"里面排除掉的异常类型）进行处理。若同时配置了 fallback 和 defaultFallback，则只有 fallback 会生效。defaultFallback 函数签名要求：\",\"返回值类型必须与原函数返回值类型一致；\",\"方法参数列表需要为空，或者可以额外多一个 Throwable 类型的参数用于接收对应的异常。\",\"defaultFallback 函数默认需要和原方法在同一个类中。若希望使用其他类的函数，则可以指定 fallbackClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否则无法解析。\",\"exceptionsToIgnore（since 1.6.0）：用于指定哪些异常被排除掉，不会计入异常统计中，也不会进入 fallback 逻辑中，而是会原样抛出。\",\"注：1.6.0 之前的版本 fallback 函数只针对降级异常（DegradeException）进行处理，不能针对业务异常进行处理。\",\"特别地，若 blockHandler 和 fallback 都进行了配置，则被限流降级而抛出 BlockException 时只会进入 blockHandler 处理逻辑。若未配置 blockHandler、fallback 和 defaultFallback，则被限流降级时会将 BlockException直接抛出。\"]},{\"header\":\"三个核心API\",\"slug\":\"三个核心api\",\"contents\":[\"SphU定义资源\",\"Tracer定义统计\",\"ContextUtil定义了上下文\"]},{\"header\":\"9、服务熔断\",\"slug\":\"_9、服务熔断\",\"contents\":[\"sentinel+ribbon+openfeign+fallback的整合\"]},{\"header\":\"1、 Ribbon 系列\",\"slug\":\"_1、ribbon系列\",\"contents\":[\"新建xiaobear-cloud-alibaba-sentinel-ribbon-provider-payment9003\",\"新建xiaobear-cloud-alibaba-sentinel-ribbon-provider-payment9004\",\"新建xiaobear-cloud-alibaba-consumer-nacos-order84\"]},{\"header\":\"1、新建9003 、9004\",\"slug\":\"_1、新建9003-、9004\",\"contents\":[\"####### 1、新建module\",\"####### 2、改Pom\",\" <dependencies> <!--SpringCloud ailibaba nacos --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <dependency> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \",\"####### 3、写yaml\",\"端口号是不同的，一个是9003，一个是9004\",\"server: port: 9003 spring: application: name: nacos-payment-provider cloud: nacos: discovery: server-addr: localhost:8848 #配置Nacos地址 management: endpoints: web: exposure: include: '*' \",\"####### 4、主启动类\",\"@SpringBootApplication @EnableDiscoveryClient public class PaymentMain9003 { public static void main(String[] args) { SpringApplication.run(PaymentMain9003.class, args); } } \",\"####### 5、业务层\",\"controller\",\"@RestController public class PaymentController { @Value(\\\"${server.port}\\\") private String serverPort; public static HashMap<Long, Payment> hashMap = new HashMap<>(); static { hashMap.put(1L,new Payment(1L,\\\"28a8c1e3bc2742d8848569891fb42181\\\")); hashMap.put(2L,new Payment(2L,\\\"bba8c1e3bc2742d8848569891ac32182\\\")); hashMap.put(3L,new Payment(3L,\\\"6ua8c1e3bc2742d8848569891xt92183\\\")); } @GetMapping(value = \\\"/paymentSQL/{id}\\\") public CommonResult<Payment> paymentSQL(@PathVariable(\\\"id\\\") Long id) { Payment payment = hashMap.get(id); CommonResult<Payment> result = new CommonResult(200,\\\"from mysql,serverPort: \\\"+serverPort,payment); return result; } } \",\"####### 6、测试\",\"我们采用的是静态数据，并没有进行数据库查询\",\"测试地址：http://localhost:9003/paymentSQL/1\"]},{\"header\":\"2、新建xiaobear-cloud-alibaba-consumer-nacos-order84\",\"slug\":\"_2、新建xiaobear-cloud-alibaba-consumer-nacos-order84\",\"contents\":[\"####### 1、新建\",\"####### 2、改pom\",\"<dependencies> <!--SpringCloud ailibaba nacos --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId> </dependency> <!--SpringCloud ailibaba sentinel --> <dependency> <groupId>com.alibaba.cloud</groupId> <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId> </dependency> <!-- 引入自己定义的api通用包，可以使用Payment支付Entity --> <dependency> <groupId>com.xiaobear</groupId> <artifactId>xiaobear-common-api-3</artifactId> <version>1.0-SNAPSHOT</version> </dependency> <!-- SpringBoot整合Web组件 --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <!--日常通用jar包配置--> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> </dependencies> \",\"####### 3、写yaml\",\"server: port: 84 spring: application: name: nacos-order-consumer cloud: nacos: discovery: server-addr: localhost:8848 sentinel: transport: #配置Sentinel dashboard地址 dashboard: localhost:8080 #默认8719端口，假如被占用会自动从8719开始依次+1扫描,直至找到未被占用的端口 port: 8719 #消费者将要去访问的微服务名称(注册成功进nacos的微服务提供者) service-url: nacos-user-service: http://nacos-payment-provider \",\"####### 4、主启动类\",\"@SpringBootApplication @EnableDiscoveryClient public class OrderNacosMain84 { public static void main(String[] args) { SpringApplication.run(OrderNacosMain84.class, args); } } \",\"####### 5、业务层\",\"config，用于负载均衡\",\"@Configuration public class ApplicationContextConfig { @Bean @LoadBalanced public RestTemplate getRestTemplate(){ return new RestTemplate(); } } \",\"controller\",\"@RestController public class CircleBreakerController { public static final String SERVICE_URL = \\\"http://nacos-payment-provider\\\"; @Resource private RestTemplate restTemplate; @RequestMapping(\\\"/consumer/fallback/{id}\\\") @SentinelResource(value = \\\"fallback\\\") //fallback负责业务异常 public CommonResult<Payment> fallback(@PathVariable Long id) { CommonResult<Payment> result = restTemplate.getForObject(SERVICE_URL + \\\"/paymentSQL/\\\"+id, CommonResult.class,id); if (id == 4) { throw new IllegalArgumentException (\\\"IllegalArgumentException,非法参数异常....\\\"); }else if (result.getData() == null) { throw new NullPointerException (\\\"NullPointerException,该ID没有对应记录,空指针异常\\\"); } return result; } } \",\"####### 6、测试\",\"fallback管运行异常\",\"blockHandler管配置违规\",\"地址：http://localhost:84/consumer/fallback/1\",\"测试结果：直接返回错误页面，不友好\",\"1、只配置fallback\",\"controller新增兜底方法\",\"package com.xiaobear.controller; import com.alibaba.csp.sentinel.annotation.SentinelResource; import com.xiaobear.entities.CommonResult; import com.xiaobear.entities.Payment; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate; import javax.annotation.Resource; /** * @Author xiaobear * @date 2021年06月08日 21:53 * @Description TODO */ @RestController public class CircleBreakerController { public static final String SERVICE_URL = \\\"http://nacos-payment-provider\\\"; @Resource private RestTemplate restTemplate; @RequestMapping(\\\"/consumer/fallback/{id}\\\") @SentinelResource(value = \\\"fallback\\\",fallback = \\\"handlerFallback\\\") //fallback负责业务异常 public CommonResult<Payment> fallback(@PathVariable Long id) { CommonResult<Payment> result = restTemplate.getForObject(SERVICE_URL + \\\"/paymentSQL/\\\"+id, CommonResult.class,id); if (id == 4) { throw new IllegalArgumentException (\\\"IllegalArgumentException,非法参数异常....\\\"); }else if (result.getData() == null) { throw new NullPointerException (\\\"NullPointerException,该ID没有对应记录,空指针异常\\\"); } return result; } public CommonResult handlerFallback(@PathVariable Long id,Throwable e) { Payment payment = new Payment(id,\\\"null\\\"); return new CommonResult<>(444,\\\"兜底异常handlerFallback,exception内容 \\\"+e.getMessage(),payment); } } \",\"重启测试：\",\"2、只配置blockHandler\",\"package com.xiaobear.controller; import com.alibaba.csp.sentinel.annotation.SentinelResource; import com.alibaba.csp.sentinel.slots.block.BlockException; import com.xiaobear.entities.CommonResult; import com.xiaobear.entities.Payment; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate; import javax.annotation.Resource; /** * @Author xiaobear * @date 2021年06月08日 21:53 * @Description TODO */ @RestController public class CircleBreakerController { public static final String SERVICE_URL = \\\"http://nacos-payment-provider\\\"; @Resource private RestTemplate restTemplate; @RequestMapping(\\\"/consumer/fallback/{id}\\\") @SentinelResource(value = \\\"fallback\\\",blockHandler = \\\"blockHandler\\\") //fallback负责业务异常 public CommonResult<Payment> fallback(@PathVariable Long id) { CommonResult<Payment> result = restTemplate.getForObject(SERVICE_URL + \\\"/paymentSQL/\\\"+id, CommonResult.class,id); if (id == 4) { throw new IllegalArgumentException (\\\"IllegalArgumentException,非法参数异常....\\\"); }else if (result.getData() == null) { throw new NullPointerException (\\\"NullPointerException,该ID没有对应记录,空指针异常\\\"); } return result; } public CommonResult handlerFallback(@PathVariable Long id,Throwable e) { Payment payment = new Payment(id,\\\"null\\\"); return new CommonResult<>(444,\\\"兜底异常handlerFallback,exception内容 \\\"+e.getMessage(),payment); } public CommonResult blockHandler(@PathVariable Long id, BlockException blockException) { Payment payment = new Payment(id,\\\"null\\\"); return new CommonResult<>(445,\\\"blockHandler-sentinel限流,无此流水: blockException \\\"+blockException.getMessage(),payment); } } \",\"sentinel配置\",\"结果\",\"3、fallback和blockHandler都配置\",\"@SentinelResource(value = \\\"fallback\\\",fallback = \\\"handlerFallback\\\",blockHandler = \\\"blockHandler\\\") \",\"若 blockHandler 和 fallback 都进行了配置，则被限流降级而抛出 BlockException 时只会进入 blockHandler 处理逻辑。\",\"4、忽略属性.......\",\"@SentinelResource(value = \\\"fallback\\\", fallback = \\\"handlerFallback\\\", blockHandler = \\\"blockHandler\\\", exceptionsToIgnore = {IllegalArgumentException.class}) \"]},{\"header\":\"2、Feign系列\",\"slug\":\"_2、feign系列\",\"contents\":[]},{\"header\":\"1、修改84模块\",\"slug\":\"_1、修改84模块\",\"contents\":[]},{\"header\":\"1、pom文件\",\"slug\":\"_1、pom文件\",\"contents\":[\"<!--SpringCloud openfeign --> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-openfeign</artifactId> </dependency> \"]},{\"header\":\"2、yaml\",\"slug\":\"_2、yaml\",\"contents\":[\"## 激活Sentinel对Feign的支持 feign: sentinel: enabled: true \"]},{\"header\":\"3、业务层\",\"slug\":\"_3、业务层\",\"contents\":[\"业务接口 带@FeignClient注解的业务接口\",\"@FeignClient(value = \\\"nacos-payment-provider\\\",fallback = PaymentFallbackService.class)//调用中关闭9003服务提供者 public interface PaymentService { @GetMapping(value = \\\"/paymentSQL/{id}\\\") public CommonResult<Payment> paymentSQL(@PathVariable(\\\"id\\\") Long id); } \",\"@Component public class PaymentFallbackService implements PaymentService { @Override public CommonResult<Payment> paymentSQL(Long id) { return new CommonResult<>(444,\\\"服务降级返回,没有该流水信息\\\",new Payment(id, \\\"errorSerial......\\\")); } } \",\"controller\",\"@Resource private PaymentService paymentService; @GetMapping(value = \\\"/consumer/openfeign/{id}\\\") public CommonResult<Payment> paymentSQL(@PathVariable(\\\"id\\\") Long id) { if(id == 4) { throw new RuntimeException(\\\"没有该id\\\"); } return paymentService.paymentSQL(id); } \"]},{\"header\":\"4、主启动\",\"slug\":\"_4、主启动\",\"contents\":[\"添加注解@EnableFeignClients\"]},{\"header\":\"5、测试\",\"slug\":\"_5、测试\",\"contents\":[\"http://localhost:84/consumer/paymentSQL/1\",\"测试84调用9003，此时故意关闭9003微服务提供者，看84消费侧自动降级，不会被耗死\"]},{\"header\":\"10、规则持久化\",\"slug\":\"_10、规则持久化\",\"contents\":[\"一旦我们重启应用，sentinel规则将消失，生产环境需要将配置规则进行持久化\",\"如何使用？\",\"将限流配置规则持久化进Nacos保存，只要刷新8401某个rest地址，sentinel控制台 的流控规则就能看到，只要Nacos里面的配置不删除，针对8401上sentinel上的流控规则持续有效\"]},{\"header\":\"1、修改8401\",\"slug\":\"_1、修改8401-1\",\"contents\":[]},{\"header\":\"1、修改pom\",\"slug\":\"_1、修改pom\",\"contents\":[\"<!--SpringCloud ailibaba sentinel-datasource-nacos --> <dependency> <groupId>com.alibaba.csp</groupId> <artifactId>sentinel-datasource-nacos</artifactId> </dependency> \"]},{\"header\":\"2、改yaml\",\"slug\":\"_2、改yaml\",\"contents\":[\"server: port: 8401 spring: application: name: xiaobear-sentinel-service cloud: #nacos注册服务中心 nacos: discovery: server-addr: localhost:8848 sentinel: transport: #配置Sentinel dashboard地址 dashboard: localhost:8080 #默认8719端口，假如被占用会自动从8719开始依次+1扫描,直至找到未被占用的端口 port: 8719 clientIp: localhost datasource: ds1: nacos: server-addr: localhost:8848 dataId: cloudalibaba-sentinel-service groupId: DEFAULT_GROUP data-type: json rule-type: flow #暴露端点 management: endpoints: web: exposure: include: '*' \"]},{\"header\":\"3、添加Nacos业务规则配置\",\"slug\":\"_3、添加nacos业务规则配置\",\"contents\":[\"[ { \\\"resource\\\": \\\"/rateLimit/byUrl\\\", \\\"limitApp\\\": \\\"default\\\", \\\"grade\\\": 1, \\\"count\\\": 1, \\\"strategy\\\": 0, \\\"controlBehavior\\\": 0, \\\"clusterMode\\\": false } ] \"]},{\"header\":\"4、启动测试\",\"slug\":\"_4、启动测试\",\"contents\":[\"测试地址：http://localhost:8401/rateLimit/byUrl\",\"停掉8401之后，在启动，发现流控规则还存在，重新配置出现了，持久化验证通过\"]}]},\"/study-tutorial/microservice/spring-cloud-alibaba/started.html\":{\"title\":\"1、Spring Cloud Alibaba入门\",\"contents\":[{\"header\":\"1、为什么会出现\",\"slug\":\"_1、为什么会出现\",\"contents\":[\"Spring Cloud netflix进入维护模式\",\"将模块置于维护模式，意味着 Spring Cloud 团队将不会再向模块添加新功能。\",\"我们将修复 block 级别的 bug 以及安全问题，我们也会考虑并审查社区的小型 pull request。\",\"进入维护模式意味着\",\"Spring Cloud Netflix 将不再开发新的组件\",\"我们都知道Spring Cloud 版本迭代算是比较快的，因而出现了很多重大ISSUE都还来不及Fix就又推另一个Release了。进入维护模式意思就是目前一直以后一段时间Spring Cloud Netflix提供的服务和功能就这么多了，不在开发新的组件和功能了。以后将以维护和Merge分支Full Request为主\",\"新组件功能将以其他替代平代替的方式实现\"]},{\"header\":\"2、什么是 Spring Cloud Alibaba\",\"slug\":\"_2、什么是spring-cloud-alibaba\",\"contents\":[\"中文文档：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md\",\"Spring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。\"]},{\"header\":\"1、功能\",\"slug\":\"_1、功能\",\"contents\":[\"服务限流降级：默认支持 WebServlet、WebFlux, OpenFeign、RestTemplate、Spring Cloud Gateway, Zuul, Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。\",\"服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。\",\"分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。\",\"消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。\",\"分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。\",\"阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。\",\"分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。\",\"阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\"]},{\"header\":\"2、组件\",\"slug\":\"_2、组件\",\"contents\":[\"Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。\",\"Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。\",\"RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。\",\"Dubbo：Apache Dubbo™ 是一款高性能 Java RPC 框架。\",\"Seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。\",\"Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。\",\"Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。\",\"Alibaba Cloud SMS: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\"]},{\"header\":\"3、官方资料\",\"slug\":\"_3、官方资料\",\"contents\":[\"官方文档：https://spring-cloud-alibaba-group.github.io/github-pages/hoxton/en-us/index.html#_introduction\",\"中文文档：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md\"]}]},\"/study-tutorial/components/activity/basic/environment.html\":{\"title\":\"3、Activiti环境\",\"contents\":[{\"header\":\"1、开发环境\",\"slug\":\"_1、开发环境\",\"contents\":[\"Jdk1.8或以上版本\",\"Mysql 5及以上的版本\",\"Tomcat8.5\",\"IDEA\",\"注意：activiti的流程定义工具插件可以安装在IDEA下，也可以安装在Eclipse工具下\"]},{\"header\":\"2、Activiti环境\",\"slug\":\"_2、activiti环境\",\"contents\":[\"我们使用：Activiti7.0.0.Beta1 默认支持spring5\"]},{\"header\":\"1、下载activiti7\",\"slug\":\"_1、下载activiti7\",\"contents\":[\"Activiti下载地址：http://activiti.org/download.html ，Maven的依赖如下：\",\"<dependencyManagement> <dependencies> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-dependencies</artifactId> <version>7.0.0.Beta1</version> <scope>import</scope> <type>pom</type> </dependency> </dependencies> </dependencyManagement> \",\"1)Database：\",\"activiti运行需要有数据库的支持，支持的数据库有：h2, mysql, oracle, postgres, mssql, db2。\"]},{\"header\":\"2、流程设计器IDEA下安装\",\"slug\":\"_2、流程设计器idea下安装\",\"contents\":[\"在IDEA的File菜单中找到子菜单”Settings”,后面我们再选择左侧的“plugins”菜单，如下图所示：\",\"此时我们就可以搜索到actiBPM插件，它就是Activiti Designer的IDEA版本，我们点击Install安装。\",\"安装好后，页面如下：\",\"提示需要重启idea，点击重启。\",\"重启完成后，再次打开Settings 下的 Plugins（插件列表），点击右侧的Installed（已安装的插件），在列表中看到actiBPM，就说明已经安装成功了，如下图所示：\",\"后面的课程里，我们会使用这个流程设计器进行Activiti的流程设计。\",\"actiBPM在低版本IDEA2020中适用，具体请自行测试，高于这个版本的，请安装Activiti BPMN visualizer插件，但这个插件只能用activiti 6，对于activiti 7的版本，还需借用外部流程图软件；\",\"具体可参考：https://blog.csdn.net/qq_37315617/article/details/122256896\"]},{\"header\":\"3、Activiti的数据库支持\",\"slug\":\"_3、activiti的数据库支持\",\"contents\":[\"Activiti 在运行时需要数据库的支持，使用25张表，把流程定义节点内容读取到数据库表中，以供后续使用。\"]},{\"header\":\"1、Activiti 支持的数据库\",\"slug\":\"_1、activiti-支持的数据库\",\"contents\":[\"activiti 支持的数据库和版本如下：\",\"数据库类型\",\"版本\",\"JDBC连接示例\",\"说明\",\"h2\",\"1.3.168\",\"jdbc:h2:tcp://localhost/activiti\",\"默认配置的数据库\",\"mysql\",\"5.1.21\",\"jdbc:mysql://localhost:3306/activiti?autoReconnect=true\",\"使用 mysql-connector-java 驱动测试\",\"oracle\",\"11.2.0.1.0\",\"jdbc:oracle:thin:@localhost:1521:xe\",\"postgres\",\"8.1\",\"jdbc:postgresql://localhost:5432/activiti\",\"db2\",\"DB2 10.1 using db2jcc4\",\"jdbc:db2://localhost:50000/activiti\",\"mssql\",\"2008 using sqljdbc4\",\"jdbc:sqlserver://localhost:1433/activiti\"]},{\"header\":\"2、在MySQL生成表\",\"slug\":\"_2、在mysql生成表\",\"contents\":[]},{\"header\":\"1、创建数据库\",\"slug\":\"_1、创建数据库\",\"contents\":[\"创建 mysql 数据库 activiti （名字任意）：\",\"CREATE DATABASE activiti DEFAULT CHARACTER SET utf8;\"]},{\"header\":\"2、使用java代码生成表\",\"slug\":\"_2、使用java代码生成表\",\"contents\":[]},{\"header\":\"1） 创建 java 工程\",\"slug\":\"_1-创建-java-工程\",\"contents\":[\"使用idea 创建 java 的maven工程，取名：activiti01。\"]},{\"header\":\"2） 加入 maven 依赖的坐标（jar 包）\",\"slug\":\"_2-加入-maven-依赖的坐标-jar-包\",\"contents\":[\"首先需要在 java 工程中加入 ProcessEngine 所需要的 jar 包，包括：\",\"activiti-engine-7.0.0.beta1.jar\",\"activiti 依赖的 jar 包： mybatis、 alf4j、 log4j 等\",\"activiti 依赖的 spring 包\",\"mysql数据库驱动\",\"第三方数据连接池 dbcp\",\"单元测试 Junit-4.12.jar\",\"我们使用 maven 来实现项目的构建，所以应当导入这些 jar 所对应的坐标到 pom.xml 文件中。\",\"完整的依赖内容如下：\",\"<properties> <slf4j.version>1.6.6</slf4j.version> <log4j.version>1.2.12</log4j.version> <activiti.version>7.0.0.Beta1</activiti.version> </properties> <dependencies> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-engine</artifactId> <version>${activiti.version}</version> </dependency> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-spring</artifactId> <version>${activiti.version}</version> </dependency> <!-- bpmn 模型处理 --> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-bpmn-model</artifactId> <version>${activiti.version}</version> </dependency> <!-- bpmn 转换 --> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-bpmn-converter</artifactId> <version>${activiti.version}</version> </dependency> <!-- bpmn json数据转换 --> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-json-converter</artifactId> <version>${activiti.version}</version> </dependency> <!-- bpmn 布局 --> <dependency> <groupId>org.activiti</groupId> <artifactId>activiti-bpmn-layout</artifactId> <version>${activiti.version}</version> </dependency> <!-- activiti 云支持 --> <dependency> <groupId>org.activiti.cloud</groupId> <artifactId>activiti-cloud-services-api</artifactId> <version>${activiti.version}</version> </dependency> <!-- mysql驱动 --> <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>5.1.40</version> </dependency> <!-- mybatis --> <dependency> <groupId>org.mybatis</groupId> <artifactId>mybatis</artifactId> <version>3.4.5</version> </dependency> <!-- 链接池 --> <dependency> <groupId>commons-dbcp</groupId> <artifactId>commons-dbcp</artifactId> <version>1.4</version> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> </dependency> <!-- log start --> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> <version>${slf4j.version}</version> </dependency> </dependencies> \"]},{\"header\":\"3） 添加log4j日志配置\",\"slug\":\"_3-添加log4j日志配置\",\"contents\":[\"我们使用log4j日志包，可以对日志进行配置\",\"在resources 下创建log4j.properties\",\"# Set root category priority to INFO and its only appender to CONSOLE. #log4j.rootCategory=INFO, CONSOLE debug info warn error fatal log4j.rootCategory=debug, CONSOLE, LOGFILE # Set the enterprise logger category to FATAL and its only appender to CONSOLE. log4j.logger.org.apache.axis.enterprise=FATAL, CONSOLE # CONSOLE is set to be a ConsoleAppender using a PatternLayout. log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %-6r[%15.15t] %-5p %30.30c %x - %m\\\\n # LOGFILE is set to be a File appender using a PatternLayout. log4j.appender.LOGFILE=org.apache.log4j.FileAppender log4j.appender.LOGFILE.File=f:\\\\act\\\\activiti.log log4j.appender.LOGFILE.Append=true log4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout log4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %-6r[%15.15t] %-5p %30.30c %x - %m\\\\n \"]},{\"header\":\"4） 添加activiti配置文件\",\"slug\":\"_4-添加activiti配置文件\",\"contents\":[\"我们使用activiti提供的默认方式来创建mysql的表。\",\"默认方式的要求是在 resources 下创建 activiti.cfg.xml 文件，注意：默认方式目录和文件名不能修改，因为activiti的源码中已经设置，到固定的目录读取固定文件名的文件。\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:context=\\\"http://www.springframework.org/schema/context\\\" xmlns:tx=\\\"http://www.springframework.org/schema/tx\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/contex http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\\\"> </beans> \"]},{\"header\":\"5） 在 activiti.cfg.xml 中进行配置\",\"slug\":\"_5-在-activiti-cfg-xml-中进行配置\",\"contents\":[\"默认方式要在在activiti.cfg.xml中bean的名字叫processEngineConfiguration，名字不可修改\",\"在这里有2中配置方式：一种是单独配置数据源，一种是不单独配置数据源\"]},{\"header\":\"1、直接配置processEngineConfiguration\",\"slug\":\"_1、直接配置processengineconfiguration\",\"contents\":[\"processEngineConfiguration 用来创建 ProcessEngine，在创建 ProcessEngine 时会执行数据库的操作。\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:context=\\\"http://www.springframework.org/schema/context\\\" xmlns:tx=\\\"http://www.springframework.org/schema/tx\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/contex http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\\\"> <!-- 默认id对应的值 为processEngineConfiguration --> <!-- processEngine Activiti的流程引擎 --> <bean id=\\\"processEngineConfiguration\\\" class=\\\"org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration\\\"> <property name=\\\"jdbcDriver\\\" value=\\\"com.mysql.jdbc.Driver\\\"/> <property name=\\\"jdbcUrl\\\" value=\\\"jdbc:mysql:///activiti\\\"/> <property name=\\\"jdbcUsername\\\" value=\\\"root\\\"/> <property name=\\\"jdbcPassword\\\" value=\\\"123456\\\"/> <!-- activiti数据库表处理策略 --> <property name=\\\"databaseSchemaUpdate\\\" value=\\\"true\\\"/> </bean> </beans> \"]},{\"header\":\"2、配置数据源后，在processEngineConfiguration 引用\",\"slug\":\"_2、配置数据源后-在processengineconfiguration-引用\",\"contents\":[\"首先配置数据源\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:context=\\\"http://www.springframework.org/schema/context\\\" xmlns:tx=\\\"http://www.springframework.org/schema/tx\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/contex http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\\\"> <!-- 这里可以使用 链接池 dbcp--> <bean id=\\\"dataSource\\\" class=\\\"org.apache.commons.dbcp.BasicDataSource\\\"> <property name=\\\"driverClassName\\\" value=\\\"com.mysql.jdbc.Driver\\\" /> <property name=\\\"url\\\" value=\\\"jdbc:mysql:///activiti\\\" /> <property name=\\\"username\\\" value=\\\"root\\\" /> <property name=\\\"password\\\" value=\\\"123456\\\" /> <property name=\\\"maxActive\\\" value=\\\"3\\\" /> <property name=\\\"maxIdle\\\" value=\\\"1\\\" /> </bean> <bean id=\\\"processEngineConfiguration\\\" class=\\\"org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration\\\"> <!-- 引用数据源 上面已经设置好了--> <property name=\\\"dataSource\\\" ref=\\\"dataSource\\\" /> <!-- activiti数据库表处理策略 --> <property name=\\\"databaseSchemaUpdate\\\" value=\\\"true\\\"/> </bean> </beans> \"]},{\"header\":\"6） java类编写程序生成表\",\"slug\":\"_6-java类编写程序生成表\",\"contents\":[\"创建一个测试类，调用activiti的工具类，生成acitivti需要的数据库表。\",\"直接使用activiti提供的工具类ProcessEngines，会默认读取classpath下的activiti.cfg.xml文件，读取其中的数据库配置，创建 ProcessEngine，在创建ProcessEngine 时会自动创建表。\",\"代码如下：\",\"package com.itheima.activiti01.test; import org.activiti.engine.ProcessEngine; import org.activiti.engine.ProcessEngineConfiguration; import org.junit.Test; public class TestDemo { /** * 生成 activiti的数据库表 */ @Test public void testCreateDbTable() { //使用classpath下的activiti.cfg.xml中的配置创建processEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); System.out.println(processEngine); } } \",\"说明： 1、运行以上程序段即可完成 activiti 表创建，通过改变 activiti.cfg.xml 中databaseSchemaUpdate 参数的值执行不同的数据表处理策略。 2 、 上 边 的 方法 getDefaultProcessEngine方法在执行时，从activiti.cfg.xml 中找固定的名称 processEngineConfiguration 。\",\"在测试程序执行过程中，idea的控制台会输出日志，说明程序正在创建数据表，类似如下,注意红线内容：\",\"执行完成后我们查看数据库， 创建了 25 张表，结果如下：\",\"到这，我们就完成activiti运行需要的数据库和表的创建。\"]},{\"header\":\"4、表结构介绍\",\"slug\":\"_4、表结构介绍\",\"contents\":[]},{\"header\":\"1、表的命名规则和作用\",\"slug\":\"_1、表的命名规则和作用\",\"contents\":[\"看到刚才创建的表，我们发现Activiti 的表都以 ACT_ 开头。\",\"第二部分是表示表的用途的两个字母标识。 用途也和服务的 API 对应。 ACT_RE ：'RE'表示 repository。 这个前缀的表包含了流程定义和流程静态资源 （图片，规则，等等）。 ACT_RU：'RU'表示 runtime。 这些运行时的表，包含流程实例，任务，变量，异步任务，等运行中的数据。 Activiti 只在流程实例执行过程中保存这些数据， 在流程结束时就会删除这些记录。 这样运行时表可以一直很小速度很快。 ACT_HI：'HI'表示 history。 这些表包含历史数据，比如历史流程实例， 变量，任务等等。 ACT_GE ： GE 表示 general。 通用数据， 用于不同场景下\"]},{\"header\":\"2、Activiti数据表介绍\",\"slug\":\"_2、activiti数据表介绍\",\"contents\":[\"表分类\",\"表名\",\"解释\",\"一般数据\",\"[ACT_GE_BYTEARRAY]\",\"通用的流程定义和流程资源\",\"[ACT_GE_PROPERTY]\",\"系统相关属性\",\"流程历史记录\",\"[ACT_HI_ACTINST]\",\"历史的流程实例\",\"[ACT_HI_ATTACHMENT]\",\"历史的流程附件\",\"[ACT_HI_COMMENT]\",\"历史的说明性信息\",\"[ACT_HI_DETAIL]\",\"历史的流程运行中的细节信息\",\"[ACT_HI_IDENTITYLINK]\",\"历史的流程运行过程中用户关系\",\"[ACT_HI_PROCINST]\",\"历史的流程实例\",\"[ACT_HI_TASKINST]\",\"历史的任务实例\",\"[ACT_HI_VARINST]\",\"历史的流程运行中的变量信息\",\"流程定义表\",\"[ACT_RE_DEPLOYMENT]\",\"部署单元信息\",\"[ACT_RE_MODEL]\",\"模型信息\",\"[ACT_RE_PROCDEF]\",\"已部署的流程定义\",\"运行实例表\",\"[ACT_RU_EVENT_SUBSCR]\",\"运行时事件\",\"[ACT_RU_EXECUTION]\",\"运行时流程执行实例\",\"[ACT_RU_IDENTITYLINK]\",\"运行时用户关系信息，存储任务节点与参与者的相关信息\",\"[ACT_RU_JOB]\",\"运行时作业\",\"[ACT_RU_TASK]v\",\"运行时任务\",\"[ACT_RU_VARIABLE]\",\"运行时变量表\"]}]},\"/study-tutorial/components/activity/basic/getting_started.html\":{\"title\":\"5、Activiti入门\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在本章内容中，我们来创建一个Activiti工作流，并启动这个流程。\",\"创建Activiti工作流主要包含以下几步：\",\"1、定义流程，按照BPMN的规范，使用流程定义工具，用流程符号把整个流程描述出来\",\"2、部署流程，把画好的流程定义文件，加载到数据库中，生成表的数据\",\"3、启动流程，使用java代码来操作数据库表中的内容\"]},{\"header\":\"1、流程符号\",\"slug\":\"_1、流程符号\",\"contents\":[\"BPMN 2.0是业务流程建模符号2.0的缩写。\",\"它由Business Process Management Initiative这个非营利协会创建并不断发展。作为一种标识，BPMN 2.0是使用一些符号来明确业务流程设计流程图的一整套符号规范，它能增进业务建模时的沟通效率。\",\"目前BPMN2.0是最新的版本，它用于在BPM上下文中进行布局和可视化的沟通。\",\"接下来我们先来了解在流程设计中常见的 符号。\",\"BPMN2.0的基本符合主要包含：\"]},{\"header\":\"事件 Event\",\"slug\":\"事件-event\",\"contents\":[]},{\"header\":\"活动 Activity\",\"slug\":\"活动-activity\",\"contents\":[\"活动是工作或任务的一个通用术语。一个活动可以是一个任务，还可以是一个当前流程的子处理流程； 其次，你还可以为活动指定不同的类型。常见活动如下：\"]},{\"header\":\"网关 GateWay\",\"slug\":\"网关-gateway\",\"contents\":[\"网关用来处理决策，有几种常用网关需要了解：\"]},{\"header\":\"排他网关 (x)\",\"slug\":\"排他网关-x\",\"contents\":[\"——只有一条路径会被选择。流程执行到该网关时，按照输出流的顺序逐个计算，当条件的计算结果为true时，继续执行当前网关的输出流；\",\"​ 如果多条线路计算结果都是 true，则会执行第一个值为 true 的线路。如果所有网关计算结果没有true，则引擎会抛出异常。\",\"​ 排他网关需要和条件顺序流结合使用，default 属性指定默认顺序流，当所有的条件不满足时会执行默认顺序流。\"]},{\"header\":\"并行网关 (+)\",\"slug\":\"并行网关\",\"contents\":[\"——所有路径会被同时选择\",\"​ 拆分 —— 并行执行所有输出顺序流，为每一条顺序流创建一个并行执行线路。\",\"​ 合并 —— 所有从并行网关拆分并执行完成的线路均在此等候，直到所有的线路都执行完成才继续向下执行。\"]},{\"header\":\"包容网关 (+)\",\"slug\":\"包容网关\",\"contents\":[\"—— 可以同时执行多条线路，也可以在网关上设置条件\",\"​ 拆分 —— 计算每条线路上的表达式，当表达式计算结果为true时，创建一个并行线路并继续执行\",\"​ 合并 —— 所有从并行网关拆分并执行完成的线路均在此等候，直到所有的线路都执行完成才继续向下执行。\"]},{\"header\":\"事件网关 (+)\",\"slug\":\"事件网关\",\"contents\":[\"—— 专门为中间捕获事件设置的，允许设置多个输出流指向多个不同的中间捕获事件。当流程执行到事件网关后，流程处于等待状态，需要等待抛出事件才能将等待状态转换为活动状态。\"]},{\"header\":\"流向 Flow\",\"slug\":\"流向-flow\",\"contents\":[\"流是连接两个流程节点的连线。常见的流向包含以下几种：\"]},{\"header\":\"2、流程设计器使用\",\"slug\":\"_2、流程设计器使用\",\"contents\":[]},{\"header\":\"Activiti-Designer使用\",\"slug\":\"activiti-designer使用\",\"contents\":[]},{\"header\":\"Palette（画板）\",\"slug\":\"palette-画板\",\"contents\":[\"在idea中安装插件即可使用，画板中包括以下结点：\",\"Connection—连接\",\"Event---事件\",\"Task---任务\",\"Gateway---网关\",\"Container—容器\",\"Boundary event—边界事件\",\"Intermediate event- -中间事件\",\"流程图设计完毕保存生成.bpmn文件\"]},{\"header\":\"新建流程(IDEA工具)\",\"slug\":\"新建流程-idea工具\",\"contents\":[\"首先选中存放图形的目录(选择resources下的bpmn目录)，点击菜单：New -> BpmnFile，如图：\",\"弹出如下图所示框，输入evection 表示 出差审批流程：\",\"起完名字evection后（默认扩展名为bpmn），就可以看到流程设计页面，如图所示：\",\"左侧区域是绘图区，右侧区域是palette画板区域\",\"鼠标先点击画板的元素即可在左侧绘图\"]},{\"header\":\"绘制流程\",\"slug\":\"绘制流程\",\"contents\":[\"使用滑板来绘制流程，通过从右侧把图标拖拽到左侧的画板，最终效果如下：\"]},{\"header\":\"指定流程定义Key\",\"slug\":\"指定流程定义key\",\"contents\":[\"流程定义key即流程定义的标识，通过properties视图查看流程的key\"]},{\"header\":\"指定任务负责人\",\"slug\":\"指定任务负责人\",\"contents\":[\"在properties视图指定每个任务结点的负责人，如：填写出差申请的负责人为 zhangsan\",\"经理审批负责人为 lisi\",\"总经理审批负责人为 wangwu\",\"财务审批负责人为 liliu\",\"因版本问题，流程配置文件如下：\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" standalone=\\\"yes\\\"?> <definitions xmlns=\\\"http://www.omg.org/spec/BPMN/20100524/MODEL\\\" xmlns:activiti=\\\"http://activiti.org/bpmn\\\" xmlns:bpmndi=\\\"http://www.omg.org/spec/BPMN/20100524/DI\\\" xmlns:omgdc=\\\"http://www.omg.org/spec/DD/20100524/DC\\\" xmlns:omgdi=\\\"http://www.omg.org/spec/DD/20100524/DI\\\" xmlns:tns=\\\"http://www.activiti.org/test\\\" xmlns:xsd=\\\"http://www.w3.org/2001/XMLSchema\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" expressionLanguage=\\\"http://www.w3.org/1999/XPath\\\" id=\\\"m1576151336547\\\" name=\\\"\\\" targetNamespace=\\\"http://www.activiti.org/test\\\" typeLanguage=\\\"http://www.w3.org/2001/XMLSchema\\\"> <process id=\\\"myEvection\\\" isClosed=\\\"false\\\" isExecutable=\\\"true\\\" name=\\\"出差申请\\\" processType=\\\"None\\\"> <startEvent id=\\\"_2\\\" name=\\\"StartEvent\\\"/> <userTask activiti:assignee=\\\"zhangsan\\\" activiti:exclusive=\\\"true\\\" id=\\\"_3\\\" name=\\\"创建出差申请\\\"/> <userTask activiti:assignee=\\\"lisi\\\" activiti:exclusive=\\\"true\\\" id=\\\"_4\\\" name=\\\"经理审批\\\"/> <userTask activiti:assignee=\\\"wangwu\\\" activiti:exclusive=\\\"true\\\" id=\\\"_5\\\" name=\\\"总经理审批\\\"/> <userTask activiti:assignee=\\\"liliu\\\" activiti:exclusive=\\\"true\\\" id=\\\"_6\\\" name=\\\"财务审批\\\"/> <endEvent id=\\\"_7\\\" name=\\\"EndEvent\\\"/> <sequenceFlow id=\\\"_8\\\" sourceRef=\\\"_2\\\" targetRef=\\\"_3\\\"/> <sequenceFlow id=\\\"_9\\\" sourceRef=\\\"_3\\\" targetRef=\\\"_4\\\"/> <sequenceFlow id=\\\"_10\\\" sourceRef=\\\"_4\\\" targetRef=\\\"_5\\\"/> <sequenceFlow id=\\\"_11\\\" sourceRef=\\\"_5\\\" targetRef=\\\"_6\\\"/> <sequenceFlow id=\\\"_12\\\" sourceRef=\\\"_6\\\" targetRef=\\\"_7\\\"/> </process> <bpmndi:BPMNDiagram documentation=\\\"background=#FFFFFF;count=1;horizontalcount=1;orientation=0;width=842.4;height=1195.2;imageableWidth=832.4;imageableHeight=1185.2;imageableX=5.0;imageableY=5.0\\\" id=\\\"Diagram-_1\\\" name=\\\"New Diagram\\\"> <bpmndi:BPMNPlane bpmnElement=\\\"myEvection\\\"> <bpmndi:BPMNShape bpmnElement=\\\"_2\\\" id=\\\"Shape-_2\\\"> <omgdc:Bounds height=\\\"32.0\\\" width=\\\"32.0\\\" x=\\\"245.0\\\" y=\\\"10.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"32.0\\\" width=\\\"32.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"_3\\\" id=\\\"Shape-_3\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"220.0\\\" y=\\\"65.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"_4\\\" id=\\\"Shape-_4\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"220.0\\\" y=\\\"155.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"_5\\\" id=\\\"Shape-_5\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"225.0\\\" y=\\\"240.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"_6\\\" id=\\\"Shape-_6\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"225.0\\\" y=\\\"320.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"85.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"_7\\\" id=\\\"Shape-_7\\\"> <omgdc:Bounds height=\\\"32.0\\\" width=\\\"32.0\\\" x=\\\"250.0\\\" y=\\\"400.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"32.0\\\" width=\\\"32.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNShape> <bpmndi:BPMNEdge bpmnElement=\\\"_12\\\" id=\\\"BPMNEdge__12\\\" sourceElement=\\\"_6\\\" targetElement=\\\"_7\\\"> <omgdi:waypoint x=\\\"266.0\\\" y=\\\"375.0\\\"/> <omgdi:waypoint x=\\\"266.0\\\" y=\\\"400.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"0.0\\\" width=\\\"0.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"_8\\\" id=\\\"BPMNEdge__8\\\" sourceElement=\\\"_2\\\" targetElement=\\\"_3\\\"> <omgdi:waypoint x=\\\"261.0\\\" y=\\\"42.0\\\"/> <omgdi:waypoint x=\\\"261.0\\\" y=\\\"65.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"0.0\\\" width=\\\"0.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"_9\\\" id=\\\"BPMNEdge__9\\\" sourceElement=\\\"_3\\\" targetElement=\\\"_4\\\"> <omgdi:waypoint x=\\\"262.5\\\" y=\\\"120.0\\\"/> <omgdi:waypoint x=\\\"262.5\\\" y=\\\"155.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"0.0\\\" width=\\\"0.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"_11\\\" id=\\\"BPMNEdge__11\\\" sourceElement=\\\"_5\\\" targetElement=\\\"_6\\\"> <omgdi:waypoint x=\\\"267.5\\\" y=\\\"295.0\\\"/> <omgdi:waypoint x=\\\"267.5\\\" y=\\\"320.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"0.0\\\" width=\\\"0.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"_10\\\" id=\\\"BPMNEdge__10\\\" sourceElement=\\\"_4\\\" targetElement=\\\"_5\\\"> <omgdi:waypoint x=\\\"265.0\\\" y=\\\"210.0\\\"/> <omgdi:waypoint x=\\\"265.0\\\" y=\\\"240.0\\\"/> <bpmndi:BPMNLabel> <omgdc:Bounds height=\\\"0.0\\\" width=\\\"0.0\\\" x=\\\"0.0\\\" y=\\\"0.0\\\"/> </bpmndi:BPMNLabel> </bpmndi:BPMNEdge> </bpmndi:BPMNPlane> </bpmndi:BPMNDiagram> </definitions> \"]}]},\"/study-tutorial/components/activity/basic/introduce.html\":{\"title\":\"1、工作流介绍\",\"contents\":[{\"header\":\"1、概念\",\"slug\":\"_1、概念\",\"contents\":[\"工作流(Workflow)，就是通过计算机对业务流程自动化执行管理。它主要解决的是“使在多个参与者之间按照某种预定义的规则自动进行传递文档、信息或任务的过程，从而实现某个预期的业务目标，或者促使此目标的实现”。\"]},{\"header\":\"2、工作流系统\",\"slug\":\"_2、工作流系统\",\"contents\":[\"一个软件系统中具有工作流的功能，我们把它称为工作流系统，一个系统中工作流的功能是什么？就是对系统的业务流程进行自动化管理，所以工作流是建立在业务流程的基础上，所以一个软件的系统核心根本上还是系统的业务流程，工作流只是协助进行业务流程管理。即使没有工作流业务系统也可以开发运行，只不过有了工作流可以更好的管理业务流程，提高系统的可扩展性。\"]},{\"header\":\"3、适用行业\",\"slug\":\"_3、适用行业\",\"contents\":[\"消费品行业，制造业，电信服务业，银证险等金融服务业，物流服务业，物业服务业，物业管理，大中型进出口贸易公司，政府事业机构，研究院所及教育服务业等，特别是大的跨国企业和集团公司。\"]},{\"header\":\"4、具体应用\",\"slug\":\"_4、具体应用\",\"contents\":[\"1、关键业务流程：订单、报价处理、合同审核、客户电话处理、供应链管理等\",\"2、行政管理类:出差申请、加班申请、请假申请、用车申请、各种办公用品申请、购买申请、日报周报等凡是原来手工流转处理的行政表单。\",\"3、人事管理类：员工培训安排、绩效考评、职位变动处理、员工档案信息管理等。\",\"4、财务相关类：付款请求、应收款处理、日常报销处理、出差报销、预算和计划申请等。\",\"5、客户服务类：客户信息管理、客户投诉、请求处理、售后服务管理等。\",\"6、特殊服务类：ISO系列对应流程、质量管理对应流程、产品数据信息管理、贸易公司报关处理、物流公司货物跟踪处理等各种通过表单逐步手工流转完成的任务均可应用工作流软件自动规范地实施。\"]},{\"header\":\"5、实现方式\",\"slug\":\"_5、实现方式\",\"contents\":[\"在没有专门的工作流引擎之前，我们之前为了实现流程控制，通常的做法就是采用状态字段的值来跟踪流程的变化情况。这样不用角色的用户，通过状态字段的取值来决定记录是否显示。\",\"针对有权限可以查看的记录，当前用户根据自己的角色来决定审批是否合格的操作。如果合格将状态字段设置一个值，来代表合格；当然如果不合格也需要设置一个值来代表不合格的情况。\",\"这是一种最为原始的方式。通过状态字段虽然做到了流程控制，但是当我们的流程发生变更的时候，这种方式所编写的代码也要进行调整。\",\"那么有没有专业的方式来实现工作流的管理呢？并且可以做到业务流程变化之后，我们的程序可以不用改变，如果可以实现这样的效果，那么我们的业务系统的适应能力就得到了极大提升。\"]}]},\"/study-tutorial/components/activity/basic/operate.html\":{\"title\":\"6、流程操作\",\"contents\":[{\"header\":\"1、流程定义\",\"slug\":\"_1、流程定义\",\"contents\":[]},{\"header\":\"概述\",\"slug\":\"概述\",\"contents\":[\"流程定义是线下按照bpmn2.0标准去描述 业务流程，通常使用idea中的插件对业务流程进行建模。\",\"使用idea下的designer设计器绘制流程，并会生成两个文件：.bpmn和.png\"]},{\"header\":\".bpmn文件\",\"slug\":\"bpmn文件\",\"contents\":[\"使用activiti-desinger设计业务流程，会生成.bpmn文件，上面我们已经创建好了bpmn文件\",\"BPMN 2.0根节点是definitions节点。 这个元素中，可以定义多个流程定义（不过我们建议每个文件只包含一个流程定义， 可以简化开发过程中的维护难度）。 注意，definitions元素 最少也要包含xmlns 和 targetNamespace的声明。 targetNamespace可以是任意值，它用来对流程实例进行分类。\",\"流程定义部分：定义了流程每个结点的描述及结点之间的流程流转。\",\"流程布局定义：定义流程每个结点在流程图上的位置坐标等信息。\"]},{\"header\":\"生成.png图片文件\",\"slug\":\"生成-png图片文件\",\"contents\":[\"IDEA工具中的操作方式\"]},{\"header\":\"1、修改文件后缀为xml\",\"slug\":\"_1、修改文件后缀为xml\",\"contents\":[\"首先将evection.bpmn文件改名为evection.xml，如下图：\",\"evection.xml修改前的bpmn文件，效果如下：\"]},{\"header\":\"2、使用designer设计器打开.xml文件\",\"slug\":\"_2、使用designer设计器打开-xml文件\",\"contents\":[\"在evection.xml文件上面，点右键并选择Diagrams菜单，再选择Show BPMN2.0 Designer…\"]},{\"header\":\"3、查看打开的文件\",\"slug\":\"_3、查看打开的文件\",\"contents\":[\"打开后，却出现乱码，如图：\"]},{\"header\":\"4、解决中文乱码\",\"slug\":\"_4、解决中文乱码\",\"contents\":[\"1、打开Settings，找到File Encodings，把encoding的选项都选择UTF-8\",\"2、打开IDEA安装路径，找到如下的安装目录\",\"根据自己所安装的版本来决定，我使用的是64位的idea，所以在idea64.exe.vmoptions文件的最后一行追加一条命令： -Dfile.encoding=UTF-8\",\"如下所示：\",\"一定注意，不要有空格，否则重启IDEA时会打不开，然后 重启IDEA。\",\"如果以上方法已经做完，还出现乱码，就再修改一个文件，并在文件的末尾添加： -Dfile.encoding=UTF-8，然后重启idea，如图：\",\"最后重新在evection.xml文件上面，点右键并选择Diagrams菜单，再选择Show BPMN2.0 Designer…，看到生成图片，如图：\",\"到此，解决乱码问题\"]},{\"header\":\"5、导出为图片文件\",\"slug\":\"_5、导出为图片文件\",\"contents\":[\"点击Export To File的小图标，打开如下窗口，注意填写文件名及扩展名，选择好保存图片的位置：\",\"然后，我们把png文件拷贝到resources下的bpmn目录，并且把evection.xml改名为evection.bpmn。\"]},{\"header\":\"2、流程定义部署\",\"slug\":\"_2、流程定义部署\",\"contents\":[]},{\"header\":\"概述\",\"slug\":\"概述-1\",\"contents\":[\"将上面在设计器中定义的流程部署到activiti数据库中，就是流程定义部署。\",\"通过调用activiti的api将流程定义的bpmn和png两个文件一个一个添加部署到activiti中，也可以将两个文件打成zip包进行部署。\"]},{\"header\":\"单个文件部署方式\",\"slug\":\"单个文件部署方式\",\"contents\":[\"分别将bpmn文件和png图片文件部署。\",\"package com.itheima.test; import org.activiti.engine.ProcessEngine; import org.activiti.engine.ProcessEngines; import org.activiti.engine.RepositoryService; import org.activiti.engine.repository.Deployment; import org.junit.Test; public class ActivitiDemo { /** * 部署流程定义 */ @Test public void testDeployment(){ // 1、创建ProcessEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 2、得到RepositoryService实例 RepositoryService repositoryService = processEngine.getRepositoryService(); // 3、使用RepositoryService进行部署 Deployment deployment = repositoryService.createDeployment() .addClasspathResource(\\\"bpmn/evection.bpmn\\\") // 添加bpmn资源 .addClasspathResource(\\\"bpmn/evection.png\\\") // 添加png资源 .name(\\\"出差申请流程\\\") .deploy(); // 4、输出部署信息 System.out.println(\\\"流程部署id：\\\" + deployment.getId()); System.out.println(\\\"流程部署名称：\\\" + deployment.getName()); } } \",\"执行此操作后activiti会将上边代码中指定的bpm文件和图片文件保存在activiti数据库。\"]},{\"header\":\"压缩包部署方式\",\"slug\":\"压缩包部署方式\",\"contents\":[\"将evection.bpmn和evection.png压缩成zip包。\",\"@Test public void deployProcessByZip() { // 定义zip输入流 InputStream inputStream = this .getClass() .getClassLoader() .getResourceAsStream( \\\"bpmn/evection.zip\\\"); ZipInputStream zipInputStream = new ZipInputStream(inputStream); // 获取repositoryService RepositoryService repositoryService = processEngine .getRepositoryService(); // 流程部署 Deployment deployment = repositoryService.createDeployment() .addZipInputStream(zipInputStream) .deploy(); System.out.println(\\\"流程部署id：\\\" + deployment.getId()); System.out.println(\\\"流程部署名称：\\\" + deployment.getName()); } \",\"执行此操作后activiti会将上边代码中指定的bpm文件和图片文件保存在activiti数据库。\"]},{\"header\":\"操作数据表\",\"slug\":\"操作数据表\",\"contents\":[\"流程定义部署后操作activiti的3张表如下：\",\"act_re_deployment 流程定义部署表，每部署一次增加一条记录\",\"act_re_procdef 流程定义表，部署每个新的流程定义都会在这张表中增加一条记录\",\"act_ge_bytearray 流程资源表\",\"接下来我们来看看，写入了什么数据：\",\"SELECT * FROM act_re_deployment #流程定义部署表，记录流程部署信息 \",\"结果：\",\"SELECT * FROM act_re_procdef #流程定义表，记录流程定义信息 \",\"结果：\",\"注意，KEY 这个字段是用来唯一识别不同流程的关键字\",\"SELECT * FROM act_ge_bytearray #资源表 \",\"结果：\",\"注意：\",\"act_re_deployment和act_re_procdef一对多关系，一次部署在流程部署表生成一条记录，但一次部署可以部署多个流程定义，每个流程定义在流程定义表生成一条记录。每一个流程定义在act_ge_bytearray会存在两个资源记录，bpmn和png。\",\"建议：一次部署一个流程，这样部署表和流程定义表是一对一有关系，方便读取流程部署及流程定义信息。\"]},{\"header\":\"3、启动流程实例\",\"slug\":\"_3、启动流程实例\",\"contents\":[\"流程定义部署在activiti后就可以通过工作流管理业务流程了，也就是说上边部署的出差申请流程可以使用了。\",\"针对该流程，启动一个流程表示发起一个新的出差申请单，这就相当于java类与java对象的关系，类定义好后需要new创建一个对象使用，当然可以new多个对象。对于请出差申请流程，张三发起一个出差申请单需要启动一个流程实例，出差申请单发起一个出差单也需要启动一个流程实例。\",\"代码如下：\",\"/** * 启动流程 */ @Test public void testStartProcess(){ //创建流程引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); //获取runtimeService RuntimeService runtimeService = processEngine.getRuntimeService(); //通过流程id启动流程 对应的act_re_procdef的id ProcessInstance evection = runtimeService.startProcessInstanceById(\\\"evection:1:4\\\"); //输出内容 System.out.println(\\\"流程定义id：\\\" + evection.getProcessDefinitionId()); System.out.println(\\\"流程实例id：\\\" + evection.getId()); System.out.println(\\\"当前活动Id：\\\" + evection.getActivityId()); } \",\"输出内容如下：\",\"操作数据表\",\"act_hi_actinst 流程实例执行历史\",\"act_hi_identitylink 流程的参与用户历史信息\",\"act_hi_procinst 流程实例历史信息\",\"act_hi_taskinst 流程任务历史信息\",\"act_ru_execution 流程执行信息\",\"act_ru_identitylink 流程的参与用户信息\",\"act_ru_task 任务信息\"]},{\"header\":\"4、任务查询\",\"slug\":\"_4、任务查询\",\"contents\":[\"流程启动后，任务的负责人就可以查询自己当前需要处理的任务，查询出来的任务都是该用户的待办任务。\",\"/** * 查询当前个人待执行的任务 */ @Test public void testFindPersonalTaskList() { // 任务负责人 String assignee = \\\"zhangsan\\\"; ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 创建TaskService TaskService taskService = processEngine.getTaskService(); // 根据流程key 和 任务负责人 查询任务 List<Task> list = taskService.createTaskQuery() .processDefinitionKey(\\\"myEvection\\\") //流程Key .taskAssignee(assignee)//只查询该任务负责人的任务 .list(); for (Task task : list) { System.out.println(\\\"流程实例id：\\\" + task.getProcessInstanceId()); System.out.println(\\\"任务id：\\\" + task.getId()); System.out.println(\\\"任务负责人：\\\" + task.getAssignee()); System.out.println(\\\"任务名称：\\\" + task.getName()); } } \",\"输出结果如下：\",\"流程实例id：2501 任务id：2505 任务负责人：zhangsan 任务名称：创建出差申请 \"]},{\"header\":\"5、流程任务处理\",\"slug\":\"_5、流程任务处理\",\"contents\":[\"任务负责人查询待办任务，选择任务进行处理，完成任务。\",\"// 完成任务 @Test public void completTask(){ // 获取引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取taskService TaskService taskService = processEngine.getTaskService(); // 根据流程key 和 任务的负责人 查询任务 // 返回一个任务对象 Task task = taskService.createTaskQuery() .processDefinitionKey(\\\"myEvection\\\") //流程Key .taskAssignee(\\\"zhangsan\\\") //要查询的负责人 .singleResult(); // 完成任务,参数：任务id taskService.complete(task.getId()); } \"]},{\"header\":\"6、流程定义信息查询\",\"slug\":\"_6、流程定义信息查询\",\"contents\":[\"查询流程相关信息，包含流程定义，流程部署，流程定义版本\",\" /** * 查询流程定义 */ @Test public void queryProcessDefinition(){ // 获取引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // repositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 得到ProcessDefinitionQuery 对象 ProcessDefinitionQuery processDefinitionQuery = repositoryService.createProcessDefinitionQuery(); // 查询出当前所有的流程定义 // 条件：processDefinitionKey =evection // orderByProcessDefinitionVersion 按照版本排序 // desc倒叙 // list 返回集合 List<ProcessDefinition> definitionList = processDefinitionQuery.processDefinitionKey(\\\"myEvection\\\") .orderByProcessDefinitionVersion() .desc() .list(); // 输出流程定义信息 for (ProcessDefinition processDefinition : definitionList) { System.out.println(\\\"流程定义 id=\\\"+processDefinition.getId()); System.out.println(\\\"流程定义 name=\\\"+processDefinition.getName()); System.out.println(\\\"流程定义 key=\\\"+processDefinition.getKey()); System.out.println(\\\"流程定义 Version=\\\"+processDefinition.getVersion()); System.out.println(\\\"流程部署ID =\\\"+processDefinition.getDeploymentId()); } } \",\"输出结果：\",\"流程定义 id=myEvection:1:4 流程定义 name=出差申请 流程定义 key=myEvection 流程定义 Version=1 流程部署ID =1 \"]},{\"header\":\"7、流程删除\",\"slug\":\"_7、流程删除\",\"contents\":[\"public void deleteDeployment() { // 流程部署id String deploymentId = \\\"1\\\"; ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 通过流程引擎获取repositoryService RepositoryService repositoryService = processEngine .getRepositoryService(); //删除流程定义，如果该流程定义已有流程实例启动则删除时出错 repositoryService.deleteDeployment(deploymentId); //设置true 级联删除流程定义，即使该流程有流程实例启动也可以删除，设置为false非级别删除方式，如果流程 //repositoryService.deleteDeployment(deploymentId, true); } \",\"说明：\",\" 使用repositoryService删除流程定义，历史表信息不会被删除 \",\" 如果该流程定义下没有正在运行的流程，则可以用普通删除。 \",\"如果该流程定义下存在已经运行的流程，使用普通删除报错，可用级联删除方法将流程及相关记录全部删除。\",\"先删除没有完成流程节点，最后就可以完全删除流程定义信息\",\"项目开发中级联删除操作一般只开放给超级管理员使用.\"]},{\"header\":\"8、流程资源下载\",\"slug\":\"_8、流程资源下载\",\"contents\":[\"现在我们的流程资源文件已经上传到数据库了，如果其他用户想要查看这些资源文件，可以从数据库中把资源文件下载到本地。\",\"解决方案有：\",\"1、jdbc对blob类型，clob类型数据读取出来，保存到文件目录\",\"2、使用activiti的api来实现\",\"使用commons-io.jar 解决IO的操作\",\"引入commons-io依赖包\",\"<dependency> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> <version>2.6</version> </dependency> \",\"通过流程定义对象获取流程定义资源，获取bpmn和png\",\"import org.apache.commons.io.IOUtils; @Test public void deleteDeployment(){ // 获取引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取repositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 根据部署id 删除部署信息,如果想要级联删除，可以添加第二个参数，true repositoryService.deleteDeployment(\\\"1\\\"); } public void queryBpmnFile() throws IOException { // 1、得到引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 2、获取repositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 3、得到查询器：ProcessDefinitionQuery，设置查询条件,得到想要的流程定义 ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery() .processDefinitionKey(\\\"myEvection\\\") .singleResult(); // 4、通过流程定义信息，得到部署ID String deploymentId = processDefinition.getDeploymentId(); // 5、通过repositoryService的方法，实现读取图片信息和bpmn信息 // png图片的流 InputStream pngInput = repositoryService.getResourceAsStream(deploymentId, processDefinition.getDiagramResourceName()); // bpmn文件的流 InputStream bpmnInput = repositoryService.getResourceAsStream(deploymentId, processDefinition.getResourceName()); // 6、构造OutputStream流 File file_png = new File(\\\"d:/evectionflow01.png\\\"); File file_bpmn = new File(\\\"d:/evectionflow01.bpmn\\\"); FileOutputStream bpmnOut = new FileOutputStream(file_bpmn); FileOutputStream pngOut = new FileOutputStream(file_png); // 7、输入流，输出流的转换 IOUtils.copy(pngInput,pngOut); IOUtils.copy(bpmnInput,bpmnOut); // 8、关闭流 pngOut.close(); bpmnOut.close(); pngInput.close(); bpmnInput.close(); } \",\"说明：\",\" deploymentId为流程部署ID \",\" resource_name为act_ge_bytearray表中NAME_列的值 \",\" 使用repositoryService的getDeploymentResourceNames方法可以获取指定部署下得所有文件的名称 \",\" 使用repositoryService的getResourceAsStream方法传入部署ID和资源图片名称可以获取部署下指定名称文件的输入流 \",\"最后的将输入流中的图片资源进行输出。\"]},{\"header\":\"9、流程历史信息的查看\",\"slug\":\"_9、流程历史信息的查看\",\"contents\":[\"即使流程定义已经删除了，流程执行的历史信息通过前面的分析，依然保存在activiti的act_hi_*相关的表中。所以我们还是可以查询流程执行的历史信息，可以通过HistoryService来查看相关的历史记录。\",\" /** * 查看历史信息 */ @Test public void findHistoryInfo(){ // 获取引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取HistoryService HistoryService historyService = processEngine.getHistoryService(); // 获取 actinst表的查询对象 HistoricActivityInstanceQuery instanceQuery = historyService.createHistoricActivityInstanceQuery(); // 查询 actinst表，条件：根据 InstanceId 查询 // instanceQuery.processInstanceId(\\\"2501\\\"); // 查询 actinst表，条件：根据 DefinitionId 查询 instanceQuery.processDefinitionId(\\\"myEvection:1:4\\\"); // 增加排序操作,orderByHistoricActivityInstanceStartTime 根据开始时间排序 asc 升序 instanceQuery.orderByHistoricActivityInstanceStartTime().asc(); // 查询所有内容 List<HistoricActivityInstance> activityInstanceList = instanceQuery.list(); // 输出 for (HistoricActivityInstance hi : activityInstanceList) { System.out.println(hi.getActivityId()); System.out.println(hi.getActivityName()); System.out.println(hi.getProcessDefinitionId()); System.out.println(hi.getProcessInstanceId()); System.out.println(\\\"<==========================>\\\"); } } \"]}]},\"/study-tutorial/components/activity/basic/overview.html\":{\"title\":\"2、Activiti7概述\",\"contents\":[{\"header\":\"1、介绍\",\"slug\":\"_1、介绍\",\"contents\":[\"Alfresco软件在2010年5月17日宣布Activiti业务流程管理（BPM）开源项目的正式启动，其首席架构师由业务流程管理BPM的专家 Tom Baeyens担任，Tom Baeyens就是原来jbpm的架构师，而jbpm是一个非常有名的工作流引擎，当然activiti也是一个工作流引擎。\",\"Activiti是一个工作流引擎， activiti可以将业务系统中复杂的业务流程抽取出来，使用专门的建模语言BPMN2.0进行定义，业务流程按照预先定义的流程进行执行，实现了系统的流程由activiti进行管理，减少业务系统由于流程变更进行系统升级改造的工作量，从而提高系统的健壮性，同时也减少了系统开发维护成本。\",\"官方网站：https://www.activiti.org/\",\"经历的版本:\",\"目前最新版本：Activiti7.0.0.Beta\"]},{\"header\":\"1、BPM\",\"slug\":\"_1、bpm\",\"contents\":[\"BPM（Business Process Management），即业务流程管理，是一种规范化的构造端到端的业务流程，以持续的提高组织业务效率。常见商业管理教育如EMBA、MBA等均将BPM包含在内。\"]},{\"header\":\"2、BPM软件\",\"slug\":\"_2、bpm软件\",\"contents\":[\"BPM软件就是根据企业中业务环境的变化，推进人与人之间、人与系统之间以及系统与系统之间的整合及调整的经营方法与解决方案的IT工具。\",\"通过BPM软件对企业内部及外部的业务流程的整个生命周期进行建模、自动化、管理监控和优化，使企业成本降低，利润得以大幅提升。\",\"BPM软件在企业中应用领域广泛，凡是有业务流程的地方都可以BPM软件进行管理，比如企业人事办公管理、采购流程管理、公文审批流程管理、财务管理等。\"]},{\"header\":\"3、BPMN\",\"slug\":\"_3、bpmn\",\"contents\":[\"BPMN（Business Process Model AndNotation）- 业务流程模型和符号 是由BPMI（BusinessProcess Management Initiative）开发的一套标准的业务流程建模符号，使用BPMN提供的符号可以创建业务流程。\",\"2004年5月发布了BPMN1.0规范.BPMI于2005年9月并入OMG（The Object Management Group对象管理组织)组织。OMG于2011年1月发布BPMN2.0的最终版本。\",\"具体发展历史如下:\",\"BPMN 是目前被各 BPM 厂商广泛接受的 BPM 标准。Activiti 就是使用 BPMN 2.0 进行流程建模、流程执行管理，它包括很多的建模符号，比如：\",\"Event\",\"用一个圆圈表示，它是流程中运行过程中发生的事情。\",\"活动用圆角矩形表示，一个流程由一个活动或多个活动组成\",\"Bpmn图形其实是通过xml表示业务流程，上边的.bpmn文件使用文本编辑器打开：\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <definitions xmlns=\\\"http://www.omg.org/spec/BPMN/20100524/MODEL\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:xsd=\\\"http://www.w3.org/2001/XMLSchema\\\" xmlns:activiti=\\\"http://activiti.org/bpmn\\\" xmlns:bpmndi=\\\"http://www.omg.org/spec/BPMN/20100524/DI\\\" xmlns:omgdc=\\\"http://www.omg.org/spec/DD/20100524/DC\\\" xmlns:omgdi=\\\"http://www.omg.org/spec/DD/20100524/DI\\\" typeLanguage=\\\"http://www.w3.org/2001/XMLSchema\\\" expressionLanguage=\\\"http://www.w3.org/1999/XPath\\\" targetNamespace=\\\"http://www.activiti.org/test\\\"> <process id=\\\"myProcess\\\" name=\\\"My process\\\" isExecutable=\\\"true\\\"> <startEvent id=\\\"startevent1\\\" name=\\\"Start\\\"></startEvent> <userTask id=\\\"usertask1\\\" name=\\\"创建请假单\\\"></userTask> <sequenceFlow id=\\\"flow1\\\" sourceRef=\\\"startevent1\\\" targetRef=\\\"usertask1\\\"></sequenceFlow> <userTask id=\\\"usertask2\\\" name=\\\"部门经理审核\\\"></userTask> <sequenceFlow id=\\\"flow2\\\" sourceRef=\\\"usertask1\\\" targetRef=\\\"usertask2\\\"></sequenceFlow> <userTask id=\\\"usertask3\\\" name=\\\"人事复核\\\"></userTask> <sequenceFlow id=\\\"flow3\\\" sourceRef=\\\"usertask2\\\" targetRef=\\\"usertask3\\\"></sequenceFlow> <endEvent id=\\\"endevent1\\\" name=\\\"End\\\"></endEvent> <sequenceFlow id=\\\"flow4\\\" sourceRef=\\\"usertask3\\\" targetRef=\\\"endevent1\\\"></sequenceFlow> </process> <bpmndi:BPMNDiagram id=\\\"BPMNDiagram_myProcess\\\"> <bpmndi:BPMNPlane bpmnElement=\\\"myProcess\\\" id=\\\"BPMNPlane_myProcess\\\"> <bpmndi:BPMNShape bpmnElement=\\\"startevent1\\\" id=\\\"BPMNShape_startevent1\\\"> <omgdc:Bounds height=\\\"35.0\\\" width=\\\"35.0\\\" x=\\\"130.0\\\" y=\\\"160.0\\\"></omgdc:Bounds> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"usertask1\\\" id=\\\"BPMNShape_usertask1\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"105.0\\\" x=\\\"210.0\\\" y=\\\"150.0\\\"></omgdc:Bounds> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"usertask2\\\" id=\\\"BPMNShape_usertask2\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"105.0\\\" x=\\\"360.0\\\" y=\\\"150.0\\\"></omgdc:Bounds> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"usertask3\\\" id=\\\"BPMNShape_usertask3\\\"> <omgdc:Bounds height=\\\"55.0\\\" width=\\\"105.0\\\" x=\\\"510.0\\\" y=\\\"150.0\\\"></omgdc:Bounds> </bpmndi:BPMNShape> <bpmndi:BPMNShape bpmnElement=\\\"endevent1\\\" id=\\\"BPMNShape_endevent1\\\"> <omgdc:Bounds height=\\\"35.0\\\" width=\\\"35.0\\\" x=\\\"660.0\\\" y=\\\"160.0\\\"></omgdc:Bounds> </bpmndi:BPMNShape> <bpmndi:BPMNEdge bpmnElement=\\\"flow1\\\" id=\\\"BPMNEdge_flow1\\\"> <omgdi:waypoint x=\\\"165.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> <omgdi:waypoint x=\\\"210.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"flow2\\\" id=\\\"BPMNEdge_flow2\\\"> <omgdi:waypoint x=\\\"315.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> <omgdi:waypoint x=\\\"360.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"flow3\\\" id=\\\"BPMNEdge_flow3\\\"> <omgdi:waypoint x=\\\"465.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> <omgdi:waypoint x=\\\"510.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> </bpmndi:BPMNEdge> <bpmndi:BPMNEdge bpmnElement=\\\"flow4\\\" id=\\\"BPMNEdge_flow4\\\"> <omgdi:waypoint x=\\\"615.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> <omgdi:waypoint x=\\\"660.0\\\" y=\\\"177.0\\\"></omgdi:waypoint> </bpmndi:BPMNEdge> </bpmndi:BPMNPlane> </bpmndi:BPMNDiagram> </definitions> \"]},{\"header\":\"2、使用步骤\",\"slug\":\"_2、使用步骤\",\"contents\":[]},{\"header\":\"部署activiti\",\"slug\":\"部署activiti\",\"contents\":[\"Activiti是一个工作流引擎（其实就是一堆jar包API），业务系统访问(操作)activiti的接口，就可以方便的操作流程相关数据，这样就可以把工作流环境与业务系统的环境集成在一起。\"]},{\"header\":\"流程定义\",\"slug\":\"流程定义\",\"contents\":[\"使用activiti流程建模工具(activity-designer)定义业务流程(.bpmn文件) 。\",\".bpmn文件就是业务流程定义文件，通过xml定义业务流程。\"]},{\"header\":\"流程定义部署\",\"slug\":\"流程定义部署\",\"contents\":[\"activiti部署业务流程定义（.bpmn文件）。\",\"使用activiti提供的api把流程定义内容存储起来，在Activiti执行过程中可以查询定义的内容\",\"Activiti执行把流程定义内容存储在数据库中\"]},{\"header\":\"启动一个流程实例\",\"slug\":\"启动一个流程实例\",\"contents\":[\"流程实例也叫：ProcessInstance\",\"启动一个流程实例表示开始一次业务流程的运行。\",\"在员工请假流程定义部署完成后，如果张三要请假就可以启动一个流程实例，如果李四要请假也启动一个流程实例，两个流程的执行互相不影响。\"]},{\"header\":\"用户查询待办任务(Task)\",\"slug\":\"用户查询待办任务-task\",\"contents\":[\"因为现在系统的业务流程已经交给activiti管理，通过activiti就可以查询当前流程执行到哪了，当前用户需要办理什么任务了，这些activiti帮我们管理了，而不需要开发人员自己编写在sql语句查询。\"]},{\"header\":\"用户办理任务\",\"slug\":\"用户办理任务\",\"contents\":[\"用户查询待办任务后，就可以办理某个任务，如果这个任务办理完成还需要其它用户办理，比如采购单创建后由部门经理审核，这个过程也是由activiti帮我们完成了。\"]},{\"header\":\"流程结束\",\"slug\":\"流程结束\",\"contents\":[\"当任务办理完成没有下一个任务结点了，这个流程实例就完成了。\"]}]},\"/study-tutorial/components/activity/basic/relation_chart.html\":{\"title\":\"4、Activiti类关系图\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"上面我们完成了Activiti数据库表的生成，java代码中我们调用Activiti的工具类，下面来了解Activiti的类关系\"]},{\"header\":\"1、类关系图\",\"slug\":\"_1、类关系图\",\"contents\":[\"在新版本中，我们通过实验可以发现IdentityService，FormService两个Serivce都已经删除了。\",\"所以后面我们对于这两个Service也不讲解了，但老版本中还是有这两个Service，同学们需要了解一下\"]},{\"header\":\"2、activiti.cfg.xml\",\"slug\":\"_2、activiti-cfg-xml\",\"contents\":[\"activiti的引擎配置文件，包括：ProcessEngineConfiguration的定义、数据源定义、事务管理器等，此文件其实就是一个spring配置文件。\"]},{\"header\":\"3、流程引擎配置类\",\"slug\":\"_3、流程引擎配置类\",\"contents\":[\"流程引擎的配置类（ProcessEngineConfiguration），通过ProcessEngineConfiguration可以创建工作流引擎ProceccEngine，常用的两种方法如下：\"]},{\"header\":\"1、StandaloneProcessEngineConfiguration\",\"slug\":\"_1、standaloneprocessengineconfiguration\",\"contents\":[\"使用StandaloneProcessEngineConfigurationActiviti可以单独运行，来创建ProcessEngine，Activiti会自己处理事务。\",\"配置文件方式：\",\"通常在activiti.cfg.xml配置文件中定义一个id为 processEngineConfiguration 的bean.\",\"方法如下：\",\"<bean id=\\\"processEngineConfiguration\\\" class=\\\"org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration\\\"> <!--配置数据库相关的信息--> <!--数据库驱动--> <property name=\\\"jdbcDriver\\\" value=\\\"com.mysql.jdbc.Driver\\\"/> <!--数据库链接--> <property name=\\\"jdbcUrl\\\" value=\\\"jdbc:mysql:///activiti\\\"/> <!--数据库用户名--> <property name=\\\"jdbcUsername\\\" value=\\\"root\\\"/> <!--数据库密码--> <property name=\\\"jdbcPassword\\\" value=\\\"123456\\\"/> <!--actviti数据库表在生成时的策略 true - 如果数据库中已经存在相应的表，那么直接使用，如果不存在，那么会创建--> <property name=\\\"databaseSchemaUpdate\\\" value=\\\"true\\\"/> </bean> \",\"还可以加入连接池:\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> <beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:context=\\\"http://www.springframework.org/schema/context\\\" xmlns:tx=\\\"http://www.springframework.org/schema/tx\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/contex http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\\\"> <bean id=\\\"dataSource\\\" class=\\\"org.apache.commons.dbcp.BasicDataSource\\\"> <property name=\\\"driverClassName\\\" value=\\\"com.mysql.jdbc.Driver\\\"/> <property name=\\\"url\\\" value=\\\"jdbc:mysql:///activiti\\\"/> <property name=\\\"username\\\" value=\\\"root\\\"/> <property name=\\\"password\\\" value=\\\"123456\\\"/> <property name=\\\"maxActive\\\" value=\\\"3\\\"/> <property name=\\\"maxIdle\\\" value=\\\"1\\\"/> </bean> <!--在默认方式下 bean的id 固定为 processEngineConfiguration--> <bean id=\\\"processEngineConfiguration\\\" class=\\\"org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration\\\"> <!--引入上面配置好的 链接池--> <property name=\\\"dataSource\\\" ref=\\\"dataSource\\\"/> <!--actviti数据库表在生成时的策略 true - 如果数据库中已经存在相应的表，那么直接使用，如果不存在，那么会创建--> <property name=\\\"databaseSchemaUpdate\\\" value=\\\"true\\\"/> </bean> </beans> \"]},{\"header\":\"2、SpringProcessEngineConfiguration\",\"slug\":\"_2、springprocessengineconfiguration\",\"contents\":[\"通过org.activiti.spring.SpringProcessEngineConfiguration 与Spring整合。\",\"创建spring与activiti的整合配置文件：\",\"activity-spring.cfg.xml（名称可修改）\",\"<beans xmlns=\\\"http://www.springframework.org/schema/beans\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns:mvc=\\\"http://www.springframework.org/schema/mvc\\\" xmlns:context=\\\"http://www.springframework.org/schema/context\\\" xmlns:aop=\\\"http://www.springframework.org/schema/aop\\\" xmlns:tx=\\\"http://www.springframework.org/schema/tx\\\" xsi:schemaLocation=\\\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.1.xsd \\\"> <!-- 工作流引擎配置bean --> <bean id=\\\"processEngineConfiguration\\\" class=\\\"org.activiti.spring.SpringProcessEngineConfiguration\\\"> <!-- 数据源 --> <property name=\\\"dataSource\\\" ref=\\\"dataSource\\\" /> <!-- 使用spring事务管理器 --> <property name=\\\"transactionManager\\\" ref=\\\"transactionManager\\\" /> <!-- 数据库策略 --> <property name=\\\"databaseSchemaUpdate\\\" value=\\\"drop-create\\\" /> <!-- activiti的定时任务关闭 --> <property name=\\\"jobExecutorActivate\\\" value=\\\"false\\\" /> </bean> <!-- 流程引擎 --> <bean id=\\\"processEngine\\\" class=\\\"org.activiti.spring.ProcessEngineFactoryBean\\\"> <property name=\\\"processEngineConfiguration\\\" ref=\\\"processEngineConfiguration\\\" /> </bean> <!-- 资源服务service --> <bean id=\\\"repositoryService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getRepositoryService\\\" /> <!-- 流程运行service --> <bean id=\\\"runtimeService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getRuntimeService\\\" /> <!-- 任务管理service --> <bean id=\\\"taskService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getTaskService\\\" /> <!-- 历史管理service --> <bean id=\\\"historyService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getHistoryService\\\" /> <!-- 用户管理service --> <bean id=\\\"identityService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getIdentityService\\\" /> <!-- 引擎管理service --> <bean id=\\\"managementService\\\" factory-bean=\\\"processEngine\\\" factory-method=\\\"getManagementService\\\" /> <!-- 数据源 --> <bean id=\\\"dataSource\\\" class=\\\"org.apache.commons.dbcp.BasicDataSource\\\"> <property name=\\\"driverClassName\\\" value=\\\"com.mysql.jdbc.Driver\\\" /> <property name=\\\"url\\\" value=\\\"jdbc:mysql://localhost:3306/activiti\\\" /> <property name=\\\"username\\\" value=\\\"root\\\" /> <property name=\\\"password\\\" value=\\\"mysql\\\" /> <property name=\\\"maxActive\\\" value=\\\"3\\\" /> <property name=\\\"maxIdle\\\" value=\\\"1\\\" /> </bean> <!-- 事务管理器 --> <bean id=\\\"transactionManager\\\" class=\\\"org.springframework.jdbc.datasource.DataSourceTransactionManager\\\"> <property name=\\\"dataSource\\\" ref=\\\"dataSource\\\" /> </bean> <!-- 通知 --> <tx:advice id=\\\"txAdvice\\\" transaction-manager=\\\"transactionManager\\\"> <tx:attributes></tx:attributes> <!-- 传播行为 --> <tx:method name=\\\"save*\\\" propagation=\\\"REQUIRED\\\" /> <tx:method name=\\\"insert*\\\" propagation=\\\"REQUIRED\\\" /> <tx:method name=\\\"delete*\\\" propagation=\\\"REQUIRED\\\" /> <tx:method name=\\\"update*\\\" propagation=\\\"REQUIRED\\\" /> <tx:method name=\\\"find*\\\" propagation=\\\"SUPPORTS\\\" read-only=\\\"true\\\" /> <tx:method name=\\\"get*\\\" propagation=\\\"SUPPORTS\\\" read-only=\\\"true\\\" /> </tx:attributes> </tx:advice> <!-- 切面，根据具体项目修改切点配置 --> <aop:config proxy-target-class=\\\"true\\\"> <aop:advisor advice-ref=\\\"txAdvice\\\" pointcut=\\\"execution(* com.itheima.ihrm.service.impl.*.(..))\\\"* /> </aop:config> </beans> \"]},{\"header\":\"创建processEngineConfiguration\",\"slug\":\"创建processengineconfiguration\",\"contents\":[\"ProcessEngineConfiguration configuration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(\\\"activiti.cfg.xml\\\") \",\"​ 上边的代码要求activiti.cfg.xml中必须有一个processEngineConfiguration的bean\",\"也可以使用下边的方法，更改bean 的名字：\",\"ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(String resource, String beanName); \"]},{\"header\":\"4、工作流引擎创建\",\"slug\":\"_4、工作流引擎创建\",\"contents\":[\"工作流引擎（ProcessEngine），相当于一个门面接口，通过ProcessEngineConfiguration创建processEngine，通过ProcessEngine创建各个service接口。\"]},{\"header\":\"1、默认创建方式\",\"slug\":\"_1、默认创建方式\",\"contents\":[\"将activiti.cfg.xml文件名及路径固定，且activiti.cfg.xml文件中有 processEngineConfiguration的配置， 可以使用如下代码创建processEngine:\",\"//直接使用工具类 ProcessEngines，使用classpath下的activiti.cfg.xml中的配置创建processEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); System.out.println(processEngine); \"]},{\"header\":\"2、一般创建方式\",\"slug\":\"_2、一般创建方式\",\"contents\":[\"//先构建ProcessEngineConfiguration ProcessEngineConfiguration configuration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(\\\"activiti.cfg.xml\\\"); //通过ProcessEngineConfiguration创建ProcessEngine，此时会创建数据库 ProcessEngine processEngine = configuration.buildProcessEngine(); \"]},{\"header\":\"5、Servcie服务接口\",\"slug\":\"_5、servcie服务接口\",\"contents\":[\"Service是工作流引擎提供用于进行工作流部署、执行、管理的服务接口，我们使用这些接口可以就是操作服务对应的数据表\"]},{\"header\":\"1、Service创建方式\",\"slug\":\"_1、service创建方式\",\"contents\":[\"通过ProcessEngine创建Service\",\"方式如下：\",\"RuntimeService runtimeService = processEngine.getRuntimeService(); RepositoryService repositoryService = processEngine.getRepositoryService(); TaskService taskService = processEngine.getTaskService(); \"]},{\"header\":\"2、Service总览\",\"slug\":\"_2、service总览\",\"contents\":[\"service名称\",\"service作用\",\"RepositoryService\",\"activiti的资源管理类\",\"RuntimeService\",\"activiti的流程运行管理类\",\"TaskService\",\"activiti的任务管理类\",\"HistoryService\",\"activiti的历史管理类\",\"ManagerService\",\"activiti的引擎管理类\",\"简单介绍：\",\"RepositoryService\",\"是activiti的资源管理类，提供了管理和控制流程发布包和流程定义的操作。使用工作流建模工具设计的业务流程图需要使用此service将流程定义文件的内容部署到计算机。\",\"除了部署流程定义以外还可以：查询引擎中的发布包和流程定义。\",\"暂停或激活发布包，对应全部和特定流程定义。 暂停意味着它们不能再执行任何操作了，激活是对应的反向操作。获得多种资源，像是包含在发布包里的文件， 或引擎自动生成的流程图。\",\"获得流程定义的pojo版本， 可以用来通过java解析流程，而不必通过xml\"]},{\"header\":\"RuntimeService\",\"slug\":\"runtimeservice\",\"contents\":[\"Activiti的流程运行管理类。可以从这个服务类中获取很多关于流程执行相关的信息\"]},{\"header\":\"TaskService\",\"slug\":\"taskservice\",\"contents\":[\"Activiti的任务管理类。可以从这个类中获取任务的信息。\"]},{\"header\":\"HistoryService\",\"slug\":\"historyservice\",\"contents\":[\"Activiti的历史管理类，可以查询历史信息，执行流程时，引擎会保存很多数据（根据配置），比如流程实例启动时间，任务的参与者， 完成任务的时间，每个流程实例的执行路径，等等。 这个服务主要通过查询功能来获得这些数据。\"]},{\"header\":\"ManagementService\",\"slug\":\"managementservice\",\"contents\":[\"Activiti的引擎管理类，提供了对 Activiti 流程引擎的管理和维护功能，这些功能不在工作流驱动的应用程序中使用，主要用于 Activiti 系统的日常维护。\"]}]},\"/study-tutorial/distributed/redis/combat/coupon_skill.html\":{\"title\":\"3、优惠券秒杀\",\"contents\":[{\"header\":\"1、全局唯一ID\",\"slug\":\"_1、全局唯一id\",\"contents\":[]},{\"header\":\"1、场景引入\",\"slug\":\"_1、场景引入\",\"contents\":[\"当用户抢购时，就会生成订单并保存到tb_voucher_order这张表中，而订单表如果使用数据库自增ID就存在一些问题:\",\"id的规律性太明显\",\"例如今天下单的订单ID为10，明天接着下一单，订单号为100，则会知道，今天----明天商城一共卖了90单\",\"受单表数据量的限制\",\"CREATE TABLE `tb_voucher_order` ( `id` bigint NOT NULL COMMENT '主键', `user_id` bigint unsigned NOT NULL COMMENT '下单的用户id', `voucher_id` bigint unsigned NOT NULL COMMENT '购买的代金券id', `pay_type` tinyint unsigned NOT NULL DEFAULT '1' COMMENT '支付方式 1：余额支付；2：支付宝；3：微信', `status` tinyint unsigned NOT NULL DEFAULT '1' COMMENT '订单状态，1：未支付；2：已支付；3：已核销；4：已取消；5：退款中；6：已退款', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '下单时间', `pay_time` timestamp NULL DEFAULT NULL COMMENT '支付时间', `use_time` timestamp NULL DEFAULT NULL COMMENT '核销时间', `refund_time` timestamp NULL DEFAULT NULL COMMENT '退款时间', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci ROW_FORMAT=COMPACT; \"]},{\"header\":\"2、全局ID生成器\",\"slug\":\"_2、全局id生成器\",\"contents\":[\"全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性：\",\"唯一性\",\"高可用\",\"高性能\",\"递增性\",\"安全性\",\"为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息：\",\"ID的组成部分：\",\"符号位：1bit，永远为0\",\"时间戳：31bit，以秒为单位，可以使用69年\",\"序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID\"]},{\"header\":\"3、代码实现\",\"slug\":\"_3、代码实现\",\"contents\":[\"@Component public class RedisIdWorker { private static final long BEGIN_TIMESTAMP = 1640995200L; private static final int COUNT_BITS = 32; @Resource private StringRedisTemplate stringRedisTemplate; /** * redis 生成器 * @param keyPrefix * @return */ public long nextId(String keyPrefix){ //1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; //生成序列号 //获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(\\\"yyyy:MM:dd\\\")); //这样可用根据日期去统计订单量 自动拆箱回产生空指针，但事实上这里并不会，redis发现没有，会自动生成 long increment = stringRedisTemplate.opsForValue().increment(\\\"icr:\\\" + keyPrefix + date); //拼接返回 // 这里拼接返回的是long类型，字符串拼接返回的是字符串 //这里需要使用 位运算 时间戳向左移动32位 在高位, 系列号采用或运算去填充 return timestamp << COUNT_BITS | increment; } } \",\"测试\",\"/** * 测试并发的情况 */ @Test public void redisIdTest() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); ThreadFactory build = new ThreadFactoryBuilder().setNamePrefix(\\\"redis_id_%d\\\").build(); //定义500个线程池 ExecutorService pool = new ThreadPoolExecutor(500, 500, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(1024), build, new ThreadPoolExecutor.AbortPolicy()); Runnable task = () ->{ for (int i = 0; i < 100; i++) { long order = redisIdWorker.nextId(\\\"order\\\"); System.out.println(\\\"id=\\\" + order); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i < 300; i++) { pool.submit(task); } latch.await(); long end = System.currentTimeMillis(); System.out.println(\\\"耗时：\\\" + (end - begin)); } \",\"全局唯一ID生成策略：\",\"UUID\",\"Redis自增\",\"snowflake算法\",\"数据库自增\",\"Redis自增ID策略：\",\"每天一个key，方便统计订单量\",\"ID构造是 时间戳 + 计数器\"]},{\"header\":\"2、实现优惠券秒杀下单\",\"slug\":\"_2、实现优惠券秒杀下单\",\"contents\":[\"@Transactional(rollbackFor = Exception.class) @Override public Result seckillVoucher(Long voucherId) { //查询优惠券信息 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); if (null == voucher){ return Result.fail(\\\"优惠券不存在！\\\"); } //判断时间是否开始或过期 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\\\"秒杀未开始！\\\"); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\\\"秒杀已结束！\\\"); } //判断库存是否充足 if (1 > voucher.getStock()){ return Result.fail(\\\"库存不足\\\"); } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).update(); if (!success){ return Result.fail(\\\"库存不足\\\"); } //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); //创建订单号 long id = redisIdWorker.nextId(\\\"order\\\"); voucherOrder.setId(id); Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); save(voucherOrder); //返回订单号 return Result.ok(id); } \"]},{\"header\":\"3、超卖问题\",\"slug\":\"_3、超卖问题\",\"contents\":[]},{\"header\":\"1、问题描述\",\"slug\":\"_1、问题描述\",\"contents\":[\"超卖问题是指卖出的数量已经超出库存了\",\"库存数据为：1\",\"线程1、线程2同时发出请求，查询库存，为1\",\"线程1判断是否大于0，此时为true，则扣减库存，此时库存为0\",\"线程2判断是否大于0，此时为true，则扣减库存，此时库存为-1\"]},{\"header\":\"2、解决方案\",\"slug\":\"_2、解决方案\",\"contents\":[\"超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：\",\"乐观锁\",\"悲观锁\"]},{\"header\":\"1、乐观锁\",\"slug\":\"_1、乐观锁\",\"contents\":[\"认为线程安全问题不一定会发生，因此不加锁，只是在更新数据时去判断有没有其它线程对数据做了修改。\",\"如果没有修改则认为是安全的，自己才更新数据。\",\"如果已经被其它线程修改说明发生了安全问题，此时可以重试或异常。\",\"乐观锁的关键是判断之前查询得到的数据是否有被修改过，常见的方式有两种：\",\"版本号法\",\"CAS法\"]},{\"header\":\"1、版本号法\",\"slug\":\"_1、版本号法\",\"contents\":[\"每次修改库存时，版本号+1，修改时根据查询的版本号进行修改\",\"初始库存为1，版本号为1\",\"线程1查询库存和版本号都是为1，判断是否大于0，大于0则进行修改且版本号+1，扣减成功\",\"线程1查询库存和版本号都是为1，判断是否大于0，大于0则进行修改且版本号+1，此时线程1已经修改版本号为2了，所以不执行\"]},{\"header\":\"2、CAS方法\",\"slug\":\"_2、cas方法\",\"contents\":[\"初始库存为1，版本号为1\",\"线程1查询库存为1，判断是否大于0，大于0则进行修改，扣减成功\",\"线程1查询库存为1，判断是否大于0，大于0则进行修改，此时线程1已经修改库存为0了，所以不执行\",\"代码实现：\",\"采用的是jmeter模拟并发请求\",\"@Transactional(rollbackFor = Exception.class) @Override public Result seckillVoucher(Long voucherId) { //查询优惠券信息 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); if (null == voucher){ return Result.fail(\\\"优惠券不存在！\\\"); } //判断时间是否开始或过期 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\\\"秒杀未开始！\\\"); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\\\"秒杀已结束！\\\"); } //判断库存是否充足 if (1 > voucher.getStock()){ return Result.fail(\\\"库存不足\\\"); } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).eq(\\\"stock\\\",voucher.getStock()).update(); if (!success){ return Result.fail(\\\"库存不足\\\"); } //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); //创建订单号 long id = redisIdWorker.nextId(\\\"order\\\"); voucherOrder.setId(id); Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); save(voucherOrder); //返回订单号 return Result.ok(id); } \",\"相比优惠券秒杀下单，修改的地方：\",\"boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).eq(\\\"stock\\\",voucher.getStock()).update(); \",\"使用多线程模拟秒杀后，发现异常比例反而增加了\",\"原因：这是因为多线程访问的时候，一个线程进行修改了，那其他99个线程都未修改成功，订单失败。这也是乐观锁的一个缺点\",\"改进方法\",\"前面是用库存代表的版本号进行控制高并发方法，但是其实只要库存量大于0的时候，都允许线程进行修改，这样就控制了高并发\",\"//扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).gt(\\\"stock\\\",0) .update(); \",\"符合预期\"]},{\"header\":\"2、悲观锁\",\"slug\":\"_2、悲观锁\",\"contents\":[\"认为线程安全问题一定会发生，因此在操作数据之前先获取锁，确保线程串行执行。\",\"例如Synchronized、Lock都属于悲观锁\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"超卖这样的线程安全问题，解决方案有哪些？\",\"悲观锁：添加同步锁，让线程串行执行 \",\"优点：简单粗暴\",\"缺点：性能一般\",\"乐观锁：不加锁，在更新时判断是否有其它线程在修改 \",\"优点：性能好\",\"缺点：存在成功率低的问题\"]},{\"header\":\"4、一人一单\",\"slug\":\"_4、一人一单\",\"contents\":[]},{\"header\":\"1、场景\",\"slug\":\"_1、场景\",\"contents\":[\"需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单\"]},{\"header\":\"1、代码实现\",\"slug\":\"_1、代码实现\",\"contents\":[\"@Transactional(rollbackFor = Exception.class) @Override public Result seckillVoucher(Long voucherId) { //查询优惠券信息 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); if (null == voucher){ return Result.fail(\\\"优惠券不存在！\\\"); } //判断时间是否开始或过期 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\\\"秒杀未开始！\\\"); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\\\"秒杀已结束！\\\"); } //判断库存是否充足 if (1 > voucher.getStock()){ return Result.fail(\\\"库存不足\\\"); } //一人一单 Long userId = UserHolder.getUser().getId(); //查询订单 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"voucher_id\\\", voucherId).count(); if (count > 0) { return Result.fail(\\\"用户已经购买过一次了！\\\"); } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).gt(\\\"stock\\\",0).update(); if (!success){ return Result.fail(\\\"库存不足\\\"); } //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); //创建订单号 long id = redisIdWorker.nextId(\\\"order\\\"); voucherOrder.setId(id); voucherOrder.setUserId(userId); save(voucherOrder); //返回订单号 return Result.ok(id); } \"]},{\"header\":\"2、结果\",\"slug\":\"_2、结果\",\"contents\":[\"发现数据库中存在10同一用户的订单\"]},{\"header\":\"3、解决方案\",\"slug\":\"_3、解决方案\",\"contents\":[\"很明显，这是线程并发问题，解决就是加锁，到底是加乐观锁呢还是悲观锁？\",\"这时我们分析一下，乐观锁认为在线程执行的时候，没有其他线程来修改之前的数据，悲观锁则是认为有人修改它的数据\",\"这里我们要控制的是同一个用户只能下一单，可以锁住用户ID来进行解决，则采用悲观锁synchronized\",\"@Override public Result seckillVoucher(Long voucherId) { //查询优惠券信息 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); if (null == voucher){ return Result.fail(\\\"优惠券不存在！\\\"); } //判断时间是否开始或过期 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\\\"秒杀未开始！\\\"); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\\\"秒杀已结束！\\\"); } //判断库存是否充足 if (1 > voucher.getStock()){ return Result.fail(\\\"库存不足\\\"); } Long userId = UserHolder.getUser().getId(); //返回字符串对象的规范表示。 //一个字符串池，最初是空的，由String类私下维护。 //当调用 intern 方法时，如果池中已经包含一个等于该String对象的字符串，由equals(Object)方法确定，则返回池中的字符串。否则，将此String对象添加到池中并返回对该String对象的引用。 //因此，对于任何两个字符串s和t ，当且仅当s.equals(t)为true时， s.intern() == t.intern()才为true 。 synchronized(userId.toString().intern()) { //获取代理对象（事务） IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } } @Override @Transactional(rollbackFor = Exception.class) public Result createVoucherOrder(Long voucherId) { //一人一单 Long userId = UserHolder.getUser().getId(); //查询订单 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"voucher_id\\\", voucherId).count(); if (count > 0) { return Result.fail(\\\"用户已经购买过一次了！\\\"); } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).gt(\\\"stock\\\",0).update(); if (!success){ return Result.fail(\\\"库存不足\\\"); } //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); //创建订单号 long id = redisIdWorker.nextId(\\\"order\\\"); voucherOrder.setId(id); voucherOrder.setUserId(userId); save(voucherOrder); //返回订单号 return Result.ok(id); } \"]},{\"header\":\"引入依赖\",\"slug\":\"引入依赖\",\"contents\":[\"<dependency> <groupId>org.aspectj</groupId> <artifactId>aspectjweaver</artifactId> </dependency> \"]},{\"header\":\"主启动类引入注解\",\"slug\":\"主启动类引入注解\",\"contents\":[\"//开启代理 @EnableAspectJAutoProxy(exposeProxy = true) @MapperScan(\\\"com.hmdp.mapper\\\") @SpringBootApplication public class HmDianPingApplication { public static void main(String[] args) { SpringApplication.run(HmDianPingApplication.class, args); } } \"]},{\"header\":\"2、集群环境下一人一单\",\"slug\":\"_2、集群环境下一人一单\",\"contents\":[]},{\"header\":\"1、创建服务集群\",\"slug\":\"_1、创建服务集群\",\"contents\":[\"1.复制一份微服务\",\"若找不到Service，可去view->Tool Windows->Services，然后Run Configure Type，选择Spring Boot项目即可\",\"2.配置名称以及覆盖之前的端口\",\"3.配置nginx作负载均衡\",\"upstream backend { server 127.0.0.1:8081 max_fails=5 fail_timeout=10s weight=1; server 127.0.0.1:8082 max_fails=5 fail_timeout=10s weight=1; } \",\"现在，用户请求会在这两个节点上负载均衡，再次测试下是否存在线程安全问题。\"]},{\"header\":\"测试\",\"slug\":\"测试\",\"contents\":[]},{\"header\":\"3、一人一单的并发问题\",\"slug\":\"_3、一人一单的并发问题\",\"contents\":[\"对于多个微服务来说，锁监视器 锁住的是线程，而不是我们自定义的代码块，每个微服务都有一个JVM，而每个JVM都会存在锁，此时并发问题还是会存在。\"]},{\"header\":\"5、分布式锁\",\"slug\":\"_5、分布式锁\",\"contents\":[]},{\"header\":\"1、场景引入\",\"slug\":\"_1、场景引入-1\",\"contents\":[\"对于一人一单并发问题，可以把锁从内部提取出来，放在外部进行监视锁，这样，不管有多少个服务，共用一把锁，即可解决并发问题\"]},{\"header\":\"2、定义\",\"slug\":\"_2、定义\",\"contents\":[\"分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。\"]},{\"header\":\"3、实现\",\"slug\":\"_3、实现\",\"contents\":[\"分布式锁的核心是实现多进程之间互斥，而满足这一点的方式有很多，常见的有三种：\",\"MySQL\",\"Redis\",\"Zookeeper\",\"**互斥 **\",\"利用mysql本身的互斥锁机制\",\"利用setnx这样的互斥命令\",\"利用节点的唯一性和有序性实现互斥\",\"**高可用 **\",\"好\",\"好\",\"好\",\"**高性能 **\",\"一般\",\"好\",\"一般\",\"**安全性 **\",\"断开连接，自动释放锁\",\"利用锁超时时间，到期释放\",\"临时节点，断开连接自动释放\"]},{\"header\":\"4、基于Redis的分布式锁\",\"slug\":\"_4、基于redis的分布式锁\",\"contents\":[\"实现分布式锁时需要实现的两个基本方法：\",\"获取锁\",\"释放锁\"]},{\"header\":\"1、获取锁\",\"slug\":\"_1、获取锁\",\"contents\":[\"互斥：确保只能有一个线程获取锁\",\"# 添加锁，利用setnx的互斥特性 SETNX lock thread1 # 添加锁过期时间，避免服务宕机引起的死锁 EXPIRE lock 10 \",\"但是如果在添加锁与添加锁的过期时间之间，redis宕机了，这是我们就灭法释放锁，这是就会一直存在\",\"SET有一次性到位的命令\",\"非阻塞：尝试一次，成功返回true，失败返回false\",\"# 添加锁，NX是互斥、EX是设置超时时间 SET lock thread1 NX EX 10 \"]},{\"header\":\"2、释放锁\",\"slug\":\"_2、释放锁\",\"contents\":[\"手动释放\",\"超时释放：获取锁时添加一个超时时间\",\"# 释放锁，删除即可 DEL key \"]},{\"header\":\"3、整体流程\",\"slug\":\"_3、整体流程\",\"contents\":[\"尝试获取锁，判断获取锁的返回结果\",\"返回OK，则表示获取锁成功，执行相应的业务，最后释放锁\",\"返回nil，则表示获取锁失败\",\"若存在业务超时或服务宕机，则会利用超时时间释放锁\"]},{\"header\":\"4、代码实现\",\"slug\":\"_4、代码实现\",\"contents\":[\"定义一个锁接口，用户获取和释放锁，利用Redis实现分布式锁功能\",\"public interface ILock { /** * 尝试获取锁 * @param timeoutSec 锁持有的超时时间，过期后自动释放 * @return true代表获取锁成功; false代表获取锁失败 **/ boolean tryLock(long timeoutSec); /** * 释放锁 */ void unLock(); } \",\"新建类实现锁接口\",\"public class SimpleRedisLock implements ILock{ private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } public static final String KEY_PREFIX = \\\"lock:\\\"; @Override public boolean tryLock(long timeoutSec) { //获取线程标识 long threadId = Thread.currentThread().getId(); //获取锁 Boolean flag = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + this.name, threadId + \\\"\\\", timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(flag); } /** * 释放锁 */ @Override public void unLock() { stringRedisTemplate.delete(KEY_PREFIX + this.name); } } \",\"修改下单的锁逻辑，之前采用的是悲观锁，现在引入自定义锁\",\"@Service public class VoucherOrderServiceImpl extends ServiceImpl<VoucherOrderMapper, VoucherOrder> implements IVoucherOrderService { @Resource private ISeckillVoucherService seckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Autowired private StringRedisTemplate stringRedisTemplate; @Override public Result seckillVoucher(Long voucherId) { //查询优惠券信息 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); if (null == voucher){ return Result.fail(\\\"优惠券不存在！\\\"); } //判断时间是否开始或过期 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\\\"秒杀未开始！\\\"); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\\\"秒杀已结束！\\\"); } //判断库存是否充足 if (1 > voucher.getStock()){ return Result.fail(\\\"库存不足\\\"); } Long userId = UserHolder.getUser().getId(); //返回字符串对象的规范表示。 //一个字符串池，最初是空的，由String类私下维护。 //当调用 intern 方法时，如果池中已经包含一个等于该String对象的字符串，由equals(Object)方法确定，则返回池中的字符串。否则，将此String对象添加到池中并返回对该String对象的引用。 //因此，对于任何两个字符串s和t ，当且仅当s.equals(t)为true时， s.intern() == t.intern()才为true 。 // synchronized(userId.toString().intern()) { // //获取代理对象（事务） // IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); // return proxy.createVoucherOrder(voucherId); // } //创建锁对象 SimpleRedisLock redisLock = new SimpleRedisLock(\\\"order\\\" + userId, stringRedisTemplate); boolean isLock = redisLock.tryLock(1200); //获取锁失败 if (!isLock){ //返回错误 return Result.fail(\\\"不允许重复下单\\\"); } try { IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 redisLock.unLock(); } } @Override @Transactional(rollbackFor = Exception.class) public Result createVoucherOrder(Long voucherId) { //一人一单 Long userId = UserHolder.getUser().getId(); //查询订单 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"voucher_id\\\", voucherId).count(); if (count > 0) { return Result.fail(\\\"用户已经购买过一次了！\\\"); } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherId).gt(\\\"stock\\\",0).update(); if (!success){ return Result.fail(\\\"库存不足\\\"); } //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); //创建订单号 long id = redisIdWorker.nextId(\\\"order\\\"); voucherOrder.setId(id); voucherOrder.setUserId(userId); save(voucherOrder); //返回订单号 return Result.ok(id); } } \"]},{\"header\":\"5、存在的问题\",\"slug\":\"_5、存在的问题\",\"contents\":[\"线程1获取锁之后，执行业务阻塞了，此时超时释放锁\",\"线程2请求获取锁成功后，线程1的业务执行完成，并释放锁\",\"线程3同理\",\"线程1在执行完业务之后，释放的锁并不是自己持有的锁，而是线程2获取到的锁，\"]},{\"header\":\"6、优化\",\"slug\":\"_6、优化\",\"contents\":[\"需求：修改之前的分布式锁实现，满足：\",\"在获取锁时存入线程标示（可以用UUID表示）\",\"在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致 \",\"如果一致则释放锁\",\"如果不一致则不释放锁\",\"修改获取锁和释放锁的实现\",\"public class SimpleRedisLock implements ILock{ private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } public static final String KEY_PREFIX = \\\"lock:\\\"; public static final String ID_PREFIX = UUID.randomUUID().toString(true) + \\\"-\\\"; @Override public boolean tryLock(long timeoutSec) { //获取线程标识 String threadId = ID_PREFIX + Thread.currentThread().getId(); //获取锁 Boolean flag = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + this.name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(flag); } /** * 释放锁 */ @Override public void unLock() { //获取线程标识 String threadId = ID_PREFIX + Thread.currentThread().getId(); //判断标识是否一致 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + this.name); if (threadId.equals(id)){ stringRedisTemplate.delete(KEY_PREFIX + this.name); } } } \"]},{\"header\":\"7、Redis的Lua脚本\",\"slug\":\"_7、redis的lua脚本\",\"contents\":[\"Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html\",\"介绍Redis提供的调用函数，语法如下：\",\"例如，我们要执行set name jack，则脚本是这样：\",\"例如，我们要先执行set name Rose，再执行get name，则脚本如下：\",\"写好脚本以后，需要用Redis命令来调用脚本，调用脚本的常见命令如下：\",\"例如，我们要执行 redis.call('set', 'name', 'jack') 这个脚本，语法如下：\",\"如果脚本中的key、value不想写死，可以作为参数传递。key类型参数会放入KEYS数组，其它参数会放入ARGV数组，在脚本中可以从KEYS和ARGV数组获取这些参数：\",\"释放锁的业务流程是这样的：\",\"获取锁中的线程标示\",\"判断是否与指定的标示（当前线程标示）一致\",\"如果一致则释放锁（删除）\",\"如果不一致则什么都不做\",\"如果用Lua脚本来表示则是这样的：\",\"-- 锁的key local key = KEYS[1] -- 当前线程标识 local threadId = ARGV[1] --- ---获取锁中的线程标识 --- local id = redis.call('get', key) -- 比较是否一致 if (id == threadId) then -- 释放锁 del key return redis.call('del', key) end ------------------------------------------------------简化版------------------------------------------------- -- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示 -- 获取锁中的标示，判断是否与当前线程标示一致 if (redis.call('GET', KEYS[1]) == ARGV[1]) then -- 一致，则删除锁 return redis.call('DEL', KEYS[1]) end -- 不一致，则直接返回 return 0 \"]},{\"header\":\"8、再次改进Redis的分布式锁\",\"slug\":\"_8、再次改进redis的分布式锁\",\"contents\":[\"需求：基于Lua脚本实现分布式锁的释放锁逻辑 提示：RedisTemplate调用Lua脚本的API如下：\",\"这里只需要更改释放锁的逻辑即可，把unlock.lua脚本放到resource目录下\",\"public class SimpleRedisLock implements ILock{ private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } public static final String KEY_PREFIX = \\\"lock:\\\"; public static final String ID_PREFIX = UUID.randomUUID().toString(true) + \\\"-\\\"; //设置为静态，不需要每次释放锁都调用一次基本 public static final DefaultRedisScript<Long> UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript<>(); //调用resource下的unlock脚本 UNLOCK_SCRIPT.setLocation(new ClassPathResource(\\\"unlock.lua\\\")); UNLOCK_SCRIPT.setResultType(Long.class); } @Override public boolean tryLock(long timeoutSec) { //获取线程标识 String threadId = ID_PREFIX + Thread.currentThread().getId(); //获取锁 Boolean flag = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + this.name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(flag); } /** * 通过lua脚本释放锁 */ @Override public void unLock() { //调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId()); } } \"]},{\"header\":\"9、总结\",\"slug\":\"_9、总结\",\"contents\":[\"基于Redis的分布式锁实现思路：\",\"利用set nx ex获取锁，并设置过期时间，保存线程标示\",\"释放锁时先判断线程标示是否与自己一致，一致则删除锁\",\"特性：\",\"利用set nx满足互斥性\",\"利用set ex保证故障时锁依然能释放，避免死锁，提高安全性\",\"利用Redis集群保证高可用和高并发特性\"]},{\"header\":\"5、基于Redis的分布式锁优化\",\"slug\":\"_5、基于redis的分布式锁优化\",\"contents\":[]},{\"header\":\"1、基于setnx实现的分布式锁存在的问题\",\"slug\":\"_1、基于setnx实现的分布式锁存在的问题\",\"contents\":[\"不可重入：同一个线程无法多次获取同一把锁\",\"例如：方法A先获取锁，调用方法B中的业务，B也想获取同一把锁，此时无法获取到锁，只能等待A释放\",\"不可重试：获取锁只尝试一次就返回false，没有重试机制\",\"很多业务下，不能返回立即失败，而是需要等待，重试\",\"超时释放：锁超时释放虽然可避免死锁，但如果业务执行耗时较长，也会导致锁释放，存在安全隐患\",\"虽然之前利用判断锁标识，Lua脚本解决了因为超时释放解决的误删问题，\",\"主从一致性：如果Redis提供了主从集群，主从同步存在延迟，当主宕机时，如果从同步主中的锁数据，则会出现锁实现\"]},{\"header\":\"2、Redisson\",\"slug\":\"_2、redisson\",\"contents\":[\"Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。\",\"官网地址： https://redisson.org\",\"GitHub地址： https://github.com/redisson/redisson\"]},{\"header\":\"1、Redisson入门\",\"slug\":\"_1、redisson入门\",\"contents\":[\"1、引入依赖\",\"<dependency> <groupId>org.redisson</groupId> <artifactId>redisson</artifactId> <version>3.17.3</version> </dependency> \",\"2、配置Redisson客户端\",\"@Component public class RedisConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); // 添加redis地址，这里添加了单点的地址，也可以使用config.useClusterServers()添加集群地址 config.useSingleServer().setAddress(\\\"redis://127.0.0.1:6379\\\").setPassword(\\\"123456\\\"); //创建redisson对象 创建客户端 return Redisson.create(config); } } \",\"3、使用Redisson的分布式锁\",\"@Resource private RedissonClient redissonClient; @Test void testRedisson() throws InterruptedException { // 获取锁（可重入），指定锁的名称 RLock lock = redissonClient.getLock(\\\"anyLock\\\"); // 尝试获取锁，参数分别是：获取锁的最大等待时间（期间会重试），锁自动释放时间，时间单位 boolean isLock = lock.tryLock(1, 10, TimeUnit.SECONDS); // 判断释放获取成功 if(isLock){ try { System.out.println(\\\"执行业务\\\"); }finally { // 释放锁 lock.unlock(); } } } \"]},{\"header\":\"2、Redisson可重入锁原理\",\"slug\":\"_2、redisson可重入锁原理\",\"contents\":[\"为了避免并发安全问题，释放不是自己的锁，这里采用的是锁计数器，获取锁+1，判断是否为自己的锁-1，最终计数为0\",\"测试类\",\"@RunWith(SpringRunner.class) @SpringBootTest @Slf4j public class RedissonTest { @Autowired private RedissonClient redissonClient; private RLock lock; @BeforeEach void setUp(){ System.out.println(121312); lock = redissonClient.getLock(\\\"order\\\"); } @Test public void method1(){ //获取锁对象 boolean tryLock = lock.tryLock(); if (!tryLock){ log.error(\\\"获取锁失败！，1\\\"); return; } try { log.info(\\\"获取锁成功，1\\\"); method2(); }finally { log.info(\\\"释放锁，1\\\"); lock.unlock(); } } public void method2(){ boolean tryLock = lock.tryLock(); if (!tryLock) { log.error(\\\"获取锁失败！，2\\\"); } try { log.info(\\\"获取锁成功！，2\\\"); }finally { log.error(\\\"释放锁！，2\\\"); lock.unlock(); } } } \",\"PS：这里@Test注解需引入import org.junit.jupiter.api.Test;而不是import org.junit.Test;，否则@BeforeEach不会执行\",\"@BeforeEach：\",\"获取锁的Lua脚本\",\"-- 锁的key local key = KEYS[1] -- 当前线程标识 local threadId = ARGV[1] --锁的自动释放时间 local releaseTime = ARGV[2] --判断是否存在 if (redis.call('exist', key) == 0) then --不存在，则释放锁 redis.call('hset',key, threadId, '1') --设置有效期 redis.call('expire', key, releaseTime) --返回结果 return 1; end --锁已经存在，判断threadId是否是自己的 if (redis.call('hexist', key, threadId) == 1) then --不存在，则获取锁，重入次数+1 redis.call('hincrby', key, threadId, '1') --设置有效期 redis.call('expire', key, releaseTime) --返回结果 return 1; end -- 代码走到这里,说明获取锁的不是自己，获取锁失败 return 0; \",\"释放锁的Lua脚本\",\"local key = KEYS[1]; -- 锁的key local threadId = ARGV[1]; -- 线程唯一标识 local releaseTime = ARGV[2]; -- 锁的自动释放时间 -- 判断当前锁是否还是被自己持有 if (redis.call('HEXISTS', key, threadId) == 0) then return nil; -- 如果已经不是自己，则直接返回 end -- 是自己的锁，则重入次数-1 local count = redis.call('HINCRBY', key, threadId, -1); -- 判断是否重入次数是否已经为0 if (count > 0) then -- 大于0说明不能释放锁，重置有效期然后返回 redis.call('EXPIRE', key, releaseTime); return nil; else -- 等于0说明可以释放锁，直接删除 redis.call('DEL', key); return nil; end; \"]},{\"header\":\"3、Redisson分布式锁原理\",\"slug\":\"_3、redisson分布式锁原理\",\"contents\":[]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[\"Redisson分布式锁原理：\",\"可重入：利用hash结构记录线程id和重入次数\",\"可重试：利用信号量和PubSub功能实现等待、唤醒，获取锁失败的重试机制\",\"超时续约：利用watchDog，每隔一段时间（releaseTime / 3），重置超时时间\"]},{\"header\":\"5、Redisson分布式锁主从一致性问题\",\"slug\":\"_5、redisson分布式锁主从一致性问题\",\"contents\":[\"Redis集群\",\"存在一瞬间，redis主机突然宕机了，且此时锁并未同步到从机，此时就会出现线程并发问题，可以从其他的主机都获取到锁\",\"Redisson解决\",\"Redisson分别从所有主机获取锁，当从所有主机获取到锁且一致时，才会获取锁成功，其他一台宕机了，则会获取锁失败，这种设计也叫做联锁。\",\"联锁（MultiLock）\",\"基于Redis的Redisson分布式联锁RedissonMultiLock对象可以将多个RLock对象关联为一个联锁，每个RLock对象实例可以来自于不同的Redisson实例。\",\"大家都知道，如果负责储存某些分布式锁的某些Redis节点宕机以后，而且这些锁正好处于锁住的状态时，这些锁会出现锁死的状态。为\",\"了避免这种情况的发生，Redisson内部提供了一个监控锁的看门狗，它的作用是在Redisson实例被关闭前，不断的延长锁的有效期。默\",\"认情况下，看门狗的检查锁的超时时间是30秒钟，也可以通过修改Config.lockWatchdogTimeout来另行指定。\",\"RLock lock1 = redissonInstance1.getLock(\\\"lock1\\\"); RLock lock2 = redissonInstance2.getLock(\\\"lock2\\\"); RLock lock3 = redissonInstance3.getLock(\\\"lock3\\\"); RedissonMultiLock lock = new RedissonMultiLock(lock1, lock2, lock3); // 同时加锁：lock1 lock2 lock3 // 所有的锁都上锁成功才算成功。 lock.lock(); ... lock.unlock(); \",\"测试\",\"1.修改Redisson配置类\",\"@Component public class RedisConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); // 添加redis地址，这里添加了单点的地址，也可以使用config.useClusterServers()添加集群地址 config.useSingleServer().setAddress(\\\"redis://127.0.0.1:6379\\\").setPassword(\\\"123456\\\"); //创建redisson对象 创建客户端 return Redisson.create(config); } @Bean public RedissonClient redissonClient1(){ Config config = new Config(); // 添加redis地址，这里添加了单点的地址，也可以使用config.useClusterServers()添加集群地址 config.useSingleServer().setAddress(\\\"redis://127.0.0.1:6380\\\"); //创建redisson对象 创建客户端 return Redisson.create(config); } } \",\"2、修改测试类\",\"@RunWith(SpringRunner.class) @SpringBootTest @Slf4j public class RedissonTest { @Autowired private RedissonClient redissonClient; @Autowired private RedissonClient redissonClient1; private RLock lock; @BeforeEach void setUp(){ System.out.println(121312); RLock lock1 = redissonClient.getLock(\\\"order\\\"); RLock lock2 = redissonClient1.getLock(\\\"order\\\"); lock = new RedissonMultiLock(lock1, lock2); } @Test public void method1(){ //获取锁对象 boolean tryLock = lock.tryLock(); if (!tryLock){ log.error(\\\"获取锁失败！，1\\\"); return; } try { log.info(\\\"获取锁成功，1\\\"); method2(); }finally { log.info(\\\"释放锁，1\\\"); lock.unlock(); } } public void method2(){ boolean tryLock = lock.tryLock(); if (!tryLock) { log.error(\\\"获取锁失败！，2\\\"); } try { log.info(\\\"获取锁成功！，2\\\"); }finally { log.error(\\\"释放锁！，2\\\"); lock.unlock(); } } } \",\"总结：\",\"不可重入Redis分布式锁： \",\"原理：利用setnx的互斥性；利用ex避免死锁；释放锁时判断线程标示\",\"缺陷：不可重入、无法重试、锁超时失效\",\"可重入的Redis分布式锁： \",\"原理：利用hash结构，记录线程标示和重入次数；利用watchDog延续锁时间；利用信号量控制锁重试等待\",\"缺陷：redis宕机引起锁失效问题\",\"Redisson的multiLock： \",\"原理：多个独立的Redis节点，必须在所有节点都获取重入锁，才算获取锁成功\",\"缺陷：运维成本高、实现复杂\"]},{\"header\":\"6、Redis优化秒杀\",\"slug\":\"_6、redis优化秒杀\",\"contents\":[]},{\"header\":\"1、场景\",\"slug\":\"_1、场景-1\",\"contents\":[\"多用户同时请求下单时，延迟过高，tomcat需要操作整套流程\"]},{\"header\":\"2、优化方案\",\"slug\":\"_2、优化方案\",\"contents\":[\"核心的处理是：判断秒杀库存和一人一单，可采用多个线程的方式完成下单\",\"整体流程：\",\"Lua脚本保证redis的原子性，要么全部成功，要么就全部不执行\",\"userId存入set集合，保证用户一人一单\"]},{\"header\":\"3、改进秒杀业务，提高并发性能\",\"slug\":\"_3、改进秒杀业务-提高并发性能\",\"contents\":[\"需求：\",\"新增秒杀优惠券的同时，将优惠券信息保存到Redis中\",\"基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功\",\"如果抢购成功，将优惠券id和用户id封装后存入阻塞队列\",\"开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能\"]},{\"header\":\"1、新增秒杀优惠券的同时，将优惠券信息保存到Redis中\",\"slug\":\"_1、新增秒杀优惠券的同时-将优惠券信息保存到redis中\",\"contents\":[\"@Override @Transactional(rollbackFor = Exception.class) public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); //保存秒杀到redis stringRedisTemplate.opsForValue().set(RedisConstants.SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } \"]},{\"header\":\"2、基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功\",\"slug\":\"_2、基于lua脚本-判断秒杀库存、一人一单-决定用户是否抢购成功\",\"contents\":[\"--优惠券ID local voucherId = ARGV[1] --用户ID local userId = ARGV[2] -- 数据key 库存key 订单key local stockKey = \\\"seckill:stock:\\\" .. voucherId local orderKey = \\\"seckill:order:\\\" .. voucherId --判断库存是否充足 tonumber函数转为数字 if (tonumber(redis.call('get',stockKey)) <= 0) then --库存不足 return 1; end --判断用户是否下单 sismember判断是否存在该字符，存在返回1，不存在返回0 if (redis.call('sismember', orderKey, userId) == 1) then --存在，说明重复下单 return 2; end --扣库存，下单 redis.call('incrby', stockKey, -1) -- 这里使用sadd往集合添加数据 redis.call('sadd', orderKey, userId) return 0; \"]},{\"header\":\"3、存入阻塞队列，异步下单\",\"slug\":\"_3、存入阻塞队列-异步下单\",\"contents\":[\"@Service public class VoucherOrderServiceImpl extends ServiceImpl<VoucherOrderMapper, VoucherOrder> implements IVoucherOrderService { @Resource private ISeckillVoucherService seckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Autowired private StringRedisTemplate stringRedisTemplate; @Resource private RedissonClient redissonClient; public static final DefaultRedisScript<Long> SECKILL_SCRIPT; static { SECKILL_SCRIPT = new DefaultRedisScript<>(); //调用resource下的seckill脚本 SECKILL_SCRIPT.setLocation(new ClassPathResource(\\\"seckill.lua\\\")); SECKILL_SCRIPT.setResultType(Long.class); } /** * 阻塞队列 */ private BlockingQueue<VoucherOrder> orderTasks = new ArrayBlockingQueue<>(1024*1024); //开启线程 private static final ExecutorService seckill_order_excutor = Executors.newSingleThreadExecutor(); // @PostConstruct表示在系统启动时调用 @PostConstruct private void init(){ seckill_order_excutor.submit(new VoucherOrderHandle()); } /** * 内部类 */ private class VoucherOrderHandle implements Runnable{ @Override public void run() { while (true){ //1.获取队列中的订单信息 try { VoucherOrder take = orderTasks.take(); handleVoucherOrder(take); } catch (InterruptedException e) { e.printStackTrace(); } //创建订单 } } private void handleVoucherOrder(VoucherOrder take) { //获取用户 Long userId = take.getUserId(); RLock redisLock = redissonClient.getLock(\\\"lock:order:\\\" + userId); boolean isLock = redisLock.tryLock(); //获取锁失败 if (!isLock){ //返回错误 return; } try { proxy.createVoucherOrder(take); } finally { //释放锁 redisLock.unlock(); } } } private IVoucherOrderService proxy; @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //执行lua脚本 Long execute = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); //判断是否返回为0 0-有购买资格 !0没有购买资格 int i = execute.intValue(); if (0 != i) { return Result.fail(1 == i ? \\\"库存不足\\\" : \\\"不能重复下单\\\"); } //0 把下单信息保存到阻塞队列 //创建订单号 long orderId = redisIdWorker.nextId(\\\"order\\\"); //创建订单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setVoucherId(voucherId); voucherOrder.setId(orderId); voucherOrder.setUserId(userId); //创建阻塞队列 orderTasks.add(voucherOrder); //获取代理对象 proxy = (IVoucherOrderService) AopContext.currentProxy(); return Result.ok(orderId); } @Override @Transactional(rollbackFor = Exception.class) public void createVoucherOrder(VoucherOrder voucherOrder) { //一人一单 Long userId = voucherOrder.getUserId(); //查询订单 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"voucher_id\\\", voucherOrder).count(); if (count > 0) { return ; } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherOrder.getVoucherId()).gt(\\\"stock\\\",0).update(); if (!success){ return; } save(voucherOrder); } } \"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结-1\",\"contents\":[\"秒杀业务的优化思路是什么？\",\"先利用Redis完成库存余量、一人一单判断，完成抢单业务\",\"再将下单业务放入阻塞队列，利用独立线程异步下单\",\"基于阻塞队列的异步秒杀存在哪些问题？\",\"内存限制问题：开启异步任务会占用CPU和内存\",\"数据安全问题：基于内存保存信息，若redis宕机了，则查询不到订单信息，可重复下单\"]},{\"header\":\"7、Redis消息队列实现异步秒杀\",\"slug\":\"_7、redis消息队列实现异步秒杀\",\"contents\":[]},{\"header\":\"1、消息队列\",\"slug\":\"_1、消息队列\",\"contents\":[\"消息队列（Message Queue），字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色：\",\"消息队列：存储和管理消息，也被称为消息代理（Message Broker）\",\"生产者：发送消息到消息队列\",\"消费者：从消息队列获取消息并处理消息\",\"Redis提供了三种不同的方式来实现消息队列：\",\"list结构：基于List结构模拟消息队列\",\"PubSub：基本的点对点消息模型\",\"Stream：比较完善的消息队列模型\"]},{\"header\":\"1、基于List结构模拟消息队列\",\"slug\":\"_1、基于list结构模拟消息队列\",\"contents\":[\"消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。\",\"队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。\",\"不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用\",\"BRPOP或者BLPOP来实现阻塞效果。\",\"基于List的消息队列有哪些优缺点？\",\"优点：\",\"利用Redis存储，不受限于JVM内存上限\",\"基于Redis的持久化机制，数据安全性有保证\",\"可以满足消息有序性\",\"缺点：\",\"无法避免消息丢失\",\"只支持单消费者\"]},{\"header\":\"2、基于PubSub的消息队列\",\"slug\":\"_2、基于pubsub的消息队列\",\"contents\":[\"PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。\",\"SUBSCRIBE channel [channel] ：订阅一个或多个频道\",\"PUBLISH channel msg ：向一个频道发送消息\",\"PSUBSCRIBE pattern[pattern] ：订阅与pattern格式匹配的所有频道\",\"基于PubSub的消息队列有哪些优缺点？\",\"优点：\",\"采用发布订阅模型，支持多生产、多消费\",\"缺点：\",\"不支持数据持久化\",\"无法避免消息丢失\",\"消息堆积有上限，超出时数据丢失\"]},{\"header\":\"3、基于Stream的消息队列\",\"slug\":\"_3、基于stream的消息队列\",\"contents\":[\"Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。\",\"发送消息的命令：\",\"例如：\",\"## 创建名为 users 的队列，并向其中发送一个消息，内容是：{name=jack,age=21}，并且使用Redis自动生成ID 127.0.0.1:6379> XADD users * name jack age 21 \\\"1644805700523-0\\\" \"]},{\"header\":\"1、XREAD\",\"slug\":\"_1、xread\",\"contents\":[\"读取消息的方式之一：XREAD\",\"例如，使用XREAD读取第一个消息：\",\"XREAD阻塞方式，读取最新的消息：\",\"在业务开发中，我们可以循环的调用XREAD阻塞方式来查询最新消息，从而实现持续监听队列的效果，伪代码如下：\",\"PS：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息到达队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题。\",\"总结：\",\"STREAM类型消息队列的XREAD命令特点：\",\"消息可回溯\",\"一个消息可以被多个消费者读取\",\"可以阻塞读取\",\"有消息漏读的风险\"]},{\"header\":\"4、基于Stream的消息队列-消费者组\",\"slug\":\"_4、基于stream的消息队列-消费者组\",\"contents\":[\"消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。具备下列特点：\",\"消息分流：队列中的消息会分流给组内的不同消费者，而不是重复消费，从而加快消息处理的速度\",\"消息标示：消费者组会维护一个标示，记录最后一个被处理的消息，哪怕消费者宕机重启，还会从标示之后读取消息。确保每一个消息都会被消费\",\"消息确认：消费者获取消息后，消息处于pending状态，并存入一个pending-list。当处理完成后需要通过XACK来确认消息，标记消息为已处理，才会从pending-list移除。\",\"创建消费者组：\",\"XGROUP CREATE key groupName ID [MKSTREAM] \",\"key：队列名称\",\"groupName：消费者组名称\",\"ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息\",\"MKSTREAM：队列不存在时自动创建队列\",\"其它常见命令：\",\"# 删除指定的消费者组 XGROUP DESTORY key groupName # 给指定的消费者组添加消费者 XGROUP CREATECONSUMER key groupname consumername # 删除消费者组中的指定消费者 XGROUP DELCONSUMER key groupname consumername \",\"从消费者组读取消息：\",\"XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] \",\"group：消费组名称\",\"consumer：消费者名称，如果消费者不存在，会自动创建一个消费者\",\"count：本次查询的最大数量\",\"BLOCK milliseconds：当没有消息时最长等待时间\",\"NOACK：无需手动ACK，获取到消息后自动确认\",\"STREAMS key：指定队列名称\",\"ID：获取消息的起始ID： \",\"\\\">\\\"：从下一个未消费的消息开始\",\"其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始\",\"消费者监听消息的基本思路：\",\"总结\",\"STREAM类型消息队列的XREADGROUP命令特点：\",\"消息可回溯\",\"可以多消费者争抢消息，加快消费速度\",\"可以阻塞读取\",\"没有消息漏读的风险\",\"有消息确认机制，保证消息至少被消费一次\"]},{\"header\":\"5、对比\",\"slug\":\"_5、对比\",\"contents\":[\"List\",\"PubSub\",\"Stream\",\"消息持久化\",\"支持\",\"不支持\",\"支持\",\"阻塞读取\",\"支持\",\"支持\",\"支持\",\"消息堆积处理\",\"受限于内存空间，可以利用多消费者加快处理\",\"受限于消费者缓冲区\",\"受限于队列长度，可以利用消费者组提高消费速度，减少堆积\",\"消息确认机制\",\"不支持\",\"不支持\",\"支持\",\"消息回溯\",\"不支持\",\"不支持\",\"支持\"]},{\"header\":\"2、基于Redis的Stream结构作为消息队列，实现异步秒杀下单\",\"slug\":\"_2、基于redis的stream结构作为消息队列-实现异步秒杀下单\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求\",\"contents\":[\"需求：\",\"创建一个Stream类型的消息队列，名为stream.orders\",\"修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId\",\"项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单\"]},{\"header\":\"2、创建一个Stream类型的消息队列，名为stream.orders\",\"slug\":\"_2、创建一个stream类型的消息队列-名为stream-orders\",\"contents\":[\"localhost:0>XGROUP CREATE stream.orders g1 0 MKSTREAM \\\"OK\\\" localhost:0> \"]},{\"header\":\"3、Java代码改造\",\"slug\":\"_3、java代码改造\",\"contents\":[\"package com.hmdp.service.impl; import cn.hutool.core.bean.BeanUtil; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import com.hmdp.dto.Result; import com.hmdp.entity.VoucherOrder; import com.hmdp.mapper.VoucherOrderMapper; import com.hmdp.service.ISeckillVoucherService; import com.hmdp.service.IVoucherOrderService; import com.hmdp.utils.RedisIdWorker; import com.hmdp.utils.UserHolder; import org.redisson.api.RLock; import org.redisson.api.RedissonClient; import org.springframework.aop.framework.AopContext; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.core.io.ClassPathResource; import org.springframework.data.redis.connection.stream.*; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.data.redis.core.script.DefaultRedisScript; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional; import javax.annotation.PostConstruct; import javax.annotation.Resource; import java.time.Duration; import java.util.Collections; import java.util.List; import java.util.Map; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; vv @Service public class VoucherOrderServiceImpl extends ServiceImpl<VoucherOrderMapper, VoucherOrder> implements IVoucherOrderService { @Resource private ISeckillVoucherService seckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Autowired private StringRedisTemplate stringRedisTemplate; @Resource private RedissonClient redissonClient; public static final DefaultRedisScript<Long> SECKILL_SCRIPT; static { SECKILL_SCRIPT = new DefaultRedisScript<>(); //调用resource下的seckill脚本 SECKILL_SCRIPT.setLocation(new ClassPathResource(\\\"seckill.lua\\\")); SECKILL_SCRIPT.setResultType(Long.class); } /** * 阻塞队列 */ private static final ExecutorService seckill_order_excutor = Executors.newSingleThreadExecutor(); @PostConstruct private void init(){ seckill_order_excutor.submit(new VoucherOrderHandle()); } /** * 内部类 */ private class VoucherOrderHandle implements Runnable{ String queueName = \\\"stream.orders\\\"; @Override public void run() { while (true){ try { //1.获取消息队列中的订单信息 List<MapRecord<String, Object, Object>> list = stringRedisTemplate.opsForStream().read( Consumer.from(\\\"g1\\\", \\\"c1\\\"), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(queueName, ReadOffset.lastConsumed()) ); //判断消息是否获取成功 if (null == list || list.isEmpty()){ //获取失败，说明没有异常，执行下一次循环 continue; } //解析消息中的订单信息 MapRecord<String, Object, Object> record = list.get(0); Map<Object, Object> values = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(values, new VoucherOrder(), true); //获取成功，可用下单 handleVoucherOrder(voucherOrder); //ACK确认 stringRedisTemplate.opsForStream().acknowledge(queueName, \\\"g1\\\", record.getId()); } catch (Exception e) { handlePendingList(); e.printStackTrace(); } } //创建订单 } private void handlePendingList() { while (true){ try { //1.获取消息队列中的订单信息 List<MapRecord<String, Object, Object>> list = stringRedisTemplate.opsForStream().read( Consumer.from(\\\"g1\\\", \\\"c1\\\"), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(queueName, ReadOffset.from(\\\"0\\\")) ); //判断消息是否获取成功 if (null == list || list.isEmpty()){ //获取失败，说明pending-list没有异常，执行下一次循环 break; } //解析消息中的订单信息 MapRecord<String, Object, Object> record = list.get(0); Map<Object, Object> values = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(values, new VoucherOrder(), true); //获取成功，可用下单 handleVoucherOrder(voucherOrder); //ACK确认 stringRedisTemplate.opsForStream().acknowledge(queueName, \\\"g1\\\", record.getId()); } catch (Exception e) { e.printStackTrace(); } } } } private void handleVoucherOrder(VoucherOrder take) { //获取用户 Long userId = take.getUserId(); RLock redisLock = redissonClient.getLock(\\\"lock:order:\\\" + userId); boolean isLock = redisLock.tryLock(); //获取锁失败 if (!isLock){ //返回错误 return; } try { proxy.createVoucherOrder(take); } finally { //释放锁 redisLock.unlock(); } } private IVoucherOrderService proxy; @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //创建订单号 long orderId = redisIdWorker.nextId(\\\"order\\\"); //执行lua脚本 Long execute = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); //判断是否返回为0 0-有购买资格 !0没有购买资格 int i = execute.intValue(); if (0 != i) { return Result.fail(1 == i ? \\\"库存不足\\\" : \\\"不能重复下单\\\"); } //获取代理对象 proxy = (IVoucherOrderService) AopContext.currentProxy(); return Result.ok(orderId); } @Override @Transactional(rollbackFor = Exception.class) public void createVoucherOrder(VoucherOrder voucherOrder) { //一人一单 Long userId = voucherOrder.getUserId(); //查询订单 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"voucher_id\\\", voucherOrder).count(); if (count > 0) { return ; } //扣减库存 boolean success = seckillVoucherService.update() .setSql(\\\"stock = stock - 1\\\") .eq(\\\"voucher_id\\\", voucherOrder.getVoucherId()).gt(\\\"stock\\\",0).update(); if (!success){ return; } save(voucherOrder); } } \"]}]},\"/study-tutorial/distributed/redis/combat/friends_follow.html\":{\"title\":\"5、好友关注\",\"contents\":[{\"header\":\"1、关注和取关\",\"slug\":\"_1、关注和取关\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求\",\"contents\":[\"需求：基于该表数据结构，实现两个接口：\",\"关注和取关接口\",\"判断是否关注的接口\",\"关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示：\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[\"@Override public Result follow(Long followUserId, Boolean isFollow) { //获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \\\"follow:\\\" + userId; //判断关注还是取关 if (isFollow){ Follow follow = new Follow(); follow.setFollowUserId(followUserId); follow.setUserId(userId); save(follow); }else { remove(new QueryWrapper<Follow>() .eq(\\\"user_id\\\", userId).eq(\\\"follow_user_id\\\", followUserId)); } return Result.ok(); } @Override public Result isFollow(Long id) { //获取登录用户 Long userId = UserHolder.getUser().getId(); //查询是否关注 Integer count = query().eq(\\\"user_id\\\", userId).eq(\\\"follow_user_id\\\", id).count(); return Result.ok(0 < count); } \"]},{\"header\":\"2、共同关注\",\"slug\":\"_2、共同关注\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求-1\",\"contents\":[\"需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同好友。\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-1\",\"contents\":[\"共同关注是指user1和user2共同关注的博主，这里可以使用SortSet，可查看两个key共同的值\",\"ZINTERSTORE destination numkeys key [key ...] \",\"计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 destination 中\",\"/** * 共同关注 * @param id * @return */ @Override public Result followCommons(Long id) { Long userId = UserHolder.getUser().getId(); String key = \\\"follow:\\\" + userId; String key2 = \\\"follow:\\\" + id; //求交集 Set<String> intersect = stringRedisTemplate.opsForSet().intersect(key, key2); if (null == intersect || intersect.isEmpty()){ return Result.ok(Collections.emptyList()); } //转换id List<Long> ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); //查询用户 List<UserDTO> users = userService.listByIds(ids) .stream() .map(user -> BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(users); } \"]},{\"header\":\"3、关注推送\",\"slug\":\"_3、关注推送\",\"contents\":[\"关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。\",\"传统模式下，用户需要的内容通过自己去查找\",\"Feed模式下，通过用户浏览的内容进行分析，然后推送给用户\"]},{\"header\":\"1、Feed流\",\"slug\":\"_1、feed流\",\"contents\":[\"Feed流产品有两种常见模式：\",\"Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈 \",\"优点：信息全面，不会有缺失。并且实现也相对简单\",\"缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低\",\"智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户 \",\"优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷\",\"缺点：如果算法不精准，可能起到反作用\"]},{\"header\":\"2、Timeline模式\",\"slug\":\"_2、timeline模式\",\"contents\":[\"该模式的实现方案有三种：\",\"拉模式\",\"推模式\",\"推拉结合\"]},{\"header\":\"1、拉模式\",\"slug\":\"_1、拉模式\",\"contents\":[\"拉模式：也叫做读扩散。简单来说就是关注者要读取的时候，实时去查询被关注者的发件箱，然后进行读取\",\"每人都有一个发件箱，收件箱，张三、李四、王五、赵六四个人，赵六关注张三和李四。\",\"现在张三和李四同时发两条信息，王五发送一条，赵六因关注张三和李四，这时会去查询张三李四的发件箱，然后进行读取，未关注王五则读取不到信息。\"]},{\"header\":\"2、推模式\",\"slug\":\"_2、推模式\",\"contents\":[\"推模式：也叫做写扩散。简单来说就是推送到每一个关注的邮箱，他们读取自己的邮箱的信息\"]},{\"header\":\"3、推拉结合\",\"slug\":\"_3、推拉结合\",\"contents\":[\"推拉结合模式：也叫做读写混合，兼具推和拉两种模式的优点。\",\"对于普通人，粉丝量不大的，采用拉模式；对于大V且是活跃粉丝，采用推模式，精准推送，活跃度不高的可采用拉模式\"]},{\"header\":\"4、对比\",\"slug\":\"_4、对比\",\"contents\":[\"拉模式\",\"推模式\",\"推拉结合\",\"写比例\",\"低\",\"高\",\"中\",\"读比例\",\"高\",\"低\",\"中\",\"用户读取延迟\",\"高\",\"低\",\"低\",\"实现难度\",\"复杂\",\"简单\",\"很复杂\",\"使用场景\",\"很少使用\",\"用户量少、没有大V\",\"过千万的用户量，有大V\"]},{\"header\":\"3、基于推模式实现关注推送功能\",\"slug\":\"_3、基于推模式实现关注推送功能\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求-2\",\"contents\":[\"需求：\",\"修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱\",\"收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现\",\"查询收件箱数据时，可以实现分页查询\"]},{\"header\":\"2、Feed流的分页问题\",\"slug\":\"_2、feed流的分页问题\",\"contents\":[\"Feed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。\",\"传统角标的效果：\",\"实际需要的效果：\"]},{\"header\":\"3、实现\",\"slug\":\"_3、实现\",\"contents\":[\"1.修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱\",\"@Override public Result saveBlog(Blog blog) { // 获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 保存探店博文 boolean save = save(blog); if (!save) { return Result.fail(\\\"新增笔记失败！\\\"); } //查询笔记所有粉丝 List<Follow> followList = followService.query().eq(\\\"follow_user_id\\\", user.getId()).list(); //笔记Id推送给粉丝 for (Follow follow : followList) { String key = RedisConstants.FEED_KEY + follow.getUserId(); stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); } // 返回id return Result.ok(blog.getId()); } \",\"@Override public Result follow(Long followUserId, Boolean isFollow) { //获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \\\"follow:\\\" + userId; //判断关注还是取关 if (isFollow){ Follow follow = new Follow(); follow.setFollowUserId(followUserId); follow.setUserId(userId); boolean isSuccess = save(follow); if (isSuccess) { //把关注用户的id存入redis set集合 stringRedisTemplate.opsForSet().add(key, followUserId.toString()); } }else { boolean remove = remove(new QueryWrapper<Follow>() .eq(\\\"user_id\\\", userId).eq(\\\"follow_user_id\\\", followUserId)); if (remove) { //从redis set集合中移除 stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); } } return Result.ok(); } \",\"2.收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现\",\"3.查询收件箱数据时，可以实现分页查询\",\"/** * 收件箱笔记滚动分页 * @param lastId * @param offset * @return */ @Override public Result queryBlogOfFollow(Long lastId, Integer offset) { //获取当前用户 Long userId = UserHolder.getUser().getId(); //查询收件箱 String key = RedisConstants.FEED_KEY + userId; Set<ZSetOperations.TypedTuple<String>> range = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, lastId, offset, 2); if (null == range || range.isEmpty()) { return Result.ok(); } //解析数据blogId,minTime时间戳 List<Long> list = new ArrayList<>(range.size()); long minTime = 0; int os = 1; for (ZSetOperations.TypedTuple<String> tuple : range) { //获取iD list.add(Long.valueOf(tuple.getValue())); //获取分数（时间戳） long time = tuple.getScore().longValue(); if (time == minTime){ os++; }else { minTime = time; os = 1; } } //根据id查询blog 返回也要根据顺序返回 List<Blog> blogs = query().in(\\\"id\\\", list) .last(\\\"order by field(id, \\\" + StrUtil.join(\\\",\\\", list) + \\\")\\\").list(); for (Blog blog : blogs) { isBlogLiked(blog); } //封装返回 ScrollResult scrollResult = new ScrollResult(); scrollResult.setList(blogs); scrollResult.setMinTime(minTime); scrollResult.setOffset(os); return Result.ok(scrollResult); } \"]}]},\"/study-tutorial/distributed/redis/combat/nearby_merchants.html\":{\"title\":\"6、附近商户\",\"contents\":[{\"header\":\"1、GEO数据结构\",\"slug\":\"_1、geo数据结构\",\"contents\":[\"GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。\",\"常见的命令有：\",\"GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member）\",\"GEODIST：计算指定的两个点之间的距离并返回\",\"GEOHASH：将指定member的坐标转为hash字符串形式并返回\",\"GEOPOS：返回指定member的坐标\",\"GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.2以后已废弃\",\"GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能\",\"GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能\"]},{\"header\":\"1、练习Redis的GEO功能\",\"slug\":\"_1、练习redis的geo功能\",\"contents\":[\"1、添加下面几条数据：\",\"北京南站（ 116.378248 39.865275 ）\",\"北京站（ 116.42803 39.903738 ）\",\"北京西站（ 116.322287 39.893729 ）\",\"localhost:0>GEOADD g1 116.378248 39.865275 bjn 116.42803 39.903738 bjz 116.322287 39.893729 bjx \\\"3\\\" \",\"2、计算北京西站到北京站的距离\",\"localhost:0>GEODIST g1 bjn bjx km \\\"5.7300\\\" localhost:0>GEODIST g1 bjz bjx km \\\"9.0916\\\" \",\"3、搜索天安门（ 116.397904 39.909005 ）附近10km内的所有火车站，并按照距离升序排序\",\"# 6.2版本之前 localhost:0>GEORADIUS g1 116.397904 39.909005 10 km WITHDIST 1) 1) \\\"bjz\\\" 2) \\\"2.6361\\\" 2) 1) \\\"bjn\\\" 2) \\\"5.1452\\\" 3) 1) \\\"bjx\\\" 2) \\\"6.6723\\\" #6.2之后的版本 localhost:0>GEOSEARCH g1 FROMLONLAT 116.397904 39.909005 BYRADIUS 10 km WITHDIST 1) 1) \\\"bjz\\\" 2) \\\"2.6361\\\" 2) 1) \\\"bjn\\\" 2) \\\"5.1452\\\" 3) 1) \\\"bjx\\\" 2) \\\"6.6723\\\" \"]},{\"header\":\"2、测试类导入redis\",\"slug\":\"_2、测试类导入redis\",\"contents\":[\"/** * 店铺数据导入redis */ @Test public void loadShopData(){ //获取商户列表 List<Shop> list = shopService.list(); //根据类型进行分组 Map<Long, List<Shop>> map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); for (Map.Entry<Long, List<Shop>> entry : map.entrySet()) { //获取类型ID Long typeId = entry.getKey(); String key = \\\"shop:geo:\\\" + typeId; //获取相同店铺类型的集合 List<Shop> value = entry.getValue(); ArrayList<RedisGeoCommands.GeoLocation<String>> locations = new ArrayList<>(value.size()); //写入redis GEOADD key 经度 纬度 member for (Shop shop : value) { locations.add(new RedisGeoCommands.GeoLocation<>( shop.getId().toString(), new Point(shop.getX(), shop.getY()) )); } stringRedisTemplate.opsForGeo().add(key, locations); } } \"]},{\"header\":\"2、附近商户搜索\",\"slug\":\"_2、附近商户搜索\",\"contents\":[\"按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可\",\"SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM文件，内容如\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-redis</artifactId> <!--排除旧版本--> <exclusions> <exclusion> <artifactId>lettuce-core</artifactId> <groupId>io.lettuce</groupId> </exclusion> <exclusion> <artifactId>spring-data-redis</artifactId> <groupId>org.springframework.data</groupId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-redis</artifactId> <version>2.6.4</version> </dependency> <dependency> <groupId>io.lettuce</groupId> <artifactId>lettuce-core</artifactId> <version>6.1.8.RELEASE</version> </dependency> \",\"代码实现\",\"@Override public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) { //是否需要根据坐标查询 if (null == x || null == y) { // 根据类型分页查询 Page<Shop> page = query() .eq(\\\"type_id\\\", typeId) .page(new Page<>(current, SystemConstants.DEFAULT_PAGE_SIZE)); // 返回数据 return Result.ok(page.getRecords()); } //计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.MAX_PAGE_SIZE; //查询redis，按照距离，分页 String key = RedisConstants.SHOP_GEO_KEY + typeId; GeoResults<RedisGeoCommands.GeoLocation<String>> results = stringRedisTemplate.opsForGeo() .search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); //解析出id if (null == results) { return Result.ok(); } List<GeoResult<RedisGeoCommands.GeoLocation<String>>> list = results.getContent(); if (list.size() < from){ return Result.ok(); } //截取from - end 的部分 List<Long> ids = new ArrayList<>(list.size()); HashMap<String, Distance> map = new HashMap<>(list.size()); list.stream().skip(from).forEach(item ->{ //获取店铺ID String name = item.getContent().getName(); ids.add(Long.valueOf(name)); //获取距离 Distance distance = item.getDistance(); map.put(name, distance); }); //根据id查询shop String join = StrUtil.join(\\\",\\\", ids); List<Shop> shopList = query().in(\\\"id\\\", ids).last(\\\"order by filed(id,\\\" + join + \\\")\\\").list(); for (Shop shop : shopList) { shop.setDistance(map.get(shop.getId().toString()).getValue()); } return Result.ok(shopList); } \"]}]},\"/study-tutorial/distributed/redis/combat/select_cache.html\":{\"title\":\"2、查询缓存\",\"contents\":[{\"header\":\"1、什么是缓存\",\"slug\":\"_1、什么是缓存\",\"contents\":[\"缓存就是数据交换的缓冲区（称作Cache [ kæʃ ] ），是存贮数据的临时地方，一般读写性能较高。\",\"浏览器缓存：缓存常用的样式css，js\",\"应用层缓存：浏览器未命中的缓存，则缓存在应用中\",\"数据库缓存：应用层未命中，则命中到数据库缓存，可缓存索引；通过索引查询时可快速检索，减少磁盘的读写\",\"CPU缓存：在Cache中的数据是内存中的一小部分，但这一小部分是短时间内CPU即将访问的，当CPU调用大量数据时，就可避开内\",\"存直接从Cache中调用，从而加快读取速度。由此可见，在CPU中加入Cache是一种高效的解决方案，这样整个内存储器（Cache+内\",\"存）就变成了既有Cache的高速度，又有内存的大容量的存储系统了。\",\"磁盘缓存：为了减少CPU透过I/O读取磁盘机的次数，提升磁盘I/O的效率，用一块内存来储存存取较频繁的磁盘内容；因为内存的存取是电子动作，而磁盘的存取是I/O动作，感觉上磁盘I/O变得较为快速。\"]},{\"header\":\"缓存的作用\",\"slug\":\"缓存的作用\",\"contents\":[\"降低后端负载\",\"提高读写效率，降低响应时间\"]},{\"header\":\"缓存的成本\",\"slug\":\"缓存的成本\",\"contents\":[\"数据一致性成本\",\"代码维护成本\",\"运维成本\"]},{\"header\":\"2、添加Redis缓存\",\"slug\":\"_2、添加redis缓存\",\"contents\":[]},{\"header\":\"1、未添加缓存作用模型\",\"slug\":\"_1、未添加缓存作用模型\",\"contents\":[\"客户端直接将请求打到数据库，数据库查询到之后直接返回给客户端\"]},{\"header\":\"2、添加缓存作用模型\",\"slug\":\"_2、添加缓存作用模型\",\"contents\":[\"客户端请求到Redis，如果redis命中则返回客户端，未命中则请求到数据库，查询到之后则返回给客户端，同时，将结果写入到redis中\"]},{\"header\":\"3、商户缓存到redis\",\"slug\":\"_3、商户缓存到redis\",\"contents\":[\"@Override public Result queryById(Long id) { //从redis查询缓存 String key = RedisConstants.CACHE_SHOP_KEY + id; String shopInfo = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isNotBlank(shopInfo)){ //存在则返回 Shop shop = JSONUtil.toBean(shopInfo, Shop.class); return Result.ok(shop); } //不存在，则查询数据库 Shop shop = getById(id); //不存在则返回错误 if (null == shop){ return Result.fail(\\\"店铺不存在\\\"); } //写入缓存 stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(shop)); //数据库存在，则返回 return Result.ok(shop); } \"]},{\"header\":\"3、缓存更新策略\",\"slug\":\"_3、缓存更新策略\",\"contents\":[\"业务场景：\",\"低一致性需求：使用内存淘汰机制。例如店铺类型的查询缓存\",\"高一致性需求：主动更新，并以超时剔除作为兜底方案。例如店铺详情查询的缓存\"]},{\"header\":\"1、主动更新\",\"slug\":\"_1、主动更新\",\"contents\":[\"Cache Aside Pattern：由缓存的调用者，在更新数据库的同时更新缓存\",\"Read/Write Through Pattern：缓存与数据库整合为一个服务，由服务维护一致性，调用者调用该服务，无需关心缓存一致性问题\",\"Write Behind Caching Pattern：调用者只操作缓存，由其他线程异步的将缓存的数据持久化到数据库，保证最终一致。\",\"PS：一般使用方案1，由缓存的调用者，在更新数据库的同时更新缓存\"]},{\"header\":\"2、Cache Aside Pattern\",\"slug\":\"_2、cache-aside-pattern\",\"contents\":[\"由缓存的调用者，在更新数据库的同时更新缓存\",\"操作缓存和数据库时有三个问题需要考虑：\",\"删除缓存还是更新缓存？\",\"如何保证缓存与数据库的操作的同时成功或失败？\",\"先操作缓存还是先操作数据库？\"]},{\"header\":\"1、删除缓存还是更新缓存？\",\"slug\":\"_1、删除缓存还是更新缓存\",\"contents\":[\"更新缓存：每次更新数据库都更新缓存，无效写操作较多 ❌\",\"如果更新数据库的次数多，而读取的次数较少，则每次更新时，都会增加无效的更新缓存操作\",\"删除缓存：更新数据库时让缓存失效，查询时再更新缓存 ✔️\"]},{\"header\":\"2、如何保证缓存与数据库的操作的同时成功或失败？\",\"slug\":\"_2、如何保证缓存与数据库的操作的同时成功或失败\",\"contents\":[\"单体系统，将缓存与数据库操作放在一个事务\",\"分布式系统，利用TCC等分布式事务方案\"]},{\"header\":\"3、先操作缓存还是先操作数据库？\",\"slug\":\"_3、先操作缓存还是先操作数据库\",\"contents\":[\"先删除缓存，再操作数据库\",\"先操作数据库，再删除缓存\"]},{\"header\":\"1、先删除缓存，再操作数据库\",\"slug\":\"_1、先删除缓存-再操作数据库\",\"contents\":[\"不存在线程安全问题场景\",\"线程1收到请求后，先删除缓存，然后更新数据库值为20\",\"线程2收到请求后，先查询缓存，并没有命中，则查询数据库的值为20，再写入缓存\",\"存在线程安全问题场景\",\"线程1收到请求后，先删除缓存\",\"线程2收到查询请求，查询缓存发现没有命中，则去查询数据库的值为10\",\"线程2将数据库的值写入缓存\",\"线程1更新数据库的值为20，此时导致数据不一致问题\"]},{\"header\":\"2、先操作数据库，再删除缓存✔️\",\"slug\":\"_2、先操作数据库-再删除缓存\",\"contents\":[\"不存在线程安全问题场景\",\"线程2收到请求后更新数据库值为20\",\"线程2更新完成后，删除缓存\",\"线程1收到请求后，查询缓存，未命中，则查询数据库\",\"线程1查询到数据后，写入缓存\",\"存在线程安全问题场景\",\"线程1收到查询请求后，查询缓存，未名中，则去查询数据库值为10\",\"线程2收到更新请求，更新数据库值为20\",\"线程2更新数据库后，删除缓存\",\"线程1写入缓存值为10，此时缓存写入的是旧数据\"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"缓存更新策略的最佳实践方案：\",\"低一致性需求：使用Redis自带的内存淘汰机制\",\"高一致性需求：主动更新，并以超时剔除作为兜底方案 \",\"读操作 \",\"缓存命中则直接返回\",\"缓存未命中则查询数据库，并写入缓存，设定超时时间\",\"写操作 \",\"先写数据库，然后再删除缓存\",\"要确保数据库与缓存操作的原子性\"]},{\"header\":\"4、实战\",\"slug\":\"_4、实战\",\"contents\":[]},{\"header\":\"1、查询设置超时时间\",\"slug\":\"_1、查询设置超时时间\",\"contents\":[\"@Override public Result queryById(Long id) { //从redis查询缓存 String key = RedisConstants.CACHE_SHOP_KEY + id; String shopInfo = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isNotBlank(shopInfo)){ //存在则返回 Shop shop = JSONUtil.toBean(shopInfo, Shop.class); return Result.ok(shop); } //不存在，则查询数据库 Shop shop = getById(id); //不存在则返回错误 if (null == shop){ return Result.fail(\\\"店铺不存在\\\"); } //写入缓存 设置超时时间为30min stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(shop), RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); //数据库存在，则返回 return Result.ok(shop); } \"]},{\"header\":\"2、更新时删除缓存\",\"slug\":\"_2、更新时删除缓存\",\"contents\":[\"@Override @Transactional public Result update(Shop shop) { Long id = shop.getId(); if (null == id){ return Result.fail(\\\"店铺id不能为空\\\"); } //1.更新数据库 updateById(shop); //2.删除缓存 stringRedisTemplate.delete(RedisConstants.CACHE_SHOP_KEY + shop.getId()); return Result.ok(); } \"]},{\"header\":\"4、缓存穿透\",\"slug\":\"_4、缓存穿透\",\"contents\":[]},{\"header\":\"1、定义\",\"slug\":\"_1、定义\",\"contents\":[\"缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。\",\"通俗点就是：key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。\"]},{\"header\":\"2、解决方案\",\"slug\":\"_2、解决方案\",\"contents\":[]},{\"header\":\"1、缓存空对象\",\"slug\":\"_1、缓存空对象\",\"contents\":[\"当客户端请求到redis后，未命中去查询数据库，数据库查询返回为null，则缓存为null\",\"优点：\",\"实现简单，维护方便\",\"缺点：\",\"额外的内存消耗\",\"若客户端请求大量数据都是不存在的，则redis会缓存大量的null数据\",\"可能造成短期的不一致\",\"客户端请求不存在的数据后，redis缓存数据为null，并设置了超时时间，此时就新增了一条数据，则再去查询时（还在TTL内），还是为null，只有当时间失效时，才会查询到\",\"可在新增时，更新缓存，可解决短期的不一致\"]},{\"header\":\"2、布隆过滤器\",\"slug\":\"_2、布隆过滤器\",\"contents\":[\"布隆过滤器是一个 bit 向量或者说 bit 数组（超长超长，记住一定要足够长）\",\"将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。\",\"优点：\",\"内存占用较少，没有多余key\",\"缺点：\",\"实现复杂\",\"存在误判可能\"]},{\"header\":\"3、实战\",\"slug\":\"_3、实战\",\"contents\":[\"@Override public Result queryById(Long id) { //从redis查询缓存 String key = RedisConstants.CACHE_SHOP_KEY + id; String shopInfo = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isNotBlank(shopInfo)){ //存在则返回 Shop shop = JSONUtil.toBean(shopInfo, Shop.class); return Result.ok(shop); } if (null != shopInfo){ return Result.fail(\\\"店铺不存在\\\"); } //不存在，则查询数据库 Shop shop = getById(id); //不存在则返回错误 if (null == shop){ //将空值写入redis stringRedisTemplate.opsForValue().set(key, \\\"\\\", RedisConstants.CACHE_NULL_TTL, TimeUnit.MINUTES); return Result.fail(\\\"店铺不存在\\\"); } //写入缓存 设置超时时间为30min stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(shop), RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); //数据库存在，则返回 return Result.ok(shop); } \"]},{\"header\":\"4、总结\",\"slug\":\"_4、总结\",\"contents\":[]},{\"header\":\"原因\",\"slug\":\"原因\",\"contents\":[\"用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力\"]},{\"header\":\"解决方案\",\"slug\":\"解决方案\",\"contents\":[\"缓存null值\",\"布隆过滤\",\"增强id的复杂度，避免被猜测id规律\",\"做好数据的基础格式校验\",\"加强用户权限校验\",\"做好热点参数的限流\"]},{\"header\":\"5、缓存雪崩\",\"slug\":\"_5、缓存雪崩\",\"contents\":[]},{\"header\":\"1、定义\",\"slug\":\"_1、定义-1\",\"contents\":[\"缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。\",\"key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。\"]},{\"header\":\"2、解决方案\",\"slug\":\"_2、解决方案-1\",\"contents\":[\"给不同的Key的TTL添加随机值\",\"利用Redis集群提高服务的可用性\",\"给缓存业务添加降级限流策略\",\"给业务添加多级缓存\"]},{\"header\":\"6、缓存击穿\",\"slug\":\"_6、缓存击穿\",\"contents\":[]},{\"header\":\"1、定义\",\"slug\":\"_1、定义-2\",\"contents\":[\"缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且缓存重建业务较复杂的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大的冲击。\",\"4个线程同时访问，且redis数据失效了，4个线程同时请求，查询缓存，发现未命中，则去查询数据库，重建缓存数据，最后写入缓存。\",\"会发现，4个线程会同时访问数据库并写入缓存。\",\"解决方案：\",\"互斥锁\",\"逻辑过期\"]},{\"header\":\"2、解决方案\",\"slug\":\"_2、解决方案-2\",\"contents\":[]},{\"header\":\"1、互斥锁\",\"slug\":\"_1、互斥锁\",\"contents\":[\"所谓互斥，就是不同线程，通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写。\",\"线程1发起请求，查询缓存发现未命中，然后获取互斥锁，成功之后，则去查询数据库重建缓存数据，写入缓存，释放锁。\",\"线程2在线程1未释放锁之前发起请求，查询缓存未命中，然后获取互斥锁，发现被线程1占用了，则获取失败，休眠一会儿，再重新获取锁（直到线程1释放），最后缓存命中。\",\"存在的问题：互斥等待时间，如果1000个线程同时访问，则只有1个获取成功，其他999个都是在等待，性能会下降\"]},{\"header\":\"2、逻辑过期\",\"slug\":\"_2、逻辑过期\",\"contents\":[\"逻辑过期：原来我们存储数据到redis中的时候，存的是k:v键值对，那逻辑过期，就是手动给value增加一个expire时间\",\"KEY\",\"VALUE\",\"heima:user:1\",\"{name:\\\"Jack\\\", age:21, expire:152141223}\",\"线程1发起请求，查询缓存，发现逻辑时间已过期，则回获取互斥锁，此时线程会开启一个新线程2（用于查询数据存入缓存），先返回过期的数据\",\"线程2查询数据库后，重建缓存数据，写入缓存后，重置逻辑过期时间，最后释放锁\",\"线程3发起请求（与线程1同步），查询缓存，发现逻辑时间已过期，获取互斥锁失败，就先返回旧数据\",\"线程4查询缓存，此时线程2已经释放锁，缓存命中，逻辑过期时间未过期，则直接返回\"]},{\"header\":\"3、比较\",\"slug\":\"_3、比较\",\"contents\":[\"解决方案\",\"优点\",\"缺点\",\"互斥锁（一致性）\",\"没有额外的内存消耗、保证一致性、实现简单\",\"线程需要等待，性能受影响可能有死锁风险\",\"逻辑过期（性能）\",\"线程无需等待，性能较好\",\"不保证一致性、有额外内存消耗、实现复杂\"]},{\"header\":\"3、实战\",\"slug\":\"_3、实战-1\",\"contents\":[]},{\"header\":\"1、基于互斥锁方式解决缓存击穿问题\",\"slug\":\"_1、基于互斥锁方式解决缓存击穿问题\",\"contents\":[\"/** * 缓存穿透解决方案 * @param id * @return */ public Shop queryByCacheMutex(Long id){ //从redis查询缓存 String key = RedisConstants.CACHE_SHOP_KEY + id; String shopInfo = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isNotBlank(shopInfo)){ //存在则返回 return JSONUtil.toBean(shopInfo, Shop.class); } if (null != shopInfo){ return null; } Shop shop = null; String lockKey = RedisConstants.LOCK_SHOP_KEY + id; try { //尝试获取互斥锁 boolean tryLock = tryLock(lockKey); //判断是否获取成功 if (!tryLock){ //不成功，则休眠，重试 Thread.sleep(10); return queryByCacheMutex(id); } //成功则查询数据库 shop = getById(id); //模拟延迟 Thread.sleep(200); //不存在则返回错误 if (null == shop){ //将空值写入redis stringRedisTemplate.opsForValue().set(key, \\\"\\\", RedisConstants.CACHE_NULL_TTL, TimeUnit.MINUTES); return null; } //写入缓存 设置超时时间为30min stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(shop), RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放锁 unLock(lockKey); } return shop; } /** * 获取锁 * @param key * @return */ private boolean tryLock(String key){ Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \\\"\\\", RedisConstants.LOCK_SHOP_TTL, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } /** * 释放锁 * @param key */ private void unLock(String key){ stringRedisTemplate.delete(key); } \"]},{\"header\":\"2、基于逻辑过期方式解决缓存击穿问题\",\"slug\":\"_2、基于逻辑过期方式解决缓存击穿问题\",\"contents\":[\"public Shop queryWithLogicalExpire(Long id){ //从redis查询缓存 String key = RedisConstants.CACHE_SHOP_KEY + id; String shopInfo = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isBlank(shopInfo)){ //存在则返回 return null; } RedisData data = JSONUtil.toBean(shopInfo, RedisData.class); LocalDateTime expireTime = data.getExpireTime(); Shop shop = JSONUtil.toBean((JSONObject) data.getData(), Shop.class); //判断是否过期 if (expireTime.isAfter(LocalDateTime.now())) { //未过期，直接返回 return shop; } //过期则获取互斥锁 String lockKey = RedisConstants.LOCK_SHOP_KEY + id; boolean lock = tryLock(lockKey); if (lock){ //获取系统处理器个数，作为线程池数量 // int nThreads = Runtime.getRuntime().availableProcessors(); //手动创建线程池 ThreadFactory build = new ThreadFactoryBuilder().setNamePrefix(\\\"demo-build-%d\\\").build(); //Common Thread Pool ExecutorService pool = new ThreadPoolExecutor(10, 12, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(1024), build, new ThreadPoolExecutor.AbortPolicy()); pool.submit(()->{ try { this.saveShopToRedis(id,20L); } catch (Exception e) { e.printStackTrace(); } finally { //释放锁 unLock(lockKey); } }); } return shop; } /** * 保存数据到redis * @param id * @param expire */ public void saveShopToRedis(Long id, Long expire){ //查询店铺数据 Shop byId = getById(id); //封装逻辑过期时间 RedisData redisData = new RedisData(); redisData.setData(byId); redisData.setExpireTime(LocalDateTime.now().plusSeconds(expire)); //写入redis stringRedisTemplate.opsForValue().set(RedisConstants.CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(redisData)); } \",\"@Data public class RedisData { private LocalDateTime expireTime; private Object data; } \"]},{\"header\":\"7、缓存工具封装\",\"slug\":\"_7、缓存工具封装\",\"contents\":[\"@Component @Slf4j public class RedisCacheClient { private final StringRedisTemplate stringRedisTemplate; public RedisCacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public void set(String key, Object value, Long time, TimeUnit timeUnit){ stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, timeUnit); } /** * 逻辑过期时间 * @param key * @param value * @param time * @param timeUnit */ public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit timeUnit){ RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(timeUnit.toSeconds(time))); //写入redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData), time, timeUnit); } /** * 缓存穿透 * @param keyPrefix * @param id * @param type * @param dbFallBack * @param time * @param timeUnit * @param <R> * @param <ID> * @return */ public <R, ID> R queryWithPassThrough( String keyPrefix, ID id, Class<R> type, Function<ID, R> dbFallBack, Long time, TimeUnit timeUnit){ //从redis查询缓存 String key = keyPrefix + id; String json = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isNotBlank(json)){ //存在则返回 return JSONUtil.toBean(json, type); } if (null != json){ return null; } //不存在，则查询数据库 R r = dbFallBack.apply(id); //不存在则返回错误 if (null == r){ //将空值写入redis stringRedisTemplate.opsForValue().set(key, \\\"\\\", RedisConstants.CACHE_NULL_TTL, TimeUnit.MINUTES); return null; } //写入缓存 设置超时时间为30min this.set(key, r, time, timeUnit); //数据库存在，则返回 return r; } /** * 缓存击穿 * @param keyPrefix * @param id * @param type * @param dbFallBack * @param time * @param timeUnit * @param <R> * @param <ID> * @return */ public <R, ID> R queryWithLogicalExpire( String keyPrefix, ID id, Class<R> type, Function<ID, R> dbFallBack, Long time, TimeUnit timeUnit){ //从redis查询缓存 String key = keyPrefix + id; String json = stringRedisTemplate.opsForValue().get(key); //判断是否存在 if (StrUtil.isBlank(json)){ //存在则返回 return null; } RedisData data = JSONUtil.toBean(json, RedisData.class); LocalDateTime expireTime = data.getExpireTime(); R r = JSONUtil.toBean((JSONObject) data.getData(), type); //判断是否过期 if (expireTime.isAfter(LocalDateTime.now())) { //未过期，直接返回 return r; } //过期则获取互斥锁 String lockKey = RedisConstants.LOCK_SHOP_KEY + id; boolean lock = tryLock(lockKey); if (lock){ //获取系统处理器个数，作为线程池数量 // int nThreads = Runtime.getRuntime().availableProcessors(); ThreadFactory build = new ThreadFactoryBuilder().setNamePrefix(\\\"demo-build-%d\\\").build(); //Common Thread Pool ExecutorService pool = new ThreadPoolExecutor(10, 12, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(1024), build, new ThreadPoolExecutor.AbortPolicy()); pool.submit(()->{ try { //查询数据库 R r1 = dbFallBack.apply(id); //写入缓存 this.setWithLogicalExpire(key, r1, time, timeUnit); } catch (Exception e) { e.printStackTrace(); } finally { //释放锁 unLock(lockKey); } }); } return r; } /** * 获取锁 * @param key * @return */ private boolean tryLock(String key){ Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \\\"\\\", RedisConstants.LOCK_SHOP_TTL, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } /** * 释放锁 * @param key */ private void unLock(String key){ stringRedisTemplate.delete(key); } } \",\"调用\",\"@Override public Result queryById(Long id) { //缓存穿透解决方案 Shop shop = redisCacheClient .queryWithPassThrough(RedisConstants.CACHE_SHOP_KEY, id, Shop.class, this::getById, RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); //缓存击穿 redisCacheClient.queryWithLogicalExpire(RedisConstants.CACHE_SHOP_KEY, id, Shop.class, this::getById, RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); if (null == shop) { return Result.fail(\\\"店铺不存在\\\"); } return Result.ok(shop); } \"]}]},\"/study-tutorial/distributed/redis/combat/sms_login.html\":{\"title\":\"1、短信登录\",\"contents\":[{\"header\":\"1、流程讲解\",\"slug\":\"_1、流程讲解\",\"contents\":[]},{\"header\":\"2、基于Session实现登录\",\"slug\":\"_2、基于session实现登录\",\"contents\":[]},{\"header\":\"1、发送短信验证码\",\"slug\":\"_1、发送短信验证码\",\"contents\":[\" /** * 发送验证码 * @param phone * @param session * @return */ @Override public Result sendCode(String phone, HttpSession session) { //【1】校验手机号 if (RegexUtils.isPhoneInvalid(phone)) { return Result.fail(\\\"手机号格式错误\\\"); } //【2】生成验证码 String numbers = RandomUtil.randomNumbers(6); //保存到session session.setAttribute(\\\"code\\\",numbers); return Result.ok(numbers); } \"]},{\"header\":\"2、短信验证码登录、注册\",\"slug\":\"_2、短信验证码登录、注册\",\"contents\":[\" /** * 用户登录 * @param loginForm * @param session * @return */ @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //【1】校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { return Result.fail(\\\"手机号格式错误\\\"); } //【2】校验验证码 String code = loginForm.getCode(); //存在session的数据 Object sessionCode = session.getAttribute(\\\"code\\\"); if (null == sessionCode || !sessionCode.equals(code)){ return Result.fail(\\\"验证码错误\\\"); } //【3】查询用户是否存在 User user = query().eq(\\\"phone\\\", phone).one(); if (null == user){ user = createUserWithPhone(phone); } session.setAttribute(\\\"user\\\", userDTO); return Result.ok(token); } /** * 根据手机号创建用户 * @param phone * @return */ private User createUserWithPhone(String phone) { User user = new User(); user.setPhone(phone); user.setNickName(SystemConstants.USER_NICK_NAME_PREFIX + RandomUtil.randomString(10)); save(user); return user; } \"]},{\"header\":\"3、校验登录状态\",\"slug\":\"_3、校验登录状态\",\"contents\":[\"我们大部分接口都需要登录才能访问，这时我们需要增加一个拦截器，拦截需要登录才能访问的功能\",\"/** * @author xiaoBear * @date 2022/5/22 15:14 * @description 登录拦截 */ public class LoginInterceptor implements HandlerInterceptor { /** * 前置拦截 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //获取session HttpSession session = request.getSession(); //获取session用户 Object user = session.getAttribute(\\\"user\\\"); //判断用户是否存在 if (null == user){ response.setStatus(401); return false; } //存在，保存在ThreadLocal UserHolder.saveUser((UserDTO) user); //放行 return true; } /** * 拦截之后 * * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { //移除用户 UserHolder.removeUser(); HandlerInterceptor.super.afterCompletion(request, response, handler, ex); } } \",\"再把拦截器加入到配置中\",\"@Configuration public class MvcConfig implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \\\"/user/code\\\", \\\"/user/login\\\", \\\"/blog/hot\\\", \\\"/shop/**\\\", \\\"/shop-type/**\\\", \\\"/voucher/**\\\" ); } } \",\"excludePathPatterns：代表放行的接口，不需要拦截\"]},{\"header\":\"3、集群的session共享问题\",\"slug\":\"_3、集群的session共享问题\",\"contents\":[\"session共享问题：多台Tomcat并不共享session存储空间，当请求切换到不同tomcat服务时导致数据丢失的问题。\",\"session的替代方案应该满足：\",\"数据共享\",\"内存存储\",\"key、value结构\",\"解决方案：使用Redis进行存储\"]},{\"header\":\"4、基于Redis实现共享session登录\",\"slug\":\"_4、基于redis实现共享session登录\",\"contents\":[\"保存登录的用户信息，可以使用String结构，以JSON字符串来保存，比较直观：\",\"KEY\",\"VALUE\",\"heima:user:1\",\"{name:\\\"Jack\\\", age:21}\",\"heima:user:2\",\"{name:\\\"Rose\\\", age:18} \",\"但是Hash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD，并且内存占用更少：\",\"Redis代替session需要考虑的问题：\",\"选择合适的数据结构\",\"选择合适的key\",\"选择合适的存储粒度\"]},{\"header\":\"1、发送短信验证码\",\"slug\":\"_1、发送短信验证码-1\",\"contents\":[\"这里采用的是StringRedisTemplate这个Bean对象\",\" /** * 发送验证码 * @param phone * @param session * @return */ @Override public Result sendCode(String phone, HttpSession session) { //【1】校验手机号 if (RegexUtils.isPhoneInvalid(phone)) { return Result.fail(\\\"手机号格式错误\\\"); } //【2】生成验证码 String numbers = RandomUtil.randomNumbers(6); //设置两分钟过期 set key value expire 120 stringRedisTemplate.opsForValue().set(RedisConstants.LOGIN_CODE_KEY + phone, numbers, RedisConstants.LOGIN_CODE_TTL, TimeUnit.MINUTES); return Result.ok(numbers); } \"]},{\"header\":\"2、短信验证码登录注册\",\"slug\":\"_2、短信验证码登录注册\",\"contents\":[\"/** * 用户登录 * @param loginForm * @param session * @return */ @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //【1】校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { return Result.fail(\\\"手机号格式错误\\\"); } //【2】校验验证码 String code = loginForm.getCode(); //从redis获取验证码并校验 String sessionCode = stringRedisTemplate.opsForValue().get(RedisConstants.LOGIN_CODE_KEY + phone); if (null == sessionCode || !sessionCode.equals(code)){ return Result.fail(\\\"验证码错误\\\"); } //【3】查询用户是否存在 User user = query().eq(\\\"phone\\\", phone).one(); if (null == user){ user = createUserWithPhone(phone); } //【4】保存用户到redis中 //生成随机token，作为登录令牌 String token = UUID.randomUUID().toString(true); //将user对象转为HashMap存储 UserDTO userDTO = new UserDTO(); BeanUtils.copyProperties(user, userDTO); //存储 Map<String, Object> map = BeanUtil.beanToMap(userDTO, new HashMap<>(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((k,v) -> v.toString())); stringRedisTemplate.opsForHash().putAll(RedisConstants.LOGIN_USER_KEY + token, map); //设置有效期 30分钟 stringRedisTemplate.expire(RedisConstants.LOGIN_USER_KEY + token, RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); return Result.ok(token); } \"]},{\"header\":\"3、校验登录状态\",\"slug\":\"_3、校验登录状态-1\",\"contents\":[\"最开始登录拦截器流程：前面session拦截的时候，只拦截需要登录的路径，然后查询用户是否存在。存在则继续，将用户信息保存到ThreadLocal中，刷新token的有效时间，放行；不存在则拦截；\",\"存在的问题：如果访问的路径并不是需要拦截的路径，token的有效期就不会一直刷新，就会导致在登录状态下，会自动退出登录。\",\"解决办法：再添加一个拦截器，用于拦截一切路径。先获取接口中的token，根据token查询Redis，若存在则保存到ThreadLocal中，刷新token时间，最后放行；而登录拦截器只用于从ThreadLocal中获取用户信息，不存在则拦截，存在则放行。\",\"public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate redisTemplate) { this.stringRedisTemplate = redisTemplate; } /** * 前置拦截 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //获取请求头的token 基于token获取redis的用户 String token = request.getHeader(\\\"authorization\\\"); if (StrUtil.isBlank(token)){ return true; } //如果用户信息为空，则放行 进行第二次拦截LoginInterceptor Map<Object, Object> entries = stringRedisTemplate.opsForHash().entries(RedisConstants.LOGIN_USER_KEY + token); if (entries.isEmpty()){ return true; } UserDTO userDTO = BeanUtil.fillBeanWithMap(entries, new UserDTO(), false); // //存在，保存在ThreadLocal UserHolder.saveUser(userDTO); //刷新过期时间 stringRedisTemplate.expire(RedisConstants.LOGIN_USER_KEY + token, RedisConstants.CACHE_SHOP_TTL, TimeUnit.MINUTES); //放行 return true; } /** * 拦截之后 * * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { //移除用户 UserHolder.removeUser(); HandlerInterceptor.super.afterCompletion(request, response, handler, ex); } } \",\"登录拦截器\",\"public class LoginInterceptor implements HandlerInterceptor { /** * 前置拦截 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //判断是否需要拦截 （ThreadLocal中存在用户） if (null == UserHolder.getUser()) { response.setStatus(401); return false; } //放行 return true; } /** * 拦截之后 * * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { //移除用户 UserHolder.removeUser(); HandlerInterceptor.super.afterCompletion(request, response, handler, ex); } } \"]}]},\"/study-tutorial/distributed/redis/combat/sortedSet.html\":{\"title\":\"4、达人探店\",\"contents\":[{\"header\":\"1、发布探店笔记\",\"slug\":\"_1、发布探店笔记\",\"contents\":[]},{\"header\":\"1、场景\",\"slug\":\"_1、场景\",\"contents\":[\"探店笔记类似点评网站的评价，往往是图文结合。对应的表有两个： tb_blog：探店笔记表，包含笔记中的标题、文字、图片等 tb_blog_comments：其他用户对探店笔记的评价\"]},{\"header\":\"2、实现查看发布探店笔记的接口\",\"slug\":\"_2、实现查看发布探店笔记的接口\",\"contents\":[\"需求：点击首页的探店笔记，会进入详情页面，实现该页面的查询接口：\",\"/** * 根据ID查询博客数据 * @param id * @return */ @Override public Result queryBlogById(Long id) { Blog blog = getById(id); if (null == blog){ return Result.fail(\\\"笔记不存在\\\"); } //查询博客用户 queryBlogUser(blog); return Result.ok(blog); } /** * 查询博客用户 * @param blog */ private void queryBlogUser(Blog blog) { Long userId = blog.getUserId(); User user = userService.getById(userId); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); } \"]},{\"header\":\"2、点赞\",\"slug\":\"_2、点赞\",\"contents\":[]},{\"header\":\"1、需求与步骤\",\"slug\":\"_1、需求与步骤\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求\",\"contents\":[\"同一个用户只能点赞一次，再次点击则取消点赞\",\"如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性）\"]},{\"header\":\"2、步骤\",\"slug\":\"_2、步骤\",\"contents\":[\"给Blog类中添加一个isLike字段，标示是否被当前用户点赞\",\"修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1\",\"修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段\",\"修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[\"SADD key member1 [member2\\\\] 向集合添加一个或多个成员 \",\"/** * 点赞 * @param id * @return */ @Override public Result likeBlog(Long id) { //获取登录用户 Long userId = UserHolder.getUser().getId(); //判断是否点赞 String key = \\\"blog:liked:\\\" + id; Boolean member = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if (BooleanUtil.isFalse(member)){ //数据库点赞数+1 boolean isSuccess = update().setSql(\\\"liked = liked + 1\\\").eq(\\\"id\\\", id).update(); if (isSuccess){ //保存到set集合 stringRedisTemplate.opsForSet().add(key, userId.toString()); } }else { //点赞取消 -1 boolean isSuccess = update().setSql(\\\"liked = liked - 1\\\").eq(\\\"id\\\", id).update(); if (isSuccess){ //从set集合中移除 stringRedisTemplate.opsForSet().remove(key, userId.toString()); } } } /** * 根据ID查询博客数据 * @param id * @return */ @Override public Result queryBlogById(Long id) { Blog blog = getById(id); if (null == blog){ return Result.fail(\\\"笔记不存在\\\"); } //查询博客用户 queryBlogUser(blog); //当期blog有没有被点赞 isBlogLiked(blog); return Result.ok(blog); } private void isBlogLiked(Blog blog) { //获取登录用户 UserDTO user = UserHolder.getUser(); //未登录不获取点赞 if (user == null) { return; } Long userId = UserHolder.getUser().getId(); //判断是否点赞 String key = RedisConstants.BLOG_LIKED_KEY + blog.getId(); Boolean member = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); blog.setIsLike(BooleanUtil.isTrue(member)); } \"]},{\"header\":\"3、点赞排行榜\",\"slug\":\"_3、点赞排行榜\",\"contents\":[\"在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜\"]},{\"header\":\"1、需求\",\"slug\":\"_1、需求-1\",\"contents\":[\"需求：按照点赞时间先后排序，返回Top5的用户\",\"** **\",\"List\",\"Set\",\"SortedSet\",\"排序方式\",\"按添加顺序排序\",\"无法排序\",\"根据score值排序\",\"唯一性\",\"不唯一\",\"唯一\",\"唯一\",\"查找方式\",\"按索引查找或首尾查找\",\"根据元素查找\",\"根据元素查找\",\"综合对比：满足唯一且排序的只有SortedSet\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-1\",\"contents\":[\"private void isBlogLiked(Blog blog) { //获取登录用户 UserDTO user = UserHolder.getUser(); //未登录不获取点赞 if (user == null) { return; } Long userId = UserHolder.getUser().getId(); //判断是否点赞 String key = RedisConstants.BLOG_LIKED_KEY + blog.getId(); Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); blog.setIsLike(null != score); } /** * 查询博客用户 * @param blog */ private void queryBlogUser(Blog blog) { Long userId = blog.getUserId(); User user = userService.getById(userId); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); } @Override public Result queryHotBlog(Integer current) { // 根据用户查询 Page<Blog> page = blogService.query() .orderByDesc(\\\"liked\\\") .page(new Page<>(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List<Blog> records = page.getRecords(); // 查询用户 records.forEach(item ->{ this.queryBlogUser(item); this.isBlogLiked(item); }); return Result.ok(records); } /** * 点赞 * @param id * @return */ @Override public Result likeBlog(Long id) { //获取登录用户 Long userId = UserHolder.getUser().getId(); //判断是否点赞 String key = \\\"blog:liked:\\\" + id; Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); if (null == score){ //数据库点赞数+1 boolean isSuccess = update().setSql(\\\"liked = liked + 1\\\").eq(\\\"id\\\", id).update(); if (isSuccess){ //保存到set集合 stringRedisTemplate.opsForZSet().add(key, userId.toString(),System.currentTimeMillis()); } }else { //点赞取消 -1 boolean isSuccess = update().setSql(\\\"liked = liked - 1\\\").eq(\\\"id\\\", id).update(); if (isSuccess){ //从set集合中移除 stringRedisTemplate.opsForZSet().remove(key, userId.toString()); } } return null; } /** * 点赞前五 * @param id * @return */ @Override public Result queryBlogLikes(Long id) { //查询top 5 zrange key 0 4 String key = RedisConstants.BLOG_LIKED_KEY + id; Set<String> top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); //解析用户iD if (null == top5 || top5.isEmpty()){ return Result.ok(Collections.emptyList()); } List<Long> list = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String ids = StrUtil.join(\\\",\\\", list); //查询用户返回 in(5,1) order by field(id, 5, 1) List<UserDTO> dtoList = userService.query().in(\\\"id\\\", list) .last(\\\"order by field(id, \\\" + ids +\\\")\\\").list() .stream() .map(user -> BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); //这里因为in 的原因，查询出来的顺序跟插入参数顺序不一致，改用order by field(id, 5, 1)，手动指定字段和s // List<UserDTO> dtoList = userService.listByIds(list) // .stream() // .map(user -> BeanUtil.copyProperties(user, UserDTO.class)) // .collect(Collectors.toList()); return Result.ok(dtoList); } \"]}]},\"/study-tutorial/distributed/redis/combat/user_sign.html\":{\"title\":\"7、用户签到\",\"contents\":[{\"header\":\"1、BitMap用法\",\"slug\":\"_1、bitmap用法\",\"contents\":[]},{\"header\":\"1、场景\",\"slug\":\"_1、场景\",\"contents\":[\"假如我们用一张表来存储用户签到信息，其结构应该如下：\",\"假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条 每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节\",\"很显然，这种做法占用空间过大，那有没有更好的做法呢？\"]},{\"header\":\"优化版\",\"slug\":\"优化版\",\"contents\":[\"我们按月来统计用户签到信息，签到记录为1，未签到则记录为0.\",\"例：张三2022年1月的签到记录，key为张三：202201，值为0或1组成的bit，那这样一个月也就2字节\",\"把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）\",\"Redis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位\"]},{\"header\":\"2、用法\",\"slug\":\"_2、用法\",\"contents\":[\"Redis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。\",\"BitMap的操作命令有：\",\"SETBIT：向指定位置（offset）存入一个0或1\",\"GETBIT ：获取指定位置（offset）的bit值\",\"BITCOUNT ：统计BitMap中值为1的bit位的数量\",\"BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值\",\"BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回\",\"BITOP ：将多个BitMap的结果做位运算（与 、或、异或）\",\"BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置\",\"localhost:0>setbit zs 0 1 \\\"0\\\" localhost:0>setbit zs 1 0 \\\"0\\\" localhost:0>getbit zs 1 \\\"0\\\" localhost:0>bitcount zs \\\"1\\\" localhost:0>bitpos zs 1 \\\"0\\\" \"]},{\"header\":\"2、签到功能\",\"slug\":\"_2、签到功能\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求\",\"contents\":[\"实现签到接口，将当前用户当天签到信息保存到Redis中\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现\",\"contents\":[\"提示：因为BitMap底层是基于String数据结构，因此其操作也都封装在字符串相关操作中了。\",\"/** * 签到 * @return */ @Override public Result sign() { //获取登录用户 Long userId = UserHolder.getUser().getId(); //获取日期，获取today是当前月的第几天 LocalDateTime time = LocalDateTime.now(); String keySuffix = time.format(DateTimeFormatter.ofPattern(\\\":yyyyMM\\\")); int dayOfMonth = time.getDayOfMonth(); //存入redis String key = RedisConstants.USER_SIGN_KEY + userId + keySuffix; stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true); return Result.ok(); } \"]},{\"header\":\"3、签到统计\",\"slug\":\"_3、签到统计\",\"contents\":[\"Q1：什么叫做连续签到天数？\",\"从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。\",\"Q2：如何得到本月到今天为止的所有签到数据？\",\"BITFIELD key GET u[dayOfMonth] 0 \",\"Q3：如何从后向前遍历每个bit位？\",\"与 1 做与运算，就能得到最后一个bit位。 随后右移1位，下一个bit位就成为了最后一个bit位。\"]},{\"header\":\"实现签到统计功能\",\"slug\":\"实现签到统计功能\",\"contents\":[]},{\"header\":\"1、需求\",\"slug\":\"_1、需求-1\",\"contents\":[\"统计当前用户截止当前时间在本月的连续签到天数\"]},{\"header\":\"2、实现\",\"slug\":\"_2、实现-1\",\"contents\":[\"@Override public Result signCount() { //获取登录用户 Long userId = UserHolder.getUser().getId(); //获取日期，获取today是当前月的第几天 LocalDateTime time = LocalDateTime.now(); String keySuffix = time.format(DateTimeFormatter.ofPattern(\\\":yyyyMM\\\")); int dayOfMonth = time.getDayOfMonth(); //存入redis String key = RedisConstants.USER_SIGN_KEY + userId + keySuffix; //获取本月截止 今天为止的所有签到记录，返回一个十进制的数字 List<Long> result = stringRedisTemplate.opsForValue().bitField(key, BitFieldSubCommands.create().get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0)); //循环遍历 if (null == result || result.isEmpty()) { return Result.ok(0); } Long num = result.get(0); if (null == num || 0 == num) { return Result.ok(0); } //让这个数字与1做与运算，得到数字的最后一个bit位 int count = 0; while (true) { //判断是否为0，如果为0，则表示未签到，结束，不为0，已签到，计数器+1 if (0 == (num & 1)) { break; }else { count++; } //把数字右移一位，抛弃最后一个bit位，继续下一个bit位 num >>>= 1; } return Result.ok(count); } \"]}]},\"/study-tutorial/distributed/redis/combat/uv_statistics.html\":{\"title\":\"8、UV统计\",\"contents\":[{\"header\":\"1、HyperLogLog用法\",\"slug\":\"_1、hyperloglog用法\",\"contents\":[\"首先我们搞懂两个概念：\",\"UV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。\",\"UV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖。\",\"Hyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0\",\"Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。\"]},{\"header\":\"2、实现UV统计\",\"slug\":\"_2、实现uv统计\",\"contents\":[\"@Test public void testHyperLogLog(){ String[] values = new String[1000]; int j = 0; for (int i = 0; i < 1000000; i++) { j = i % 1000; values[j] = \\\"user_\\\" + i; if (999 == j){ stringRedisTemplate.opsForHyperLogLog().add(\\\"hl\\\",values); } } Long count = stringRedisTemplate.opsForHyperLogLog().size(\\\"hl\\\"); System.out.println(count); } \"]},{\"header\":\"3、总结\",\"slug\":\"_3、总结\",\"contents\":[\"HyperLogLog的作用：\",\"做海量数据的统计工作\",\"HyperLogLog的优点：内存占用极低、性能非常好\",\"HyperLogLog的缺点：有一定的误差\"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/dubbo_zookeeper.html\":{\"title\":\"3、dubbo+zookeeper\",\"contents\":[{\"header\":\"1、dubbo\",\"slug\":\"_1、dubbo\",\"contents\":[\"Apache Dubbo |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。\",\"官网：https://dubbo.apache.org/en-us/\",\"从github上下载dubbo-admin，解压到当前文件夹，然后命令行执行mvn clean package进行打包\"]},{\"header\":\"2、Zookeeper(动物园管理员)\",\"slug\":\"_2、zookeeper-动物园管理员\",\"contents\":[\"一句话：ZooKeeper 是一个分布式协调技术、高性能的，开源的分布式系统的协调(Coordination)服务，是 Google的 Chubby 一个开源的实现，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用程序一致性和分布式协调技术服务的软件\",\"ZooKeeper=类似Unix 文件系统+通知机制+Znode 节点\",\"作用：服务注册+分布式系统的一致性通知协调\",\"官网：https://zookeeper.apache.org/\"]},{\"header\":\"1、统一命名服务\",\"slug\":\"_1、统一命名服务\",\"contents\":[\"统一命名服务（Name Service 如Dubbo 服务注册中心)\",\"Dubbo 是一个分布式服务框架，致力于提供高性能和透明化的RPC 远程服务调用方案，是阿里巴巴SOA 服务化治\",\"理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的成\",\"员站点。\",\"在Dubbo 实现中：\",\"服务提供者在启动的时候，向ZK 上的指定节点/dubbo/${serviceName}/providers 目录下写入自己的URL 地址，\",\"这个操作就完成了服务的发布。服务消费者启动的时候， 订阅/dubbo/${serviceName}/providers 目录下的提供\",\"者URL 地址， 并向/dubbo/${serviceName} /consumers 目录下写入自己的URL 地址。\",\"注意，所有向ZooKeeper 上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的\",\"变化。另外，Dubbo 还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者\",\"的信息。\"]},{\"header\":\"2、Zookeeper安装（Windows）\",\"slug\":\"_2、zookeeper安装-windows\",\"contents\":[\"从官网中下载解压\",\"配置文件相关参数\",\"**1、tickTime：**通信心跳数,Zookeeper 服务器心跳时间，单位毫秒\",\"ZooKeeper 使用的基本时间， 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime 时\",\"间就会发送一个心跳,时间单位为毫秒。\",\"它用于心跳机制，并且设置最小的session 超时时间为两倍心跳时间.(session 的最小超时时间是2*tickTime。)\",\"2、initLimit：\",\"这个配置项是用来配置ZooKeeper 接收Follower 客户端（这里所说的客户端不是用户链接Zookeeper 服务\",\"器的客户端，而是ZooKeeper 服务器集群中连接到leader 的Follower 服务器,Follower 在启动过程中，会从\",\"Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。Leader 允许Follower 在initLimit 时间内完成\",\"这个工作）初始化连接是最长能忍受多少个心跳的时间间隔数。\",\"当已经超过10 个心跳的时间（也就是tickTime）长度后Zookeeper 服务器还没有收到客户端返回的信息，那么\",\"表明这个客户端连接失败。总的时间长度就是102000=20 秒\",\"3、syncLimit：\",\"LF 同步通信时限。集群中Leader 与Follower 之间的最大响应时间单位。\",\"在运行过程中，Leader 负责与ZK 集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活\",\"状态,假如响应超过syncLimit * tickTime(假设syncLimit=5 ，请求和应答时间长度，最长不能超过多少个tickTime 的时间长度，总的时间长度就是5*2000=10 秒。)，Leader 认为Follwer 死掉，从服务器列表中删除Follwer。\",\"4、dataDir：\",\"数据文件目录+数据持久化路径。\",\"保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。\",\"5、clientPort：\",\"客户端连接端口，监听客户端连接的端口。\",\"注：\",\"启动Zookeeper 服务之前需要先安装好Java 环境\"]},{\"header\":\"3、dubbo+zookeeper测试\",\"slug\":\"_3、dubbo-zookeeper测试\",\"contents\":[\"启动zkServer.cmd以及执行dubbo的jar包，访问http://localhost:8080/#/，输入用户名密码root-root\"]},{\"header\":\"4、整合\",\"slug\":\"_4、整合\",\"contents\":[\"导入依赖\",\" <!-- https://mvnrepository.com/artifact/org.apache.dubbo/dubbo-spring-boot-starter --> <dependency> <groupId>org.apache.dubbo</groupId> <artifactId>dubbo-spring-boot-starter</artifactId> <version>2.7.5</version> </dependency> <!-- https://mvnrepository.com/artifact/com.github.sgroschupf/zkclient --> <dependency> <groupId>com.github.sgroschupf</groupId> <artifactId>zkclient</artifactId> <version>0.1</version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.curator/curator-framework --> <dependency> <groupId>org.apache.curator</groupId> <artifactId>curator-framework</artifactId> <version>4.3.0</version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.curator/curator-recipes --> <dependency> <groupId>org.apache.curator</groupId> <artifactId>curator-recipes</artifactId> <version>4.3.0</version> </dependency> <!-- 可能会有日志冲突，排除 --> <dependency> <groupId>org.apache.zookeeper</groupId> <artifactId>zookeeper</artifactId> <version>3.6.0</version> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>org.slf4j:slf4j-log4j12:1.7.30</artifactId> </exclusion> </exclusions> </dependency> \",\"provider-service\",\"public interface TicketService { public String getTicket(); } \",\"@Service //import org.apache.dubbo.config.annotation.Service; @Component //使用dubbo，尽量不用service注解 public class TicketServiceImpl implements TicketService { @Override public String getTicket() { return \\\"XIAOBEAR\\\"; } } \",\"server.port=8082 #服务应用名字 dubbo.application.name=provider-server #注册中心地址 dubbo.registry.address=zookeeper://127.0.0.1:2181 #哪些需要被注册 dubbo.scan.base-packages=com.xiaobear.service \",\"client-service\",\"@Service //放到容器中 import org.springframework.stereotype.Service; public class UserService { //去注册中心拿到Ticket @Reference //pom坐标 可以定义相同的接口名 //import org.apache.dubbo.config.annotation.Reference; TicketService ticketService; public void buyTicket() { String ticket = ticketService.getTicket(); System.out.println(\\\"在注册中心拿到一张票=>\\\"+ticket); } } \",\"public interface TicketService { public String getTicket(); } \",\"启动项目provider-service和client-service，访问http://localhost:8080/，success！！！\",\"当然，这只是简单的部署，还需深入学习！！！\"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/jedis.html\":{\"title\":\"4、Jedis\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"导入依赖\",\"<!-- https://mvnrepository.com/artifact/redis.clients/jedis --> <dependency> <groupId>redis.clients</groupId> <artifactId>jedis</artifactId> <version>3.2.0</version> </dependency> \",\"JedisConfig配置类\",\"@Configuration //相当于XML public class JedisConfig { /** logger */ private Logger LOGGER = LoggerFactory.getLogger(JedisConfig.class); @Value(\\\"${spring.redis.host}\\\") private String host; @Value(\\\"${spring.redis.port}\\\") private int port; @Value(\\\"${spring.redis.password}\\\") private String password; @Value(\\\"${spring.redis.timeout}\\\") private int timeout; @Value(\\\"${spring.redis.jedis.pool.max-active}\\\") private int maxActive; @Value(\\\"${spring.redis.jedis.pool.max-idle}\\\") private int maxIdle; @Value(\\\"${spring.redis.jedis.pool.min-idle}\\\") private int minIdle; @Bean public JedisPool jedisPool(){ JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxIdle(maxIdle); jedisPoolConfig.setMinIdle(minIdle); jedisPoolConfig.setMaxTotal(maxActive); JedisPool jedisPool = new JedisPool(jedisPoolConfig,host,port,timeout,password); LOGGER.info(\\\"JedisPool连接成功！！！\\\"+host+\\\"\\\\t\\\"+port); return jedisPool; } } \",\"User类\",\"@Data public class User implements Serializable { private String id; private String name; private Integer age; } \",\"UserService\",\"public interface UserService { /* * Redis String * 用户输入一个key * 先判断有无该数据 * 有则查询，无则在mysql中查询并返回 * * */ public String getString(String key); public void expireStr(String key,String value); public User selectById(String id); } \",\"UserServiceImpl\",\"@Component @Log // <-> private Logger LOGGER = LoggerFactory.getLogger(JedisConfig.class); public class UserServiceImpl implements UserService { @Autowired private JedisPool jedisPool; @Autowired private JedisUtil jedisUtil; /*redis中有什么命令<->Jedis中有什么方法 * Redis String * 用户输入一个key * 先判断有无该数据 * 有则查询，无则在mysql中查询并返回 * * */ @Override public String getString(String key) { String val =null; Jedis resource = jedisPool.getResource(); //判断是否存在 if (resource.exists(key)){ log.info(\\\"查询redis中的数据\\\"); val=resource.get(key); }else { val=\\\"yhxlovelwh\\\"; log.info(\\\"查询mysql中的数据\\\"+val); resource.set(key, val); } //关闭连接 resource.close(); return val; } @Override public void expireStr(String key, String value) { Jedis jedis = jedisUtil.getJedis(); jedis.set(key, value); jedis.expire(key,20); log.info(key+\\\"\\\\t 设置值\\\"+value+\\\"\\\\t\\\"+20); jedisUtil.close(jedis); } @Override public User selectById(String id){ String key = \\\"user:\\\"+id; //得到jedis对象 User user = new User(); Jedis jedis = jedisUtil.getJedis(); //实现业务逻辑判断 if (jedis.exists(key)){ log.info(\\\"查询的是redis的数据！\\\"); Map<String, String> map = jedis.hgetAll(key); user.setId(map.get(\\\"id\\\")); user.setName(map.get(\\\"name\\\")); user.setAge(Integer.parseInt(map.get(\\\"age\\\"))); }else { user.setId(\\\"10\\\"); user.setName(\\\"lwh\\\"); user.setAge(18); log.info(\\\"查询的是mysql的数据！\\\"); Map<String, String> map = new HashMap<>(); map.put(\\\"id\\\",user.getId()); map.put(\\\"name\\\",user.getName()); map.put(\\\"age\\\",user.getAge()+\\\"\\\"); log.info(\\\"在redis中存取数据！\\\"); jedis.hmset(key,map); } //关闭连接 jedisUtil.close(jedis); return user; } } \",\"application.yml\",\"server: port: 8080 spring: redis: host: 192.168.199.129 port: 6379 password: 密码 jedis: pool: max-idle: 6 #\\\\u6700\\\\u5927\\\\u7A7A\\\\u95F2\\\\u6570 max-active: 10 #\\\\u6700\\\\u5927\\\\u8FDE\\\\u63A5\\\\u6570 min-idle: 2 #\\\\u6700\\\\u5C0F\\\\u7A7A\\\\u95F2\\\\u6570 timeout: 2000 #\\\\u8FDE\\\\u63A5\\\\u8D85\\\\u65F6 \",\"测试类\",\"@SpringBootTest class SpringbootJedisApplicationTests { @Autowired private JedisPool jedisPool; @Autowired private UserService userService; @Test void contextLoads() { String result = userService.getString(\\\"name\\\"); System.out.println(result); } @Test void Test(){ String key=\\\"testkey\\\"; String val=\\\"测试\\\"; userService.expireStr(key,val); } @Test void Test2(){ User user = userService.selectById(\\\"1004\\\"); System.out.println(user); } } \"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/lettuce.html\":{\"title\":\"5、lettuce（Redis）\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"现建项目\",\"导入依赖\",\"<!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-redis --> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-redis</artifactId> <version>2.2.6.RELEASE</version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.commons/commons-pool2 --> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-pool2</artifactId> <version>2.8.0</version> </dependency> \",\"重写RedisTemplate模板\",\"@Configuration public class RedisConfig extends JCacheConfigurerSupport { @Bean public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate<String, Object> redisTemplate = new RedisTemplate<>(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize 替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和 key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); //hash的value序列化采用jackson2 redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; } } \",\"User实体类\",\"@Data public class User implements Serializable { private String id; private String name; private Integer age; public static String getKeyName(){ return \\\"user\\\"; } } \",\"UserService实现接口\",\"public interface UserService { /* * Redis String * 用户输入一个key * 先判断有无该数据 * 有则查询，无则在mysql中查询并返回 * */ public String getString(String key); public void expireStr(String key,String value); public User selectById(String id); //根据id查询 } \",\"UserServiceImpl实现类\",\"@Service @Slf4j public class UserServiceImpl implements UserService{ @Autowired private RedisTemplate<String,Object> redisTemplate; /* *测试String类型 * */ @Override public String getString(String key) { if(redisTemplate.hasKey(key)){ log.info(\\\"redis中查询数据！\\\"); return (String) redisTemplate.opsForValue().get(key); }else { String val = \\\"redisTemplate学习lettuce客户端\\\"; log.info(\\\"MYSQL中查询数据！\\\"+val); redisTemplate.opsForValue().set(key,val); log.info(\\\"存入redis中\\\"); return val; } } @Override public void expireStr(String key, String value) { redisTemplate.opsForValue().set(key, value); redisTemplate.expire(key,2, TimeUnit.HOURS);//设置key的时间 } //测试hash类型 @Override public User selectById(String id) { // redisTemplate.hasKey(key); if (redisTemplate.opsForHash().hasKey(KeyNameUtil.USER,id)) { return (User) redisTemplate.opsForHash().get(\\\"user\\\",id); }else { log.info(\\\"查询MYSQL数据库！！！\\\"); User user = new User(); user.setId(\\\"1\\\"); user.setName(\\\"yhx\\\"); user.setAge(18); //.put()用户实体user 主键id 整个对象 redisTemplate.opsForHash().put(User.getKeyName(),id,user); return user; } } /* * 问题1：出现了很多的字符串->提取出来 * ->1.声明一个工具类 *->2.在实体bean，声明一个方法 * 问题2：强制类型转换问题 * 问题3： redisTemplate.opsForHash()写很长一串 * @Resource(name = \\\"redisTemplate\\\") * private RedisTemplate<String,String> redis; * */ } \",\"KeyNameUtil工具类\",\"public interface KeyNameUtil { String USER = \\\"user\\\"; } \",\"配置文件\",\"server: port: 8081 spring: redis: host: 192.168.199.129 port: 6379 password: 密码 lettuce: pool: max-idle: 6 max-active: 10 min-idle: 2 timeout: 2000 \",\"Jedis与Redis的区别：\"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/shiro.html\":{\"title\":\"1、shiro\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"shiro简介\",\"Apache Shiro™是一个强大且易用的Java安全框架,能够用于身份验证、授权、加密和会话管理。Shiro拥有易于理解的API,您可以快速、轻松地获得任何应用程序——从最小的移动应用程序到最大的网络和企业应用程序。\",\"简而言之，Apache Shiro 是一个强大灵活的开源安全框架，可以完全处理身份验证、授权、加密和会话管理。\"]},{\"header\":\"1、功能\",\"slug\":\"_1、功能\",\"contents\":[\"验证用户身份\",\"用户访问权限控制，比如：1、判断用户是否分配了一定的安全角色。2、判断用户是否被授予完成某个操作的权限\",\"在非 Web 或 EJB 容器的环境下可以任意使用Session API\",\"可以响应认证、访问控制，或者 Session 生命周期中发生的事件\",\"可将一个或以上用户安全数据源数据组合成一个复合的用户 “view”(视图)\",\"支持单点登录(SSO)功能\",\"支持提供“Remember Me”服务，获取用户关联信息而无需登录 ···\"]},{\"header\":\"2、特点\",\"slug\":\"_2、特点\",\"contents\":[\"易于使用——易用性是项目的最终目标。应用程序安全非常令人困惑和沮丧,被认为是“不可避免的灾难”。如果你让它简化到新手都可以使用它,它就将不再是一种痛苦了。\",\"全面——没有其他安全框架的宽度范围可以同Apache Shiro一样,它可以成为你的“一站式”为您的安全需求提供保障。\",\"灵活——Apache Shiro可以在任何应用程序环境中工作。虽然在网络工作、EJB和IoC环境中可能并不需要它。但Shiro的授权也没有任何规范,甚至没有许多依赖关系。\",\"Web支持——Apache Shiro拥有令人兴奋的web应用程序支持,允许您基于应用程序的url创建灵活的安全策略和网络协议(例如REST),同时还提供一组JSP库控制页面输出。\",\"低耦合——Shiro干净的API和设计模式使它容易与许多其他框架和应用程序集成。你会看到Shiro无缝地集成Spring这样的框架, 以及Grails, Wicket, Tapestry, Mule, Apache Camel, Vaadin…等。\",\"被广泛支持——Apache Shiro是Apache软件基金会的一部分。项目开发和用户组都有友好的网民愿意帮助。这样的商业公司如果需要Katasoft还提供专业的支持和服务。\"]},{\"header\":\"3、Apache Shiro Features 特性\",\"slug\":\"_3、apache-shiro-features-特性\",\"contents\":[\"Shiro以Shiro开发团队所谓的“应用程序安全性的四个基石”为目标-身份验证，授权，会话管理和密码术：\",\"**身份验证：**有时称为“登录”，这是证明用户就是他们所说的身份的行为。\",\"**授权：**访问控制的过程，即确定“谁”有权访问“什么”。\",\"**会话管理：**即使在非Web或EJB应用程序中，也可以管理用户特定的会话。\",\"**密码术：**使用密码算法保持数据安全，同时仍然易于使用。\",\"具体参见官网https://shiro.apache.org/\"]},{\"header\":\"4、shiro认证过程\",\"slug\":\"_4、shiro认证过程\",\"contents\":[]},{\"header\":\"5、shiro快速开始\",\"slug\":\"_5、shiro快速开始\",\"contents\":[\"创建一个maven或者Spring Boot工程\",\"导入依赖\",\" <!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-core --> <dependency> <groupId>org.apache.shiro</groupId> <artifactId>shiro-core</artifactId> <version>1.5.2</version> </dependency> <!-- configure logging --> <dependency> <groupId>org.slf4j</groupId> <artifactId>jcl-over-slf4j</artifactId> <version>1.7.29</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> <version>1.7.29</version> </dependency> <!-- https://mvnrepository.com/artifact/log4j/log4j --> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>1.2.17</version> </dependency> \",\"日志文件logging.properties\",\"log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m %n ## General Apache libraries log4j.logger.org.apache=WARN ## Spring log4j.logger.org.springframework=WARN ## Default Shiro logging log4j.logger.org.apache.shiro=INFO ## Disable verbose logging log4j.logger.org.apache.shiro.util.ThreadContext=WARN log4j.logger.org.apache.shiro.cache.ehcache.EhCache=WARN \",\"在类路径下创建shiro.ini，不下载ini插件的话，就是一个普通的文本，建议下载一下\",\"[users] ## user 'root' with password 'secret' and the 'admin' role root = secret, admin ## user 'guest' with the password 'guest' and the 'guest' role guest = guest, guest ## user 'presidentskroob' with password '12345' (\\\"That's the same combination on ## my luggage!!!\\\" ;)), and role 'president' presidentskroob = 12345, president ## user 'darkhelmet' with password 'ludicrousspeed' and roles 'darklord' and 'schwartz' darkhelmet = ludicrousspeed, darklord, schwartz ## user 'lonestarr' with password 'vespa' and roles 'goodguy' and 'schwartz' lonestarr = vespa, goodguy, schwartz ## ----------------------------------------------------------------------------- ## Roles with assigned permissions # ## Each line conforms to the format defined in the ## org.apache.shiro.realm.text.TextConfigurationRealm#setRoleDefinitions JavaDoc ## ----------------------------------------------------------------------------- [roles] ## 'admin' role has all permissions, indicated by the wildcard '*' admin = * ## The 'schwartz' role can do anything (*) with any lightsaber: schwartz = lightsaber:* ## The 'goodguy' role is allowed to 'drive' (action) the winnebago (type) with ## license plate 'eagle5' (instance specific id) goodguy = winnebago:drive:eagle5 \",\"在java目录下（Spring Boot跟启动类在同一个包下）创建Quickstart.java\",\"public class Quickstart { private static final transient Logger log = LoggerFactory.getLogger(Quickstart.class); public static void main(String[] args) { Factory<SecurityManager> factory = new IniSecurityManagerFactory(\\\"classpath:shiro.ini\\\"); SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); //获取当前的用户对象subject Subject currentUser = SecurityUtils.getSubject(); //通过当前用户拿到session Session session = currentUser.getSession(); session.setAttribute(\\\"someKey\\\", \\\"aValue\\\"); String value = (String) session.getAttribute(\\\"someKey\\\"); if (value.equals(\\\"aValue\\\")) { log.info(\\\"Retrieved the correct value! [\\\" + value + \\\"]\\\"); } // 判断当前的用户是否被认证 if (!currentUser.isAuthenticated()) { //token令牌，没有获取，随机设置 UsernamePasswordToken token = new UsernamePasswordToken(\\\"lonestarr\\\", \\\"vespa\\\"); token.setRememberMe(true);//记住我 try { currentUser.login(token);//执行了登录操作 } catch (UnknownAccountException uae) {//未知用户 log.info(\\\"There is no user with username of \\\" + token.getPrincipal()); } catch (IncorrectCredentialsException ice) {//错误密码 log.info(\\\"Password for account \\\" + token.getPrincipal() + \\\" was incorrect!\\\"); } catch (LockedAccountException lae) {//用户被锁定 log.info(\\\"The account for username \\\" + token.getPrincipal() + \\\" is locked. \\\" + \\\"Please contact your administrator to unlock it.\\\"); } // ... catch more exceptions here (maybe custom ones specific to your application? catch (AuthenticationException ae) { //unexpected condition? error? } } //say who they are: //print their identifying principal (in this case, a username): log.info(\\\"User [\\\" + currentUser.getPrincipal() + \\\"] logged in successfully.\\\"); //test a role:测试角色 if (currentUser.hasRole(\\\"schwartz\\\")) { log.info(\\\"May the Schwartz be with you!\\\"); } else { log.info(\\\"Hello, mere mortal.\\\"); } //test a typed permission (not instance-level) if (currentUser.isPermitted(\\\"lightsaber:wield\\\")) { log.info(\\\"You may use a lightsaber ring. Use it wisely.\\\"); } else { log.info(\\\"Sorry, lightsaber rings are for schwartz masters only.\\\"); } //a (very powerful) Instance Level permission: if (currentUser.isPermitted(\\\"winnebago:drive:eagle5\\\")) { log.info(\\\"You are permitted to 'drive' the winnebago with license plate (id) 'eagle5'. \\\" + \\\"Here are the keys - have fun!\\\"); } else { log.info(\\\"Sorry, you aren't allowed to drive the 'eagle5' winnebago!\\\"); } //all done - log out!注销 currentUser.logout(); System.exit(0); } } \",\"启动Quickstart.java的main方法\",\"18:09:26.277 [main] INFO com.xiaobear.springboot.Quickstart - User [lonestarr] logged in successfully. 18:09:26.277 [main] INFO com.xiaobear.springboot.Quickstart - May the Schwartz be with you! 18:09:26.277 [main] INFO com.xiaobear.springboot.Quickstart - You may use a lightsaber ring. Use it wisely. 18:09:26.278 [main] INFO com.xiaobear.springboot.Quickstart - You are permitted to 'drive' the winnebago with license plate (id) 'eagle5'. Here are the keys - have fun! 18:09:26.278 [main] DEBUG org.apache.shiro.mgt.DefaultSecurityManager - Logging out subject with primary principal lonestarr 18:09:26.278 [main] DEBUG org.apache.shiro.session.mgt.AbstractSessionManager - Stopping session with id [8fce998d-1600-406e-b3db-1015dae0d42d] \",\"注意：\",\"如果没有导入日志门面log4j或者其他的，控制台就会爆红或者什么也不输出\"]},{\"header\":\"6、Spring Boot+Mybatis+Shiro\",\"slug\":\"_6、spring-boot-mybatis-shiro\",\"contents\":[\"新建项目，导入依赖\",\"<!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-spring-boot-web-starter --> <dependency> <groupId>org.apache.shiro</groupId> <artifactId>shiro-spring</artifactId> <version>1.5.2</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-devtools</artifactId> <scope>runtime</scope> <optional>true</optional> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <optional>true</optional> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-jpa</artifactId> </dependency> \",\"配置文件application.yml\",\"spring: datasource: url: jdbc:mysql://localhost:3306/mybatis?useSSL=true&useUnicode=true&characterEncoding=UTF-8&serverTimezone=GMT username: root password: 密码 driver-class-name: com.mysql.cj.jdbc.Driver mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: com.xiaobear.springbootshiro \",\"实体类\",\"@Data @AllArgsConstructor @NoArgsConstructor @Entity public class User { private int id; private String name; private String pwd; private String perms; } \",\"快速搭建mybatis环境\",\"UserMapper接口\",\"@Mapper @Repository public interface UserMapper { User queryUserByName(String name); } \",\"UserService接口\",\"public interface UserService { User queryUserByName(String name); } \",\"UserServiceImpl\",\"@Service public class UserServiceImpl implements UserService { @Autowired UserMapper userMapper; @Override public User queryUserByName(String name) { return userMapper.queryUserByName(name); } } \",\"UserMapper.xml\",\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" ?> <!DOCTYPE mapper PUBLIC \\\"-//mybatis.org//DTD Mapper 3.0//EN\\\" \\\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\\\"> <mapper namespace=\\\"com.xiaobear.springbootshiro.mapper.UserMapper\\\"> <select id=\\\"queryUserByName\\\" resultType=\\\"com.xiaobear.springbootshiro.pojo.User\\\"> select * from mybatis.user where name = #{name} </select> </mapper> \",\"UserRealm：自定义的Realm->授权与认证\",\"public class UserRealm extends AuthorizingRealm { @Autowired UserService userService; // 授权 @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { System.out.println(\\\"执行了=>授权doGetAuthorizationInfo\\\"); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermission(\\\"user:add\\\");//增加了授权add页面 //拿到当前登录的对象 Subject subject = SecurityUtils.getSubject(); User currentUser = (User) subject.getPrincipal();//拿到User对象 info.addStringPermission(currentUser.getPerms());//设置当前用户的权限 return info; } /*认证*/ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { System.out.println(\\\"执行了=>认证doGetAuthorizationInfo\\\"); //连接真实的数据库 UsernamePasswordToken usernamePasswordToken = (UsernamePasswordToken) authenticationToken; User user = userService.queryUserByName(usernamePasswordToken.getUsername()); if (user ** null) { return null; } Subject subject1 = SecurityUtils.getSubject(); Session session = subject1.getSession(); session.setAttribute(\\\"LoginUser\\\",user); //密码认证 return new SimpleAuthenticationInfo(user,user.getPwd(),\\\"\\\"); /* 内存中访问数据 String name=\\\"root\\\"; String password=\\\"123456\\\"; UsernamePasswordToken usernamePasswordToken = (UsernamePasswordToken) authenticationToken; if(!usernamePasswordToken.getUsername().equals(name)){ return null; } //密码认证 return new SimpleAuthenticationInfo(\\\"\\\",password,\\\"\\\"); */ } } \",\"ShiroConfig：\",\"@Configuration public class ShiroConfig { @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Qualifier(\\\"securityManager\\\") DefaultWebSecurityManager defaultWebSecurityManager){ ShiroFilterFactoryBean bean = new ShiroFilterFactoryBean(); // 设置安全管理器 bean.setSecurityManager(defaultWebSecurityManager); /*添加shiro内置过滤器 anon:例子/admins/**=anon 没有参数，表示可以匿名使用。 authc:例如/admins/user/**=authc表示需要认证(登录)才能使用，没有参数 authcBasic:例如/admins/user/**=authcBasic,没有参数表示httpBasic认证 perms :（权限）例子/admins/user/=perms[user:add:],参数可以写多个，多个时必须加上引号，并且参数之间用逗号分割，例如/admins/user/=perms[“user:add:,user:modify:*”]，当有多个参数时必须每个参数都通过才通过，想当于isPermitedAll()方法。 port :例子/admins/user/**=port[8081],当请求的url的端口不是8081是跳转到schemal://serverName:8081?queryString,其中schmal是协议http或https等，serverName是你访问的host,8081是url配置里port的端口，queryString rest :例子/admins/user/=rest[user],根据请求的方法，相当于/admins/user/=perms[user:method] ,其中method为post，get，delete等。 roles :(角色)例子/admins/user/=roles[admin],参数可以写多个，多个时必须加上引号，并且参数之间用逗号分割，当有多个参数时，例如admins/user/=roles[“admin,guest”],每个参数通过才算通过，相当于hasAllRoles()方法。 ssl :例子/admins/user/**=ssl没有参数，表示安全的url请求，协议为https user :例如/admins/user/**=user没有参数表示必须存在用户，当登入操作时不做检查 */ HashMap<String, String> filterMap= new LinkedHashMap<>(); /* filterMap.put(\\\"/user/add\\\",\\\"authc\\\"); filterMap.put(\\\"/user/delete\\\",\\\"authc\\\"); */ filterMap.put(\\\"/user/add\\\",\\\"perms[user:add]\\\"); filterMap.put(\\\"/user/delete\\\",\\\"perms[user:delete]\\\"); filterMap.put(\\\"/user/*\\\",\\\"authc\\\"); bean.setFilterChainDefinitionMap(filterMap); //登录页面 bean.setLoginUrl(\\\"/tologin\\\"); //未授权页面 bean.setUnauthorizedUrl(\\\"/unauthorized\\\"); return bean; } @Bean(name = \\\"securityManager\\\") public DefaultWebSecurityManager getDefaultWebSecurityManager(@Qualifier(\\\"userRealm\\\") UserRealm userRealm){ DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); /*关联userRealm*/ securityManager.setRealm(userRealm); return securityManager; } @Bean public UserRealm userRealm(){ return new UserRealm(); } //整合shiroDialect @Bean public ShiroDialect getshiroDialect(){ return new ShiroDialect(); } } \",\"Controller层\",\"@Controller public class MyController { @RequestMapping({\\\"/\\\",\\\"/index\\\"}) public String ToIndex(Model model){ model.addAttribute(\\\"msg\\\",\\\"hello,shiro\\\"); return \\\"index\\\"; } @RequestMapping(\\\"/user/add\\\") public String add(){ return \\\"user/add\\\"; } @RequestMapping(\\\"/user/delete\\\") public String delete(){ return \\\"user/delete\\\"; } @RequestMapping(\\\"/tologin\\\") public String toLogin() { return \\\"login\\\"; } @RequestMapping(\\\"/login\\\") public String login(String username, String password, Model model){ Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(username,password); try { subject.login(token);//执行登录的方法 return \\\"index\\\"; }catch (UnknownAccountException e) { model.addAttribute(\\\"msg\\\",\\\"用户名错误\\\"); }catch (IncorrectCredentialsException e){ model.addAttribute(\\\"msg\\\",\\\"密码错误\\\"); } return \\\"login\\\"; } @RequestMapping(\\\"/unauthorized\\\") @ResponseBody public String unauthorized(){ return \\\"未经授权无法访问\\\"; } } \",\"页面准备，因只用于学习测试，页面很简陋\",\"index.html\",\"<!DOCTYPE html> <html lang=\\\"en\\\" xmlns:th=\\\"http://www.thymeleaf.org\\\" xmlns:shiro=\\\"http://www.w3.org/1999/xhtml\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>首页</title> </head> <body> <h1>hello xiaobear</h1> <p th:text=\\\"${msg}\\\"></p> <hr> <div th:if=\\\"${session.LoginUser**null}\\\"> <a th:href=\\\"@{/tologin}\\\">登录</a> </div> <div shiro:hasPermission=\\\"user:add\\\"> <a th:href=\\\"@{/user/add}\\\">add</a> </div> <div shiro:haspermission=\\\"user:delete\\\"> <a th:href=\\\"@{/user/delete}\\\">delete</a> </div> </body> </html> \",\"login.html\",\"<!DOCTYPE html> <html lang=\\\"en\\\" xmlns:th=\\\"http://www.w3.org/1999/xhtml\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>login</title> </head> <body> <p th:text=\\\"${msg}\\\" style=\\\"color: red\\\"></p> <form th:action=\\\"@{/login}\\\"> <p>用户名 <input type=\\\"text\\\" name=\\\"username\\\"></p> <p>密码 <input type=\\\"text\\\" name=\\\"password\\\"></p> <p><input type=\\\"submit\\\"></p> </form> </body> </html> \",\"user/add.html\",\"<!DOCTYPE html> <html lang=\\\"en\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>add</title> </head> <body> <h1>add</h1> </body> </html> \",\"user/delete.html\",\"<!DOCTYPE html> <html lang=\\\"en\\\"> <head> <meta charset=\\\"UTF-8\\\"> <title>delete</title> </head> <body> <h1>delete</h1> </body> </html> \"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/spring_boot_admin.html\":{\"title\":\"6、Spring-Boot-admin\",\"contents\":[{\"header\":\"What is Spring Boot Admin?\",\"slug\":\"what-is-spring-boot-admin\",\"contents\":[\"codecentric’s Spring Boot Admin is a community project to manage and monitor your Spring Boot ® applications. The applications register with our Spring Boot Admin Client (via HTTP) or are discovered using Spring Cloud ® (e.g. Eureka, Consul). The UI is just a Vue.js application on top of the Spring Boot Actuator endpoints.\"]},{\"header\":\"1、设置Spring Boot Admin Server\",\"slug\":\"_1、设置spring-boot-admin-server\",\"contents\":[\"导入依赖\",\"<!-- https://mvnrepository.com/artifact/de.codecentric/spring-boot-admin-server --> <dependency> <groupId>de.codecentric</groupId> <artifactId>spring-boot-admin-server</artifactId> <version>2.2.2</version> </dependency> \",\"添加@EnableAdminServer到配置中来引入Spring Boot Admin Server 配置\",\"@EnableAdminServer @SpringBootApplication public class SpringBootDemoAdminServerApplication { public static void main(String[] args) { SpringApplication.run(SpringBootDemoAdminServerApplication.class, args); } } \",\"配置文件application.yaml配置端口\",\"server: port: 8081 \"]},{\"header\":\"2、Spring Boot Admin Client\",\"slug\":\"_2、spring-boot-admin-client\",\"contents\":[\"导入依赖\",\"<!-- https://mvnrepository.com/artifact/de.codecentric/spring-boot-admin-starter-client --> <dependency> <groupId>de.codecentric</groupId> <artifactId>spring-boot-admin-starter-client</artifactId> <version>2.2.2</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-security</artifactId> </dependency> \",\"配置application.yml\",\"server: port: 8080 servlet: context-path: /demo spring: application: ## Spring Boot Admin展示的客户端项目名，不设置，会使用自动生成的随机id name: spring-boot-demo-admin-client boot: admin: client: ## Spring Boot Admin 服务端地址 url: \\\"http://localhost:8000/\\\" instance: metadata: ## 客户端端点信息的安全认证信息 user.name: ${spring.security.user.name} user.password: ${spring.security.user.password} security: user: name: xiaobear password: 123456 management: endpoint: health: ## 端点健康情况，默认值\\\"never\\\"，设置为\\\"always\\\"可以显示硬盘使用情况和线程情况 show-details: always endpoints: web: exposure: ## 设置端点暴露的哪些内容，默认[\\\"health\\\",\\\"info\\\"]，设置\\\"*\\\"代表暴露所有可访问的端点 include: \\\"*\\\" \",\"编写简单的Controller类\",\"@RestController public class IndexController { @GetMapping(value = {\\\"\\\", \\\"/\\\"}) public String index() { return \\\"Hello,yhx\\\"; } } \",\"同时启动项目\",\"一个简单的Spring Boot Admin整合就完成了，当然还可以整合更多的，具体参照官方文档！！！\",\"https://codecentric.github.io/spring-boot-admin/2.1.6/#getting-started\"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/swagger.html\":{\"title\":\"2、Swagger\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"号称世界上最流行的Api框架\",\"RestFul Api 文档在线自动生成工具-->Api文档与Api定义同步更新\",\"直接运行，可以在线测试API接口\",\"支持多种语言（Java、php）\",\"Simplify API development for users, teams, and enterprises with the Swagger open source and professional toolset. Find out how Swagger can help you design and document your APIs at scale.\",\"借助Swagger开源和专业工具集，为用户，团队和企业简化API开发。了解Swagger如何帮助您大规模设计和记录API。\"]},{\"header\":\"1、快速开始Swagger\",\"slug\":\"_1、快速开始swagger\",\"contents\":[\"新建Spring Boot项目\",\"创建hello工程\",\"导入Swagger相关依赖\",\"<!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger-ui --> <dependency> <groupId>io.springfox</groupId> <artifactId>springfox-swagger-ui</artifactId> <version>2.9.2</version> </dependency> <!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --> <dependency> <groupId>io.springfox</groupId> <artifactId>springfox-swagger2</artifactId> <version>2.9.2</version> </dependency> \",\"HelloController层\",\"@RestController public class HelloController { @RequestMapping(value = \\\"/hello\\\") public String hello() { return \\\"xiaobear,hello\\\"; } } \",\"测试：http://localhost:8080/hello以及http://localhost:8080/swagger-ui.html\"]},{\"header\":\"2、Swagger配置\",\"slug\":\"_2、swagger配置\",\"contents\":[\"@Configuration @EnableSwagger2 //开启swagger2 public class SwaggerConfig { // 配置了swagger的docket实例 @Bean public Docket decket() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()); } // 配置swagger信息=apiInfo private ApiInfo apiInfo() { //作者信息 Contact contact = new Contact(\\\"xiaobear\\\",\\\"http://xiaobear.cn/\\\",\\\"2861184805@qq.com\\\"); return new ApiInfo( \\\"Spring Boot整合Swagger\\\", \\\"你什么都想要，凭什么不努力\\\", \\\"1.0\\\", \\\"http://xiaobear.cn/\\\", contact, \\\"Apache 2.0\\\", \\\"http://www.apache.org/licenses/LICENSE-2.0\\\", new ArrayList()); } } \"]},{\"header\":\"3、Swagger配置扫描接口\",\"slug\":\"_3、swagger配置扫描接口\",\"contents\":[\"@Configuration @EnableSwagger2 //开启swagger2 public class SwaggerConfig { // 配置了swagger的docket实例 @Bean public Docket decket() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) // enable是否开启swagger，if false，咋不能在浏览器中访问 //.enable(false) .select() //RequestHandlerSelectors：配置要扫描接口的方式 //basePackage：指定扫描的包 //any：扫描全部 //none：不扫描 //withClassAnnotation：扫描类上的注解，参数是一个注解的反射对象 //withMethodAnnotation：扫描方法上的注解 .apis(RequestHandlerSelectors.basePackage(\\\"com.xiaobear.swagger.controller\\\")) //paths：过滤路径 .paths(PathSelectors.any()) .build(); } \",\"swagger如何在生产环境中使用，发布环境不用？\",\"判断是否为生产环境 flag = false\",\"注入enable（false）\",\" @Bean public Docket decket(Environment environment) { //设置要显示的swagger Profiles profiles = Profiles.of(\\\"dev\\\"); //获取项目的环境，environment.acceptsProfiles 判断是否处在自己设定的环境当中 boolean acceptsProfiles = environment.acceptsProfiles(profiles); System.out.println(acceptsProfiles); return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) // enable是否开启swagger，if false，咋不能在浏览器中访问 .enable(acceptsProfiles) .select() //RequestHandlerSelectors：配置要扫描接口的方式 //basePackage：指定扫描的包 //any：扫描全部 //none：不扫描 //withClassAnnotation：扫描类上的注解，参数是一个注解的反射对象 //withMethodAnnotation：扫描方法上的注解 .apis(RequestHandlerSelectors.basePackage(\\\"com.xiaobear.swagger.controller\\\")) //paths：过滤路径 .paths(PathSelectors.any()) .build(); } \",\"配置API文档的分组\",\" .groupName(\\\"xiaobear\\\") \",\"配置多个分组文档\",\" @Bean public Docket docker1(){ return new Docket(DocumentationType.SWAGGER_2).groupName(\\\"A\\\"); } @Bean public Docket docker2(){ return new Docket(DocumentationType.SWAGGER_2).groupName(\\\"B\\\"); } @Bean public Docket docker3(){ return new Docket(DocumentationType.SWAGGER_2).groupName(\\\"C\\\"); } \",\"实体类\",\"//@Api(注释) @ApiModel(\\\"用户实体类\\\") public class User { @ApiModelProperty(\\\"用户名\\\") public String username; @ApiModelProperty(\\\"密码\\\") public String password; } \",\"Controller\",\"@RestController public class HelloController { @ApiOperation(\\\"hello控制\\\")//用来标记方法作用的注解 @GetMapping(value = \\\"/hello\\\") public String hello() { return \\\"xiaobear,hello\\\"; } @GetMapping(value = \\\"/hello1\\\") public String hello1(@ApiParam(\\\"用户名\\\") String username) { return username+\\\"hello\\\"; } @ApiOperation(\\\"测试类\\\") @PostMapping(value = \\\"/hello2\\\") public User hello2(@ApiParam(\\\"用户名\\\") User user) { return user; } //只要在我们的接口中，返回值存在实体类，就会被扫描到swagger中 @PostMapping(value = \\\"/user\\\") public User user() { return new User(); } } \"]},{\"header\":\"4、在Spring Security的配置\",\"slug\":\"_4、在spring-security的配置\",\"contents\":[\"如果我们的项目集成了Spring Security，swagger2可能会被拦截，所有我们必须把它排除在外，在Spring Security中重写configure方法\",\" public void configure(WebSecurity web)throws Exception { web.ignoring() .antMatchers(\\\"swagger-ui.html\\\") .antMatchers(\\\"/v2/**\\\") .antMatchers(\\\"swagger-resoures/**\\\"); } \",\"**注：**发布的时候，关闭Swagger！！！\"]}]},\"/study-tutorial/microservice/spring-boot2/integrate/upload.html\":{\"title\":\"7、Spring Boot upload\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"配置文件application.yaml，设置自己的端口号以及上传信息\",\"server: port: 8081 servlet: context-path: /demo qiniu: ### 此处填写你自己的七牛云 access key accessKey: ### 此处填写你自己的七牛云 secret key secretKey: ### 此处填写你自己的七牛云 bucket bucket: ### 此处填写你自己的七牛云 域名 prefix: spring: servlet: multipart: enabled: true location: #上传位置 file-size-threshold: 5MB max-file-size: 20MB \",\"配置类\",\"@Configuration @ConditionalOnClass({Servlet.class, StandardServletMultipartResolver.class, MultipartConfigElement.class}) //在特定指定类加载使用 @ConditionalOnProperty(prefix = \\\"spring.http.multipart\\\", name = \\\"enabled\\\", matchIfMissing = true) @EnableConfigurationProperties(MultipartProperties.class) public class UploadConfig { @Value(\\\"${qiniu.accessKey}\\\") private String accessKey; @Value(\\\"${qiniu.secretKey}\\\") private String secretKey; private final MultipartProperties multipartProperties; @Autowired public UploadConfig(MultipartProperties multipartProperties) { this.multipartProperties = multipartProperties; } // 上传配置 @Bean @ConditionalOnMissingBean public MultipartConfigElement multipartConfigElement() { return this.multipartProperties.createMultipartConfig(); } // 注册解析器 @Bean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) @ConditionalOnMissingBean(MultipartResolver.class) public StandardServletMultipartResolver multipartResolver() { StandardServletMultipartResolver multipartResolver = new StandardServletMultipartResolver(); multipartResolver.setResolveLazily(this.multipartProperties.isResolveLazily()); return multipartResolver; } // 机房 @Bean public com.qiniu.storage.Configuration qiniuConfig() { return new com.qiniu.storage.Configuration(Zone.zone0()); } //构建一个七牛上传工具实例 @Bean public UploadManager uploadManager() { return new UploadManager(qiniuConfig()); } //认证信息实例 @Bean public Auth auth() { return Auth.create(accessKey, secretKey); } // 构建七牛空间管理实例 @Bean public BucketManager bucketManager() { return new BucketManager(auth(), qiniuConfig()); } } \",\"Service层\",\"Service\",\"public interface IQiNiuService { /* * 七牛云上传文件 * @param file 文件 * @return 七牛上传Response * @throws QiniuException 七牛异常 */ Response uploadFile(File file) throws QiniuException; } \",\"ServiceImpl\",\"@Service @Slf4j public class QiNiuServiceImpl implements IQiNiuService, InitializingBean { private final UploadManager uploadManager; private final Auth auth; @Value(\\\"${qiniu.bucket}\\\") private String bucket; private StringMap putPolicy; @Autowired public QiNiuServiceImpl(UploadManager uploadManager, Auth auth) { this.uploadManager = uploadManager; this.auth = auth; } /** * 七牛云上传文件 * @param file 文件 * @return 七牛上传Response * @throws QiniuException 七牛异常 */ @Override public Response uploadFile(File file) throws QiniuException { Response response = this.uploadManager.put(file, file.getName(), getUploadToken()); int retry = 0; while (response.needRetry() && retry < 3) { response = this.uploadManager.put(file, file.getName(), getUploadToken()); retry++; } return response; } @Override public void afterPropertiesSet() { this.putPolicy = new StringMap(); putPolicy.put(\\\"returnBody\\\", \\\"{\\\\\\\"key\\\\\\\":\\\\\\\"$(key)\\\\\\\",\\\\\\\"hash\\\\\\\":\\\\\\\"$(etag)\\\\\\\",\\\\\\\"bucket\\\\\\\":\\\\\\\"$(bucket)\\\\\\\",\\\\\\\"width\\\\\\\":$(imageInfo.width), \\\\\\\"height\\\\\\\":${imageInfo.height}}\\\"); } /** * 获取上传凭证 * @return 上传凭证 */ private String getUploadToken() { return this.auth.uploadToken(bucket, null, 3600, putPolicy); } } \",\"Controller\",\"@RestController @Slf4j @RequestMapping(\\\"/upload\\\") public class UploadController { @Value(\\\"${spring.servlet.multipart.location}\\\") private String fileTempPath; @Value(\\\"${qiniu.prefix}\\\") private String prefix; private final IQiNiuService qiNiuService; @Autowired public UploadController(IQiNiuService qiNiuService) { this.qiNiuService = qiNiuService; } @PostMapping(value = \\\"/local\\\", consumes = MediaType.MULTIPART_FORM_DATA_VALUE) public Dict local(@RequestParam(\\\"file\\\") MultipartFile file) { if (file.isEmpty()) { return Dict.create().set(\\\"code\\\", 400).set(\\\"message\\\", \\\"文件内容为空\\\"); } String fileName = file.getOriginalFilename(); String rawFileName = StrUtil.subBefore(fileName, \\\".\\\", true); String fileType = StrUtil.subAfter(fileName, \\\".\\\", true); String localFilePath = StrUtil.appendIfMissing(fileTempPath, \\\"/\\\") + rawFileName + \\\"-\\\" + DateUtil.current(false) + \\\".\\\" + fileType; try { file.transferTo(new File(localFilePath)); } catch (IOException e) { log.error(\\\"【文件上传至本地】失败，绝对路径：{}\\\", localFilePath); return Dict.create().set(\\\"code\\\", 500).set(\\\"message\\\", \\\"文件上传失败\\\"); } log.info(\\\"【文件上传至本地】绝对路径：{}\\\", localFilePath); return Dict.create().set(\\\"code\\\", 200).set(\\\"message\\\", \\\"上传成功\\\").set(\\\"data\\\", Dict.create().set(\\\"fileName\\\", fileName).set(\\\"filePath\\\", localFilePath)); } @PostMapping(value = \\\"/yun\\\", consumes = MediaType.MULTIPART_FORM_DATA_VALUE) public Dict yun(@RequestParam(\\\"file\\\") MultipartFile file) { if (file.isEmpty()) { return Dict.create().set(\\\"code\\\", 400).set(\\\"message\\\", \\\"文件内容为空\\\"); } String fileName = file.getOriginalFilename(); String rawFileName = StrUtil.subBefore(fileName, \\\".\\\", true); String fileType = StrUtil.subAfter(fileName, \\\".\\\", true); String localFilePath = StrUtil.appendIfMissing(fileTempPath, \\\"/\\\") + rawFileName + \\\"-\\\" + DateUtil.current(false) + \\\".\\\" + fileType; try { file.transferTo(new File(localFilePath)); Response response = qiNiuService.uploadFile(new File(localFilePath)); if (response.isOK()) { JSONObject jsonObject = JSONUtil.parseObj(response.bodyString()); String yunFileName = jsonObject.getStr(\\\"key\\\"); String yunFilePath = StrUtil.appendIfMissing(prefix, \\\"/\\\") + yunFileName; FileUtil.del(new File(localFilePath)); log.info(\\\"【文件上传至七牛云】绝对路径：{}\\\", yunFilePath); return Dict.create().set(\\\"code\\\", 200).set(\\\"message\\\", \\\"上传成功\\\").set(\\\"data\\\", Dict.create().set(\\\"fileName\\\", yunFileName).set(\\\"filePath\\\", yunFilePath)); } else { log.error(\\\"【文件上传至七牛云】失败，{}\\\", JSONUtil.toJsonStr(response)); FileUtil.del(new File(localFilePath)); return Dict.create().set(\\\"code\\\", 500).set(\\\"message\\\", \\\"文件上传失败\\\"); } } catch (IOException e) { log.error(\\\"【文件上传至七牛云】失败，绝对路径：{}\\\", localFilePath); return Dict.create().set(\\\"code\\\", 500).set(\\\"message\\\", \\\"文件上传失败\\\"); } } } \",\"index.html\",\"<!doctype html> <html lang=\\\"en\\\"> <head> <meta charset=\\\"UTF-8\\\"> <meta name=\\\"viewport\\\" content=\\\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\\\"> <meta http-equiv=\\\"X-UA-Compatible\\\" content=\\\"ie=edge\\\"> <title>spring-boot-demo-upload</title> <!-- import Vue.js --> <script src=\\\"https://cdn.bootcss.com/vue/2.5.17/vue.min.js\\\"></script> <!-- import stylesheet --> <link href=\\\"https://cdn.bootcss.com/iview/3.1.4/styles/iview.css\\\" rel=\\\"stylesheet\\\"> <!-- import iView --> <script src=\\\"https://cdn.bootcss.com/iview/3.1.4/iview.min.js\\\"></script> </head> <body> <div id=\\\"app\\\"> <Row :gutter=\\\"16\\\" style=\\\"background:#eee;padding:10%\\\"> <i-col span=\\\"12\\\"> <Card style=\\\"height: 300px\\\"> <p slot=\\\"title\\\"> <Icon type=\\\"ios-cloud-upload\\\"></Icon> 本地上传 </p> <div style=\\\"text-align: center;\\\"> <Upload :before-upload=\\\"handleLocalUpload\\\" action=\\\"/demo/upload/local\\\" ref=\\\"localUploadRef\\\" :on-success=\\\"handleLocalSuccess\\\" :on-error=\\\"handleLocalError\\\" > <i-button icon=\\\"ios-cloud-upload-outline\\\">选择文件</i-button> </Upload> <i-button type=\\\"primary\\\" @click=\\\"localUpload\\\" :loading=\\\"local.loadingStatus\\\" :disabled=\\\"!local.file\\\"> {{ local.loadingStatus ? '本地文件上传中' : '本地上传' }} </i-button> </div> <div> <div v-if=\\\"local.log.status != 0\\\">状态：{{local.log.message}}</div> <div v-if=\\\"local.log.status **= 200\\\">文件名：{{local.log.fileName}}</div> <div v-if=\\\"local.log.status **= 200\\\">文件路径：{{local.log.filePath}}</div> </div> </Card> </i-col> <i-col span=\\\"12\\\"> <Card style=\\\"height: 300px;\\\"> <p slot=\\\"title\\\"> <Icon type=\\\"md-cloud-upload\\\"></Icon> 七牛云上传 </p> <div style=\\\"text-align: center;\\\"> <Upload :before-upload=\\\"handleYunUpload\\\" action=\\\"/demo/upload/yun\\\" ref=\\\"yunUploadRef\\\" :on-success=\\\"handleYunSuccess\\\" :on-error=\\\"handleYunError\\\" > <i-button icon=\\\"ios-cloud-upload-outline\\\">选择文件</i-button> </Upload> <i-button type=\\\"primary\\\" @click=\\\"yunUpload\\\" :loading=\\\"yun.loadingStatus\\\" :disabled=\\\"!yun.file\\\"> {{ yun.loadingStatus ? '七牛云文件上传中' : '七牛云上传' }} </i-button> </div> <div> <div v-if=\\\"yun.log.status != 0\\\">状态：{{yun.log.message}}</div> <div v-if=\\\"yun.log.status **= 200\\\">文件名：{{yun.log.fileName}}</div> <div v-if=\\\"yun.log.status **= 200\\\">文件路径：{{yun.log.filePath}}</div> </div> </Card> </i-col> </Row> </div> <script> new Vue({ el: '#app', data: { local: { // 选择文件后，将 beforeUpload 返回的 file 保存在这里，后面会用到 file: null, // 标记上传状态 loadingStatus: false, log: { status: 0, message: \\\"\\\", fileName: \\\"\\\", filePath: \\\"\\\" } }, yun: { // 选择文件后，将 beforeUpload 返回的 file 保存在这里，后面会用到 file: null, // 标记上传状态 loadingStatus: false, log: { status: 0, message: \\\"\\\", fileName: \\\"\\\", filePath: \\\"\\\" } } }, methods: { // beforeUpload 在返回 false 或 Promise 时，会停止自动上传，这里我们将选择好的文件 file 保存在 data里，并 return false handleLocalUpload(file) { this.local.file = file; return false; }, // 这里是手动上传，通过 $refs 获取到 Upload 实例，然后调用私有方法 .post()，把保存在 data 里的 file 上传。 // iView 的 Upload 组件在调用 .post() 方法时，就会继续上传了。 localUpload() { this.local.loadingStatus = true; // 标记上传状态 this.$refs.localUploadRef.post(this.local.file); }, // 上传成功后，清空 data 里的 file，并修改上传状态 handleLocalSuccess(response) { this.local.file = null; this.local.loadingStatus = false; if (response.code **= 200) { this.$Message.success(response.message); this.local.log.status = response.code; this.local.log.message = response.message; this.local.log.fileName = response.data.fileName; this.local.log.filePath = response.data.filePath; this.$refs.localUploadRef.clearFiles(); } else { this.$Message.error(response.message); this.local.log.status = response.code; this.local.log.message = response.message; } }, // 上传失败后，清空 data 里的 file，并修改上传状态 handleLocalError() { this.local.file = null; this.local.loadingStatus = false; this.$Message.error('上传失败'); }, // beforeUpload 在返回 false 或 Promise 时，会停止自动上传，这里我们将选择好的文件 file 保存在 data里，并 return false handleYunUpload(file) { this.yun.file = file; return false; }, // 这里是手动上传，通过 $refs 获取到 Upload 实例，然后调用私有方法 .post()，把保存在 data 里的 file 上传。 // iView 的 Upload 组件在调用 .post() 方法时，就会继续上传了。 yunUpload() { this.yun.loadingStatus = true; // 标记上传状态 this.$refs.yunUploadRef.post(this.yun.file); }, // 上传成功后，清空 data 里的 file，并修改上传状态 handleYunSuccess(response) { this.yun.file = null; this.yun.loadingStatus = false; if (response.code **= 200) { this.$Message.success(response.message); this.yun.log.status = response.code; this.yun.log.message = response.message; this.yun.log.fileName = response.data.fileName; this.yun.log.filePath = response.data.filePath; this.$refs.yunUploadRef.clearFiles(); } else { this.$Message.error(response.message); this.yun.log.status = response.code; this.yun.log.message = response.message; } }, // 上传失败后，清空 data 里的 file，并修改上传状态 handleYunError() { this.yun.file = null; this.yun.loadingStatus = false; this.$Message.error('上传失败'); } } }) </script> </body> </html> \"]}]},\"/architecture/basic/\":{\"title\":\"Basic\",\"contents\":[]},\"/architecture/distributed/\":{\"title\":\"Distributed\",\"contents\":[]},\"/dev-necessary/common/\":{\"title\":\"Common\",\"contents\":[]},\"/dev-necessary/problem/\":{\"title\":\"Problem\",\"contents\":[]},\"/interview/cheats/\":{\"title\":\"Cheats\",\"contents\":[]},\"/interview/javaBasics/\":{\"title\":\"Java Basics\",\"contents\":[]},\"/interview/javaHighLevel/\":{\"title\":\"Java High Level\",\"contents\":[]},\"/interview/sourceCode/\":{\"title\":\"Source Code\",\"contents\":[]},\"/study-tutorial/advanced/\":{\"title\":\"Advanced\",\"contents\":[]},\"/study-tutorial/devTools/\":{\"title\":\"Dev Tools\",\"contents\":[]},\"/study-tutorial/route/\":{\"title\":\"Route\",\"contents\":[]},\"/interview/sourceCode/java/\":{\"title\":\"Java\",\"contents\":[]},\"/interview/sourceCode/spring/\":{\"title\":\"Spring\",\"contents\":[]},\"/lick-brick-java/avoid/code/\":{\"title\":\"Code\",\"contents\":[]},\"/lick-brick-java/avoid/\":{\"title\":\"Avoid\",\"contents\":[]},\"/lick-brick-java/java-dev/AlibabaDevelopmentManual/\":{\"title\":\"Alibaba Development Manual\",\"contents\":[]},\"/study-tutorial/components/report/\":{\"title\":\"Report\",\"contents\":[]},\"/study-tutorial/basic/features/\":{\"title\":\"Features\",\"contents\":[]},\"/study-tutorial/basic/\":{\"title\":\"Basic\",\"contents\":[]},\"/study-tutorial/basic/dataAndAlgorithm/\":{\"title\":\"Data And Algorithm\",\"contents\":[]},\"/study-tutorial/database/mysql/\":{\"title\":\"Mysql\",\"contents\":[]},\"/study-tutorial/database/\":{\"title\":\"Database\",\"contents\":[]},\"/study-tutorial/devTools/idea/\":{\"title\":\"Idea\",\"contents\":[]},\"/study-tutorial/distributed/elasticsearch/\":{\"title\":\"Elasticsearch\",\"contents\":[]},\"/study-tutorial/distributed/docker/\":{\"title\":\"Docker\",\"contents\":[]},\"/study-tutorial/distributed/rabbitmq/\":{\"title\":\"Rabbitmq\",\"contents\":[]},\"/study-tutorial/distributed/redis/\":{\"title\":\"Redis\",\"contents\":[]},\"/study-tutorial/frame/javaweb/\":{\"title\":\"Javaweb\",\"contents\":[]},\"/study-tutorial/frame/\":{\"title\":\"Frame\",\"contents\":[]},\"/study-tutorial/frame/mybatis/\":{\"title\":\"Mybatis\",\"contents\":[]},\"/study-tutorial/frame/spring-mvc/\":{\"title\":\"Spring Mvc\",\"contents\":[]},\"/study-tutorial/frame/mybatis-plus/\":{\"title\":\"Mybatis Plus\",\"contents\":[]},\"/study-tutorial/microservice/spring-cloud/\":{\"title\":\"Spring Cloud\",\"contents\":[]},\"/study-tutorial/microservice/spring-boot2/\":{\"title\":\"Spring Boot2\",\"contents\":[]},\"/study-tutorial/microservice/spring-cloud-alibaba/\":{\"title\":\"Spring Cloud Alibaba\",\"contents\":[]},\"/study-tutorial/components/activity/basic/\":{\"title\":\"Basic\",\"contents\":[]},\"/study-tutorial/components/activity/\":{\"title\":\"Activity\",\"contents\":[]},\"/study-tutorial/distributed/redis/combat/\":{\"title\":\"Combat\",\"contents\":[]}}}");self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
